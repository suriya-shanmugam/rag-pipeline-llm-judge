{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f14f9ba9764e4e3bba4a2dc577811642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_02453c2708cd43948c8fac6734ceaa50"
          }
        },
        "1c7fbc02aef3475bb1cd411c3f7450ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10192d70118f436b9b6afddd0338613e",
            "placeholder": "​",
            "style": "IPY_MODEL_9dec940de31f4eb887aa4c56dc703484",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b0bc5cc3213e49608c789c7f09a93bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_feca446b7ab24f258eabc60386b00c42",
            "placeholder": "​",
            "style": "IPY_MODEL_90e7ba2270f848dab033ae935600f061",
            "value": ""
          }
        },
        "fcaa3327e9e54f51b61a1487b7341267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9507ac2382004fc69eeaf97cdc2bf867",
            "style": "IPY_MODEL_719c29a9f9944354a79ac3a5e76b7016",
            "value": false
          }
        },
        "9605f02b781d4917a0548ff8d49fbee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4499978deb4f4bc69709aad9ddb482f6",
            "style": "IPY_MODEL_863b5d895fd948c1b5ca16a03ae9bc8d",
            "tooltip": ""
          }
        },
        "d8e44bd206ef4698abd07743838150cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63e391718d2744e4ad734ff1303c5366",
            "placeholder": "​",
            "style": "IPY_MODEL_81f22bf4a5224ab8ae58b292703530be",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "02453c2708cd43948c8fac6734ceaa50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "10192d70118f436b9b6afddd0338613e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dec940de31f4eb887aa4c56dc703484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "feca446b7ab24f258eabc60386b00c42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e7ba2270f848dab033ae935600f061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9507ac2382004fc69eeaf97cdc2bf867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "719c29a9f9944354a79ac3a5e76b7016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4499978deb4f4bc69709aad9ddb482f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "863b5d895fd948c1b5ca16a03ae9bc8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "63e391718d2744e4ad734ff1303c5366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f22bf4a5224ab8ae58b292703530be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98bece27365744d9acd47cb6eaa045b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d88b587a319149cfbc49012952a02f73",
            "placeholder": "​",
            "style": "IPY_MODEL_8b4bb28ac61a42249aeece4b369b2f50",
            "value": "Connecting..."
          }
        },
        "d88b587a319149cfbc49012952a02f73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b4bb28ac61a42249aeece4b369b2f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6121fbc2f5ba4a0a8d961f8ae16c17a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba68a9ae40344f6cbe90aab6c05d85bc",
              "IPY_MODEL_17424a225a324f588def43a1be6528a0",
              "IPY_MODEL_46dcd2eb4de34ca999edc15d8b30cb0a"
            ],
            "layout": "IPY_MODEL_3beecd0133c848068e4a0532ef21123b"
          }
        },
        "ba68a9ae40344f6cbe90aab6c05d85bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5691bfaea40a490a8f39a4c890ba7024",
            "placeholder": "​",
            "style": "IPY_MODEL_a29fc01a88834204ab7f64ac76853e17",
            "value": "README.md: 100%"
          }
        },
        "17424a225a324f588def43a1be6528a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9f0f701aeb7497da1888bea0ffa3467",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_057e64c7e5304f55842a5d25a9016f27",
            "value": 21
          }
        },
        "46dcd2eb4de34ca999edc15d8b30cb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5bfc304ccf547b08b708ad3193ce947",
            "placeholder": "​",
            "style": "IPY_MODEL_ae338f2d313342ba8c0d4683d584f8b2",
            "value": " 21.0/21.0 [00:00&lt;00:00, 2.59kB/s]"
          }
        },
        "3beecd0133c848068e4a0532ef21123b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5691bfaea40a490a8f39a4c890ba7024": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a29fc01a88834204ab7f64ac76853e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9f0f701aeb7497da1888bea0ffa3467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "057e64c7e5304f55842a5d25a9016f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5bfc304ccf547b08b708ad3193ce947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae338f2d313342ba8c0d4683d584f8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af789acb6496492c904511b869983a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22842bfa27114c3a9ba95aab04d44519",
              "IPY_MODEL_f59660bb1a8042729d36ec8c8de56b07",
              "IPY_MODEL_27bc5ae4b19d436faa4aaea0bb726428"
            ],
            "layout": "IPY_MODEL_c7aaef1883824cec9eea18fdd041afb3"
          }
        },
        "22842bfa27114c3a9ba95aab04d44519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b922af01e7f4330aaefb8883faf7769",
            "placeholder": "​",
            "style": "IPY_MODEL_9d9ca24e1e2d438a9367c8535183101b",
            "value": "huggingface_doc.csv: 100%"
          }
        },
        "f59660bb1a8042729d36ec8c8de56b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e0ad9b8c4c41d69b017ea6faf0a7ff",
            "max": 21954601,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad34dc10d9374194ad69fbf1258f1edf",
            "value": 21954601
          }
        },
        "27bc5ae4b19d436faa4aaea0bb726428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62e39a84b0384830b12ba167a0fe3b02",
            "placeholder": "​",
            "style": "IPY_MODEL_17bd357d3a724c3f9b29cbae79db02bc",
            "value": " 22.0M/22.0M [00:00&lt;00:00, 27.6MB/s]"
          }
        },
        "c7aaef1883824cec9eea18fdd041afb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b922af01e7f4330aaefb8883faf7769": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9ca24e1e2d438a9367c8535183101b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6e0ad9b8c4c41d69b017ea6faf0a7ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad34dc10d9374194ad69fbf1258f1edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62e39a84b0384830b12ba167a0fe3b02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17bd357d3a724c3f9b29cbae79db02bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27641494a29d4fdf8e900fbc1c696a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_166f5d56ef284abdb8a0c713778b0e4d",
              "IPY_MODEL_05c4d42e76864b9695a98baa05083f65",
              "IPY_MODEL_f041f7a7106444228eb1614b910b6eb8"
            ],
            "layout": "IPY_MODEL_bb1b3bf0366d4e79b74a977558ca3ef0"
          }
        },
        "166f5d56ef284abdb8a0c713778b0e4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0855e6758c0d490bbbae50dc2ca352e3",
            "placeholder": "​",
            "style": "IPY_MODEL_471ee207dae74ab49cdefd3c309bbe1f",
            "value": "Generating train split: 100%"
          }
        },
        "05c4d42e76864b9695a98baa05083f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c3fb43ddb8f487885a6128fdb7be5fb",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebd9faafe063419e8ef851be0bec4ac9",
            "value": 2647
          }
        },
        "f041f7a7106444228eb1614b910b6eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bae4023390a443fea094956523bfc891",
            "placeholder": "​",
            "style": "IPY_MODEL_091562942fdf408890011469655a6d2f",
            "value": " 2647/2647 [00:00&lt;00:00, 4585.46 examples/s]"
          }
        },
        "bb1b3bf0366d4e79b74a977558ca3ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0855e6758c0d490bbbae50dc2ca352e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471ee207dae74ab49cdefd3c309bbe1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c3fb43ddb8f487885a6128fdb7be5fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebd9faafe063419e8ef851be0bec4ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bae4023390a443fea094956523bfc891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091562942fdf408890011469655a6d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef93a70bdb644c58a01b2599c8948427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da2187c6e57b460c9f8c504649d23214",
              "IPY_MODEL_5bd0d548f9f4449bb2a2fcd8ae62c80a",
              "IPY_MODEL_51d05b7e5f4443dab5d1f5c15c842864"
            ],
            "layout": "IPY_MODEL_810df4a65b904c8f91eb190ae418e398"
          }
        },
        "da2187c6e57b460c9f8c504649d23214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50a8b4a507fb4cb7bbec91ec8ddf7a85",
            "placeholder": "​",
            "style": "IPY_MODEL_a6538bd53ddd4aa7a6c3ba3d3ae5057a",
            "value": "100%"
          }
        },
        "5bd0d548f9f4449bb2a2fcd8ae62c80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_396174a7fb0748e4a0290a3b47706f5a",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94adb6cffaf94615896907623845898a",
            "value": 2647
          }
        },
        "51d05b7e5f4443dab5d1f5c15c842864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3876a0d02d254726bf3dcd65b6f6cf23",
            "placeholder": "​",
            "style": "IPY_MODEL_a000eeaf6e5644e7916b0817462db412",
            "value": " 2647/2647 [00:00&lt;00:00, 12978.51it/s]"
          }
        },
        "810df4a65b904c8f91eb190ae418e398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a8b4a507fb4cb7bbec91ec8ddf7a85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6538bd53ddd4aa7a6c3ba3d3ae5057a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "396174a7fb0748e4a0290a3b47706f5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94adb6cffaf94615896907623845898a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3876a0d02d254726bf3dcd65b6f6cf23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a000eeaf6e5644e7916b0817462db412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da49499b70e4474da7bf4e2e1e2d9d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc6ae7e41b51494885829028c3183e26",
              "IPY_MODEL_8eb0ba8c35904cc4a710c568131ac6b2",
              "IPY_MODEL_48747261c89843c181d4260d0cdc50b6"
            ],
            "layout": "IPY_MODEL_d38ea8d4f8ee42969b7319f104a046e3"
          }
        },
        "bc6ae7e41b51494885829028c3183e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61f37901ed9b442ca425cb85bcf45c52",
            "placeholder": "​",
            "style": "IPY_MODEL_2e8538100e344de599f8e0d6c521a264",
            "value": "100%"
          }
        },
        "8eb0ba8c35904cc4a710c568131ac6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_421cd7fe8b474c528d1a5a249360696e",
            "max": 16776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d42ab1894da445fa6aed02f88b33547",
            "value": 16776
          }
        },
        "48747261c89843c181d4260d0cdc50b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07a8268adcdf4d27937b7b11224c35d7",
            "placeholder": "​",
            "style": "IPY_MODEL_739d2200bb8d463fbfc4ffce5606af22",
            "value": " 16776/16776 [00:20&lt;00:00, 944.51it/s]"
          }
        },
        "d38ea8d4f8ee42969b7319f104a046e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61f37901ed9b442ca425cb85bcf45c52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e8538100e344de599f8e0d6c521a264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "421cd7fe8b474c528d1a5a249360696e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d42ab1894da445fa6aed02f88b33547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07a8268adcdf4d27937b7b11224c35d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "739d2200bb8d463fbfc4ffce5606af22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe42ddb217e84c32b306f86e08e314b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e108c3085bd493a94c0a33d79aab835",
              "IPY_MODEL_0699c0a52bb34b6f81dbebd09bf5ce07",
              "IPY_MODEL_b5dae48529c3461397db5943da6114bb"
            ],
            "layout": "IPY_MODEL_8e0d2d065c714d699d8b415c639d037d"
          }
        },
        "7e108c3085bd493a94c0a33d79aab835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9bc730201e948b1bb1497a32635e3a7",
            "placeholder": "​",
            "style": "IPY_MODEL_f3adf322cfb3404889fd77d03174f372",
            "value": "100%"
          }
        },
        "0699c0a52bb34b6f81dbebd09bf5ce07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29542d66fecd49a48432510bb72e4617",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efe56e57e1124ef49fd4ef5052735b47",
            "value": 2647
          }
        },
        "b5dae48529c3461397db5943da6114bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8694e4356504eae8a3378da5d507b68",
            "placeholder": "​",
            "style": "IPY_MODEL_f66bcc1c26f84406bcade7125958e85a",
            "value": " 2647/2647 [00:00&lt;00:00, 10697.84it/s]"
          }
        },
        "8e0d2d065c714d699d8b415c639d037d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9bc730201e948b1bb1497a32635e3a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3adf322cfb3404889fd77d03174f372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29542d66fecd49a48432510bb72e4617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe56e57e1124ef49fd4ef5052735b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8694e4356504eae8a3378da5d507b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66bcc1c26f84406bcade7125958e85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8becc5ea39124d0ebc453808589c5a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d6f4775b6204fd49f04cac08229c914",
              "IPY_MODEL_1795b23359d34a36b2bb4206c0fe9f3c",
              "IPY_MODEL_f8dfc32b8a1446f28446a7ef4cbd7ab1"
            ],
            "layout": "IPY_MODEL_b10b23c9f99a457f91e45228c87b662d"
          }
        },
        "3d6f4775b6204fd49f04cac08229c914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aeaff77971443c982656c7db1d7cda7",
            "placeholder": "​",
            "style": "IPY_MODEL_4cf211fb0c8847ec83f91d1cfccfa101",
            "value": "100%"
          }
        },
        "1795b23359d34a36b2bb4206c0fe9f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ccff14cd6ee48449b07c12933622e8a",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bea59f3f8bb04e07ae3fe66f2045982c",
            "value": 10
          }
        },
        "f8dfc32b8a1446f28446a7ef4cbd7ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d12390fbc62410aa2641ac6c1c9a919",
            "placeholder": "​",
            "style": "IPY_MODEL_13153e5a2f874629a83b77808b50a2a7",
            "value": " 10/10 [00:09&lt;00:00,  1.01s/it]"
          }
        },
        "b10b23c9f99a457f91e45228c87b662d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aeaff77971443c982656c7db1d7cda7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cf211fb0c8847ec83f91d1cfccfa101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ccff14cd6ee48449b07c12933622e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bea59f3f8bb04e07ae3fe66f2045982c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d12390fbc62410aa2641ac6c1c9a919": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13153e5a2f874629a83b77808b50a2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74463bc872bd40cd9fec800f9f26e085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5306bf055ed44ed2b927eaa352af3246",
              "IPY_MODEL_ccdcaa9466a64a8f96997954ff5a9b4a",
              "IPY_MODEL_371c37e49f634155b1055f269197ff61"
            ],
            "layout": "IPY_MODEL_500f5a10354143edba44820c3cc92f97"
          }
        },
        "5306bf055ed44ed2b927eaa352af3246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063d8d17af354619869622cfdf4e72f6",
            "placeholder": "​",
            "style": "IPY_MODEL_6417ef798dc14ecda835d3d0c75ae1ee",
            "value": " 60%"
          }
        },
        "ccdcaa9466a64a8f96997954ff5a9b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e17de1d66445b795c7938922620caa",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7017625800c543c9b8f9be2ae20d5369",
            "value": 6
          }
        },
        "371c37e49f634155b1055f269197ff61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29f7cc67ea6346f2ba492fb4ba5475ca",
            "placeholder": "​",
            "style": "IPY_MODEL_0d9a7643f28d41d4b4699973652f6bc0",
            "value": " 6/10 [00:26&lt;00:18,  4.73s/it]"
          }
        },
        "500f5a10354143edba44820c3cc92f97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063d8d17af354619869622cfdf4e72f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6417ef798dc14ecda835d3d0c75ae1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55e17de1d66445b795c7938922620caa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7017625800c543c9b8f9be2ae20d5369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29f7cc67ea6346f2ba492fb4ba5475ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d9a7643f28d41d4b4699973652f6bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1f6c01a895d41bb93cdbbe2cab43acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9942e0217d41450f90741e2756942db7",
              "IPY_MODEL_b33a9de34e164b11b3439d2cce85817a",
              "IPY_MODEL_0f5a36943f30458c9474499fb523424e"
            ],
            "layout": "IPY_MODEL_3c78a0199cfd4b448aa9a310285a6b38"
          }
        },
        "9942e0217d41450f90741e2756942db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b357a59177fd476c9e098829385d0a6a",
            "placeholder": "​",
            "style": "IPY_MODEL_ecf56464164745959acf3edcf3e5aa31",
            "value": "100%"
          }
        },
        "b33a9de34e164b11b3439d2cce85817a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_529ced00d4684177b2d8ff7b3678d4cd",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbf7da0971a14cf5b035cb153c52f948",
            "value": 10
          }
        },
        "0f5a36943f30458c9474499fb523424e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c119386623f74d7bb2353ddb6c33012c",
            "placeholder": "​",
            "style": "IPY_MODEL_de72614d783c469994cbfc0d2d9becf0",
            "value": " 10/10 [01:57&lt;00:00, 11.32s/it]"
          }
        },
        "3c78a0199cfd4b448aa9a310285a6b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b357a59177fd476c9e098829385d0a6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecf56464164745959acf3edcf3e5aa31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "529ced00d4684177b2d8ff7b3678d4cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbf7da0971a14cf5b035cb153c52f948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c119386623f74d7bb2353ddb6c33012c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de72614d783c469994cbfc0d2d9becf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22ebf8903aac497ab7aa5390376c925b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f030132f5d74e0abcce8ddce3f66574",
              "IPY_MODEL_7c5d2cccd66743a9a7151109f18cf48b",
              "IPY_MODEL_b3a1289acd994b3cb86198f3de48c9f0"
            ],
            "layout": "IPY_MODEL_cd3425b0697b429cbecddcebf906db4e"
          }
        },
        "4f030132f5d74e0abcce8ddce3f66574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83cc393b17d8493bb7f78efd8ccd74bb",
            "placeholder": "​",
            "style": "IPY_MODEL_bf7a0824b43c447886615aaadfc8730d",
            "value": "100%"
          }
        },
        "7c5d2cccd66743a9a7151109f18cf48b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ecb235348024d65ab92fbad3de336c8",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dce7bc331af5483fb5706d3e6ce056c6",
            "value": 10
          }
        },
        "b3a1289acd994b3cb86198f3de48c9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b2941fc537b489c90952c2a86b85009",
            "placeholder": "​",
            "style": "IPY_MODEL_39cf0e05499c4a35a9678f39bd780c27",
            "value": " 10/10 [00:40&lt;00:00,  4.40s/it]"
          }
        },
        "cd3425b0697b429cbecddcebf906db4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83cc393b17d8493bb7f78efd8ccd74bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf7a0824b43c447886615aaadfc8730d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ecb235348024d65ab92fbad3de336c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce7bc331af5483fb5706d3e6ce056c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b2941fc537b489c90952c2a86b85009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39cf0e05499c4a35a9678f39bd780c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMdxgEnugfk2",
        "outputId": "f86a3c00-6f6b-42d6-b6eb-172cd5c2c311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m2.4/2.5 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.9/332.9 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "import datasets\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "metadata": {
        "id": "nDLrVfkmhuVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "f14f9ba9764e4e3bba4a2dc577811642",
            "1c7fbc02aef3475bb1cd411c3f7450ee",
            "b0bc5cc3213e49608c789c7f09a93bb5",
            "fcaa3327e9e54f51b61a1487b7341267",
            "9605f02b781d4917a0548ff8d49fbee7",
            "d8e44bd206ef4698abd07743838150cd",
            "02453c2708cd43948c8fac6734ceaa50",
            "10192d70118f436b9b6afddd0338613e",
            "9dec940de31f4eb887aa4c56dc703484",
            "feca446b7ab24f258eabc60386b00c42",
            "90e7ba2270f848dab033ae935600f061",
            "9507ac2382004fc69eeaf97cdc2bf867",
            "719c29a9f9944354a79ac3a5e76b7016",
            "4499978deb4f4bc69709aad9ddb482f6",
            "863b5d895fd948c1b5ca16a03ae9bc8d",
            "63e391718d2744e4ad734ff1303c5366",
            "81f22bf4a5224ab8ae58b292703530be",
            "98bece27365744d9acd47cb6eaa045b6",
            "d88b587a319149cfbc49012952a02f73",
            "8b4bb28ac61a42249aeece4b369b2f50"
          ]
        },
        "id": "2r-ETbBYhxnY",
        "outputId": "29c6d001-c943-43ee-a0b6-079e0feafa71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f14f9ba9764e4e3bba4a2dc577811642"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "6121fbc2f5ba4a0a8d961f8ae16c17a5",
            "ba68a9ae40344f6cbe90aab6c05d85bc",
            "17424a225a324f588def43a1be6528a0",
            "46dcd2eb4de34ca999edc15d8b30cb0a",
            "3beecd0133c848068e4a0532ef21123b",
            "5691bfaea40a490a8f39a4c890ba7024",
            "a29fc01a88834204ab7f64ac76853e17",
            "a9f0f701aeb7497da1888bea0ffa3467",
            "057e64c7e5304f55842a5d25a9016f27",
            "b5bfc304ccf547b08b708ad3193ce947",
            "ae338f2d313342ba8c0d4683d584f8b2",
            "af789acb6496492c904511b869983a13",
            "22842bfa27114c3a9ba95aab04d44519",
            "f59660bb1a8042729d36ec8c8de56b07",
            "27bc5ae4b19d436faa4aaea0bb726428",
            "c7aaef1883824cec9eea18fdd041afb3",
            "5b922af01e7f4330aaefb8883faf7769",
            "9d9ca24e1e2d438a9367c8535183101b",
            "e6e0ad9b8c4c41d69b017ea6faf0a7ff",
            "ad34dc10d9374194ad69fbf1258f1edf",
            "62e39a84b0384830b12ba167a0fe3b02",
            "17bd357d3a724c3f9b29cbae79db02bc",
            "27641494a29d4fdf8e900fbc1c696a22",
            "166f5d56ef284abdb8a0c713778b0e4d",
            "05c4d42e76864b9695a98baa05083f65",
            "f041f7a7106444228eb1614b910b6eb8",
            "bb1b3bf0366d4e79b74a977558ca3ef0",
            "0855e6758c0d490bbbae50dc2ca352e3",
            "471ee207dae74ab49cdefd3c309bbe1f",
            "1c3fb43ddb8f487885a6128fdb7be5fb",
            "ebd9faafe063419e8ef851be0bec4ac9",
            "bae4023390a443fea094956523bfc891",
            "091562942fdf408890011469655a6d2f"
          ]
        },
        "id": "Uf5H0XziiaV4",
        "outputId": "924dbc71-df22-4250-fb8f-556e6bf88a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6121fbc2f5ba4a0a8d961f8ae16c17a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af789acb6496492c904511b869983a13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27641494a29d4fdf8e900fbc1c696a22"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX0Lh7uUif0J",
        "outputId": "635e784d-4549-4555-aa72-5eb8174b5d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n',\n",
              " 'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVV3RFTcijh5",
        "outputId": "0ef1e16d-241a-4929-f6c1-0174271382ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2647"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ef93a70bdb644c58a01b2599c8948427",
            "da2187c6e57b460c9f8c504649d23214",
            "5bd0d548f9f4449bb2a2fcd8ae62c80a",
            "51d05b7e5f4443dab5d1f5c15c842864",
            "810df4a65b904c8f91eb190ae418e398",
            "50a8b4a507fb4cb7bbec91ec8ddf7a85",
            "a6538bd53ddd4aa7a6c3ba3d3ae5057a",
            "396174a7fb0748e4a0290a3b47706f5a",
            "94adb6cffaf94615896907623845898a",
            "3876a0d02d254726bf3dcd65b6f6cf23",
            "a000eeaf6e5644e7916b0817462db412"
          ]
        },
        "id": "otnfP8ZAizz0",
        "outputId": "1fd7eab3-7011-4d63-dc60-dea6cc79b1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef93a70bdb644c58a01b2599c8948427"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]"
      ],
      "metadata": {
        "id": "kfV2lt-FpPqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: str,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPARATORS,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique"
      ],
      "metadata": {
        "id": "KpE31EYHi3Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = split_documents(512, RAW_KNOWLEDGE_BASE,\"thenlper/gte-small\")"
      ],
      "metadata": {
        "id": "w_AIJBkcjLFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_jXGNvPlIuP",
        "outputId": "d1eb0e2a-4089-4d6e-eb34-5888ca67473e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16776"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeCwG8YgnTUZ",
        "outputId": "8af2fbbb-6209-45fc-e66c-f16541d130ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 1}, page_content='Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [len(chunk.page_content) for chunk in chunks]"
      ],
      "metadata": {
        "id": "QkV-qKo3lgje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(chunks)]\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484,
          "referenced_widgets": [
            "da49499b70e4474da7bf4e2e1e2d9d54",
            "bc6ae7e41b51494885829028c3183e26",
            "8eb0ba8c35904cc4a710c568131ac6b2",
            "48747261c89843c181d4260d0cdc50b6",
            "d38ea8d4f8ee42969b7319f104a046e3",
            "61f37901ed9b442ca425cb85bcf45c52",
            "2e8538100e344de599f8e0d6c521a264",
            "421cd7fe8b474c528d1a5a249360696e",
            "3d42ab1894da445fa6aed02f88b33547",
            "07a8268adcdf4d27937b7b11224c35d7",
            "739d2200bb8d463fbfc4ffce5606af22"
          ]
        },
        "id": "ZLo2K0M8rWNQ",
        "outputId": "04d36dbc-9d5e-4a66-e25a-4ecb0dc60947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16776 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da49499b70e4474da7bf4e2e1e2d9d54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "import os\n",
        "\n",
        "\n",
        "def load_embeddings(\n",
        "    langchain_docs: List[LangchainDocument],\n",
        "    chunk_size: int,\n",
        "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
        "\n",
        "    Args:\n",
        "        langchain_docs: list of documents\n",
        "        chunk_size: size of the chunks to split the documents into\n",
        "        embedding_model_name: name of the embedding model to use\n",
        "\n",
        "    Returns:\n",
        "        FAISS index\n",
        "    \"\"\"\n",
        "    # load embedding_model\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=embedding_model_name,\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
        "    )\n",
        "\n",
        "    # Check if embeddings already exist on disk\n",
        "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
        "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
        "    if os.path.isdir(index_folder_path):\n",
        "        return FAISS.load_local(\n",
        "            index_folder_path,\n",
        "            embedding_model,\n",
        "            distance_strategy=DistanceStrategy.COSINE,\n",
        "            allow_dangerous_deserialization=True,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        print(\"Index not found, generating it...\")\n",
        "        docs_processed = split_documents(\n",
        "            chunk_size,\n",
        "            langchain_docs,\n",
        "            embedding_model_name,\n",
        "        )\n",
        "        knowledge_index = FAISS.from_documents(\n",
        "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "        )\n",
        "        knowledge_index.save_local(index_folder_path)\n",
        "        return knowledge_index"
      ],
      "metadata": {
        "id": "vegyQmpPmhQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name= \"thenlper/gte-small\",\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    chunks, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")"
      ],
      "metadata": {
        "id": "cJjbLeQoBZ6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pacmap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg-VlcGAD-QT",
        "outputId": "697e0f20-b0eb-4357-e949-d5d13b021176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pacmap\n",
            "  Downloading pacmap-0.8.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.12/dist-packages (from pacmap) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.12/dist-packages (from pacmap) (0.60.0)\n",
            "Collecting annoy>=1.11 (from pacmap)\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from pacmap) (2.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.57->pacmap) (0.43.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20->pacmap) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20->pacmap) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20->pacmap) (3.6.0)\n",
            "Downloading pacmap-0.8.0-py3-none-any.whl (21 kB)\n",
            "Building wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp312-cp312-linux_x86_64.whl size=549354 sha256=879206be0e3f8c3a07a8c81c918a0993935318fc04cd2a36907522803aa74b02\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/b9/53/a3b2d1fe1743abadddec6aa541294b24fdbc39d7800bc57311\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy, pacmap\n",
            "Successfully installed annoy-1.17.3 pacmap-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"How to create a pipeline object?\"\n",
        "query_vector = embedding_model.embed_query(user_query)"
      ],
      "metadata": {
        "id": "PkExf11-Eebt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pacmap\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
        "\n",
        "embeddings_2d = [\n",
        "    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0]) for idx in range(len(chunks))\n",
        "] + [query_vector]\n",
        "\n",
        "# Fit the data (the index of transformed data corresponds to the index of the original data)\n",
        "documents_projected = embedding_projector.fit_transform(np.array(embeddings_2d), init=\"pca\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9VnC3oXByA0",
        "outputId": "88f06f6e-d7c5-48a6-e0fb-4e9691ad5a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pacmap.pacmap:Warning: random state is set to 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(\n",
        "    [\n",
        "        {\n",
        "            \"x\": documents_projected[i, 0],\n",
        "            \"y\": documents_projected[i, 1],\n",
        "            \"source\": docs_processed[i].metadata[\"source\"].split(\"/\")[1],\n",
        "            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n",
        "            \"symbol\": \"circle\",\n",
        "            \"size_col\": 4,\n",
        "        }\n",
        "        for i in range(len(docs_processed))\n",
        "    ]\n",
        "    + [\n",
        "        {\n",
        "            \"x\": documents_projected[-1, 0],\n",
        "            \"y\": documents_projected[-1, 1],\n",
        "            \"source\": \"User query\",\n",
        "            \"extract\": user_query,\n",
        "            \"size_col\": 100,\n",
        "            \"symbol\": \"star\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Visualize the embedding\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x=\"x\",\n",
        "    y=\"y\",\n",
        "    color=\"source\",\n",
        "    hover_data=\"extract\",\n",
        "    size=\"size_col\",\n",
        "    symbol=\"symbol\",\n",
        "    color_discrete_map={\"User query\": \"black\"},\n",
        "    width=1000,\n",
        "    height=700,\n",
        ")\n",
        "fig.update_traces(\n",
        "    marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\")),\n",
        "    selector=dict(mode=\"markers\"),\n",
        ")\n",
        "fig.update_layout(\n",
        "    legend_title_text=\"<b>Chunk source</b>\",\n",
        "    title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "seJBp8KCEwxq",
        "outputId": "6ab96570-c87a-44c6-b466-769a8a320ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"044727af-3d89-4bfc-99ab-f7a15391e29c\" class=\"plotly-graph-div\" style=\"height:700px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"044727af-3d89-4bfc-99ab-f7a15391e29c\")) {                    Plotly.newPlot(                        \"044727af-3d89-4bfc-99ab-f7a15391e29c\",                        [{\"customdata\":[[\"Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](htt...\"],[\"## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fraw.githu...\"],[\"Access and read Logs\\n\\nHugging Face Endpoints provides access to the logs of your Endpoints through t...\"],[\"Hugging Face Inference Endpoints documentation\\n\\n## Setup\\n\\n```bash\\npip install hf-doc-builder==0.4.0 ...\"],[\"Pricing\\n\\n\\u003cdiv class=\\\"flex md:justify-start mb-2 text-gray-400 items-center\\\"\\u003e\\n  \\u003ca href=\\\"https:\\u002f\\u002fui.e...\"],[\"| Provider | Instance Size | hourly rate | vCPUs | Memory | Architecture          |\\n| -------- | ---...\"],[\"| Provider | Instance Size | hourly rate | GPUs | Memory | Architecture |\\n| -------- | -------------...\"],[\"**hourly cost**\\n```\\ninstance hourly rate * ((hours * # min replica) + (scale-up hrs * # additional r...\"],[\"Supported Transformers & Diffusers Tasks\\n\\nInference Endpoints offers out-of-the-box support for Mach...\"],[\"| Task                           | Framework             | Out of the box Support |\\n|---------------...\"],[\"| Table Question Answering       | Transformers          | ✅                      |\\n| Conversational...\"],[\"## Example Request payloads\\n\\nSee the following request examples for some of the tasks:\\n\\n### Custom H...\"],[\"### Text Generation\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"This sound track was beautiful! It paints the senery in ...\"],[\"### Audio Classification\\n\\nAudio Classification can receive `json` payloads or binary data from a `au...\"],[\"### Text To Image\\n\\n```json\\n{        \\n  \\\"inputs\\\": \\\"realistic render portrait realistic render portrai...\"],[\"Access and view Metrics\\n\\nHugging Face Endpoints provides access to the metrics and analytics of your...\"],[\"# FAQs \\n\\n\\n\\n### Q: In which regions are Inference Endpoints available?\\n\\nA: Inference Endpoints are cu...\"],[\"### Q: I would like to deploy a model which is not in the supported tasks, is this possible?\\n\\nA:  Ye...\"],[\"A: Please contact us if you feel your model would do better on a different instance type than what i...\"],[\"Help & Support \\n\\nWe have a variety of Inference Endpoints blog posts to help you at https:\\u002f\\u002fhuggingf...\"],[\"Pause and Resume your Endpoint\\n\\nYou can `pause` & `resume` endpoints to save cost and configurations...\"],[\"\\u003cimg\\n  src=\\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fhf-endpoints-documentation\\u002fmain\\u002fassets\\u002fpau...\"],[\"API Reference (Swagger)\\n\\n🤗 Inference Endpoints can be used through the [UI](https:\\u002f\\u002fui.endpoints.hug...\"],[\"Use a custom Container Image\\n\\n\\nInference Endpoints not only allows you to [customize your inference ...\"],[\"Autoscaling\\n\\nAutoscaling allows you to dynamically adjust the number of endpoint replicas running yo...\"],[\"## Scaling to 0\\n\\nInference Endpoints also supports autoscaling to 0, which means reducing the number...\"],[\"Create a Private Endpoint with AWS PrivateLink\\n\\nSecurity and secure inference are key principles of ...\"],[\"\\u003cimg\\n  src=\\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fhf-endpoints-documentation\\u002fmain\\u002fassets\\u002f6_p...\"],[\"Security & Compliance\\n\\n🤗 Inference Endpoints is built with security and secure inference at its core...\"],[\"## Inference Endpoint Security level\\n\\nWe currently offer three types of endpoints, in order or incre...\"],[\"Send Requests to Endpoints\\n\\nYou can send requests to Inference Endpoints using the UI leveraging the...\"],[\"This means for an NLP task, the payload is represented as the `inputs` key and additional pipeline p...\"],[\"Change Organization or Account\\n\\nInference Endpoints uses your [Hugging Face](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"Update your Endpoint\\n\\nYou can update `running` Endpoints to change some of the configurations. Howev...\"],[\"Advanced Setup (Instance Types, Auto Scaling, Versioning)\\n\\nWe have seen how fast and easy it is to d...\"],[\"_Default: PyTorch if available._\\n\\n**Revision**\\n\\nCreate your Endpoint targeting a specific revision c...\"],[\"Inference Endpoints Version\\n\\nHugging Face Inference Endpoints comes with a default serving container...\"],[\"Serialization & Deserialization for Requests\\n\\nHugging Face Inference Endpount comes with a default s...\"],[\"| Content-Type           | Payload                        | \\n| ---------------------- | ------------...\"],[\"Below is a list of supported `accept` headers and the serialized payload is returned.\\n\\n\\n| Accept    ...\"],[\"🤗 Inference Endpoints\\n\\n🤗 Inference Endpoints offers a secure production solution to easily deploy an...\"],[\"### Guides\\n\\n* [Access the solution (UI)](\\u002fdocs\\u002finference-endpoints\\u002fguides\\u002faccess)\\n* [Create your fir...\"],[\"Access 🤗 Inference Endpoints\\n\\nTo access the [Inference Endpoints web application](https:\\u002f\\u002fui.endpoin...\"],[\"Add custom Dependencies\\n\\nInference Endpoints’ base image includes all required libraries to run infe...\"],[\"Create custom Inference Handler\\n\\nHugging Face Endpoints supports all of the Transformers and Sentenc...\"],[\"Included examples are for:\\n\\n* [Optimum and ONNX Runtime](https:\\u002f\\u002fhuggingface.co\\u002fphilschmid\\u002fdistilber...\"],[\"### 1. Set up Development Environment\\n\\nThe easiest way to develop our custom handler is to set up a ...\"],[\"The first step is to create our `handler.py` in the local clone of our repository.\\n\\n```\\n!cd distilbe...\"],[\"Next, we have to adjust our `handler.py` and `EndpointHandler` to match our condition.\\n\\n```python\\nfr...\"],[\"### 5. Push the Custom Handler to your repository\\n\\nAfter you have successfully tested your handler l...\"]],\"hovertemplate\":\"source=hf-endpoints-documentation\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"hf-endpoints-documentation, circle\",\"marker\":{\"color\":\"#EF553B\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"hf-endpoints-documentation, circle\",\"showlegend\":true,\"x\":[4.2266135,3.9947836,0.23528005,16.44687,4.9956827,-13.3095045,-15.042382,-21.733667,-3.9147434,-3.616515,-3.731614,-1.2752374,-3.9664268,8.6225815,8.999328,6.5297184,-1.489887,-2.1149712,-1.1218644,6.934947,3.2374787,-5.532912,3.1129637,-2.973268,-3.8640907,-3.4604728,-1.234876,9.0214405,-6.7574167,0.09384961,8.283952,8.302054,-6.885622,-6.5186563,15.762339,15.770793,0.8962245,-4.9187126,-5.4904466,-0.7167917,1.5308454,8.637178,3.1159992,7.9332695,-0.8803638,-1.3994551,-0.9150706,-1.1668599,0.38319924,-7.1425586],\"xaxis\":\"x\",\"y\":[-0.5885506,-0.9187065,5.0857825,2.8431144,1.9363083,-1.0557214,-2.21547,8.554317,5.1519747,5.2796307,4.9760284,3.5228245,4.7127485,2.6696765,3.069336,-0.6514034,-3.3841772,-2.732415,-3.0821168,-9.4140215,-2.506652,-5.342726,3.1052341,2.1421294,2.3392556,-4.812767,7.5843964,-11.379145,0.75746906,-5.909171,4.4394455,4.6209936,2.764666,7.902342,-16.044762,-16.043581,2.1325586,1.3983055,1.5505744,5.765622,0.57439923,-10.550685,2.5701387,-9.940468,2.7366483,0.5637251,-0.02814357,-0.6624387,5.7768826,2.4654732],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Choosing a metric for your task\\n\\n**So you've trained your model and want to see how well it’s doing ...\"],[\"### Task-specific metrics\\n\\nPopular ML tasks like Machine Translation and Named Entity Recognition ha...\"],[\"If you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use i...\"],[\"--\\ntitle: poseval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"`zero_division`: Which value to substitute as a metric value when encountering zero division. Should...\"],[\"`f1`: the average [F1 score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002ff1), on a scale between 0.0 and 1.0.\\n\\n\\n#...\"],[\"--\\ntitle: MAPE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"### Output Values\\nThis metric outputs a dictionary, containing the mean absolute error score, which ...\"],[\"## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n  title={Scikit-learn: Machine Learning in {P}ython}...\"],[\"--\\ntitle: ROUGE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"One can also pass a custom tokenizer which is especially useful for non-latin languages.\\n```python\\n\\u003e...\"],[\"### Output Values\\nThe output is a dictionary with one entry for each rouge type in the input list `r...\"],[\"The same example, but only calculating `rouge_1`:\\n```python\\n\\u003e\\u003e\\u003e rouge = evaluate.load('rouge')\\n\\u003e\\u003e\\u003e p...\"],[\"--\\ntitle: Word Length\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_f...\"],[\"## Citation(s)\\n\\n\\n## Further References\\n- [NLTK's `word_tokenize`](https:\\u002f\\u002fwww.nltk.org\\u002fapi\\u002fnltk.toke...\"],[\"Working with Keras and Tensorflow\\n\\n\\n\\nEvaluate can be easily intergrated into your Keras and Tensorfl...\"],[\"## Callbacks\\n\\nSuppose we want to keep track of model metrics while a model is training. We can use a...\"],[\"--\\ntitle: CharCut\\nemoji: 🔤\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"### Output Values\\n- **charcut_mt**: the CharCut evaluation score (lower is better)\\n\\n### Output Examp...\"],[\"--\\ntitle: IndicGLUE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"The output of the metric depends on the IndicGLUE subset chosen, consisting of a dictionary that con...\"],[\"Partial match for the CVIT-Mann Ki Baat subset (which outputs `precision@10`) \\n\\n```python\\n\\u003e\\u003e\\u003e indic_...\"],[\"--\\ntitle: Google BLEU\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"The minimum value of precision and recall is then returned as the score.\\n\\n\\n## Intended Uses\\nThis met...\"],[\"This metric can take on values from 0 to 1, inclusive. Higher scores are better, with 0 indicating n...\"],[\"Example with multiple references for the first sample:\\n```python\\n\\u003e\\u003e\\u003e predictions = ['It is a guide t...\"],[\"Example with multiple references for the first sample, with `min_len` adjusted to `2`, instead of th...\"],[\"## Citation\\n```bibtex\\n@misc{wu2016googles,\\ntitle={Google's Neural Machine Translation System: Bridgi...\"],[\"--\\ntitle: \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\np...\"],[\"`max_length`: the maximum sequence length (default value is `128`).\\n\\n`device`: either \\\"gpu\\\" or \\\"cpu\\\"...\"],[\"| FrugalScore                                        | Student     | Teacher        | Method     |\\n|...\"],[\"| [moussaKam\\u002ffrugalscore_tiny_bert-base_mover-score](https:\\u002f\\u002fhuggingface.co\\u002fmoussaKam\\u002ffrugalscore_ti...\"],[\"Depending on the size of the model picked, the loading time will vary: the `tiny` models will load v...\"],[\"--\\ntitle: Mean IoU\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"**Optional inputs**\\n- `nan_to_num` (`int`): If specified, NaN values will be replaced by the number ...\"],[\"### Examples\\n\\n```python\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e mean_iou = evaluate.load(\\\"mean_iou\\\")\\n\\u003e\\u003e\\u003e # suppos...\"],[\"--\\ntitle: SuperGLUE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"Format of `predictions`:\\n- for `record`: list of question-answer dictionaries with the following key...\"],[\"### Values from popular papers\\nThe [original SuperGLUE paper](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f1905.00537.pdf) ...\"],[\"## Limitations and bias\\nThis metric works only with datasets that have the same format as the [Super...\"],[\"🤗 Transformers\\n\\nTo run the 🤗 Transformers examples make sure you have installed the following librar...\"],[\"trainer.train()\\n```\\n\\n## Seq2SeqTrainer\\n\\nWe can use the [`~transformers.Seq2SeqTrainer`] for sequence...\"],[\"# rougeLSum expects newline after each sentence\\n    decoded_preds = [\\\"\\\\n\\\".join(nltk.sent_tokenize(pr...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"---\\n\\n## Adding a new element to the navigation bar\\n\\nAccepted files are Markdown (.md or .mdx).\\n\\nCrea...\"],[\"### Adding a new tutorial\\n\\nAdding a new tutorial or section is done in two steps:\\n\\n- Add a new file ...\"],[\"```\\n## XXXTokenizer\\n\\n[[autodoc]] XXXTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_speci...\"],[\"If the description is too long to fit in one line, another indentation is necessary before writing t...\"],[\"We follow the [doctest](https:\\u002f\\u002fdocs.python.org\\u002f3\\u002flibrary\\u002fdoctest.html) syntax for the examples to a...\"],[\"#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that no fi...\"],[\"--\\ntitle: Spearman Correlation Coefficient Metric \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradi...\"],[\"## How to Use\\nAt minimum, this metric only requires a `list` of predictions and a `list` of referenc...\"],[\"## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@book{kokoska2000crc,\\n  title={CRC standard probabil...\"],[\"--\\ntitle: TREC Eval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"### Examples\\n\\nA minimal example of looks as follows:\\n```Python\\nqrel = {\\n    \\\"query\\\": [0],\\n    \\\"q0\\\": ...\"],[\"## Limitations and Bias\\nThe `trec_eval` metric requires the inputs to be in the TREC run and qrel fo...\"],[\"A quick tour\\n\\n🤗 Evaluate provides access to a wide range of evaluation tools. It covers a range of m...\"],[\"## Load\\n\\nAny metric, comparison, or measurement is loaded with the `evaluate.load` function:\\n\\n```py\\n...\"],[\"Let's have a look at a few examples. First, let's look at the `description` attribute of the accurac...\"],[\"## Compute\\n\\nNow that we know how the evaluation module works and what should go in there we want to ...\"],[\"```py\\n\\u003e\\u003e\\u003e for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\\n\\u003e\\u003e\\u003e     accuracy.add_batch(reference...\"],[\"## Combining several evaluations\\n\\nOften one wants to not only evaluate a single metric but a range o...\"],[\"The content of the JSON file look like the following:\\n\\n```json\\n{\\n    \\\"experiment\\\": \\\"run 42\\\",\\n    \\\"ac...\"],[\"To make an evaluation with the `evaluator` let's load a `transformers` pipeline (but you can pass yo...\"],[\"\\u003e\\u003e\\u003e print(results)\\n{'accuracy':\\n    {\\n      'confidence_interval': (0.906, 0.9406749892841922),\\n    ...\"],[\"## Running evaluation on a suite of tasks\\n\\nIt can be useful to evaluate models on a variety of diffe...\"],[\"Evaluation can be run by loading the `EvaluationSuite` and calling the `run()` method with a model o...\"],[\"Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pledge...\"],[\"Community leaders have the right and responsibility to remove, edit, or reject\\ncomments, commits, co...\"],[\"### 2. Warning\\n\\n**Community Impact**: A violation through a single incident or series\\nof actions.\\n\\n*...\"],[\"[homepage]: https:\\u002f\\u002fwww.contributor-covenant.org\\n[v2.0]: https:\\u002f\\u002fwww.contributor-covenant.org\\u002fversio...\"],[\"Using the `evaluator` with custom pipelines\\n\\nThe evaluator is designed to work with `transformer` pi...\"],[\"pipe = ScikitEvalPipeline(text_clf)\\n```\\n\\nWe can now pass this `pipeline` to the `evaluator`:\\n\\n```py\\n...\"],[\"pipe = SpacyEvalPipeline(nlp)\\n```\\n\\nThat class is compatible with the `evaluator` and we can use the ...\"],[\"--\\ntitle: Matthews Correlation Coefficient\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_ve...\"],[\"### Inputs\\n- **`predictions`** (`list` of `int`s): Predicted class labels.\\n- **`references`** (`list...\"],[\"## Limitations and Bias\\n*Note any limitations or biases that the metric has.*\\n\\n\\n## Citation\\n```bibte...\"],[\"Evaluator\\n\\nThe evaluator classes for automatic evaluation.\\n\\n## Evaluator classes\\n\\nThe main entry poi...\"],[\"Using the `evaluator`\\n\\nThe `Evaluator` classes allow to evaluate a  triplet of model, dataset, and m...\"],[\"The text classification evaluator can be used to evaluate text models on classification datasets suc...\"],[\"eval_results = task_evaluator.compute(\\n    model_or_pipeline=model,\\n    data=data,\\n    label_mapping...\"],[\"```python\\nimport evaluate\\n\\neval_results = task_evaluator.compute(\\n    model_or_pipeline=\\\"lvwerra\\u002fdis...\"],[\"```python\\nimport pandas as pd\\nfrom datasets import load_dataset\\nfrom evaluate import evaluator\\nfrom ...\"],[\"df = pd.DataFrame(results, index=models)\\ndf[[\\\"overall_f1\\\", \\\"overall_accuracy\\\", \\\"total_time_in_second...\"],[\"### Visualizing results\\n\\nYou can feed in the `results` list above into the `plot_radar()` function t...\"],[\"Let's have a look how we can evaluate QA models and compute confidence intervals at the same time.\\n\\n...\"],[\"The evaluator can be used on large datasets! Below, an example shows how to use it on ImageNet-1k fo...\"],[\"--\\ntitle: Exact Match\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"### Inputs\\n- **`predictions`** (`list` of `str`): List of predicted texts.\\n- **`references`** (`list...\"],[\"Ignoring regexes \\\"the\\\" and \\\"yell\\\", as well as ignoring case and punctuation:\\n```python\\n\\u003e\\u003e\\u003e exact_mat...\"],[\"## Limitations and Bias\\nThis metric is limited in that it outputs the same score for something that ...\"],[\"--\\ntitle: Wilcoxon\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file:...\"],[\"--\\ntitle: SQuAD v2\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"## How to use \\n\\nThe metric takes two files or two lists - one representing model predictions and the...\"],[\"```python\\nfrom evaluate import load\\nsquad_metric = load(\\\"squad_v2\\\")\\nresults = squad_metric.compute(p...\"],[\"The range of `total` depends on the length of predictions\\u002freferences: its minimal value is 0, and ma...\"],[\"Partial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom evaluate import load\\nsquad_metric = lo...\"],[\"- [The Stanford Question Answering Dataset: Background, Challenges, Progress (blog post)](https:\\u002f\\u002fra...\"],[\"--\\ntitle: BLEU\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"Scores are calculated for individual translated segments—generally sentences—by comparing them with ...\"],[\"The default tokenizer is based on whitespace and regexes. It can be replaced by any function that ta...\"],[\"The [Attention is All you Need paper](https:\\u002f\\u002fproceedings.neurips.cc\\u002fpaper\\u002f2017\\u002ffile\\u002f3f5ee243547dee9...\"],[\"## Limitations and Bias\\nThis metric has multiple known limitations:\\n- BLEU compares overlap in token...\"],[\"--\\ntitle: Pearson Correlation Coefficient \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_ve...\"],[\"## How to Use\\n\\nThis metric takes a list of predictions and a list of references as input\\n\\n```python\\n...\"],[\"Example 2-The same as Example 1, but that also returns the `p-value`.\\n```python\\n\\u003e\\u003e\\u003e pearsonr_metric ...\"],[\"--\\ntitle: Code Eval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"N.B.\\nThis metric exists to run untrusted model-generated code. Users are strongly encouraged not to ...\"],[\"## Limitations and bias\\n\\nAs per the warning included in the metric code itself:\\n\\u003e This program exist...\"],[\"More information about the limitations of the code can be found on the [Human Eval Github repository...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fevaluate\\u002fmedia\\u002fresolve\\u002fmain...\"],[\"🤗 Evaluate is a library that makes evaluating and comparing models and reporting their performance e...\"],[\"# Installation\\n\\n## With pip\\n\\n🤗 Evaluate can be installed from PyPi and has to be installed in a virt...\"],[\"Types of Evaluations in 🤗 Evaluate\\n\\nThe goal of the 🤗 Evaluate library is to support different types...\"],[\"Comparisons have yet to be systematically used when comparing and reporting model performance, howev...\"],[\"--\\ntitle: WER\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"# Metric Card for WER\\n\\n## Metric description\\nWord error rate (WER) is a common metric of the perform...\"],[\"The **lower** the value, the **better** the performance of the ASR system, with a WER of 0 being a p...\"],[\"## Limitations and bias\\n\\nWER is a valuable tool for comparing different systems as well as for evalu...\"],[\"--\\ntitle: chrF\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"See the [sacreBLEU README.md](https:\\u002f\\u002fgithub.com\\u002fmjpost\\u002fsacreBLEU#chrf--chrf) for more information.\\n...\"],[\"### Output Values\\nThe output is a dictionary containing the following fields:\\n- **`'score'`** (`floa...\"],[\"The same chrF++ example as above, but with `lowercase=True` to normalize all case:\\n```python\\n\\u003e\\u003e\\u003e pre...\"],[\"## Citation\\n```bibtex\\n@inproceedings{popovic-2015-chrf,\\n    title = \\\"chr{F}: character n-gram {F}-sc...\"],[\"--\\ntitle: Regard\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ...\"],[\"### Output Values\\n\\n**With a single input**:\\n\\n`regard` : the regard scores of each string in the inpu...\"],[\"Example 2 (comparison mode):\\n```python\\n\\u003e\\u003e\\u003e regard = evaluate.load(\\\"regard\\\", \\\"compare\\\")\\n\\u003e\\u003e\\u003e group1 = ...\"],[\"## Citation(s)\\n@article{https:\\u002f\\u002fdoi.org\\u002f10.48550\\u002farxiv.1909.01326,\\n  doi = {10.48550\\u002fARXIV.1909.0132...\"],[\"--\\ntitle: Honest\\nemoji: 🤗\\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ap...\"],[\"| Model Name       | Top K =1 | Top K =5 |Top K =20 |\\n| ---------------- | -------- | -------- | ---...\"],[\"Example 2: Calculating HONEST with 2 groups (e.g. male\\u002ffemale)\\n```python\\n\\u003e\\u003e\\u003e honest = evaluate.load(...\"],[\"## Further References\\n- Bassignana, Elisa, Valerio Basile, and Viviana Patti. [\\\"Hurtlex: A multiling...\"],[\"--\\ntitle: SQuAD\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"The range of `f1` is 0-1 -- its lowest possible value is 0, if either the precision or the recall is...\"],[\"Partial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom evaluate import load\\nsquad_metric = lo...\"],[\"--\\ntitle: Recall\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): The predicted labels.\\n- **references** (`list` of `i...\"],[\"- **sample_weight** (`list` of `float`): Sample weights Defaults to `None`.\\n- **zero_division** (): ...\"],[\"### Output Values\\n- **recall**(`float`, or `array` of `float`, for multiclass targets): Either the g...\"],[\"Example 4-A multiclass example, using different averages.\\n```python\\n\\u003e\\u003e\\u003e recall_metric = evaluate.loa...\"],[\"--\\ntitle: BERT Score\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file...\"],[\"```python\\nfrom evaluate import load\\nbertscore = load(\\\"bertscore\\\")\\npredictions = [\\\"hello there\\\", \\\"gen...\"],[\"`f1`: The [F1 score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002ff1) for each sentence from the `predictions` + `...\"],[\"Furthermore, not all languages are supported by the metric -- see the [BERTScore supported language ...\"],[\"--\\ntitle: Competition MATH\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\nap...\"],[\"More recent progress on the dataset can be found on the [dataset leaderboard](https:\\u002f\\u002fpaperswithcode...\"],[\"## Citation\\n\\n```bibtex\\n@article{hendrycksmath2021,\\n  title={Measuring Mathematical Problem Solving W...\"],[\"--\\ntitle: MSE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"### Output Values\\nThis metric outputs a dictionary, containing the mean squared error score, which i...\"],[\"## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n  title={Scikit-learn: Machine Learning in {P}ython}...\"],[\"--\\ntitle: WikiSplit\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"`sacrebleu`: the [SacreBLEU](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002fsacrebleu) score, which can take any val...\"],[\"No match between prediction and reference:\\n\\n```python\\n\\u003e\\u003e\\u003e wiki_split = evaluate.load(\\\"wiki_split\\\")\\n\\u003e...\"],[\"--\\ntitle: r_squared\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ...\"],[\"Here's an example of how to calculate the R-squared value:\\n```python\\nr_squared = 1 - (SSR\\u002fSST)\\n```\\n\\n...\"],[\"- [Khan Academy: R-Squared](https:\\u002f\\u002fwww.khanacademy.org\\u002fmath\\u002fstatistics-probability\\u002fdescribing-relat...\"],[\"--\\ntitle: Precision\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): Predicted class labels.\\n- **references** (`list` of ...\"],[\"- 'samples': Calculate metrics for each instance, and find their average (only meaningful for multil...\"],[\"### Output Values\\n- **precision**(`float` or `array` of `float`): Precision score or list of precisi...\"],[\"Example 4-A multiclass example, with different values for the `average` input.\\n```python\\n\\u003e\\u003e\\u003e predict...\"],[\"--\\ntitle: XTREME-S\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"It also has two optional arguments: \\n\\n- `bleu_kwargs`: a `dict` of keywords to be passed when comput...\"],[\"- `cer`:  Character error rate (CER) is similar to WER, but operates on character instead of word. T...\"],[\"For the `fleurs-lang_id` subset (which outputs `accuracy`):\\n\\n```python\\n\\u003e\\u003e\\u003e xtreme_s_metric = evaluat...\"],[\"--\\ntitle: CER\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"This metric outputs a float representing the character error rate.\\n\\n```\\nprint(cer_score)\\n0.341463414...\"],[\"CER above 1 due to insertion errors:\\n\\n```python\\nfrom evaluate import load\\ncer = load(\\\"cer\\\")\\npredicti...\"],[\"--\\ntitle: {{ cookiecutter.module_name }}\\ndatasets:\\n- {{ cookiecutter.dataset_name }} \\ntags:\\n- evalua...\"],[\"### Examples\\n*Give code examples of the {{ cookiecutter.module_type }} being used. Try to include ex...\"],[\"--\\ntitle: McNemar\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ...\"],[\"## Limitations and bias\\n\\nThe McNemar test is a non-parametric test, so it has relatively few assumpt...\"],[\"--\\ntitle: F1\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): Predicted labels.\\n- **references** (`list` of `int`)...\"],[\"### Output Values\\n- **f1**(`float` or `array` of `float`): F1 score or list of f1 scores, depending ...\"],[\"Example 4-A multiclass example, with different values for the `average` input.\\n```python\\n\\u003e\\u003e\\u003e predict...\"],[\"Considerations for model evaluation\\n\\nDeveloping an ML model is rarely a one-shot deal: it often invo...\"],[\"## The impact of class imbalance\\n\\nWhile many academic datasets, such as the [IMDb dataset](https:\\u002f\\u002fh...\"],[\"Using accuracy in an imbalanced setting is less ideal, since it is not sensitive to minority classes...\"],[\"Other metrics, such as [BLEU](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fevaluate-metric\\u002fexact_match) are harder ...\"],[\"When doing online model evaluation, there is often a trade-off to be done between inference speed an...\"],[\"--\\ntitle: Mahalanobis Distance\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19....\"],[\"## Limitations and Bias\\n\\nThe Mahalanobis distance is only able to capture linear relationships betwe...\"],[\"--\\ntitle: MAUVE\\nemoji: 🤗\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"It also has several optional arguments:\\n\\n`num_buckets`: the size of the histogram to quantize P and ...\"],[\"`p_hist`: a discrete distribution, which is a quantized version of the text distribution `p_text`.\\n ...\"],[\"It is a good idea to use at least 1000 samples for each distribution to compute MAUVE (the original ...\"],[\"--\\ntitle: BLEURT\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"### Inputs\\n- **predictions** (`list` of `str`s): List of generated sentences to score.\\n- **reference...\"],[\"Example with the `\\\"bleurt-base-128\\\"` model checkpoint:\\n```python\\n\\u003e\\u003e\\u003e predictions = [\\\"hello there\\\", \\\"...\"],[\"--\\ntitle: Label Distribution\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0....\"],[\"#### Values from Popular Papers\\n\\n\\n### Examples\\nCalculating the label distribution of a dataset with ...\"],[\"--\\ntitle: XNLI\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"### Values from popular papers\\nThe [original XNLI paper](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f1809.05053.pdf) repor...\"],[\"## Citation\\n\\n```bibtex\\n@InProceedings{conneau2018xnli,\\n  author = \\\"Conneau, Alexis\\n                 ...\"],[\"--\\ntitle: Text Duplicates\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\na...\"],[\"```python\\n\\u003e\\u003e\\u003e data = [\\\"foo\\\", \\\"bar\\\", \\\"foobar\\\"]\\n\\u003e\\u003e\\u003e duplicates = evaluate.load(\\\"text_duplicates\\\")\\n\\u003e\\u003e\\u003e ...\"],[\"Creating an EvaluationSuite\\n\\nIt can be useful to evaluate models on a variety of different tasks to ...\"],[\"The mandatory attributes for a new `SubTask` are `task_type` and `data`.\\n1. [`task_type`] maps to th...\"],[\"An `EvaluationSuite` can be loaded by name from the Hugging Face Hub, or locally by providing a path...\"],[\"--\\ntitle: MAE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"### Output Values\\nThis metric outputs a dictionary, containing the mean absolute error score, which ...\"],[\"## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n  title={Scikit-learn: Machine Learning in {P}ython}...\"],[\"--\\ntitle: GLUE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"`f1`: the harmonic mean of the precision and recall (see [F1 score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002ff...\"],[\"For more recent model performance, see the [dataset leaderboard](https:\\u002f\\u002fpaperswithcode.com\\u002fdataset\\u002f...\"],[\"## Citation\\n\\n```bibtex\\n @inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Ana...\"],[\"Scikit-Learn\\n\\nTo run the scikit-learn examples make sure you have installed the following library:\\n\\n...\"],[\"Append classifier to preprocessing pipeline. Now we have a full prediction pipeline.\\n\\n```python\\nclf ...\"],[\"Logging methods\\n\\n🤗 Evaluate strives to be transparent and explicit about how it works, but this can ...\"],[\"[[autodoc]] evaluate.logging.set_verbosity\\n\\n[[autodoc]] evaluate.logging.set_verbosity_info\\n\\n[[autod...\"],[\"Installation\\n\\nBefore you start, you will need to setup your environment and install the appropriate ...\"],[\"--\\ntitle: Word Count\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_fi...\"],[\"Example for a dataset from 🤗 Datasets:\\n\\n```python\\n\\u003e\\u003e\\u003e imdb = datasets.load_dataset('imdb', split = '...\"],[\"--\\ntitle: seqeval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"`sample_weight`: An array-like of shape (n_samples,) that provides weights for individual samples. T...\"],[\"More recently, seqeval continues being used for reporting performance on tasks such as [named entity...\"],[\"Partial match:\\n\\n```python\\n\\u003e\\u003e\\u003e seqeval = evaluate.load('seqeval')\\n\\u003e\\u003e\\u003e predictions = [['O', 'O', 'B-MI...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fevaluate\\u002fmedia\\u002fresolve\\u002fmain...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003e\\u003cdiv class=\\\"w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 fo...\"],[\"--\\ntitle: SARI\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"`F1_keep` is the n-gram F1 score for keep operations \\n\\n`P_del` is the n-gram precision score for del...\"],[\"### Values from popular papers\\n\\nThe [original paper that proposes the SARI metric](https:\\u002f\\u002faclanthol...\"],[\"## Citation\\n\\n```bibtex\\n@inproceedings{xu-etal-2016-optimizing,\\ntitle = {Optimizing Statistical Machi...\"],[\"--\\ntitle: METEOR\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"METEOR is based on a generalized concept of unigram matching between the machine-produced translatio...\"],[\"## Output values\\n\\nThe metric outputs a dictionary containing the METEOR score. Its values range from...\"],[\"Multiple `references` per `prediction`, partial match:\\n\\n```python\\n\\u003e\\u003e\\u003e meteor = evaluate.load('meteor...\"],[\"--\\ntitle: CharacTER\\nemoji: 🔤\\ncolorFrom: orange\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file...\"],[\"# Corpus example\\npreds = [\\\"this week the saudis denied information published in the new york times\\\",...\"],[\"## Further References\\n- Repackaged version that is used in this HF implementation: [https:\\u002f\\u002fgithub.c...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Submitting a new issue or feature request\\n\\nFollowing these guidelines when submitting an issue or...\"],[\"### Do you want to request a new feature (that is not a metric)?\\n\\nWe would appreciate it if your fea...\"],[\"1. Fork the [repository](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fevaluate) by\\n   clicking on the 'Fork' butto...\"],[\"🤗 Evaluate also uses `flake8` and a few custom scripts to check for coding mistakes. Quality\\n   cont...\"],[\"6. Once you are satisfied, go to the webpage of your fork on GitHub. Click on 'Pull request' to send...\"],[\"**This guide was heavily inspired by the awesome [scikit-learn guide to contributing](https:\\u002f\\u002fgithub...\"],[\"--\\ntitle: MASE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"Optional arguments:\\n- `periodicity`: the seasonal periodicity of training data. The default is 1.\\n- ...\"],[\"Example with multi-dimensional lists, and the `raw_values` config:\\n```python\\n\\u003e\\u003e\\u003e mase_metric = evalu...\"],[\"--\\ntitle: sMAPE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"### Output Values\\nThis metric outputs a dictionary, containing the mean absolute error score, which ...\"],[\"## Citation(s)\\n\\n```bibtex\\n@article{article,\\n    author = {Chen, Zhuo and Yang, Yuhong},\\n    year = {...\"],[\"Saving methods\\n\\nMethods for saving evaluations results:\\n\\n## Save\\n\\n[[autodoc]] evaluate.save...\"],[\"--\\ntitle: Exact Match \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: green\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_f...\"],[\"--\\ntitle: Perplexity\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_fi...\"],[\"```python\\nfrom evaluate import load\\nperplexity = load(\\\"perplexity\\\",  module_type= \\\"measurement\\\")\\nres...\"],[\"The range of this metric is [0, inf). A lower score is better.\\n\\n#### Values from Popular Papers\\n\\n\\n##...\"],[\"--\\ntitle: Accuracy\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"Output Example(s):\\n```python\\n{'accuracy': 1.0}\\n```\\n\\nThis metric outputs a dictionary, containing the...\"],[\"## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n  title={Scikit-learn: Machine Learning in {P}ython}...\"],[\"--\\ntitle: RL Reliability\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_...\"],[\"### Output Values\\n\\nIn `\\\"online\\\"` mode:\\n- HighFreqEnergyWithinRuns: High Frequency across Time (DT)\\n-...\"],[\"## Limitations and Bias\\nThis implementation of RL reliability metrics does not compute permutation t...\"],[\"--\\ntitle: \\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\np...\"],[\"9\\tWord sense\\tThis is the word sense of the word in Column 3.\\n  10\\tSpeaker\\u002fAuthor\\tThis is the speaker...\"],[\"## Metric description\\n\\nCoVal is a coreference evaluation tool for the [CoNLL](https:\\u002f\\u002fhuggingface.co...\"],[\"```python\\nfrom evaluate import load\\ncoval = load('coval')\\nwords = ['bc\\u002fcctv\\u002f00\\u002fcctv_0005   0   0    ...\"],[\"## Output values\\n\\nThe metric outputs a dictionary with the following key-value pairs:\\n\\n`mentions`: n...\"],[\"## Examples \\n\\nMaximal values\\n\\n```python\\nfrom evaluate import load\\ncoval = load('coval')\\nwords = ['bc...\"],[\"| Column | Type                  | Description                                                      ...\"],[\"| 5      | Part-of-Speech        |                                                                  ...\"],[\"| 9      | Word sense            | This is the word sense of the word in Column 3.                  ...\"],[\"| N      | Coreference           | Coreference chain information encoded in a parenthesis structure....\"],[\"## Citations\\n\\n```bibtex\\n@InProceedings{moosavi2019minimum,\\n  author = { Nafise Sadat Moosavi, Leo Bo...\"],[\"```bibtex\\n@inproceedings{moosavi-strube-2016-coreference,\\n    title = \\\"Which Coreference Evaluation ...\"],[\"--\\ntitle: CUAD\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"`references`: a list of question-answer dictionaries with the following key-values:\\n - `id`: the id ...\"],[\"`aupr`: The Area Under the Precision-Recall curve, with a range between 0.0 and 1.0, with a higher v...\"],[\"Minimal values:\\n\\n```python\\nfrom evaluate import load\\ncuad_metric = load(\\\"cuad\\\")\\npredictions = [{'pre...\"],[\"In terms of the metric itself, the accuracy of AUPR has been debated because its estimates are quite...\"],[\"Visualization methods\\n\\nMethods for visualizing evaluations results:\\n\\n## Radar Plot\\n\\n[[autodoc]] eval...\"],[\"--\\ntitle: TER\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.p...\"],[\"See the README.md file at https:\\u002f\\u002fgithub.com\\u002fmjpost\\u002fsacreBLEU#ter for more information.\\n\\n\\n## How to ...\"],[\"### Output Values\\nThis metric returns the following:\\n- **`score`** (`float`): TER score (num_edits \\u002f...\"],[\"Example ignoring punctuation and capitalization, and everything matches:\\n```python\\n\\u003e\\u003e\\u003e predictions =...\"],[\"## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@inproceedings{snover-etal-2006-study,\\n    title = \\\"...\"],[\"--\\ntitle: Toxicity\\nemoji: 🤗\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: ap...\"],[\"Args:\\n    `predictions` (list of str): prediction\\u002fcandidate sentences\\n    `toxic_label` (str) (optio...\"],[\"`toxicity_ratio` : the percentage of predictions with toxicity \\u003e= 0.5 (if `aggregation` = `ratio`)\\n\\n...\"],[\"```bibtex\\n@article{gehman2020realtoxicityprompts,\\n  title={Realtoxicityprompts: Evaluating neural to...\"],[\"Hub methods\\n\\nMethods for using the Hugging Face Hub:\\n\\n## Push to hub \\n\\n[[autodoc]] evaluate.push_to_...\"],[\"--\\ntitle: COMET\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"Alternative models that can be chosen include `wmt20-comet-qe-da`, `wmt21-comet-mqm`, `wmt21-cometin...\"],[\"## Examples\\n\\nFull match:\\n\\n```python\\nfrom evaluate import load\\ncomet_metric = load('comet') \\nsource =...\"],[\"## Limitations and bias\\n\\nThe models provided for calculating the COMET metric are built on top of XL...\"],[\"### Interpreting Scores:\\n\\nWhen using COMET to evaluate machine translation, it's important to unders...\"],[\"```bibtex\\n@inproceedings{rei-EtAl:2020:WMT,\\n   author    = {Rei, Ricardo  and  Stewart, Craig  and  ...\"],[\"--\\ntitle: Brier Score\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"Output Example(s):\\n```python\\n{'brier_score': 0.5}\\n```\\n\\n#### Values from Popular Papers\\n\\n\\n### Example...\"],[\"--\\ntitle: Perplexity\\nemoji: 🤗\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```python\\nfrom evaluate import load\\nperplexity = load(\\\"perplexity\\\", module_type=\\\"metric\\\")\\nresults = ...\"],[\"The range of this metric is [0, inf). A lower score is better.\\n\\n#### Values from Popular Papers\\n\\n\\n##...\"],[\"See Meister and Cotterell, [\\\"Language Model Evaluation Beyond Perplexity\\\"]( https:\\u002f\\u002farxiv.org\\u002fabs\\u002f21...\"],[\"Loading methods\\n\\nMethods for listing and loading evaluation modules:\\n\\n## List\\n\\n[[autodoc]] evaluate....\"],[\"Creating and sharing a new evaluation\\n\\n## Setup\\n\\nBefore you can create a new metric make sure you ha...\"],[\"3. [`EvaluationModuleInfo.inputs_description`] describes the expected inputs and outputs. It may als...\"],[\"This method is used when you call `.compute()` later on.\\n\\n## Readme\\n\\nWhen you use the `evalute-cli` ...\"],[\"Main classes\\n\\n## EvaluationModuleInfo\\n\\nThe base class `EvaluationModuleInfo` implements a the logic ...\"],[\"--\\ntitle: SacreBLEU\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"### Inputs\\n- **`predictions`** (`list` of `str`): list of translations to score. Each translation sh...\"],[\"### Output Values\\n- `score`: BLEU score\\n- `counts`: Counts\\n- `totals`: Totals\\n- `precisions`: Precis...\"],[\"--\\ntitle: ROC AUC\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"This metric has three separate use cases:\\n- **binary**: The case in which there are only two differe...\"],[\"### Inputs\\n- **`references`** (array-like of shape (n_samples,) or (n_samples, n_classes)): Ground t...\"],[\"- **`sample_weight`** (array-like of shape (n_samples,)): Sample weights. Defaults to None.\\n- **`max...\"],[\"### Output Values\\nThis metric returns a dict containing the `roc_auc` score. The score is a `float`,...\"],[\"Example 3, the **multilabel** use case:\\n```python\\n\\u003e\\u003e\\u003e roc_auc_score = evaluate.load(\\\"roc_auc\\\", \\\"mult...\"],[\"```bibtex\\n@article{scikit-learn,\\ntitle={Scikit-learn: Machine Learning in {P}ython},\\nauthor={Pedrego...\"],[\"--\\ntitle: NIST_MT\\nemoji: 🤗 \\ncolorFrom: purple\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"### Inputs\\n- **predictions**: tokenized predictions to score. For sentence-level NIST, a list of tok...\"]],\"hovertemplate\":\"source=evaluate\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"evaluate, circle\",\"marker\":{\"color\":\"#00cc96\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"evaluate, circle\",\"showlegend\":true,\"x\":[0.9931312,0.62539124,-0.037520055,13.361428,4.1790056,4.310334,-13.195936,-12.92195,-12.833122,-12.789121,-12.505728,-12.301299,-12.29818,-4.419823,-4.892678,16.229898,15.970695,8.907539,16.458248,16.512466,16.41037,11.620329,15.694774,16.37819,12.67052,16.295206,16.856256,16.78972,16.178778,15.374718,19.37687,16.921066,19.226252,-5.1498585,-4.045782,-7.302661,-6.2809176,-3.1546547,7.472102,9.129399,16.406326,15.685489,17.166264,19.271498,18.099257,17.530602,-1.0311373,-1.0851501,-0.9560146,-7.555012,19.129648,19.234816,17.565042,-0.81571835,-3.7392328,-3.6102247,-3.7317584,-3.5086317,-0.8929411,-3.3878243,-3.4268887,-3.7268565,-3.242197,-3.4123623,-0.17758526,0.48568913,0.20676136,8.200801,-2.4875028,-9.499962,-2.7826896,17.145464,19.35111,18.38977,-2.8992362,-8.047084,-7.5666738,-6.059802,-1.4402982,4.6306496,14.23186,9.206089,10.65188,-7.6349864,6.4176006,-6.281268,-9.575957,-13.605852,-17.414532,14.465477,-5.680526,5.935899,-3.2247133,-3.8231642,-4.0739536,-2.0439806,-2.0992856,-0.9316024,9.30953,9.310894,9.312283,9.308538,9.313466,-10.1505575,-10.005717,-9.955142,-8.1859455,-7.394545,-4.2484455,3.5715022,-1.5228965,-1.7094796,-7.8910556,-0.8069264,-0.4643437,0.34396285,0.18401721,-1.2163283,-2.178574,1.0437282,-4.614086,7.9590425,0.28214172,-0.060425725,-1.1670479,2.6052725,-2.231358,-0.84873956,-0.6020969,-9.402424,0.13902579,-0.73817825,13.244761,13.422743,13.775733,1.0495765,-1.050233,-6.446789,-6.485371,-6.4370327,-12.706696,-12.457171,-12.4178915,-12.486897,-1.4546758,-2.2630603,-2.3044999,-21.731565,-18.367733,-18.681566,-3.4572608,-2.0373192,-2.311376,-0.44801274,-6.0725145,-5.7687182,-6.2461314,-6.33555,-5.977857,-6.432956,9.0603,5.0868015,5.027186,4.2378473,9.256056,10.958428,-7.634336,-0.5748584,9.989774,5.2189236,-1.1334133,-9.401356,5.3887963,3.4542968,2.8540852,0.12478672,-3.5365794,-3.977485,-1.2020458,-1.2126445,-1.2118968,-6.0349946,-2.407253,15.779287,15.784988,15.78013,15.768459,15.779095,15.776991,15.775908,-5.7083716,-5.7077613,-2.641038,-2.2840443,-1.8635887,-9.292683,-7.5791397,-0.92643785,-0.4934138,-0.28731525,13.045688,-3.3924975,8.540935,5.3619385,-3.5726547,-1.8697525,11.589717,6.036248,13.372382,2.4347384,2.4408293,-14.292049,0.30156744,0.55804336,12.81385,12.853001,-2.1301565,-1.2521082,-1.484412,11.718151,12.5045805,-3.6347399,-3.2703712,-9.242761,-2.4007423,-6.562184,-6.5365915,-3.7369187,-14.518391,-21.730963,-18.040174,3.7566962,-2.3873327,-0.4549367,-1.1718179,-1.3175032,-1.0245172,-0.80962074,-1.5566273,-10.629219,-10.598724,-10.194792,11.788628,12.04649,11.223156,11.681321,-2.2854023,-5.1108274,-7.12935,-3.9290335,19.699865,19.627228,-14.900518,-5.80605,-5.6052785,-5.7183237,-14.497636,-21.735033,1.2873017,0.81136405,0.77345634,-3.6987958,-4.1624346,-4.8163433,-4.478545,-4.590015,-5.0524645,-5.2175064,-4.019543,-4.133635,-4.061648,-4.0281796,-4.1465673,-3.125309,2.7181332,3.745109,7.696241,7.702469,1.3323599,-1.0875283,0.6547408,-0.79880565,-2.4644063,15.749151,-2.1347423,-0.8387311,-7.6054335,-1.5644537,-4.9276977,7.749135,3.2112606,3.5788178,-0.20896687,-1.1822442,0.29828638,-3.4242117,8.737542,-3.2257502,-2.026007,-0.38789618,-3.4767394,-9.703808,-6.0389547,-6.275652,-1.8399812,-1.3307899,-2.7723787,0.53584373,-1.8781557,-2.0863903,-1.3421489,-7.966501,3.5044858],\"xaxis\":\"x\",\"y\":[6.49173,7.9265013,7.132928,5.4202056,0.8349308,0.5346287,-18.24872,-18.120365,-18.136251,-18.144835,-18.371998,-17.847471,-17.85074,-1.8134707,-1.4901932,3.174326,3.2455702,2.2016199,2.9272985,2.4207652,2.4155233,4.398523,3.5374517,2.3728712,3.8233128,3.2383807,1.9471688,1.9554254,3.0780203,3.3165548,2.4198565,2.84775,2.513203,2.793697,-6.28282,-0.017436882,2.1130788,-0.306751,2.653679,1.7679864,3.3368645,3.4366264,3.2420864,2.4062994,1.9208779,2.754774,-2.372898,1.4100438,-1.7888634,-1.6196325,3.282264,2.730671,2.434427,-1.8284436,0.81821746,-3.436247,-3.592348,-2.497936,-2.9907966,-2.5601764,-3.1081197,-5.8146944,-4.5844436,-4.2302375,9.996093,6.9655914,7.3909073,3.2186852,3.9656372,0.5505497,3.8125646,3.0284007,2.4439645,2.7368495,-6.8492556,-1.9946901,-2.224104,2.3975382,4.2143674,1.3450367,3.6144044,2.758088,3.2181516,-1.5701319,1.7975196,6.0500903,-0.7084596,19.45066,14.811181,5.1556034,-1.3706799,1.5265605,3.7338314,4.3419414,2.9785419,1.0381724,2.5833004,3.152509,-19.999279,-20.000275,-19.999598,-20.001732,-19.998625,0.42321634,0.5168008,0.95147985,3.0156255,2.0071938,1.503707,2.962857,-7.7184987,-7.505393,3.1719737,4.84299,5.3951755,5.4886775,5.292309,3.2515972,-1.0255883,4.7274346,0.8385271,4.019561,4.903386,5.0843306,-3.7194223,-0.21072939,-5.476099,-7.005579,7.5437007,-0.52142954,2.552886,2.0492682,3.554827,3.6203048,3.3827724,4.6004567,5.2520742,6.974703,7.7882004,7.845898,-18.397217,-17.994225,-17.587454,-17.911572,0.61724937,-2.3535013,-2.1151276,8.552606,-4.2239842,-4.2649097,-0.58480924,0.24854226,0.15982865,0.5171803,0.042747237,0.26200348,2.6591003,2.6984162,2.225867,2.8416607,2.7656856,3.718119,3.8447852,-0.82845163,3.0306132,3.484136,-1.7794815,7.5508356,3.5275872,-0.21211526,-0.83269364,-1.0829942,1.544804,2.6940904,3.1903028,3.9690285,2.5854,1.2374089,2.0778039,1.8515508,1.23602,1.642144,5.154653,-16.038311,-16.031862,-16.036022,-16.037813,-16.03523,-16.036707,-16.037333,2.4572349,2.2471879,-6.5855436,-6.826711,-7.4843345,0.059909962,6.2940397,3.492307,4.043198,4.263498,3.8022912,2.8583097,-10.406224,1.8679373,-6.6856155,-6.3345165,2.0423331,2.06473,5.4258423,3.5916986,3.6761897,-1.8789986,8.201893,8.169971,4.3427615,4.4668503,-2.2910569,-7.009623,0.356389,3.4048896,3.5509737,-3.3227024,-7.6876326,-0.17136486,1.2314112,2.8514404,3.0700169,-7.480548,-2.2510355,8.551886,-4.130051,1.5960344,-12.394272,-0.15915222,5.6658115,3.3207722,3.6636617,-0.20394932,3.3093,-0.60207444,-0.46949646,-0.5290233,2.4135478,2.333697,2.538186,2.9326692,3.5403907,0.26105618,1.7859237,5.53841,2.3335087,2.3782525,-2.0554533,-6.4091177,-6.392781,-6.430714,-2.242575,8.554897,5.5207043,4.6859813,5.174538,6.6964974,-8.172935,-5.676562,-5.772515,-5.732297,-2.125251,-2.3306053,-3.0760703,-3.2761834,-2.5610843,-3.1600108,-3.2109606,-8.072724,4.4646864,4.455291,3.430909,3.4312139,4.271007,1.8130319,7.173484,2.779313,1.2670938,-16.044073,3.5489573,3.6225278,2.1848774,3.4255202,2.6159294,3.6358466,5.240472,0.864692,9.7148905,6.84648,8.006374,-5.642816,-11.5114,0.1258449,-1.7631056,0.011460868,-1.9305708,-0.49373257,3.9054437,4.001296,4.8397326,0.8052143,1.5505248,6.311339,1.6555482,1.3041615,2.719464,-1.6786988,-1.961551],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio 的主要特点：\\n\\n1. [添加示例输入](#example-inputs)\\n2. [传递自定义错误消息](#erro...\"],[\"例如，对于上面显示的计算器界面，我们将在下面的旗标目录中存储标记的数据：\\n\\n```directory\\n+-- calculator.py\\n+-- flagged\\u002f\\n|   +-- logs.csv\\n`...\"],[\"## 队列 (Queuing)\\n\\n如果您的应用程序预计会有大量流量，请 with `queue()` 方法来控制处理速率。这将排队处理调用，因此一次只处理一定数量的请求。队列使用 Websockets...\"],[\"## 批处理函数 (Batch Functions)\\n\\nGradio 支持传递*批处理*函数。批处理函数只是接受输入列表并返回预测列表的函数。\\n\\n例如，这是一个批处理函数，它接受两个输入列表（一个单词...\"],[\"Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef...\"],[\"State in Blocks\\n\\nWe covered [State in Interfaces](https:\\u002f\\u002fgradio.app\\u002finterface-state), this guide ta...\"],[\"如何使用地图组件绘制图表\\n\\nRelated spaces:\\nTags: PLOTS, MAPS\\n\\n## 简介\\n\\n本指南介绍如何使用 Gradio 的 `Plot` 组件在地图上绘制地理数据。Gradi...\"],[\"fig.update_layout(\\n    mapbox_style=\\\"open-street-map\\\",\\n    hovermode='closest',\\n    mapbox=dict(\\n   ...\"],[\"Gradio Demo: examples_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"Gradio Demo: number_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr....\"],[\"Gradio Demo: map_airbnb\\n### Display an interactive map of AirBnB locations with Plotly. Data is host...\"],[\"return fig\\n\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_p...\"],[\"Gradio Demo: question-answering\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nimport gra...\"],[\"`@gradio\\u002fbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { Button } from \\\"@gradio\\u002fbutton\\\";\\n\\u003c\\u002fscript\\u003e\\n\\n\\u003cbutton type...\"],[\"Gradio Demo: sales_projections\\n\\n\\n```\\n!pip install -q gradio pandas numpy matplotlib\\n```\\n\\n\\n```\\nimport...\"],[\"Gradio and W&B Integration\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fakhaliq\\u002fJoJoGAN\\nTags: WAND...\"],[\"Now, let's walk you through how to do this on your own. We'll make the assumption that you're new to...\"],[\"# reset generator\\n   del generator\\n   generator = deepcopy(original_generator)\\n\\n   g_optim = optim.A...\"],[\"alpha = 1.0\\nalpha = 1-alpha\\n\\npreserve_color = True\\nnum_iter = 100\\nlog_interval = 50\\n\\nsamples = []\\nco...\"],[\"with torch.no_grad():\\n        real_feat = discriminator(targets)\\n    fake_feat = discriminator(img)\\n...\"],[\"torch.save({\\\"g\\\": generator.state_dict()}, \\\"your-model-name.pt\\\")\\n\\nfiles.download('your-model-name.pt'...\"],[\"6. Integrate Gradio into your W&B Dashboard\\n\\n   The last step—integrating your Gradio demo with your...\"],[\"- Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web.\\n\\n## ...\"],[\"Gradio Demo: duplicatebutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n...\"],[\"Gradio Demo: upload_button_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as...\"],[\"@gradio\\u002fimageeditor\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6809](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f680...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Fixes\\n\\n- [#6502](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"Gradio Demo: chatinterface_system_prompt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr...\"],[\"Gradio Demo: streaming_wav2vec\\n\\n\\n```\\n!pip install -q gradio torch transformers \\n```\\n\\n\\n```\\nfrom trans...\"],[\"component-styles\\n\\n## Textbox\\n\\n| name        | type                                 | description    ...\"],[\"## Checkbox Group\\n\\n| name             | type                                 | description          ...\"],[\"## File\\n\\n| name      | type                                 | description         |\\n| --------- | --...\"],[\"## HTML\\n\\nNothing\\n\\n## Gallery\\n\\n| name        | type                                      | descriptio...\"],[\"Gradio Demo: blocks_webcam\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\n\\nimport gradio...\"],[\"Gradio Demo: on_listener_live\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"主题 Theming\\n\\nTags: THEMES\\n\\n## 介绍\\n\\nGradio 具有内置的主题引擎，可让您自定义应用的外观和感觉。您可以选择各种主题，或者创建自己的主题。要这样做，请将 `theme=...\"],[\"\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-1.hf.space?__theme=light\\\"\\n\\tfr...\"],[\"\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-3.hf.space?__theme=light\\\"\\n\\tfr...\"],[\"在上面的示例中，我们将 `button_primary_border_radius` 变量设置为`*radius_xl`。此变量将设置为中等半径大小范围的 `xl` 设置。\\n\\n#### 引用其他变量\\n...\"],[\"\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-new-step-2.hf.space?__theme=light\\\"\\n\\tframebo...\"],[\"### 发现主题\\n\\n[主题库](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002ftheme-gallery)显示了所有公开的 gradio 主题。在发布主题之后，\\n它将在几分...\"],[\"his demo shows how you can build an interactive dashboard with gradio. Click on a python library on ...\"],[\"gradio\\n\\n## 4.11.0\\n\\n### Features\\n\\n- [#6842](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6842) [`846d52d...\"],[\"### Fixes\\n\\n- [#6829](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6829) [`50496f9`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"### Fixes\\n\\n- [#6799](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6799) [`c352811`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"### Fixes\\n\\n- [#6525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6525) [`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6680](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6680) [`cfd5700`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6569](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6569) [`4d1cbbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"### Fixes...\"],[\"- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6556](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6556) [`d76bcaa`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.8.0\\n\\n### Features\\n\\n- [#6624](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6624) [`1751f14`](https:...\"],[\"- [#6607](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6607) [`13ace03`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6550](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6550) [`3156598`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.7.1\\n\\n### Features\\n\\n- [#6537](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6537) [`6d3fecfa4`](http...\"],[\"- [#6532](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6532) [`96290d304`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6518](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6518) [`d4e3a5189`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6528](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6528) [`f53b01cbf`](https:\\u002f\\u002fgithub.co...\"],[\"A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Fixes\\n\\n- [#6497](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#6428](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6428) [`ac4ca59c9`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6456](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6456) [`3953a1467`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6441](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6441) [`2f805a7dd`](https:\\u002f\\u002fgithub.co...\"],[\"## 4.3.0\\n\\n### Features\\n\\n- [#6395](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6395) [`8ef48f852`](http...\"],[\"- [#6412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6412) [`649f3ceb6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6379](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6379) [`de998b281`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.2.0\\n\\n### Features\\n\\n- [#6333](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6333) [`42f76aeeb`](http...\"],[\"### Fixes\\n\\n- [#6368](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6368) [`8a3f45c26`](https:\\u002f\\u002fgithub.co...\"],[\"- [#6310](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6310) [`dfdaf1092`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6314](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6314) [`fad92c29d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.1.1\\n\\n### Fixes\\n\\n- [#6288](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6288) [`92278729e`](https:\\u002f...\"],[\"- [#6261](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6261) [`8bbeca0e7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6255](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6255) [`e3ede2ff7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes...\"],[\"- [#6266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6266) [`e32bac894`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6229](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6229) [`5cddd6588`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.0.2\\n\\n### Fixes\\n\\n- [#6191](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6191) [`b555bc09f`](https:\\u002f...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fmedia0.giphy.com\\u002fmedia\\u002fKv1bAN7MX3ya5krkEU\\u002fgiphy.gif\\\"\\u003e\\n\\n**3. Server Side Events**: ...\"],[\"Gradio 4.0 is a new major version, and includes breaking changes from 3.x. Here's a list of all the ...\"],[\"**Other changes related to the `gradio` library**:\\n\\n* Removes the deprecated `status_tracker` parame...\"],[\"For example, if your code looks like this:\\n\\n```py\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n   ...\"],[\"In Gradio 4.0, the `concurrency_count` parameter has been removed. You can still control the number ...\"],[\"To summarize migration:\\n\\n* For events that execute quickly or don't use much CPU or GPU resources, y...\"],[\"```py\\ngr.Image(source=\\\"canvas\\\", tools=\\\"sketch\\\")\\n```\\n\\nNow, you should write:\\n\\n```py\\ngr.ImageEditor(so...\"],[\"- [#6184](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6184) [`86edc0199`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6171](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6171) [`28322422c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6118](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6118) [`88bccfdba`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6069](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6069) [`bf127e124`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5955](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5955) [`825c9cddc`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6073](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6073) [`abff6fb75`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6077](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6077) [`35a227fbf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.0-beta.13\\n\\n### Features\\n\\n- [#5964](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5964) [`5fbda0b...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5937](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5937) [`dcf13d750`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.co...\"],[\"- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5819](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5819) [`5f1cbc436`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes...\"],[\"- [#5840](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5840) [`4e62b8493`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5897](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5897) [`0592c301d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.47.1\\n\\n### Fixes\\n\\n- [#5816](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5816) [`796145e2c`](https:...\"],[\"For more information check the [`FileExplorer` documentation](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002ffileexplorer)....\"],[\"### Fixes\\n\\n- [#5798](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5798) [`a0d3cc45c`](https:\\u002f\\u002fgithub.co...\"],[\"### Fixes\\n\\n- [#5775](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5775) [`e2874bc3c`](https:\\u002f\\u002fgithub.co...\"],[\"### Fixes\\n\\n- [#5735](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5735) [`abb5e9df4`](https:\\u002f\\u002fgithub.co...\"],[\"## 3.45.2\\n\\n### Features\\n\\n- [#5722](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5722) [`dba651904`](htt...\"],[\"- [#5714](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5714) [`a0fc5a296`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5732](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5732) [`3a48490bc`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.1\\n\\n### Fixes\\n\\n- [#5701](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5701) [`ee8eec1e5`](https:...\"],[\"- [#5675](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5675) [`b619e6f6e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5642](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5642) [`21c7225bd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5240) [`da05e59a5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes...\"],[\"- [#5625](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5625) [`9ccc4794a`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5690](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5690) [`6b8c8afd9`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.44.4\\n\\n### Features\\n\\n- [#5514](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5514) [`52f783175`](htt...\"],[\"## 3.44.3\\n\\n### Fixes\\n\\n- [#5562](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5562) [`50d9747d0`](https:...\"],[\"## 3.44.1\\n\\n### Fixes\\n\\n- [#5516](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5516) [`c5fe8eba`](https:\\u002f...\"],[\"- [#5505](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5505) [`9ee20f49`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5510](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5510) [`afcf3c48`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5459](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5459) [`bd2fda77`](https:\\u002f\\u002fgithub.com...\"],[\"## 3.43.0\\n\\n### Features\\n\\n- [#5165](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5165) [`c77f05ab`](http...\"],[\"Thanks [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82)!\\n\\n#### Added the ability to attach event lis...\"],[\"Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n### Features\\n\\n- [#5334](https:\\u002f\\u002fgithub.com\\u002fgradi...\"],[\"### Fixes\\n\\n- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com...\"],[\"## 3.41.2\\n\\n### Features\\n\\n- [#5284](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5284) [`5f25eb68`](http...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"We now have an event `render` on the \\u003cgradio-app\\u003e web component, which is triggered once the embedde...\"],[\"- [#5268](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5268) [`f49028cf`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5280](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5280) [`a2f42e28`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5221](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5221) [`f344592a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes...\"],[\"- [#5256](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5256) [`933db53e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5312](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5312) [`f769cb67`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5276](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5276) [`502f1015`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.40.0\\n\\n### Highlights\\n\\n#### Client.predict will now return the final output for streaming endpoi...\"],[\"demo.queue().launch()\\n```\\n\\nFrom the backend, streamed outputs are served from the `\\u002fstream\\u002f` endpoin...\"],[\"- [#5081](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5081) [`d7f83823`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5076](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5076) [`2745075a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5136) [`eaa1ce14`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes...\"],[\"- [#5062](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5062) [`7d897165`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5061](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5061) [`136adc9c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.39.0\\n\\n### Highlights\\n\\n#### Create Discord Bots from Gradio Apps 🤖 ([#4960](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"But once again, you can deploy ANY `gr.ChatInterface` app exposed on the internet! So don't hesitate...\"],[\"## 3.38\\n\\n### New Features:\\n\\n- Provide a parameter `animate` (`False` by default) in `gr.make_wavefor...\"],[\"### Bug Fixes:\\n\\n- Fixes `cancels` for generators so that if a generator is canceled before it is com...\"],[\"And a corresponding easy-to-use API at `\\u002fchat`:\\n\\n\\u003cimg width=\\\"1164\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fgithub.c...\"],[\"### Bug Fixes:\\n\\n- The `.change()` event is fixed in `Video` and `Image` so that it only fires once b...\"],[\"### Other Changes:\\n\\n- Warning on mobile that if a user leaves the tab, websocket connection may brea...\"],[\"### Other Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3....\"],[\"- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg...\"],[\"- Fixes an HTML sanitization issue in DOMPurify where links in markdown were not opening in a new wi...\"],[\"- Removed uncessessary `type` deprecation warning by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyabou...\"],[\"### Other Changes:\\n\\n- Add `.git-blame-ignore-revs` by [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in [PR 4586](ht...\"],[\"## 3.35.2\\n\\n### New Features:\\n\\nNo changes to highlight.\\n\\n### Bug Fixes:\\n\\n- Fix chatbot streaming by [...\"],[\"demo.launch()\\n```\\n\\n- Min and max value for gr.Number by [@artegoser](https:\\u002f\\u002fgithub.com\\u002fartegoser) a...\"],[\"- Add support for PAUSED state in the JS client by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 4...\"],[\"- `show_label` will not automatically be set to `True` in `gr.BarPlot.update` by [@freddyaboulton](h...\"],[\"### Other Changes:\\n\\n- Change styling of status and toast error components by [@hannahblair](https:\\u002f\\u002f...\"],[\"### Breaking Changes:\\n\\n- The behavior of the `Clear` button has been changed for `Slider`, `Checkbox...\"],[\"### Bug Fixes:\\n\\n- Remove target=\\\"\\\\_blank\\\" override on anchor tags with internal targets by [@hannahb...\"],[\"### Other Changes:\\n\\n- When running on Spaces, handler functions will be transformed by the [PySpaces...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.33.0\\n\\n### New Features:\\n\\n- Introduced `gradio ...\"],[\"### Bug Fixes:\\n\\n- Fix bug where Label change event was triggering itself by [@freddyaboulton](https:...\"],[\"### Other Changes:\\n\\n- Remove flicker of loading bar by adding opacity transition, by [@aliabid94](ht...\"],[\"### Bug Fixes:\\n\\n- Fixed Gallery\\u002fAnnotatedImage components not respecting GRADIO_DEFAULT_DIR variable...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.31.0\\n\\n### New Features:\\n\\n- The reloader comman...\"],[\"### Bug Fixes:\\n\\n- Fix \\\"TypeError: issubclass() arg 1 must be a class\\\" When use Optional[Types] by [@...\"],[\"### Other Changes:\\n\\n- Change `gr.Chatbot()` markdown parsing to frontend using `marked` library and ...\"],[\"### Bug Fixes:\\n\\n- Records username when flagging by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR ...\"],[\"- Returning language agnostic types in the `\\u002finfo` route by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffre...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"## 3.28.2\\n\\n### Bug Fixes\\n\\n- Code component visual updates by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\n- `gr.Hugg...\"],[\"- Fix duplicate play commands in full-screen mode of 'video'. by [@tomchang25](https:\\u002f\\u002fgithub.com\\u002fto...\"],[\"- Fix `gr.ChatBot` to handle image url [tye-singwa](https:\\u002f\\u002fgithub.com\\u002ftye-signwa) in [PR 3953](http...\"],[\"### Documentation Changes:\\n\\n- Make use of `gr` consistent across the docs by [@duerrsimon](https:\\u002f\\u002fg...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.27.0\\n\\n### New Features:\\n\\n###### Annotated...\"],[\"```py\\nwith gr.Blocks() as demo:\\n    gr.Video((\\\"video.mp4\\\", \\\"captions.srt\\\"))\\n```\\n\\n### Bug Fixes:\\n\\n- F...\"],[\"![Recording 2023-04-08 at 17 44 39](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f230748572-90a2...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"```python\\n  import gradio as gr\\n  gr.themes.builder()\\n  ```\\n\\n  ![Theme Builder](https:\\u002f\\u002fuser-images....\"],[\"- Fixed bug where text for altair plots was not legible in dark mode by [@freddyaboulton](https:\\u002f\\u002fgi...\"],[\"- Support using an empty list as `gr.Dataframe` value, by [@space-nuko](https:\\u002f\\u002fgithub.com\\u002fspace-nuk...\"],[\"### Documentation Changes:\\n\\n- Makes some fixes to the Theme Guide related to naming of variables, by...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Mobile responsive iframes in...\"],[\"By [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3428](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"###### `elem_classes`\\n\\nAdd keyword argument `elem_classes` to Components to control class names of c...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.21.0\\n\\n### New Features:\\n\\n###### Theme Sha...\"],[\"by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3428](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    gallery = gr.Gallery([\\\"images\\u002f1.jpg\\\", \\\"...\"],[\"### Documentation Changes:\\n\\n- Added a section on security and access when sharing Gradio apps by [@a...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Prevent in-place updates of ...\"],[\"No changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Bre...\"],[\"By [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3297](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"- Ensure `mirror_webcam` is always respected by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 3245](http...\"],[\"- Fix change event listed twice in image docs by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3318](h...\"],[\"### Documentation Changes:\\n\\n- Added the `types` field to the dependency field in the config by [@fre...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 3205](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f3205)\\n\\n...\"],[\"```python\\ngr.Chatbot([(\\\"Hi, I'm DialoGPT. Try asking me a question.\\\", None)])\\n```\\n\\nBy [@dawoodkhan82...\"],[\"### Documentation Changes:\\n\\n- Sort components in docs by alphabetic order by [@aliabd](https:\\u002f\\u002fgithu...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### New Features:\\n\\n###### Revamped Stop Button for Interfaces 🛑\\n\\nIf your Interface function is a gen...\"],[\"- Fixes URL resolution on Windows by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 3108](https:\\u002f\\u002fg...\"],[\"- Fixed bug where the font color of axis labels and titles for native plots did not respond to dark ...\"],[\"### Documentation Changes:\\n\\n- Added a guide on the 4 kinds of Gradio Interfaces by [@yvrjsharma](htt...\"],[\"###### Run on Kaggle kernels 🧪\\n\\nA share link will automatically be created when running on Kaggle ke...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"By [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 3014](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"- Fixes bug where interpretation event was not configured correctly by [@freddyaboulton](https:\\u002f\\u002fgit...\"],[\"- Fix forwarding for guides after SEO renaming by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3017](...\"],[\"- Adding missing embedded components on docs by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3027](ht...\"],[\"- Fixes bug where tabs selected attribute not working if manually change tab by [@tomchang25](https:...\"],[\"### Documentation Changes:\\n\\n- SEO improvements to guides by[@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.16.2\\n\\n### New Features:\\n\\nNo changes to hi...\"],[\"No changes to highlight.\\n\\n### Full Changelog:\\n\\nNo changes to highlight.\\n\\n### Contributors Shoutout:\\n...\"],[\"```python\\ngr.Dropdown([\\\"angola\\\", \\\"pakistan\\\", \\\"canada\\\"], multiselect=True, value=[\\\"angola\\\"])\\n```\\n\\n\\u003cim...\"],[\"### Documentation Changes:\\n\\n- Added a Guide on using Google Sheets to create a real-time dashboard w...\"],[\"For an example of the api see below:\\n\\n```python\\ngr.LinePlot(stocks,\\n            x=\\\"date\\\",\\n          ...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"```py\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    df = gr.DataFrame(run_query, every=60*60)\\n\\n...\"],[\"You don't need to do anything differently, but when you set `share=True` in `launch()`,\\nyou'll get t...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n- Fixed typo in parameter `visible` in classes...\"],[\"```python\\nimport gradio as gr\\nimport altair as alt\\nfrom vega_datasets import data\\n\\ncars = data.cars(...\"],[\"###### Add Brazilian Portuguese translation\\n\\nAdd Brazilian Portuguese translation (pt-BR.json) by [@...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.12.0\\n\\n### New Features:\\n\\n###### The `Chat...\"],[\"```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Accordion(label=\\\"Open for greet...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"demo.launch()\\n```\\n\\n###### Revamped API documentation page\\n\\nNew API Docs page with in-browser playgro...\"],[\"### Documentation Changes:\\n\\n- Updated documentation for embedding Gradio demos on Spaces as web comp...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.10.0\\n\\n- Add support for `'password'` and ...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.9.1\\n\\n### New Features:\\n\\nNo changes to hig...\"],[\"###### Calling functions by api_name in loaded apps\\n\\nWhen you load an upstream app with `gr.Blocks.l...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.8.2\\n\\n### Bug Fixes:\\n\\n- Ensure gradio apps...\"],[\"![live_demo](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f198357377-633ce460-4e31-47bd-8202-14...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\\n\\ndemo.queue()\\ndem...\"],[\"### Documentation Changes:\\n\\n- Added an example interactive dashboard to the \\\"Tabular & Plots\\\" sectio...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.6\\n\\n### New Features:\\n\\n###### Cancelling R...\"],[\"gr.Interface(iteration,\\n             inputs=gr.Slider(minimum=1, maximum=10, step=1, value=5),\\n     ...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"[@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 2459](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f2459)\\n\\n###...\"],[\"- Speeds up Gallery component by using temporary files instead of base64 representation in the front...\"],[\"- Change \\\"return ValueError\\\" to \\\"raise ValueError\\\" by [@vzakharov](https:\\u002f\\u002fgithub.com\\u002fvzakharov) in ...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4.1\\n\\n### New Features:\\n\\n###### 1. See Pas...\"],[\"### Documentation Changes:\\n\\n1. New Guide: Connecting to a Database 🗄️\\n\\n   A new guide by [@freddyabo...\"],[\"- Create a guide on how to connect an app to a database hosted on the cloud by [@freddyaboulton](htt...\"],[\"- Fixed small typos in the docs [@julien-c](https:\\u002f\\u002fgithub.com\\u002fjulien-c) in [PR 2373](https:\\u002f\\u002fgithub...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4\\n\\n### New Features:\\n\\n###### 1. Gallery C...\"],[\"![color-sketch](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192410500-3c8c3e64-a5fd-4df2-a991-...\"],[\"### Documentation Changes:\\n\\n1. Adding a Playground Tab to the Website by [@aliabd](https:\\u002f\\u002fgithub.co...\"],[\"- Website fixes and refactoring by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 2280](https:\\u002f\\u002fgithub....\"],[\"- release 3.4b3 by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 2328](https:\\u002f\\u002fgithub.com\\u002fgradio-a...\"],[\"### Contributors Shoutout:\\n\\n- [@SkyTNT](https:\\u002f\\u002fgithub.com\\u002fSkyTNT) made their first contribution in ...\"],[\"![187936493-5c90c01d-a6dd-400f-aa42-833a096156a1](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f...\"],[\"- safari fixes by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 2138](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- Fix bugs with gr.update by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 2157](https...\"],[\"### Contributors Shoutout:\\n\\n- [@lamhoangtung](https:\\u002f\\u002fgithub.com\\u002flamhoangtung) made their first cont...\"],[\"###### 3. Component Fixes 🧱\\n\\n- Specify the width and height of an image in its style tag (thanks to ...\"],[\"```python\\nimport gradio as gr\\ndemo = gr.Interface(lambda x:x, gr.Slider(0, 10, randomize=True), \\\"num...\"],[\"- Reset components to original state by setting value to None by [@freddyaboulton](https:\\u002f\\u002fgithub.co...\"],[\"- Fix TimeSeries examples not properly displayed in UI by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodk...\"],[\"### Contributors Shoutout:\\n\\n- [@chrisemezue](https:\\u002f\\u002fgithub.com\\u002fchrisemezue) made their first contri...\"],[\"If your demo code is in a script named `app.py`, instead of running `python app.py` you can now run ...\"],[\"gr.Examples takes two required parameters:\\n\\n- `examples` which takes in a nested list\\n- `inputs` whi...\"],[\"- File component: list multiple files and allow for download #1446 by [@dawoodkhan82](https:\\u002f\\u002fgithub...\"],[\"- Remove usage of deprecated gr.inputs and gr.outputs from website by [@freddyaboulton](https:\\u002f\\u002fgith...\"],[\"### Contributors Shoutout:\\n\\n- [@nhankiet](https:\\u002f\\u002fgithub.com\\u002fnhankiet) made their first contribution...\"],[\"![kitchen_sink](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f168686333-7a6e3096-3e23-4309-abf2-...\"],[\"- Gradio dash fe by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 807](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- Fixing external.py in blocks-dev to reflect the new HF Spaces paths by [@abidlabs](https:\\u002f\\u002fgithub....\"],[\"- Restore Interpretation, Live, Auth, Queueing by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR ...\"],[\"- Blocks analytics by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 947](https:\\u002f\\u002fgithub.com\\u002fgradio...\"],[\"- indentation fix by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 993](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"- Model3D + Plot Components by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 1010](https:\\u002f...\"],[\"- Slackbot web tracker fix by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1043](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"- Update PULL_REQUEST_TEMPLATE.md by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 1068](h...\"],[\"- Website Reload: README in demos docker by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1100](https:...\"],[\"- ONNX guide fixes by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1131](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- Issue #768: Support passing none to resize and crop image by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fda...\"],[\"- use secondary buttons in interface by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1173](https:\\u002f\\u002fgith...\"],[\"- Automatic word-break in highlighted text, combine_adjacent support by [@aliabid94](https:\\u002f\\u002fgithub....\"],[\"- Layout bugs by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1246](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"- update logo by [@gary149](https:\\u002f\\u002fgithub.com\\u002fgary149) in [PR 1266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"- New meta img by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1289](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"### Contributors Shoutout:\\n\\n- [@JefferyChiang](https:\\u002f\\u002fgithub.com\\u002fJefferyChiang) made their first co...\"],[\"@gradio\\u002fstatustracker\\n\\n## 0.4.3\\n\\n### Features\\n\\n- [#6814](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6...\"],[\"\\u003cvideo src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f12937446\\u002f284027169-31188926-fd16-4a1c-8718-998...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"create-svelte\\n\\nEverything you need to build a Svelte project, powered by [`create-svelte`](https:\\u002f\\u002fg...\"],[\"imple image classification in Pytorch with Gradio's Image input and Label output....\"],[\"`@gradio\\u002fatoms`\\n\\n```html\\n\\u003cscript lang=\\\"ts\\\"\\u003e\\n\\timport { Block, BlockTitle, BlockLabel, IconButton, Emp...\"],[\"his text generation demo works like autocomplete. There's only one textbox and it's used for both th...\"],[\"Gradio Demo: sound_alert\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo r...\"],[\"Gradio Demo: theme_extended_step_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"实时语音识别\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fstreaming-asr-paused, https:\\u002f\\u002fhugging...\"],[\"确保您至少安装了其中之一，以便您可以跟随本教程操作。如果您尚未安装 `ffmpeg`，请在[系统上下载并安装](https:\\u002f\\u002fwww.ffmpeg.org\\u002fdownload.html)，以便从麦克风...\"],[\"让我们看看它的效果吧！（录制一段短音频并点击提交，或[在新标签页打开](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002ffull-context-asr)）：\\n\\n\\u003cifr...\"],[\"请注意，我们还进行了另一个更改，即我们设置了 `live=True`。这使得 Gradio 接口保持持续运行，因此它可以自动转录音频，而无需用户反复点击提交按钮。\\n\\n让我们看看它的效果（在下方尝试或[...\"],[\"尝试下面的演示，查看差异（或[在新标签页中打开](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fstreaming-asr-paused)）！\\n\\n\\u003ciframe src...\"],[\"model = Model(model_file_path)\\nmodel.enableExternalScorer(lm_file_path)\\nmodel.setScorerAlphaBeta(lm_...\"],[\"Gradio Demo: sine_curve\\n\\n\\n```\\n!pip install -q gradio plotly\\n```\\n\\n\\n```\\nimport math\\nimport gradio as g...\"],[\"his text generation demo takes in input text and returns generated text. It uses the Transformers li...\"],[\"Gradio Demo: color_generator\\n\\n\\n```\\n!pip install -q gradio opencv-python numpy\\n```\\n\\n\\n```\\nimport gradi...\"],[\"@gradio\\u002fclient\\n\\n## 0.9.3\\n\\n### Features\\n\\n- [#6814](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6814) [`...\"],[\"## 0.8.1\\n\\n### Fixes\\n\\n- [#6383](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6383) [`324867f63`](https:\\u002f...\"],[\"## 0.7.0-beta.1\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.5.1\\n\\n### Fixes\\n\\n- [#5816](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5816) [`796145e2c`](https:\\u002f...\"],[\"## 0.4.2\\n\\n### Features\\n\\n- [#5124](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5124) [`6e56a0d9b`](http...\"],[\"## 0.2.1\\n\\n### Features\\n\\n- [#5173](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5173) [`730f0c1d`](https...\"],[\"- [#4315](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4315) [`b525b122`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"@gradio\\u002flabel\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.1.0\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Case Study: A Component to Display PDFs\\n\\nLet's work through an example of building a custom gradio c...\"],[\"From within the `frontend` directory, run `npm install @gradio\\u002fclient @gradio\\u002fupload @gradio\\u002ficons @...\"],[\"After launching the dev server, you should see a link printed to your console that says `Frontend Se...\"],[\"let _value = value;\\n    let old_value = _value;\\n```\\n\\n\\nTip: The `gradio`` object passed in here conta...\"],[\"Create a new file called `PdfUploadText.svelte` and copy the following code.\\nIts creating a new div ...\"],[\"With that out of the way, let's start off by importing `pdfjs` and loading the code of the pdf worke...\"],[\"Tip: The `$:` syntax in svelte is how you declare statements to be reactive. Whenever any of the inp...\"],[\"Now we will run these functions whenever the `Upload` component uploads a file and whenever the `Mod...\"],[\"Now we will add them underneath the canvas in a separate `\\u003cdiv\\u003e`\\n\\n```ts\\n    ...\\n\\n    \\u003cModifyUpload i...\"],[\"async function get_doc(url: string) {\\n\\t\\tconst loadingTask = pdfjsLib.getDocument(url);\\n\\t\\tpdfDoc = aw...\"],[\"When all is said an done, your component's backend code should look like this:\\n\\n```python\\nfrom __fut...\"],[\"def example_inputs(self):\\n        return \\\"https:\\u002f\\u002fgradio-builds.s3.amazonaws.com\\u002fassets\\u002fpdf-guide\\u002ffw...\"],[\"Tip: You may need to add the following lines to the `Dockerfile` of your HuggingFace Space.\\n\\n```Dock...\"],[\"Gradio Demo: stream_audio_out\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"with gr.Column():\\n            stream_as_bytes_btn = gr.Button(\\\"Stream as Bytes\\\")\\n            stream_...\"],[\"gradio-ui\\n\\nThis folder contains all of the Gradio UI and component source code.\\n\\n- [set up](#setup)\\n...\"],[\"```bash\\npnpm format:write\\n```\\n\\n### type checking\\n\\nWe use [TypeScript](https:\\u002f\\u002fwww.typescriptlang.org...\"],[\"Gradio Demo: gallery_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"Gradio Demo: blocks_scroll\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndemo = gr.B...\"],[\"website\\n\\n## 0.20.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fcode@0.3.3\\n\\n## 0.20.2\\n...\"],[\"## 0.17.0\\n\\n### Features\\n\\n- [#6533](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6533) [`e2810fcfc`](htt...\"],[\"## 0.13.0\\n\\n### Features\\n\\n- [#6351](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6351) [`294414d9f`](htt...\"],[\"## 0.11.0-beta.1\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6...\"],[\"## 0.11.0-beta.0\\n\\n### Features\\n\\n- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af3...\"],[\"## 0.9.0\\n\\n### Features\\n\\n- [#5386](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5386) [`0312c990f`](http...\"],[\"## 0.6.0\\n\\n### Features\\n\\n- [#5565](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5565) [`f0514fc49`](http...\"],[\"## 0.2.2\\n\\n### Features\\n\\n- [#5284](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5284) [`5f25eb68`](https...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5298](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"### Fixes\\n\\n- [#5007](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5007) [`71c90394`](https:\\u002f\\u002fgithub.com...\"],[\"Gradio Demo: live_dashboard\\n### This demo shows how you can build a live interactive dashboard with ...\"],[\"Image Classification in TensorFlow and Keras\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs...\"],[\"In the case of our pretrained model, it will look like this:\\n\\n```python\\nimport requests\\n\\n# Download ...\"],[\"gr.Interface(fn=classify_image,\\n             inputs=gr.Image(shape=(224, 224)),\\n             outputs...\"],[\"Gradio Demo: queue_full_e2e_test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ...\"],[\"Gradio Demo: blocks_simple_squares\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo...\"],[\"Gradio Demo: blocks_static\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.Bl...\"],[\"Gradio Demo: main_note\\n\\n\\n```\\n!pip install -q gradio scipy numpy matplotlib\\n```\\n\\n\\n```\\n# Downloading f...\"],[\"@gradio\\u002fatoms\\n\\n## 0.4.1\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"\\u003cvideo src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f12937446\\u002f284027169-31188926-fd16-4a1c-8718-998...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5864](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5864) [`e70805d54`](http...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5216](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"demo for predicting the depth of an image and generating a 3D model of it....\"],[\"Gradio Demo: video_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the de...\"],[\"TensorFlow 和 Keras 中的图像分类\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fkeras-image-classifier\\n标签：VIS...\"],[\"让我们来详细了解一下。该函数接受一个参数：\\n\\n- `inp`：输入图像的 `numpy` 数组\\n\\n然后，函数添加一个批次维度，通过模型进行处理，并返回：\\n\\n- `confidences`：预测结果，以...\"],[\"his demo identifies if two speakers are the same person using Gradio's Audio and HTML components....\"],[\"Gradio Demo: image_classifier_interface_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading f...\"],[\"@gradio\\u002fimage\\n\\n## 0.5.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.4.2\\n\\n### Fixes\\n\\n- [#6635](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6635) [`b639e04`](https:\\u002f\\u002fg...\"],[\"```py\\n\\ndef fn(im):\\n    im[\\\"composite\\\"] # the full canvas\\n    im[\\\"background\\\"] # the background image...\"],[\"## 0.3.4\\n\\n### Features\\n\\n- [#6363](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6363) [`4d3aad33a`](http...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.9\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](http...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: latex\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks() as ...\"],[\"Gradio Demo: video_identity\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"@gradio\\u002ffallback\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.0-beta.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.2.0-beta.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`0b4fd5b6d`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highli...\"],[\"🚀 Creating Discord Bots from Gradio Apps 🚀\\n\\nTags: NLP, TEXT, CHAT\\n\\nWe're excited to announce that Gr...\"],[\"```python\\nimport gradio as gr\\n\\ndef slow_echo(message, history):\\n    return message\\n\\ndemo = gr.ChatIn...\"],[\"Visit the space of your discord bot. You should see \\\"Add this bot to your server by clicking this li...\"],[\"## 🦜 Additional LLMs 🦜\\n\\nIn addition to Meta's 70 billion Llama 2 model, we have prepared template sp...\"],[\"Gradio Demo: blocks_essay\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ncountries_cit...\"],[\"Gradio Demo: generate_tone\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport numpy as np\\nimport gr...\"],[\"`@gradio\\u002fform`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { Form } from \\\"@gradio\\u002fform\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nForm\\n```javasc...\"],[\"Gradio Demo: hello_world_3\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Components: The Key Concepts\\n\\nIn this section, we discuss a few important concepts when it co...\"],[\"### What you need to remember\\n\\n* Gradio will use the interactive version (if available) of a compone...\"],[\"demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \\\"image\\\")\\ndemo.launch()\\n```\\n\\nThis will create ...\"],[\"* As a component author, **YOU** control the format of the data displayed in the frontend as well as...\"],[\"Real Time Speech Recognition\\n\\nTags: ASR, SPEECH, STREAMING\\n\\n## Introduction\\n\\nAutomatic speech recogn...\"],[\"Here's how to build a real time speech recognition (ASR) app:\\n\\n1. [Set up the Transformers ASR Model...\"],[\"1. Set `streaming=True` in the `Audio` component\\n2. Set `live=True` in the `Interface`\\n3. Add a `sta...\"],[\"Gradio Demo: unified_demo_text_generation\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\n...\"],[\"Gradio Demo: calculator_blocks_cached\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\n...\"],[\"Gradio Demo: image-simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"`@gradio\\u002fimageeditor`...\"],[\"Gradio Demo: chatbot_multimodal\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"Gradio Demo: dataframe_block-ui-test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwi...\"],[\"Gradio Demo: on_listener_test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"The Backend 🐍\\n\\nThis guide will cover everything you need to know to implement your custom component'...\"],[\"```python\\n    @abstractmethod\\n    def preprocess(self, x: Any) -\\u003e Any:\\n        \\\"\\\"\\\"\\n        Convert f...\"],[\"```python\\n@abstractmethod\\ndef example_inputs(self) -\\u003e Any:\\n    \\\"\\\"\\\"\\n    The example inputs for this c...\"],[\"This is best explained with an example. Let's look at the core `Video` component, which stores the v...\"],[\"If your component expects uploaded files as input, or returns saved files to the frontend, you **MUS...\"],[\"Gradio Demo: blocks_flag\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport numpy as np\\nimport grad...\"],[\"Gradio Demo: concurrency_without_queue\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\ni...\"],[\"Gradio Demo: dashboard\\n### This demo shows how you can build an interactive dashboard with gradio. C...\"],[\"def create_issue_plot(libraries, issue_choices):\\n    if \\\"Issue\\\" not in issue_choices:\\n        return...\"],[\"@gradio\\u002fslider\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"@gradio\\u002fmodel3d\\n\\n## 0.4.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6240) [`dd901c1b0`](http...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5373](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5373) [`79d8f9d8`](https...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"mport { Meta } from \\\"@storybook\\u002fblocks\\\";\\n\\n\\u003cMeta title=\\\"Introduction\\\" \\u002f\\u003e\\n\\n\\u003cstyle\\u003e\\n\\t{`\\n    img {\\n     ...\"],[\"simple demo showcasing the upload button used with its `upload` event trigger....\"],[\"`@gradio\\u002fhighlightedtext`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseStaticHighlightedText, BaseInteractiveH...\"],[\"Gradio Demo: image_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwit...\"],[\"Gradio Demo: reverse_audio\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: blocks_page_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef prin...\"],[\"Gradio & LLM Agents 🤝\\n\\nLarge Language Models (LLMs) are very impressive but they can be made even mo...\"],[\"## gradio_tools - An end-to-end example\\n\\nTo get started with `gradio_tools`, all you need to do is i...\"],[\"You'll note that we are using some pre-built tools that come with `gradio_tools`. Please see this [d...\"],[\"@abstractmethod\\n    def create_job(self, query: str) -\\u003e Job:\\n        pass\\n\\n    @abstractmethod\\n    d...\"],[\"And that's it!\\n\\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We ...\"],[\"def _block_output(self, gr) -\\u003e \\\"gr.components.Component\\\":\\n        return gr.Image()\\n```\\n\\nSome notes ...\"],[\"his simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear...\"],[\"`@gradio\\u002fvideo`\\n\\n```javascript\\n\\u003cscript\\u003e\\n\\timport { BaseInteractiveVideo, BaseStaticVideo, BasePlayer ...\"],[\"utomatic speech recognition English. Record from your microphone and the app will transcribe the aud...\"],[\"Gradio Demo: longest_word\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef longest_...\"],[\"`@gradio\\u002fcolorpicker`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseColorPicker, BaseExample } from \\\"@gradio\\u002fco...\"],[\"Gradio Demo: annotatedimage_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nim...\"],[\"Running a Gradio App on your Web Server with Nginx\\n\\nTags: DEPLOYMENT, WEB SERVER, NGINX\\n\\n## Introduc...\"],[\"3. Paste the following into your file editor:\\n\\n```bash\\nserver {\\n    listen 80;\\n    server_name examp...\"],[\"@gradio\\u002fupload\\n\\n## 0.5.6\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`732...\"],[\"A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.4.2\\n\\n### Fixes\\n\\n- [#6441](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"```python\\nimport gradio as gr\\nfrom pydub import AudioSegment\\n\\ndef stream_audio(audio_file):\\n    audi...\"],[\"Getting Started with the Gradio Python client\\n\\nTags: CLIENT, API, SPACES\\n\\nThe Gradio Python client m...\"],[\"## Connecting to a Hugging Face Space\\n\\n```python\\nfrom gradio_client import Client\\n\\nclient = Client(\\\"...\"],[\"## Connecting a general Gradio app\\n\\nIf your app is running somewhere else, just provide the full URL...\"],[\"```python\\nfrom gradio_client import Client\\n\\nclient = Client(\\\"gradio\\u002fcalculator\\\")\\nclient.predict(4, \\\"...\"],[\"client = Client(space=\\\"abidlabs\\u002fen2fr\\\")\\n\\njob = client.submit(\\\"Hello\\\", api_name=\\\"\\u002fpredict\\\", result_ca...\"],[\"```py\\nfrom gradio_client import Client\\n\\nclient = Client(src=\\\"gradio\\u002fcount_generator\\\")\\njob = client.s...\"],[\"Frequently Asked Questions\\n\\n## What do I need to install before using Custom Components?\\nBefore usin...\"],[\"## Why is it important to use `FileData` for components dealing with file uploads?\\n\\nUtilizing `FileD...\"],[\"Create Your Own Friends with a GAN\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fNimaBoscarino\\u002fcryp...\"],[\"### Prerequisites\\n\\nMake sure you have the `gradio` Python package already [installed](\\u002fgetting_start...\"],[\"```python\\nfrom torch import nn\\n\\nclass Generator(nn.Module):\\n    # Refer to the link below for explan...\"],[\"## Step 2 — Defining a `predict` function\\n\\nThe `predict` function is the key to making Gradio work! ...\"],[\"With `gr.Interface()`, we can define all of that with a single function call:\\n\\n```python\\nimport grad...\"],[\"The `examples` parameter takes a list of lists, where each item in the sublists is ordered in the sa...\"],[\"def predict(seed, num_punks):\\n    torch.manual_seed(seed)\\n    z = torch.randn(num_punks, 100, 1, 1)\\n...\"],[\"Gradio Demo: blocks_component_shortcut\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n...\"],[\"Gradio Demo: progress_component\\n\\n\\n```\\n!pip install -q gradio tqdm\\n```\\n\\n\\n```\\nimport gradio as gr\\nimpo...\"],[\"Gradio Demo: calculator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo re...\"],[\"Gradio Demo: cancel_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport time\\nimport gradio as gr\\n\\n...\"],[\"Gradio Demo: live_with_vars\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.I...\"],[\"反应式界面 (Reactive Interfaces)\\n\\n本指南介绍了如何使 Gradio 界面自动刷新或连续流式传输数据。\\n\\n## 实时界面 (Live Interfaces)\\n\\n您可以通过在界面中...\"],[\"Gradio Demo: gpt2_xl\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ntitle = \\\"gpt2-xl\\\"\\n...\"],[\"Gradio Demo: uploadbutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef...\"],[\"Vision Transformers 图像分类\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fvision-transformer\\n标签：VISION, ...\"],[\"gr.Interface.load(\\n             \\\"huggingface\\u002fgoogle\\u002fvit-base-patch16-224\\\",\\n             examples=[\\\"a...\"],[\"Gradio Demo: blocks_speech_text_sentiment\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\n...\"],[\"Gradio Demo: spectogram\\n\\n\\n```\\n!pip install -q gradio scipy numpy matplotlib\\n```\\n\\n\\n```\\nimport matplot...\"],[\"Gradio Demo: clustering\\n### This demo built with Blocks generates 9 plots based on the input.\\n      ...\"],[\"def get_noise(n_clusters):\\n    np.random.seed(SEED)\\n    X, labels = np.random.rand(N_SAMPLES, 2), np...\"],[\"def get_dbscan(X, labels, n_clusters, **kwargs):\\n    model = DBSCAN(eps=0.3)\\n    model.set_params(**...\"],[\"def plot_clusters(ax, X, labels):\\n    set_clusters = set(labels)\\n    set_clusters.discard(-1)  # -1 ...\"],[\"with gr.Blocks(title=title) as demo:\\n    gr.HTML(f\\\"\\u003cb\\u003e{title}\\u003c\\u002fb\\u003e\\\")\\n    gr.Markdown(description)\\n\\n  ...\"],[\"@gradio\\u002fhtml\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"`@gradio\\u002fgallery`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseGallery } from \\\"@gradio\\u002fgallery\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nB...\"],[\"命名实体识别 （Named-Entity Recognition）\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002frajistics\\u002fbiobert_ner_demo，htt...\"],[\"Blocks and Event Listeners\\n\\nWe briefly descirbed the Blocks class in the [Quickstart](\\u002fmain\\u002fguides\\u002fq...\"],[\"```python\\noutput = gr.Textbox(label=\\\"Output\\\", interactive=True)\\n```\\n\\n_Note_: What happens if a Gradi...\"],[\"Let's see an example of each:\\n$code_calculator_list_and_dict\\n\\nBoth `add()` and `sub()` take `a` and ...\"],[\"```python\\nwith gr.Blocks() as demo:\\n    food_box = gr.Number(value=10, label=\\\"Food Count\\\")\\n    statu...\"],[\"You can also set `cache_examples=True` similar to `gr.Interface`, in which case two additional argum...\"],[\"## Running Events Continuously\\n\\nYou can run events on a fixed schedule using the `every` parameter o...\"],[\"$code_on_listener_basic\\n$demo_on_listener_basic\\n\\nYou can use decorator syntax as well:\\n\\n$code_on_lis...\"],[\"Gradio Demo: no_input\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport random\\n\\nsen...\"],[\"Gradio Demo: hello_world_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Demo: blocks_plug\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef change_ta...\"],[\"PyTorch 图像分类\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fpytorch-image-classifier, https...\"],[\"```python\\nimport requests\\nfrom PIL import Image\\nfrom torchvision import transforms\\n\\n# 下载ImageNet的可读标...\"],[\"Gradio Demo: video_subtitle\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"@gradio\\u002fstate\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"分享您的应用\\n\\n如何分享您的 Gradio 应用：\\n\\n1. [使用 share 参数分享演示](#sharing-demos)\\n2. [在 HF Spaces 上托管](#hosting-on-hf-...\"],[\"有两种方法可以嵌入您的 Gradio 演示。您可以在 Hugging Face Space 页面的“嵌入此空间”下拉选项中直接找到这两个选项的快速链接：\\n\\n![嵌入此空间下拉选项](\\u002fassets\\u002fg...\"],[\"以下是使用这些属性创建一个懒加载且初始高度为 0px 的 Gradio 应用的示例。\\n\\n```html\\n&lt;gradio-app space=\\\"gradio\\u002fEchocardiogram-Segm...\"],[\"## 直接访问网络请求\\n\\n当用户向您的应用程序进行预测时，您可能需要底层的网络请求，以获取请求标头（例如用于高级身份验证）、记录客户端的 IP 地址或其他原因。Gradio 支持与 FastAPI 类...\"],[\"Gradio Demo: english_translator\\n\\n\\n```\\n!pip install -q gradio transformers torch\\n```\\n\\n\\n```\\nimport gra...\"],[\"@gradio\\u002frow\\n\\n## 0.1.1\\n\\n### Features\\n\\n- [#6399](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6399) [`053...\"],[\"Gradio Demo: image_editor\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"分块状态 (State in Blocks)\\n\\n我们已经介绍了[接口状态](https:\\u002f\\u002fgradio.app\\u002finterface-state)，这篇指南将介绍分块状态，它的工作原理大致相同。\\n\\n#...\"],[\"ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full c...\"],[\"Gradio Demo: matrix_transpose\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\n\\nimport gra...\"],[\"Gradio Demo: blocks_textbox_max_lines\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\n...\"],[\"ecreate the viral AnimeGAN image transformation demo....\"],[\"Gradio Demo: calculator_list_and_dict\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nw...\"],[\"区块和事件监听器 (Blocks and Event Listeners)\\n\\n我们在[快速入门](https:\\u002f\\u002fgradio.app\\u002fquickstart\\u002f#blocks-more-flexibil...\"],[\"使用哪种语法是个人偏好！对于具有许多输入组件的函数，选项 2 可能更容易管理。\\n\\n$demo_calculator_list_and_dict\\n\\n## 函数返回列表与字典 (Function Retu...\"],[\"您可以使用事件监听器的 `every` 参数按固定计划运行事件。这将在客户端连接打开的情况下，每隔一定秒数运行一次事件。如果连接关闭，事件将在下一次迭代后停止运行。\\n请注意，这不考虑事件本身的运行时间...\"],[\"Gradio Demo: checkbox_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith g...\"],[\"@gradio\\u002ficons\\n\\n## 0.3.2\\n\\n### Features\\n\\n- [#6399](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6399) [`0...\"],[\"```py\\n\\ndef fn(im):\\n    im[\\\"composite\\\"] # the full canvas\\n    im[\\\"background\\\"] # the background image...\"],[\"## 0.2.0-beta.2\\n\\n### Features\\n\\n- [#5966](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5966) [`9cad2127b...\"],[\"Gradio Demo: audio_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwit...\"],[\"Gradio Demo: chatinterface_random_response\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport random\\nimp...\"],[\"Gradio Demo: colorpicker_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwit...\"],[\"Gradio Demo: blocks_update\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Bloc...\"],[\"Getting Started with the Gradio JavaScript client\\n\\nTags: CLIENT, API, SPACES\\n\\nThe Gradio JavaScript ...\"],[\"Start by connecting instantiating a `client` instance and connecting it to a Gradio app that is runn...\"],[\"**Note:** if the original Space uses GPUs, your private Space will as well, and your Hugging Face ac...\"],[\"We should also provide the `api_name='\\u002fpredict'` argument to the `predict()` method. Although this i...\"],[\"```js\\nimport { client } from \\\"@gradio\\u002fclient\\\";\\n\\nfunction log_result(payload) {\\n\\tconst {\\n\\t\\tdata: [tra...\"],[\"## Generator Endpoints\\n\\nSome Gradio API endpoints do not return a single value, rather they return a...\"],[\"更多示例 (More on Examples)\\n\\n本指南介绍了有关示例的更多内容：从目录中加载示例，提供部分示例和缓存。如果你对示例还不熟悉，请查看 [关键特性](..\\u002fkey-features\\u002f#e...\"],[\"从 Supabase 数据创建仪表盘\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Supabase](https:\\u002f\\u002fsupabase.com\\u002f) 是一个基于云的开源后端，提...\"],[\"# 将数据写入表中\\ndata = client.table('Product').insert(main_list).execute()\\n```\\n\\n返回 Supabase 仪表板并刷新页面，您将看到 ...\"],[\"Gradio Demo: model3d_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"@gradio\\u002fmarkdown\\n\\n## 0.6.0\\n\\n### Features\\n\\n- [#6842](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6842) ...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6071](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6071) [`f08da1a6f...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n  - @gradio\\u002fatoms@0....\"],[\"## 0.1.2\\n\\n### Fixes\\n\\n- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002f...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"Gradio Demo: video_identity_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef video...\"],[\"`gradio_client`: Use a Gradio app as an API -- in 3 lines of Python\\n\\nThis directory contains the sou...\"],[\"```python\\nfrom gradio_client import Client\\n\\nclient = Client(\\\"abidlabs\\u002fmy-private-space\\\", hf_token=\\\"....\"],[\"```\\nClient.predict() Usage Info\\n---------------------------\\nNamed API endpoints: 1\\n\\n - predict(input...\"],[\"\\u003e\\u003e \\\"My thought I have nobody by a beauty and will as you poured. Mr. Rochester is serve in that so d...\"],[\"Gradio Demo: image_classification\\n### Simple image classification in Pytorch with Gradio's Image inp...\"],[\"Gradio Demo: label_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"自定义的 JS 和 CSS\\n\\n本指南介绍了如何更灵活地为 Blocks 添加样式，并添加 JavaScript 代码到事件监听器中。\\n\\n**警告**：在自定义的 JS 和 CSS 中使用查询选择器不能...\"],[\"Gradio Demo: bokeh_plot\\n\\n\\n```\\n!pip install -q gradio bokeh\\u003e=3.0 xyzservices\\n```\\n\\n\\n```\\nimport gradio ...\"],[\"SPECIES = sorted(data.species.unique())\\n        MARKERS = [\\\"hex\\\", \\\"circle_x\\\", \\\"triangle\\\"]\\n\\n        p...\"],[\"# JavaScript Client Library\\n\\nA javascript (and typescript) client to call Gradio APIs.\\n\\n## Installat...\"],[\"**Additional context**\\n\\nApplications hosted on Hugging Face spaces can be in a number of different s...\"],[\"`predict` accepts two parameters, `endpoint` and `payload`. It returns a promise that resolves to th...\"],[\"##### `on`\\n\\nThe `on` method allows you to subscribe to events related to the submitted API request. ...\"],[\"const submission = app.submit(\\\"\\u002fpredict\\\", payload).on(\\\"data\\\", handle_data);\\n\\n\\u002f\\u002f later\\nsubmission.off...\"],[\"```ts\\nimport { client } from \\\"@gradio\\u002fclient\\\";\\n\\nconst app = await client(\\\"user\\u002fspace-name\\\");\\nconsole...\"],[\"```ts\\nimport { duplicate } from \\\"@gradio\\u002fclient\\\";\\n\\nconst app = await duplicate(\\\"user\\u002fspace-name\\\", {\\n...\"],[\"iles in this directory are used in:\\n\\n- tests for the gradio library\\n- example inputs in the view API...\"],[\"How to Use the 3D Model Component\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fModel3D, htt...\"],[\"if __name__ == \\\"__main__\\\":\\n    demo.launch()\\n```\\n\\nLet's break down the code above:\\n\\n`load_mesh`: Thi...\"],[\"@gradio\\u002fdataset\\n\\n## 0.1.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.0.4\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.2\\n\\n## 0.0.3\\n\\n### Patch...\"],[\"Gradio Demo: blocks_kinematics\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport pandas as pd\\nimport nu...\"],[\"Gradio Demo: blocks_neural_instrument_coding\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading f...\"],[\"io3 = gr.Interface(\\n    lambda x, y, z: os.path.join(os.path.abspath(''),\\\"trombone.wav\\\"),\\n    [\\n    ...\"],[\"gr.Button(\\\"Generate a new saxophone recording\\\")\\n\\n    m(\\n        \\\"\\\"\\\"\\n    ### Inputs to the model\\n    ...\"],[\"Gradio Demo: image_classifier\\n\\n\\n```\\n!pip install -q gradio numpy tensorflow\\n```\\n\\n\\n```\\n# Downloading ...\"],[\"Using Gradio Blocks Like Functions\\n\\nTags: TRANSLATION, HUB, SPACES\\n\\n**Prerequisite**: This Guide bui...\"],[\"The following code snippet and demo shows how to use `Blocks.load`.\\n\\nNote that the variable `english...\"],[\"Gradio Demo: animeganv2\\n### Recreate the viral AnimeGAN image transformation demo.\\n        \\n\\n\\n```\\n!p...\"],[\"demo = gr.Interface(\\n    fn=inference, \\n    inputs=[gr.Image(type=\\\"pil\\\"),gr.Radio(['version 1 (🔺 sty...\"],[\"Gradio Demo: image_segmentation\\n### Simple image segmentation using gradio's AnnotatedImage componen...\"],[\"section_btn.click(section, [img_input, num_boxes, num_segments], img_output)\\n\\n    def select_section...\"],[\"Gradio Demo: stable-diffusion\\n### Note: This is a simplified version of the code needed to create th...\"],[\"advanced_button = gr.Button(\\\"Advanced options\\\", elem_id=\\\"advanced-btn\\\")\\n\\n        with gr.Row(elem_id...\"],[\"`@gradio\\u002fstatustracker`\\n\\n```html\\n\\u003cscript\\u003e\\n    import {StatusTracker, Toast, Loader} from `@gradio\\u002fst...\"],[\"@gradio\\u002fsimpletextbox\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgith...\"],[\"## 0.1.0\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"Gradio Demo: chatbot_consecutive\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ...\"],[\"Gradio Demo: file_explorer\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nfrom pathlib ...\"],[\"运行后台任务\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002ffreddyaboulton\\u002fgradio-google-forms\\nTags: TASKS...\"],[\"让我们还写一个函数，在 gradio 应用程序加载时加载最新的评论 :\\n\\n```python\\ndef load_data():\\n    db = sqlite3.connect(DB_FILE)\\n  ...\"],[\"备份数据的函数如下 :\\n\\n```python\\nfrom apscheduler.schedulers.background import BackgroundScheduler\\n\\ndef backup...\"],[\"Gradio Demo: file_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Bl...\"],[\"Gradio Demo: blocks_group\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(nam...\"],[\"gr.Markdown(\\\"### container=False removes label, padding, and block border, placing elements 'directl...\"],[\"通过自动重载实现更快的开发\\n\\n**先决条件**：本指南要求您了解块的知识。请确保[先阅读块指南](https:\\u002f\\u002fgradio.app\\u002fquickstart\\u002f#blocks-more-flexibil...\"],[\"inp.change(fn=lambda x: f\\\"欢迎，{x}！\\\",\\n               inputs=inp,\\n               outputs=out)\\n\\nif __nam...\"],[\"Notebook Magic 现在是作者构建 Gradio 演示的首选方式。无论您如何编写 Python 代码，我们都希望这两种方法都能为您提供更好的 Gradio 开发体验。\\n\\n---\\n\\n## 下一...\"],[\"Gradio Demo: sentiment_analysis\\n### This sentiment analaysis demo takes in input text and returns it...\"],[\"Gradio Demo: image_mod\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo rep...\"],[\"Gradio Demo: hello_blocks\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(nam...\"],[\"Installing Gradio in a Virtual Environment\\n\\nTags: INSTALLATION\\n\\nIn this guide, we will describe step...\"],[\"3. **Activate the Virtual Environment**:\\n   To activate the virtual environment, run:\\n\\n   ```bash\\n  ...\"],[\"```bash\\n   pip install gradio\\n   ```\\n\\n5. **Verification**:\\n   To verify the installation, run `pytho...\"],[\"Gradio Demo: score_tracker\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nscores = []\\n...\"],[\"`@gradio\\u002ffile`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseFile, BaseFileUpload, FilePreview, BaseExample } from...\"],[\"Gradio Demo: scatterplot_component\\n\\n\\n```\\n!pip install -q gradio vega_datasets\\n```\\n\\n\\n```\\nimport gradi...\"],[\"Gradio Demo: xgboost-income-prediction-with-explainability\\n### This demo takes in 12 inputs from the...\"],[\"unique_class = sorted(X_train[\\\"workclass\\\"].unique())\\nunique_education = sorted(X_train[\\\"education\\\"]....\"],[\"with gr.Blocks() as demo:\\n    gr.Markdown(\\\"\\\"\\\"\\n    **Income Classification with XGBoost 💰**:  This de...\"],[\")\\n            capital_gain = gr.Slider(\\n                label=\\\"Capital Gain\\\",\\n                minimu...\"],[\"demo.launch()\\n\\n```...\"],[\"his demo built with Blocks generates 9 plots based on the input....\"],[\"Gradio & LLM Agents 🤝\\n\\n非常强大的大型语言模型（LLM），如果我们能赋予它们完成专门任务的技能，它们将变得更加强大。\\n\\n[gradio_tools](https:\\u002f\\u002fgithub...\"],[\"agent = initialize_agent(tools, llm, memory=memory, agent=\\\"conversational-react-description\\\", verbos...\"],[\"就是这样！\\n\\n一旦您创建了自己的工具，请在`gradio_tools`存储库上发起拉取请求！我们欢迎所有贡献。\\n\\n## 示例工具 - 稳定扩散\\n\\n以下是作为示例的稳定扩散工具代码：\\n\\nfrom gra...\"],[\"Gradio Demo: barplot_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport pa...\"],[\"Gradio Demo: blocks_hello\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef welcome(n...\"],[\"his is a fake GAN that shows how to create a text-to-image interface for image generation. Check out...\"],[\"Gradio Demo: loginbutton_component\\n\\n\\n```\\n!pip install -q gradio gradio[oauth]\\n```\\n\\n\\n```\\nimport gradi...\"],[\"@gradio\\u002fgallery\\n\\n## 0.4.14\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.3\\n\\n### Fixes\\n\\n- [#6288](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6288) [`92278729e`](https:\\u002f...\"],[\"## 0.4.0-beta.9\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.5.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b67115e8e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5554](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5554) [`75ddeb390`](http...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"在 Web 服务器上使用 Nginx 运行 Gradio 应用\\n\\n标签：部署，Web 服务器，Nginx\\n\\n## 介绍\\n\\nGradio 是一个 Python 库，允许您快速创建可定制的 Web 应用程...\"],[\"## 重新启动 Nginx\\n\\n1. 如果您在 tmux 会话中，请通过键入 CTRL + B（或 CMD + B），然后按下 \\\"D\\\" 键来退出。\\n\\n2. 最后，通过运行 `sudo systemctl...\"],[\"@gradio\\u002fhighlightedtext\\n\\n## 0.4.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgi...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.4.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"Gradio Demo: stream_frames\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport numpy ...\"],[\"@gradio\\u002futils\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f...\"],[\"从 Google Sheets 创建实时仪表盘\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n[Google Sheets](https:\\u002f\\u002fwww.google.com\\u002fshee...\"],[\"到此为止！您现在拥有一个仪表盘，每 5 秒刷新一次，从 Google Sheets 中获取数据。\\n\\n## 私有 Google Sheets\\n\\n对于私有 Google Sheets，流程需要更多的工作量...\"],[\"```python\\nimport gspreadimport pandas as pd\\n# 与 Google 进行身份验证并获取表格URL = 'https:\\u002f\\u002fdocs.google.com\\u002fspr...\"],[\"@gradio\\u002fradio\\n\\n## 0.3.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5384](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5384) [`ddc02268`](https...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: markdown_example\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ncss = (\\n ...\"],[\"## Tech\\n\\nDillinger uses a number of open source projects to work properly:\\n\\n- [AngularJS] - HTML enh...\"],[\"#### Building for source\\n\\nFor production release:\\n\\n```bash\\ngulp build --prod\\n```\\n\\nGenerating pre-bui...\"],[\"[dill]: \\u003chttps:\\u002f\\u002fgithub.com\\u002fjoemccann\\u002fdillinger\\u003e\\n   [git-repo-url]: \\u003chttps:\\u002f\\u002fgithub.com\\u002fjoemccann\\u002fdi...\"],[\"gradio_client\\n\\n## 0.7.3\\n\\n### Fixes\\n\\n- [#6693](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6693) [`34f9...\"],[\"## 0.7.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.7.0-beta.1\\n\\n### Features\\n\\n- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af33...\"],[\"Thanks to a new capability that allows components to communicate directly with the server _without_ ...\"],[\"The `gradio_client` now supports streaming file outputs 🌊\\n\\nNo new syntax! Connect to a gradio demo t...\"],[\"Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n### Features\\n\\n- [#5076](https:\\u002f\\u002fgithub...\"],[\"Currently we have template spaces for:\\n\\n- [Llama-2-70b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Pinned dependencies to major...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\nNo changes to highlight.\\n\\n# 0....\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"Gradio Demo: blocks_form\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks...\"],[\"Gradio Demo: tictactoe\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks()...\"],[\"Gradio Demo: fake_gan_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo re...\"],[\"simple dashboard ranking spaces by number of likes....\"],[\"@gradio\\u002fjson\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"`gradio\\u002fmodel3d`\\n\\n```html\\n\\u003cscript\\u003e\\n    import {BaseModel3D, BaseModel3DUpload, BaseExample } from `@...\"],[\"@gradio\\u002ftheme\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"## 0.0.3\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Gradio Demo: automatic-speech-recognition\\n### Automatic speech recognition English. Record from your...\"],[\"Gradio Demo: event_trigger\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"video_btn = gr.Button(\\\"Change interactive\\\")\\n\\n            with gr.Column():\\n                video1 = ...\"],[\"radio2.change(fn=change_image, inputs=radio2, outputs=image1)\\n        image1.change(fn=alert_change,...\"],[\"`@gradio\\u002fbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseChatBot } from \\\"@gradio\\u002fchatbot\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\n\\nB...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n[\\u003cimg src=\\\"readme_files\\u002fgradio.svg\\\" alt=\\\"gradio\\\" width=400\\u003e](https:\\u002f\\u002fgradio.app...\"],[\"If you like Gradio, please leave us a ⭐ on GitHub!\\n\\n## Open Source Stack\\n\\nGradio is built on top of ...\"],[\"Gradio Demo: theme_new_step_3\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nfrom __future__ import annotat...\"],[\"class Seafoam(Base):\\n    def __init__(\\n        self,\\n        *,\\n        primary_hue: colors.Color | ...\"],[\"slider_color_dark=\\\"*secondary_600\\\",\\n            block_title_text_weight=\\\"600\\\",\\n            block_bor...\"],[\"seafoam = Seafoam()\\n\\nwith gr.Blocks(theme=seafoam) as demo:\\n    textbox = gr.Textbox(label=\\\"Name\\\")\\n ...\"],[\"Gradio Demo: dataframe_datatype\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport p...\"],[\"Gradio Demo: Echocardiogram-Segmentation\\n\\n\\n```\\n!pip install -q gradio -f https:\\u002f\\u002fdownload.pytorch.or...\"],[\"print(\\\"loading weights from \\\", os.path.join(destination_for_weights, \\\"deeplabv3_resnet50_random\\\"))\\n\\n...\"],[\"import gradio as gr\\n\\ni = gr.Image(label=\\\"Echocardiogram\\\")\\no = gr.Image(label=\\\"Segmentation Mask\\\")\\n\\ne...\"],[\"`@gradio\\u002fbutton`\\n\\n```javascript\\n\\u003cscript\\u003e\\n\\timport { BaseButton } from \\\"@gradio\\u002fbutton\\\";\\n\\timport { cre...\"],[\"Gradio Demo: reverse_audio_2\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport gradio as gr\\nimport...\"],[\"Gradio Demo: model3d_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nw...\"],[\"Gradio Demo: sentence_builder\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef sent...\"],[\"@gradio\\u002faudio\\n\\n## 0.6.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.5.4\\n\\n### Fixes\\n\\n- [#6546](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6546) [`a424fdbb2`](https:\\u002f...\"],[\"## 0.4.3\\n\\n### Fixes\\n\\n- [#6317](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6317) [`19af2806a`](https:\\u002f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.4.0-beta.9\\n\\n### Features\\n\\n- [#6153](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6153) [`1162ed621...\"],[\"## 0.4.0-beta.8\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](http...\"],[\"## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#4993](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4993) [`dc07a9f9`](https...\"],[\"Gradio Demo: waveform\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport random\\n\\n\\nCO...\"],[\"Gradio Demo: hello_login\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport argparse...\"],[\"@gradio\\u002ffileexplorer\\n\\n## 0.3.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgith...\"],[\"## 0.3.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0-beta.1\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"![output](https:\\u002f\\u002fgithub.com\\u002fpngwn\\u002fMDsveX\\u002fassets\\u002f12937446\\u002fef108f0b-0e84-4292-9984-9dc66b3e144d)\\n\\nFor...\"],[\"Using Gradio and Comet\\n\\nTags: COMET, SPACES\\nContributed by the Comet team\\n\\n## Introduction\\n\\nIn this ...\"],[\"If you're running these examples as a script, you can either export your credentials as environment ...\"],[\"# Download human-readable labels for ImageNet.\\nresponse = requests.get(\\\"https:\\u002f\\u002fgit.io\\u002fJJkYN\\\")\\nlabel...\"],[\"Go to your Comet Project page, and head over to the Panels tab. Click the `+ Add` button to bring up...\"],[\"Once you have added your Panel, click Edit to access to the Panel Options page and paste in the path...\"],[\"if torch.cuda.is_available():\\n    device = \\\"cuda\\\"\\nelse:\\n    device = \\\"cpu\\\"\\n\\nMODEL_NAME = \\\"gpt2\\\"\\n\\nmod...\"],[\"input_text = gr.Textbox(label=\\\"Input Text\\\", lines=5, interactive=True)\\n    submit_btn = gr.Button(\\\"S...\"],[\"`@gradio\\u002fmarkdown`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseMarkdown, MarkdownCode, BaseExample } from `@g...\"],[\"Gradio Demo: calculator_live\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef calcul...\"],[\"Named-Entity Recognition\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002frajistics\\u002fbiobert_ner_demo, ...\"],[\"```py\\nfrom transformers import pipeline\\nner_pipeline = pipeline(\\\"ner\\\")\\nner_pipeline(\\\"Does Chicago ha...\"],[\"Gradio Demo: fraud_detector\\n\\n\\n```\\n!pip install -q gradio pandas\\n```\\n\\n\\n```\\n# Downloading files from t...\"],[\"!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides\\u002f1)getting_start...\"],[\"### Installation\\n\\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https:\\u002f\\u002fwww.python.org\\u002fdo...\"],[\"Type your name in the textbox on the left, drag the slider, and then press the Submit button. You sh...\"],[\"\\u003e [!TIP]\\n \\u003e For the `inputs` and `outputs` arguments, you can pass in the name of these components a...\"],[\"### An Overview of Gradio\\n\\nSo far, we've been discussing the `Interface` class, which is a high-leve...\"],[\"* [Gradio Python Client](https:\\u002f\\u002fwww.gradio.app\\u002fguides\\u002fgetting-started-with-the-python-client) (`gra...\"],[\"If you like Gradio, please leave us a ⭐ on GitHub!\\n\\n## Open Source Stack\\n\\nGradio is built on top of ...\"],[\"Building a FastAPI App with the Gradio Python Client\\n\\nTags: CLIENT, API, WEB APP\\n\\nIn this blog post,...\"],[\"client = Client(\\\"abidlabs\\u002fmusic-separation\\\")\\n\\ndef acapellify(audio_path):\\n    result = client.predic...\"],[\"new_audio = acapellify(old_audio)\\n\\n    new_video = f\\\"acap_{video_path}\\\"\\n    subprocess.call(['ffmpeg...\"],[\"In this example, the FastAPI app has two routes: `\\u002f` and `\\u002fuploadvideo\\u002f`.\\n\\nThe `\\u002f` route returns an ...\"],[\"```html\\n&lt;!DOCTYPE html\\u003e &lt;html\\u003e &lt;head\\u003e &lt;title\\u003eVideo Gallery&lt;\\u002ftitle\\u003e\\n&lt;style\\u003e body { ...\"],[\"## Step 4: Run your FastAPI app\\n\\nFinally, we are ready to run our FastAPI app, powered by the Gradio...\"],[\"Developing Faster with Auto-Reloading\\n\\n**Prerequisite**: This Guide requires you to know about Block...\"],[\"In the terminal, run `gradio run.py`. That's it!\\n\\nNow, you'll see that after you'll see something li...\"],[\"🔥 If your application accepts command line arguments, you can pass them in as well. Here's an exampl...\"],[\"Here's what it looks like in a jupyter notebook:\\n\\n![](https:\\u002f\\u002fgradio-builds.s3.amazonaws.com\\u002fdemo-fi...\"],[\"Running Background Tasks\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002ffreddyaboulton\\u002fgradio-google...\"],[\"The code will look like this:\\n\\n```python\\nDB_FILE = \\\".\\u002freviews.db\\\"\\ndb = sqlite3.connect(DB_FILE)\\n\\n# C...\"],[\"## Step 2 - Create a gradio app ⚡\\n\\nNow that we have our database logic defined, we can use gradio cr...\"],[\"Note that you'll have to get an access token from the \\\"Settings\\\" tab of your HuggingFace for the abo...\"],[\"How to Use the Plot Component for Maps\\n\\nTags: PLOTS, MAPS\\n\\n## Introduction\\n\\nThis guide explains how ...\"],[\"In the code above, we first load the csv data into a pandas dataframe. Let's begin by defining a fun...\"],[\"More info [here](https:\\u002f\\u002fplotly.com\\u002fpython\\u002fscattermapbox\\u002f) on scatter plots using Mapbox and Plotly....\"],[\"Gradio Demo: blocks_style\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Block...\"],[\"Using Flagging\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fcalculator-flagging-crowdsource...\"],[\"There are [four parameters](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002f#interface-header) in `gradio.Interface` that co...\"],[\"Here's an example: The code below creates the calculator interface embedded below it:\\n\\n```python\\nimp...\"],[\"If we go back to the calculator example, the following code will create the interface embedded below...\"],[\"iface.launch()\\n```\\n\\nNotice that we define our own\\ninstance of `gradio.HuggingFaceDatasetSaver` using...\"],[\"Here is an example with an image sepia filter Blocks demo that lets you flag\\ndata using the default ...\"],[\"his demo shows how you can build a live interactive dashboard with gradio.\\nThe current time is refre...\"],[\"Gradio Demo: plot_component\\n\\n\\n```\\n!pip install -q gradio matplotlib numpy\\n```\\n\\n\\n```\\nimport gradio as...\"],[\"Gradio Demo: request_ip_headers\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef pr...\"],[\"连接到数据库\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fchicago-bike-share-dashboard\\n标签：TABULAR, PLOTS\\n\\n##...\"],[\"connection_string = f\\\"postgresql:\\u002f\\u002f{DB_USER}:{DB_PASSWORD}@{DB_HOST}?port={PORT}&dbname={DB_NAME}\\\"\\n\\n...\"],[\"demo.launch()\\n```\\n\\n## 步骤 3 - 部署\\n\\n如果您运行上述代码，您的应用程序将在本地运行。\\n您甚至可以通过将 `share=True` 参数传递给 `launch` 来获得一个临...\"],[\"Gradio Demo: chatinterface_streaming_echo\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport time\\nimport...\"],[\"Gradio Demo: hangman\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nsecret_word = \\\"gra...\"],[\"The 4 Kinds of Gradio Interfaces\\n\\nSo far, we've always assumed that in order to build an Gradio demo...\"],[\"$code_fake_gan_no_input\\n$demo_fake_gan_no_input\\n\\n## Input-only demos\\n\\nSimilarly, to create a demo th...\"],[\"`@gradio\\u002fdataframe`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseDataFrame, BaseExample } from \\\"@gradio\\u002fdatafr...\"],[\"Gradio Demo: line_plot\\n\\n\\n```\\n!pip install -q gradio vega_datasets pandas\\n```\\n\\n\\n```\\nimport gradio as ...\"],[\"def line_plot_fn(dataset):\\n    if dataset == \\\"stocks\\\":\\n        return gr.LinePlot(\\n            stock...\"],[\"Contributing to Gradio\\n\\n![GitHub issues by-label](https:\\u002f\\u002fimg.shields.io\\u002fgithub\\u002fissues\\u002fgradio-app\\u002fgr...\"],[\"### 📦 Using dev containers\\n\\nYou can alternatively use dev containers. This is supported on all platf...\"],[\"## 🚀 Run a Gradio app\\n\\nYou can get started by creating an `app.py` file in the root:\\n\\n```\\nimport gra...\"],[\"## 🕸️ Gradio Website\\n\\nWe also welcome any contributions to our [website](https:\\u002f\\u002fwww.gradio.app). \\n\\n...\"],[\"```\\nbash scripts\\u002fformat_backend.sh\\n```\\n\\n```\\nbash scripts\\u002fformat_frontend.sh\\n```\\n\\nThank you for takin...\"],[\"Gradio Demo: leaderboard\\n### A simple dashboard ranking spaces by number of likes.\\n        \\n\\n\\n```\\n!p...\"],[\"Gradio 界面的 4 种类型\\n\\n到目前为止，我们一直假设构建 Gradio 演示需要同时具备输入和输出。但对于机器学习演示来说，并不总是如此：例如，*无条件图像生成模型*不需要任何输入，但会生成一...\"],[\"Gradio Demo: calculator_blocks\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef cal...\"],[\"@gradio\\u002fdataframe\\n\\n## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`846d52d`](https:\\u002f\\u002fgithub.c...\"],[\"## 0.3.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fbutton@0.2.1\\n  - @gradio\\u002fupload@...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"### Fixes\\n\\n- [#5755](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5755) [`e842a561a`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`05892302`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"### Fixes\\n\\n- [#5256](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5256) [`933db53e`](https:\\u002f\\u002fgithub.com...\"],[\"Gradio Demo: rows_and_columns\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"Gradio Demo: lineplot_component\\n\\n\\n```\\n!pip install -q gradio vega_datasets\\n```\\n\\n\\n```\\nimport gradio a...\"],[\"@gradio\\u002fbox\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`73268ee`](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"## 0.1.0-beta.5\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"Gradio Demo: webcam\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef snap(image, v...\"],[\"Gradio Demo: theme_new_step_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nfrom __future__ import annotat...\"],[\"Gradio 和 ONNX 在 Hugging Face 上\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fonnx\\u002fEfficientNet-Lite...\"],[\"### Hugging Face 模型\\n\\nHugging Face 模型中心还支持 ONNX 模型，并且可以通过[ONNX 标签](https:\\u002f\\u002fhuggingface.co\\u002fmodels?libr...\"],[\"# 从ONNX模型仓库加载ONNX模型\\nmodel = hub.load(\\\"efficientnet-lite4\\\")\\n# 加载标签文本文件\\nlabels = json.load(open(\\\"label...\"],[\"title = \\\"EfficientNet-Lite4\\\"\\ndescription = \\\"EfficientNet-Lite 4是最大的变体，也是EfficientNet-Lite模型集合中最准确的。它...\"],[\"Gradio Demo: slider_release\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef identi...\"],[\"How to add support for more languages\\n\\nWe would love to support more languages for Gradio 🌎\\n\\nTo add ...\"],[\"Setting Up a Demo for Maximum Performance\\n\\nTags: CONCURRENCY, LATENCY, PERFORMANCE\\n\\nLet's say that y...\"],[\"(2) They allow the server to send multiple updates to the frontend. This means, for example, that th...\"],[\"If you want to change this behavior, there are several parameters that can be used to configure the ...\"],[\"You can also set the number of requests that can be processed in parallel for each event individuall...\"],[\"### The `max_batch_size` parameter in events\\n\\nAnother way to increase the parallelism of your Gradio...\"],[\"## Upgrading your Hardware (GPUs, TPUs, etc.)\\n\\nIf you have done everything above, and your demo is s...\"],[\"Gradio Demo: blocks_outputs\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef make_m...\"],[\"with gr.Blocks() as demo:\\n    with gr.Column():\\n        txt = gr.Textbox(label=\\\"Small Textbox\\\", line...\"],[\"interactive=True, headers=[\\\"One\\\", \\\"Two\\\", \\\"Three\\\", \\\"Four\\\"], col_count=4\\n        )\\n        df = gr.Dat...\"],[\"if __name__ == \\\"__main__\\\":\\n    demo.launch()\\n\\n```...\"],[\"Gradio Demo: slider_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr....\"],[\"`@gradio\\u002fradio`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseRadio, BaseExample } from \\\"@gradio\\u002fradio\\\"; \\n\\u003c\\u002fscr...\"],[\"Gradio Demo: clearbutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\nwit...\"],[\"`@gradio\\u002fhtml`\\n\\n```javascript\\nimport { BaseHTML } from \\\"@gradio\\u002fhtml\\\";\\n```\\n\\nBaseHTML\\n```javascript\\n\\t...\"],[\"Gradio Demo: hello_world\\n### The simplest possible Gradio demo. It wraps a 'Hello {name}!' function ...\"],[\"Gradio Demo: ner_pipeline\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nfrom transformer...\"],[\"接口状态 (Interface State)\\n\\n本指南介绍了 Gradio 中如何处理状态。了解全局状态和会话状态的区别，以及如何同时使用它们。\\n\\n## 全局状态 (Global State)\\n\\n您的...\"],[\"使用 Hugging Face 集成\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fhelsinki_translation_en_es\\n标签：HUB，SPAC...\"],[\"让我们尝试使用推理 API 而不是自己加载模型的方式进行相同的演示。鉴于 Inference API 支持的 Hugging Face 模型，Gradio 可以自动推断出预期的输入和输出，并进行底层服...\"],[\"您还可以在 Hugging Face Spaces 上使用和混合现有的 Gradio 演示。例如，您可以将两个现有的 Gradio 演示放在单独的选项卡中并创建一个新的演示。您可以在本地运行此新演示，...\"],[\"Gradio Demo: chatbot_dialogpt\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nimport gradi...\"],[\"Gradio Demo: blocks_joined\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: zip_to_json\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nfrom zipfile import ZipFile\\n\\nimport...\"],[\"@gradio\\u002fgroup\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`2...\"],[\"Gradio Demo: titanic_survival\\n\\n\\n```\\n!pip install -q gradio scikit-learn numpy pandas\\n```\\n\\n\\n```\\n# Dow...\"],[\"clf = RandomForestClassifier()\\nclf.fit(X_train, y_train)\\npredictions = clf.predict(X_test)\\n\\n\\ndef pre...\"],[\"`@gradio\\u002fuploadbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseUploadButton } from \\\"@gradio\\u002fuploadbutton\\\"...\"],[\"Gradio Demo: translation\\n### This translation demo takes in the text, source and target languages, a...\"],[\"Using Hugging Face Integrations\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fen2es\\nTags: HU...\"],[\"You might notice that the first inference takes about 20 seconds. This happens since the Inference A...\"],[\"Here's an example that does exactly that:\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n...\"],[\"demo = gr.Interface.from_pipeline(pipe)\\ndemo.launch()\\n```\\n\\nThe previous code produces the following ...\"],[\"使用 Gradio 块像函数一样\\n\\nTags: TRANSLATION, HUB, SPACES\\n\\n**先决条件**: 本指南是在块介绍的基础上构建的。请确保[先阅读该指南](https:\\u002f\\u002fgrad...\"],[\"english_generator(text, fn_index=1)[0][\\\"generated_text\\\"]\\n\\nGradio 空间中的函数是从零开始索引的，所以西班牙语翻译器将是我的空间中的第二个...\"],[\"Gradio Demo: unispeech-speaker-verification\\n\\n\\n```\\n!pip install -q gradio git+https:\\u002f\\u002fgithub.com\\u002fhugg...\"],[\"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\\nSTYLE = \\\"\\\"\\\"\\n\\u003clink rel=\\\"styles...\"],[\"def similarity_fn(path1, path2):\\n    if not (path1 and path2):\\n        return '\\u003cb style=\\\"color:red\\\"\\u003e...\"],[\"inputs = [\\n    gr.Audio(sources=[\\\"microphone\\\"], type=\\\"filepath\\\", label=\\\"Speaker #1\\\"),\\n    gr.Audio(s...\"],[\"Gradio Demo: white_noise_vid_not_playable\\n\\n\\n```\\n!pip install -q gradio opencv-python\\n```\\n\\n\\n```\\nimpor...\"],[\"Gradio Demo: logoutbutton_component\\n\\n\\n```\\n!pip install -q gradio gradio[oauth]\\n```\\n\\n\\n```\\nimport grad...\"],[\"`@gradio\\u002fbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { Button } from \\\"@gradio\\u002fbutton\\\";\\n\\u003c\\u002fscript\\u003e\\n\\n\\u003cbutton type...\"],[\"Gradio Demo: chicago-bikeshare-dashboard\\n\\n\\n```\\n!pip install -q gradio psycopg2 matplotlib SQLAlchemy...\"],[\"If data were added to the database, the plots in this demo would update\\n    whenever the webpage is ...\"],[\"Gradio Demo: textbox_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"使用标记\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fcalculator-flagging-crowdsourced, https:\\u002f\\u002fhuggingfac...\"],[\"iface = gr.Interface(\\n    calculator,\\n    [\\\"number\\\", gr.Radio([\\\"add\\\", \\\"subtract\\\", \\\"multiply\\\", \\\"divid...\"],[\"通过 `flagging_callback` 参数，我们使这变得非常简单。\\n\\n例如，下面我们将会将标记的数据从我们的计算器示例导入到 Hugging Face 数据集中，以便我们可以构建一个“众包”数...\"],[\"Image Classification in PyTorch\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fpytorch-imag...\"],[\"In the case of our pretrained model, it will look like this:\\n\\n```python\\nimport requests\\nfrom PIL imp...\"],[\"This produces the following interface, which you can try right here in your browser (try uploading y...\"],[\"Theming\\n\\nTags: THEMES\\n\\n## Introduction\\n\\nGradio features a built-in theming engine that lets you cust...\"],[\"### Core Colors\\n\\nThe first 3 constructor arguments set the colors of the theme and are `gradio.theme...\"],[\"- `spacing_size`: This sets the padding within and spacing between elements. In the default theme, t...\"],[\"You could modify these values such as the following:\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Defau...\"],[\"Of course, many CSS variable names are shorter than this, such as `table_border_color`, or `input_sh...\"],[\"```python\\ntheme = gr.themes.Default().set(\\n    button_primary_background_fill=\\\"#FF0000\\\",\\n    button_...\"],[\"Our new theme class will inherit from `gradio.themes.Base`, a theme that sets a lot of convenient de...\"],[\"## Sharing Themes\\n\\nOnce you have created a theme, you can upload it to the HuggingFace Hub to let ot...\"],[\"The theme preview for our seafoam theme is here: [seafoam preview](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgra...\"],[\"@gradio\\u002ftabs\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`28...\"],[\"## 0.0.5\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.1\\n\\n## 0.0.4\\n\\n### Patch...\"],[\"如何创建一个新组件\\n\\n## 简介\\n\\n本指南旨在说明如何添加一个新组件，你可以在 Gradio 应用程序中使用该组件。该指南将通过代码片段逐步展示如何添加[ColorPicker](https:\\u002f\\u002fgr...\"],[\"def __init__(\\n        self,\\n        value: str = None,\\n        *,\\n        label: Optional[str] = Non...\"],[\"# 输入功能\\n    def preprocess(self, x: str | None) -\\u003e Any:\\n        \\\"\\\"\\\"\\n        Any preprocessing needed ...\"],[\"```python\\nclass TestColorPicker(unittest.TestCase):\\n    def test_component_functions(self):\\n        ...\"],[\"## 2. 创建一个新的 Svelte 组件\\n\\n让我们来看看创建新组件的前端并将其与其 Python 代码映射起来的步骤：\\n\\n- 在 [js 文件夹](https:\\u002f\\u002fgithub.com\\u002fgradi...\"],[\"在这里，您将拥有三个文件，第一个文件用于 Svelte 应用程序，具体如下所示：\\n\\n```typescript\\n\\u003csvelte:options accessors={true} \\u002f\\u003e\\n\\n\\u003cscript...\"],[\"const item: HTMLInputElement = getByDisplayValue(\\\"#000000\\\");\\n\\t\\tassert.equal(item.value, \\\"#000000\\\");\\n...\"],[\"Gradio Demo: concurrency_with_queue\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpo...\"],[\"Gradio Demo: blocks_essay_simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef c...\"],[\"@gradio\\u002fform\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`73268ee`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.0.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`fe057300`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: blocks_chained_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"Gradio Demo: dataframe_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith g...\"],[\"Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\\n\\nTags: SERVERLESS, BROWSER, PYODIDE\\n...\"],[\"### 2. Create the `\\u003cgradio-lite\\u003e` tags\\n\\nSomewhere in the body of your HTML page (wherever you'd like...\"],[\"What if you want to create a Gradio app that spans multiple files? Or that has custom Python require...\"],[\"async def classify(text):\\n\\treturn await pipe(text)\\n\\ndemo = gr.Interface(classify, \\\"textbox\\\", \\\"json\\\")...\"],[\"## Try it out!\\n\\nYou can immediately try out `@gradio\\u002flite` by copying and pasting this code in a loc...\"],[\"控制布局 (Controlling Layout)\\n\\n默认情况下，块中的组件是垂直排列的。让我们看看如何重新排列组件。在幕后，这种布局结构使用了[Web 开发的 flexbox 模型](https:\\u002f...\"],[\"## 可见性 (Visibility)\\n\\n组件和布局元素都有一个 `visible` 参数，可以在初始时设置，并使用 `gr.update()` 进行更新。在 Column 上设置 `gr.updat...\"],[\"Gradio Demo: blocks_kitchen_sink\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ...\"],[\"var link_elem = document.createElement('link');\\n                link_elem.classList.add('link-css');...\"],[\"with gr.Row():\\n        slider1 = gr.Slider(label=\\\"Slider 1\\\")\\n        slider2 = gr.Slider(label=\\\"Slid...\"],[\"def clear():\\n                    time.sleep(0.2)\\n                    return None\\n\\n                cl...\"],[\"gr.Markdown(\\\"## Media Files\\\")\\n\\n    with gr.Tabs() as tabs:\\n        with gr.Tab(\\\"Audio\\\"):\\n           ...\"],[\"def chat(history):\\n                time.sleep(2)\\n                yield [[\\\"How are you?\\\", \\\"I am good....\"],[\"gr.Markdown(\\\"## Dataset Examples\\\")\\n\\n    component_example_set = [\\n        (gr.Audio(render=False), j...\"],[\"isplay an interactive map of AirBnB locations with Plotly. Data is hosted on HuggingFace Datasets....\"],[\"使用 Gradio 和 Comet\\n\\nTags: COMET, SPACES\\n由 Comet 团队贡献\\n\\n## 介绍\\n\\n在这个指南中，我们将展示您可以如何使用 Gradio 和 Comet。我们将介绍...\"],[\"在这个例子中，我们将介绍如何将您的 Gradio 应用程序记录到 Comet，并使用 Gradio 自定义面板与其进行交互。\\n\\n我们先通过使用 `resnet18` 构建一个简单的图像分类示例。\\n\\n`...\"],[\"如果您要长期托管 Gradio 应用程序，可以使用 Gradio Panel Extended 自定义面板进行嵌入 UI。\\n\\n转到您的 Comet 项目页面，转到面板选项卡。单击“+ 添加”按钮以打开...\"],[\"[![在 Colab 中打开](https:\\u002f\\u002fcolab.research.google.com\\u002fassets\\u002fcolab-badge.svg)](https:\\u002f\\u002fcolab.research.go...\"],[\"def predict(text, state, message):\\n    experiment = state\\n\\n    shap_values = explainer([text])\\n    p...\"],[\"@gradio\\u002fcolumn\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`...\"],[\"Gradio Demo: asr\\n\\n\\n```\\n!pip install -q gradio torch torchaudio transformers\\n```\\n\\n\\n```\\nimport gradio ...\"],[\"he simplest possible Gradio demo. It wraps a 'Hello {name}!' function in an Interface that accepts a...\"],[\"his  demo converts text to speech in 14 languages....\"],[\"Contributing a Guide\\n\\nWant to help teach Gradio? Consider contributing a Guide! 🤗\\n\\nBroadly speaking,...\"],[\"That's it! We're looking forward to reading your Guide 🥳...\"],[\"Gradio Demo: blocks_xray\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\ndi...\"],[\"Gradio Demo: fake_gan\\n### This is a fake GAN that shows how to create a text-to-image interface for ...\"],[\"如何使用 3D 模型组件\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fdawood\\u002fModel3D, https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fradam...\"],[\"\\u003cgradio-app space=\\\"radames\\u002fPIFu-Clothed-Human-Digitization\\\"\\u003e \\u003c\\u002fgradio-app\\u003e\\n\\n---\\n\\n搞定！这就是构建 Model3D 模型...\"],[\"@gradio\\u002faccordion\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.c...\"],[\"## 0.2.0-beta.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.0.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.1\\n  - @gradio\\u002fstatustr...\"],[\"Gradio Demo: highlightedtext_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n...\"],[\"Gradio Demo: on_listener_decorator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith...\"],[\"Gradio Demo: zip_files\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo rep...\"],[\"his demo uses a fake model to showcase iterative output. The Image output will update every time a g...\"],[\"Gradio Demo: color_picker\\n\\n\\n```\\n!pip install -q gradio Pillow\\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"Gradio Demo: tax_calculator\\n### Calculate taxes using Textbox, Radio, and Dataframe components\\n     ...\"],[\"Customizing your demo with CSS and Javascript\\n\\nGradio allows you to customize your demo in several w...\"],[\"```python\\nwith gr.Blocks(css=\\\".gradio-container {background: url('file=clouds.jpg')}\\\") as demo:\\n    ...\"],[\"Below is an example of adding custom js to show an animated welcome message when the demo first load...\"],[\"The `Interface` class\\n\\nAs mentioned in the [Quickstart](\\u002fmain\\u002fguides\\u002fquickstart), the `gr.Interface`...\"],[\"## An Image Example\\n\\nGradio supports many types of components, such as `Image`, `DataFrame`, `Video`...\"],[\"Continue learning about examples in the [More On Examples](https:\\u002f\\u002fgradio.app\\u002fguides\\u002fmore-on-example...\"],[\"For example, with the calculator interface shown above, we would have the flagged data stored in the...\"],[\"Gradio Demo: variable_outputs\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nmax_textb...\"],[\"Security Policy\\n\\n## Reporting a Vulnerability\\n\\nIf you discover a security vulnerability, we would be...\"],[\"Configuring Your Custom Component\\n\\nThe custom components workflow focuses on [convention over config...\"],[\"## Python Dependencies\\n\\nYou can add python dependencies by modifying the `dependencies` key in `pypr...\"],[\"Gradio Demo: blocks_gpt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\napi = gr.load(\\\"...\"],[\"imple image segmentation using gradio's AnnotatedImage component....\"],[\"Gradio Demo: save_file_no_output\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport random\\nimport string...\"],[\"@gradio\\u002fcolorpicker\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.1.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustr...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: code_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blo...\"],[\"Gradio Demo: fake_diffusion\\n### This demo uses a fake model to showcase iterative output. The Image ...\"],[\"Gradio Demo: altair_plot\\n\\n\\n```\\n!pip install -q gradio altair vega_datasets\\n```\\n\\n\\n```\\nimport altair a...\"],[\"circ = rect.mark_point().encode(\\n            alt.ColorValue('grey'),\\n            alt.Size('count()',...\"],[\"lines = base.mark_line().encode(\\n            size=alt.condition(~highlight, alt.value(1), alt.value(...\"],[\"Gradio Demo: native_plots\\n\\n\\n```\\n!pip install -q gradio vega_datasets\\n```\\n\\n\\n```\\n# Downloading files f...\"],[\"create-svelte\\n\\nEverything you need to build a Svelte project, powered by [`create-svelte`](https:\\u002f\\u002fg...\"],[\"Gradio Demo: same-person-or-different\\n### This demo identifies if two speakers are the same person u...\"],[\"OUTPUT_OK = (\\n    \\\"\\\"\\\"\\n    \\u003cdiv class=\\\"container\\\"\\u003e\\n        \\u003cdiv class=\\\"row\\\"\\u003e\\u003ch1 style=\\\"text-align: ce...\"],[\"wav1, _ = apply_effects_file(path1, EFFECTS)\\n    wav2, _ = apply_effects_file(path2, EFFECTS)\\n    pr...\"],[\"How to Create a Custom Chatbot with Gradio Blocks\\n\\nTags: NLP, TEXT, CHAT\\nRelated spaces: https:\\u002f\\u002fhug...\"],[\"We have a single function, `respond()`, which takes in the entire history of the chatbot, appends a ...\"],[\"3. The third method makes the input field interactive again so that users can send another message t...\"],[\"## Adding Markdown, Images, Audio, or Videos\\n\\nThe `gr.Chatbot` component supports a subset of markdo...\"],[\"Gradio Demo: video_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwit...\"],[\"Gradio Demo: bar_plot\\n\\n\\n```\\n!pip install -q gradio pandas\\n```\\n\\n\\n```\\nimport gradio as gr\\nimport panda...\"],[\"def bar_plot_fn(display):\\n    if display == \\\"simple\\\":\\n        return gr.BarPlot(\\n            simple,...\"],[\"with gr.Blocks() as bar_plot:\\n    with gr.Row():\\n        with gr.Column():\\n            display = gr....\"],[\"Gradio Demo: progress_simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time...\"],[\"@gradio\\u002fuploadbutton\\n\\n## 0.3.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithu...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.0.9\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6e56a0d9b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"The Frontend 🌐⭐️\\n\\nThis guide will cover everything you need to know to implement your custom compone...\"],[\"* `mode` is how the parent Gradio app tells your component whether the `interactive` or `static` ver...\"],[\"* `selected`: You can also adjust how the examples are displayed if a user \\\"selects\\\" a particular ex...\"],[\"async function handle_upload(file_data: FileData[]): Promise\\u003cvoid\\u003e {\\n        await tick();\\n        u...\"],[\"```typescript\\n\\u003cscript\\u003e\\n\\timport { type FileData, normalise_file, Upload, ModifyUpload } from \\\"@gradio...\"],[\"@gradio\\u002fsimpledropdown\\n\\n## 0.1.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgit...\"],[\"## 0.1.0\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"Test Coverage\\n\\nJust a little reference docs to understand what is tested\\u002f needs testing. Perhaps tem...\"],[\"| Component       | `value` | `visible` | `elem_id` | `elem_classes` | `container` | `label` | `show...\"],[\"| Image           | `❌`    | `❌`      | `❌`      | `❌`           | `❌`        | `❌`    | `❌`        ...\"],[\"### Events...\"],[\"| Component       | `value` | `visible` | `elem_id` | `elem_classes` | `container` | `label` | `show...\"],[\"| HighlightedText | `❌`    | `❌`      | `❌`      | `❌`           | `❌`        | `❌`    | `❌`        ...\"],[\"### `AnnotatedImage`\\n\\n### `Audio`\\n\\n### `BarPlot`\\n\\n### `Button`\\n\\n### `Chatbot`\\n\\n### `Checkbox`\\n\\n### `...\"],[\"@gradio\\u002fcode\\n\\n## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.2.5\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fupload@0.4.1\\n\\n## 0.2.4\\n\\n### Fixe...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: text_analysis\\n### This simple demo takes advantage of Gradio's HighlightedText, JSON an...\"],[\"Gradio Demo: diff_texts\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nfrom difflib import Differ\\n\\nimport g...\"],[\"Gradio Demo: code\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\nimp...\"],[\"Reactive Interfaces\\n\\nFinally, we cover how to get Gradio demos to refresh automatically or continuou...\"],[\"`@gradio\\u002ftheme`\\n\\ncss for gradio...\"],[\"Gradio Demo: chatbot_simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport rando...\"],[\"his sentiment analaysis demo takes in input text and returns its classification for either positive,...\"],[\"Gradio Demo: count_generator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time...\"],[\"Creating a Real-Time Dashboard from Google Sheets\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Google Sheets](...\"],[\"def get_data():\\n    return pd.read_csv(csv_url)\\n```\\n\\n3\\\\. The data query is a function, which means t...\"],[\"2\\\\. In the Cloud Console, click on the hamburger menu in the top-left corner and select \\\"APIs & Serv...\"],[\"```html\\nhttps:\\u002f\\u002fdocs.google.com\\u002fspreadsheets\\u002fd\\u002f1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU\\u002fedit#gid...\"],[\"demo.queue().launch()  # Run the demo with queuing enabled\\n```\\n\\nYou now have a Dashboard that refres...\"],[\"@gradio\\u002fnumber\\n\\n## 0.3.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fatoms@0.1.3\\n  - @gradio\\u002fstatustr...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"@gradio\\u002fplot\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.2\\n\\n### Fixes\\n\\n- [#5795](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5795) [`957ba5cfd`](https:\\u002f...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"Gradio Demo: chatbot_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr....\"],[\"Gradio Demo: interface_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\nd...\"],[\"his demo identifies musical instruments from an audio file. It uses Gradio's Audio and Label compone...\"],[\"Gradio Demo: blocks_js_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef welcome...\"],[\"Gradio Demo: image_mod_default_image\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files fro...\"],[\"Gradio Demo: on_listener_basic\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr....\"],[\"Gradio Demo: reversible_flow\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef increa...\"],[\"@gradio\\u002fcheckboxgroup\\n\\n## 0.3.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgith...\"],[\"## 0.3.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"Gradio Demo: digit_classifier\\n\\n\\n```\\n!pip install -q gradio tensorflow\\n```\\n\\n\\n```\\nfrom urllib.request ...\"],[\"@gradio\\u002fwasm\\n\\n## 0.4.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67...\"],[\"## 0.2.0-beta.1\\n\\n### Features\\n\\n- [#5963](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5963) [`174b73619...\"],[\"### Fixes\\n\\n- [#5919](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5919) [`1724918f0`](https:\\u002f\\u002fgithub.co...\"],[\"@gradio\\u002fcheckbox\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n  - @gradio\\u002fatoms@0....\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"Gradio Demo: input_output\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef image_mo...\"],[\"Key Features\\n\\nLet's go through some of the key features of Gradio. This guide is intended to be a hi...\"],[\"**Static and Interactive Components**\\n\\nEvery component has a _static_ version that is designed to *d...\"],[\"Images are converted to NumPy arrays because they are a common format for machine learning workflows...\"],[\"```python\\ndemo = gr.Interface(...).queue(default_concurrency_limit=5)\\ndemo.launch()\\n```\\n\\nThis limits...\"],[\"$code_fake_diffusion\\n$demo_fake_diffusion\\n\\nNote that we've added a `time.sleep(1)` in the iterator t...\"],[\"For additional styling ability, you can pass any CSS (as well as custom JavaScript) to your Gradio a...\"],[\"```python\\ndemo = gr.Interface(\\n    fn=trim_words, \\n    inputs=[\\\"textbox\\\", \\\"number\\\"], \\n    outputs=[\\\"...\"],[\"Gradio Demo: file_explorer_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nw...\"],[\"Gradio Demo: timeseries-forecasting-with-prophet\\n### A simple dashboard showing pypi stats for pytho...\"],[\"Custom Components in 5 minutes\\n\\nGradio 4.0 introduces Custom Components -- the ability for developer...\"],[\"```bash\\ngradio cc create MyComponent --template SimpleTextbox\\n```\\n\\nInstead of `MyComponent`, give yo...\"],[\"Tip: You don't have to run dev mode from your custom component directory. The first argument to `dev...\"],[\"Test Strategy\\n\\nVery brief, mildly aspirational test strategy document. This isn't where we are but i...\"],[\"We should not focus on code coverage but on test coverage following the below criteria:\\n\\n- The docum...\"],[\"These test are usually either unit or integration tests. They are generally pretty quick to write (e...\"],[\"## Supported environments and versions\\n\\nAll operating systems refer to the current runner variants s...\"],[\"- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. Th...\"],[\"Gradio Demo: audio_debugger\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"`@gradio\\u002fupload`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { Upload, ModifyUpload, normalise_file, get_fetchable_...\"],[\"Gradio Demo: hello_blocks_decorator\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\nwi...\"],[\"Connecting to a Database\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fchicago-bikeshare-das...\"],[\"Once your database is created, download the dataset from Kaggle and upload it to your database.\\nFor ...\"],[\"def get_most_popular_stations():\\n\\n    df = pd.read_sql(\\n        \\\"\\\"\\\"\\n    SELECT COUNT(ride_id) as n, ...\"],[\"But what if you want to a permanent deployment solution?\\nLet's deploy our Gradio app to the free Hug...\"],[\"Gradio Demo: image_classifier_2\\n\\n\\n```\\n!pip install -q gradio pillow torch torchvision\\n```\\n\\n\\n```\\n# Do...\"],[\"Gradio Demo: scatter_plot\\n\\n\\n```\\n!pip install -q gradio vega_datasets pandas\\n```\\n\\n\\n```\\nimport gradio ...\"],[\"with gr.Blocks() as scatter_plot:\\n    with gr.Row():\\n        with gr.Column():\\n            dataset =...\"],[\"使用Gradio Python客户端构建FastAPI应用\\n\\nTags: CLIENT, API, WEB APP\\n\\n在本博客文章中，我们将演示如何使用 `gradio_client` [Python...\"],[\"我们的视频处理工作流包含三个步骤：\\n\\n1. 首先，我们从视频文件路径开始，并使用`ffmpeg`提取音频。\\n2. 然后，我们通过上面的`acapellify()`函数传入音频文件。\\n3. 最后，我们将...\"],[\"`\\u002f` 路由返回一个显示所有上传视频的画廊的HTML模板。\\n\\n`\\u002fuploadvideo\\u002f` 路由接受一个带有`UploadFile`对象的 `POST` 请求，表示上传的视频文件。视频文件通过`pr...\"],[\"```csv\\n├── main.py\\n├── templates\\n│   └── home.html\\n```\\n\\n将以下内容写入`home.html`文件中：\\n\\n```html\\n&lt;!DOCTYPE...\"],[\"## 第4步：运行 FastAPI 应用\\n\\n最后，我们准备好运行由 Gradio Python 客户端提供支持的 FastAPI 应用程序。\\n\\n打开终端并导航到包含 `main.py` 文件的目录，然...\"],[\"Gradio Demo: theme_extended_step_3\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"@gradio\\u002ftabitem\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [...\"],[\"## 0.0.4\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.1\\n  - @gradio\\u002ftabs@0.0...\"],[\"从 BigQuery 数据创建实时仪表盘\\n\\nTags: 表格 , 仪表盘 , 绘图\\n\\n[Google BigQuery](https:\\u002f\\u002fcloud.google.com\\u002fbigquery) 是一个基...\"],[\"## 使用 BigQuery 客户端\\n\\n获得凭据后，您需要使用 BigQuery Python 客户端使用您的凭据进行身份验证。为此，您需要在终端中运行以下命令安装 BigQuery Python 客...\"],[\"demo.queue().launch()  # Run the demo with queuing enabled\\n```...\"],[\"Gradio Demo: progress\\n\\n\\n```\\n!pip install -q gradio tqdm datasets\\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"# track iterable of unknown length\\n    def load_random(data, progress=gr.Progress()):\\n        def yi...\"],[\"Gradio Demo: theme_new_step_1\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nfrom gradi...\"],[\"Interface State\\n\\nSo far, we've assumed that your demos are *stateless*: that they do not persist inf...\"],[\"Notice how the state persists across submits within each page, but if you load this demo in another ...\"],[\"Gradio Demo: tabbed_interface_lite\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nhell...\"],[\"Gradio Demo: theme_extended_step_1\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"使用 GAN 创建您自己的朋友\\n\\nspaces\\u002fNimaBoscarino\\u002fcryptopunks, https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fnateraw\\u002fcryptopunks...\"],[\"生成器不断训练以创建对鉴别器更难以识别的图像，而鉴别器每次正确检测到伪造图像时，都会为生成器设置更高的门槛。随着网络之间的这种竞争（**adversarial 对抗性！**），生成的图像改善到了对人眼...\"],[\"## 步骤 2 - 定义“predict”函数\\n\\n`predict` 函数是使 Gradio 工作的关键！我们通过 Gradio 界面选择的任何输入都将通过我们的 `predict` 函数传递，该函数...\"],[\"## 第四步—更多 punk！\\n\\n每次生成 4 个 punk 是一个好的开始，但是也许我们想控制每次想生成多少。通过简单地向我们传递给 `gr.Interface` 的 `inputs` 列表添加另一...\"],[\"供参考，这是我们的完整代码 :\\n\\n```python\\nimport torch\\nfrom torch import nn\\nfrom huggingface_hub import hf_hub_down...\"],[\"Gradio Demo: blocks_flipper\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\nimport gradio...\"],[\"@gradio\\u002fannotatedimage\\n\\n## 0.3.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgi...\"],[\"## 0.3.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`aaa55ce85`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0-beta.1\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Gradio Demo: stt_or_tts\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ntts_examples = ...\"],[\"gradio_client\\n\\n## 0.7.3\\n\\n### Fixes\\n\\n- [#6693](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6693) [`34f9...\"],[\"## 0.7.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.7.0-beta.1\\n\\n### Features\\n\\n- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af33...\"],[\"Thanks to a new capability that allows components to communicate directly with the server _without_ ...\"],[\"The `gradio_client` now supports streaming file outputs 🌊\\n\\nNo new syntax! Connect to a gradio demo t...\"],[\"Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n### Features\\n\\n- [#5076](https:\\u002f\\u002fgithub...\"],[\"Currently we have template spaces for:\\n\\n- [Llama-2-70b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Pinned dependencies to major...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\nNo changes to highlight.\\n\\n# 0....\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"Gradio and W&B Integration\\n\\n相关空间：https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fakhaliq\\u002fJoJoGAN\\n标签：WANDB, SPACES\\n由 Gr...\"],[\"让我们开始吧！\\n\\n1. 创建 W&B 账号\\n\\n   如果您还没有 W&B 账号，请按照[这些快速说明](https:\\u002f\\u002fapp.wandb.ai\\u002flogin)创建免费账号。这不应该超过几分钟的时间。一...\"],[\"img = generator(in_latent, input_is_latent=True)\\n\\n       with torch.no_grad():\\n           real_feat ...\"],[\"files.download('your-model-name.pt')\\n\\n   latent_dim = 512\\n   device=\\\"cuda\\\"\\n   model_path_s = hf_hub_...\"],[\"```python\\n\\n   demo.integrate(wandb=wandb)\\n   ```\\n\\n   调用集成之后，将创建一个演示，您可以将其集成到仪表板或报告中\\n\\n   在 W&B 之外，使用 ...\"],[\"Gradio Demo: dataset\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\n...\"],[\"c_2 = gr.CheckboxGroup(visible=False, choices=['a', 'b', 'c'])\\n    gr.Dataset(\\n        label=\\\"Checkb...\"],[\"gr.Dataset(\\n        components=[i],\\n        label=\\\"Image\\\",\\n        samples=[[img], [img], [img], [im...\"],[\"if __name__ == \\\"__main__\\\":\\n    demo.launch()\\n\\n```...\"],[\"`@gradio\\u002fdropdown`\\n\\n```html\\n\\u003cscript\\u003e\\n    import {BaseDropdown, BaseMultiselect, BaseExample } from \\\"...\"],[\"Gradio Demo: fake_diffusion_with_gif\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files fro...\"],[\"Gradio Demo: theme_soft\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\nwit...\"],[\"Gradio Demo: radio_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"Gradio Demo: image_selections\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport num...\"],[\"使用 Gradio Python 客户端入门\\n\\nTags: CLIENT, API, SPACES\\n\\nGradio Python 客户端使得将任何 Gradio 应用程序作为 API 使用变得非常容易...\"],[\"client = Client.duplicate(\\\"abidlabs\\u002fwhisper\\\", hf_token=HF_TOKEN)\\nclient.predict(\\\"audio_sample.wav\\\")\\n...\"],[\"```python\\nfrom gradio_client import Client\\n\\nclient = Client(\\\"abidlabs\\u002fwhisper\\\")\\nclient.predict(\\\"http...\"],[\"如果第一个作业已开始处理，则它将不会被取消。如果第二个作业尚未开始，则它将成功取消并从队列中删除。\\n\\n## 生成器端点 （Generator Endpoints）\\n\\n某些Gradio API端点不返回...\"],[\"Gradio Demo: markdown_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr...\"],[\"Gradio Demo: gpt2_xl_unified\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ncomponent ...\"],[\"How to Create a Chatbot with Gradio\\n\\nTags: NLP, TEXT, CHAT\\n\\n## Introduction\\n\\nChatbots are a popular ...\"],[\"Here's our chat function:\\n\\n```python\\nimport random\\n\\ndef random_response(message, history):\\n    retur...\"],[\"If you're familiar with Gradio's `Interface` class, the `gr.ChatInterface` includes many of the same...\"],[\"The `additional_inputs` parameters accepts a component or a list of components. You can pass the com...\"],[\"## Using your chatbot via an API\\n\\nOnce you've built your Gradio chatbot and are hosting it on [Huggi...\"],[\"Of course, we could also use the `openai` library directy. Here a similar example, but this time wit...\"],[\"def predict(message, history):\\n\\n    history_transformer_format = history + [[message, \\\"\\\"]]\\n    stop ...\"],[\"Create a Dashboard from Supabase Data\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Supabase](https:\\u002f\\u002fsupabase....\"],[\"5\\\\. Click Save to save the table schema.\\n\\nOur table is now ready!\\n\\n## Write data to Supabase\\n\\nThe ne...\"],[\"```python\\nimport supabase\\nimport pandas as pd\\n\\nclient = supabase.create_client('SUPABASE_URL', 'SUPA...\"],[\"Gradio Demo: hello_world_4\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Demo: fake_gan_no_input\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport time\\n\\nimport gradio as...\"],[\"gradio_test\\n\\n## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"@gradio\\u002fpreview\\n\\n## 0.6.0\\n\\n### Features\\n\\n- [#6738](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6738) [...\"],[\"A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.2.2\\n\\n### Features\\n\\n- [#6467](https:\\u002f\\u002fgithub.c...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.1.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.1.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.co...\"],[\"Gradio Demo: blocks_js_methods\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nblocks =...\"],[\"Gradio Demo: theme_extended_step_4\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"@gradio\\u002ftextbox\\n\\n## 0.4.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.4.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5417](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5417) [`d14d63e3`](https...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.1.0\\n\\n### Features\\n\\n- [#5005](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"Gradio Demo: dataset_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr....\"],[\"Gradio Demo: blocks_layout\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndemo = gr.B...\"],[\"@gradio\\u002fapp\\n\\n## 1.17.0\\n\\n### Features\\n\\n- [#6831](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6831) [`f3...\"],[\"## 1.16.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67ddd40`](https...\"],[\"## 1.13.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 1.12.0\\n\\n### Features\\n\\n- [#6427](https:\\u002f\\u002fgithub.c...\"],[\"## 1.10.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6204ccac5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 1.10.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`4b1011bab`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 1.10.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`92278729e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 1.9.1\\n\\n### Features\\n\\n- [#6137](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6137) [`2ba14b284`](http...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.co...\"],[\"## 1.9.0-beta.3\\n\\n### Features\\n\\n- [#6124](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6124) [`a7435ba9e...\"],[\"## 1.9.0-beta.2\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"### Fixes\\n\\n- [#6065](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6065) [`7d07001e8`](https:\\u002f\\u002fgithub.co...\"],[\"## 1.9.0-beta.0\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 1.7.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`796145e2c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 1.6.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abb5e9df4`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Fixes\\n\\n- [#5705](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5705) [`78e7cf516`](https:\\u002f\\u002fgithub.co...\"],[\"## 1.5.3\\n\\n### Fixes\\n\\n- [#5562](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5562) [`50d9747d0`](https:\\u002f...\"],[\"## 1.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6e381c4f`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"## 1.4.0\\n\\n### Features\\n\\n- [#5267](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5267) [`119c8343`](https...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"Thanks [@hannahblair](https:\\u002f\\u002fgithub.com\\u002fhannahblair)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"### Fixes\\n\\n- [#5285](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5285) [`cdfd4217`](https:\\u002f\\u002fgithub.com...\"],[\"Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n### Features\\n\\n- [#5025](https:\\u002f\\u002fgithub...\"],[\"## 1.1.0\\n\\n### Features\\n\\n- [#4995](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4995) [`3f8c210b`](https...\"],[\"Gradio Demo: kitchen_sink\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"def fn(\\n    text1,\\n    text2,\\n    num,\\n    slider1,\\n    slider2,\\n    single_checkbox,\\n    checkboxes...\"],[\"(\\\"testing\\\", None),\\n            (\\\"lazy\\\", -0.1),\\n            (\\\"dogs\\\", 0.4),\\n            (\\\".\\\", 0),\\n    ...\"],[\"demo = gr.Interface(\\n    fn,\\n    inputs=[\\n        gr.Textbox(value=\\\"Lorem ipsum\\\", label=\\\"Textbox\\\"),\\n...\"],[\"[\\n            \\\"the quick brown fox\\\",\\n            \\\"jumps over the lazy dog\\\",\\n            10,\\n        ...\"],[\"if __name__ == \\\"__main__\\\":\\n    demo.launch()\\n\\n```...\"],[\"`@gradio\\u002futils`\\n\\nGeneral functions for handling events in Gradio Svelte components\\n\\n\\n```javascript\\ne...\"],[\"Gradio Demo: audio_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Bl...\"],[\"Gradio Demo: musical_instrument_identification\\n### This demo identifies musical instruments from an ...\"],[\"demo = gr.Interface(fn=predict,\\n                    inputs=gr.Audio(type=\\\"filepath\\\"),\\n              ...\"],[\"Gradio Demo: change_vs_input\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the de...\"],[\"with gr.Column(min_width=200):\\n            gr.Markdown(\\\"# ON:CHANGE\\\")\\n            text_ch = gr.Textb...\"],[\"text.input(lambda x:x, text, text_in)\\n    num.input(lambda x:x, num, num_in)\\n    slider.input(lambda...\"],[\"text_ch.change(lambda x:x, text_ch, text_ch2)\\n    num_ch.change(lambda x:x, num_ch, num_ch2)\\n    sli...\"],[\"@gradio\\u002fchatbot\\n\\n## 0.5.5\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`846d52d`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.5.0\\n\\n### Features\\n\\n- [#6537](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6537) [`6d3fecfa4`](http...\"],[\"## 0.4.3\\n\\n### Fixes\\n\\n- [#6316](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6316) [`4b1011bab`](https:\\u002f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.4.0-beta.9\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.4.0-beta.7\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.4.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`ee8eec1e5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.0\\n\\n### Highlights\\n\\n#### Like\\u002fDislike Button for Chatbot ([#5391](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"### Fixes\\n\\n- [#5242](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5242) [`2b397791`](https:\\u002f\\u002fgithub.com...\"],[\"@gradio\\u002fbutton\\n\\n## 0.2.13\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com...\"],[\"### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.5\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"Gradio Demo: dataframe_colorful\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport pandas as pd \\nimport ...\"],[\"How to Style the Gradio Dataframe\\n\\nTags: DATAFRAME, STYLE, COLOR\\n\\n## Introduction\\n\\nData visualizatio...\"],[\"# Displaying the styled dataframe in Gradio\\nwith gr.Blocks() as demo:\\n    gr.DataFrame(styler)\\n    \\n...\"],[\"Apart from highlighting cells, you might want to color specific text within the cells. Here's how yo...\"],[\"In this script, the format method of the Styler object is used to set the precision of numbers to tw...\"],[\"Image Classification with Vision Transformers\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlab...\"],[\"## Step 2 — Loading the Vision Transformer Model with Gradio\\n\\nWhen using a model from the Hugging Fa...\"],[\"Gradio Demo: text_generation\\n### This text generation demo takes in input text and returns generated...\"],[\"Gradio Demo: diffusers_with_batching\\n\\n\\n```\\n!pip install -q gradio torch transformers diffusers\\n```\\n\\n...\"],[\"Gradio Demo: depth_estimation\\n### A demo for predicting the depth of an image and generating a 3D mo...\"],[\"# forward pass\\n    with torch.no_grad():\\n        outputs = model(**encoding)\\n        predicted_depth...\"],[\"pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\\n        rgbd_image, camera_intrinsic)\\n\\n    pri...\"],[\"title = \\\"Demo: zero-shot depth estimation with DPT + 3D Point Cloud\\\"\\ndescription = \\\"This demo is a v...\"],[\"Using Gradio for Tabular Data Science Workflows\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fsciki...\"],[\"def infer(input_dataframe):\\n  return pd.DataFrame(model.predict(input_dataframe))\\n\\ngr.Interface(fn =...\"],[\"inputs = [gr.Dataframe(label=\\\"Supersoaker Production Data\\\")]\\noutputs = [gr.Gallery(label=\\\"Profiling ...\"],[\"gr.load(\\\"huggingface\\u002fscikit-learn\\u002ftabular-playground\\\", title=title, description=description).launch(...\"],[\"Controlling Layout\\n\\nBy default, Components in Blocks are arranged vertically. Let's take a look at h...\"],[\"$code_rows_and_columns\\n$demo_rows_and_columns\\n\\nSee how the first column has two Textboxes arranged v...\"],[\"## Tabs and Accordions\\n\\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any co...\"],[\"The solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the co...\"],[\"gradio\\n\\n## 4.11.0\\n\\n### Features\\n\\n- [#6842](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6842) [`846d52d...\"],[\"### Fixes\\n\\n- [#6829](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6829) [`50496f9`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"### Fixes\\n\\n- [#6799](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6799) [`c352811`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"### Fixes\\n\\n- [#6525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6525) [`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6680](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6680) [`cfd5700`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6569](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6569) [`4d1cbbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"### Fixes...\"],[\"- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6556](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6556) [`d76bcaa`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.8.0\\n\\n### Features\\n\\n- [#6624](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6624) [`1751f14`](https:...\"],[\"- [#6607](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6607) [`13ace03`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6550](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6550) [`3156598`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.7.1\\n\\n### Features\\n\\n- [#6537](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6537) [`6d3fecfa4`](http...\"],[\"- [#6532](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6532) [`96290d304`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6518](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6518) [`d4e3a5189`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6528](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6528) [`f53b01cbf`](https:\\u002f\\u002fgithub.co...\"],[\"A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Fixes\\n\\n- [#6497](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#6428](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6428) [`ac4ca59c9`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6456](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6456) [`3953a1467`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6441](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6441) [`2f805a7dd`](https:\\u002f\\u002fgithub.co...\"],[\"## 4.3.0\\n\\n### Features\\n\\n- [#6395](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6395) [`8ef48f852`](http...\"],[\"- [#6412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6412) [`649f3ceb6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6379](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6379) [`de998b281`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.2.0\\n\\n### Features\\n\\n- [#6333](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6333) [`42f76aeeb`](http...\"],[\"### Fixes\\n\\n- [#6368](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6368) [`8a3f45c26`](https:\\u002f\\u002fgithub.co...\"],[\"- [#6310](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6310) [`dfdaf1092`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6314](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6314) [`fad92c29d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.1.1\\n\\n### Fixes\\n\\n- [#6288](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6288) [`92278729e`](https:\\u002f...\"],[\"- [#6261](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6261) [`8bbeca0e7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6255](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6255) [`e3ede2ff7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes...\"],[\"- [#6266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6266) [`e32bac894`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6229](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6229) [`5cddd6588`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.0.2\\n\\n### Fixes\\n\\n- [#6191](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6191) [`b555bc09f`](https:\\u002f...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fmedia0.giphy.com\\u002fmedia\\u002fKv1bAN7MX3ya5krkEU\\u002fgiphy.gif\\\"\\u003e\\n\\n**3. Server Side Events**: ...\"],[\"Gradio 4.0 is a new major version, and includes breaking changes from 3.x. Here's a list of all the ...\"],[\"**Other changes related to the `gradio` library**:\\n\\n* Removes the deprecated `status_tracker` parame...\"],[\"For example, if your code looks like this:\\n\\n```py\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n   ...\"],[\"In Gradio 4.0, the `concurrency_count` parameter has been removed. You can still control the number ...\"],[\"To summarize migration:\\n\\n* For events that execute quickly or don't use much CPU or GPU resources, y...\"],[\"```py\\ngr.Image(source=\\\"canvas\\\", tools=\\\"sketch\\\")\\n```\\n\\nNow, you should write:\\n\\n```py\\ngr.ImageEditor(so...\"],[\"- [#6184](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6184) [`86edc0199`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6171](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6171) [`28322422c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6118](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6118) [`88bccfdba`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6069](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6069) [`bf127e124`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5955](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5955) [`825c9cddc`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6073](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6073) [`abff6fb75`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6077](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6077) [`35a227fbf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.0-beta.13\\n\\n### Features\\n\\n- [#5964](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5964) [`5fbda0b...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5937](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5937) [`dcf13d750`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.co...\"],[\"- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5819](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5819) [`5f1cbc436`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes...\"],[\"- [#5840](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5840) [`4e62b8493`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5897](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5897) [`0592c301d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.47.1\\n\\n### Fixes\\n\\n- [#5816](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5816) [`796145e2c`](https:...\"],[\"For more information check the [`FileExplorer` documentation](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002ffileexplorer)....\"],[\"### Fixes\\n\\n- [#5798](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5798) [`a0d3cc45c`](https:\\u002f\\u002fgithub.co...\"],[\"### Fixes\\n\\n- [#5775](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5775) [`e2874bc3c`](https:\\u002f\\u002fgithub.co...\"],[\"### Fixes\\n\\n- [#5735](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5735) [`abb5e9df4`](https:\\u002f\\u002fgithub.co...\"],[\"## 3.45.2\\n\\n### Features\\n\\n- [#5722](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5722) [`dba651904`](htt...\"],[\"- [#5714](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5714) [`a0fc5a296`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5732](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5732) [`3a48490bc`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.1\\n\\n### Fixes\\n\\n- [#5701](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5701) [`ee8eec1e5`](https:...\"],[\"- [#5675](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5675) [`b619e6f6e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5642](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5642) [`21c7225bd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5240) [`da05e59a5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes...\"],[\"- [#5625](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5625) [`9ccc4794a`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5690](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5690) [`6b8c8afd9`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.44.4\\n\\n### Features\\n\\n- [#5514](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5514) [`52f783175`](htt...\"],[\"## 3.44.3\\n\\n### Fixes\\n\\n- [#5562](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5562) [`50d9747d0`](https:...\"],[\"## 3.44.1\\n\\n### Fixes\\n\\n- [#5516](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5516) [`c5fe8eba`](https:\\u002f...\"],[\"- [#5505](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5505) [`9ee20f49`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5510](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5510) [`afcf3c48`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5459](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5459) [`bd2fda77`](https:\\u002f\\u002fgithub.com...\"],[\"## 3.43.0\\n\\n### Features\\n\\n- [#5165](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5165) [`c77f05ab`](http...\"],[\"Thanks [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82)!\\n\\n#### Added the ability to attach event lis...\"],[\"Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n### Features\\n\\n- [#5334](https:\\u002f\\u002fgithub.com\\u002fgradi...\"],[\"### Fixes\\n\\n- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com...\"],[\"## 3.41.2\\n\\n### Features\\n\\n- [#5284](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5284) [`5f25eb68`](http...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"We now have an event `render` on the \\u003cgradio-app\\u003e web component, which is triggered once the embedde...\"],[\"- [#5268](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5268) [`f49028cf`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5280](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5280) [`a2f42e28`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5221](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5221) [`f344592a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes...\"],[\"- [#5256](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5256) [`933db53e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5312](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5312) [`f769cb67`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5276](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5276) [`502f1015`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.40.0\\n\\n### Highlights\\n\\n#### Client.predict will now return the final output for streaming endpoi...\"],[\"demo.queue().launch()\\n```\\n\\nFrom the backend, streamed outputs are served from the `\\u002fstream\\u002f` endpoin...\"],[\"- [#5081](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5081) [`d7f83823`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5076](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5076) [`2745075a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5136) [`eaa1ce14`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes...\"],[\"- [#5062](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5062) [`7d897165`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5061](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5061) [`136adc9c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.39.0\\n\\n### Highlights\\n\\n#### Create Discord Bots from Gradio Apps 🤖 ([#4960](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"But once again, you can deploy ANY `gr.ChatInterface` app exposed on the internet! So don't hesitate...\"],[\"## 3.38\\n\\n### New Features:\\n\\n- Provide a parameter `animate` (`False` by default) in `gr.make_wavefor...\"],[\"### Bug Fixes:\\n\\n- Fixes `cancels` for generators so that if a generator is canceled before it is com...\"],[\"And a corresponding easy-to-use API at `\\u002fchat`:\\n\\n\\u003cimg width=\\\"1164\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fgithub.c...\"],[\"### Bug Fixes:\\n\\n- The `.change()` event is fixed in `Video` and `Image` so that it only fires once b...\"],[\"### Other Changes:\\n\\n- Warning on mobile that if a user leaves the tab, websocket connection may brea...\"],[\"### Other Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3....\"],[\"- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg...\"],[\"- Fixes an HTML sanitization issue in DOMPurify where links in markdown were not opening in a new wi...\"],[\"- Removed uncessessary `type` deprecation warning by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyabou...\"],[\"### Other Changes:\\n\\n- Add `.git-blame-ignore-revs` by [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in [PR 4586](ht...\"],[\"## 3.35.2\\n\\n### New Features:\\n\\nNo changes to highlight.\\n\\n### Bug Fixes:\\n\\n- Fix chatbot streaming by [...\"],[\"demo.launch()\\n```\\n\\n- Min and max value for gr.Number by [@artegoser](https:\\u002f\\u002fgithub.com\\u002fartegoser) a...\"],[\"- Add support for PAUSED state in the JS client by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 4...\"],[\"- `show_label` will not automatically be set to `True` in `gr.BarPlot.update` by [@freddyaboulton](h...\"],[\"### Other Changes:\\n\\n- Change styling of status and toast error components by [@hannahblair](https:\\u002f\\u002f...\"],[\"### Breaking Changes:\\n\\n- The behavior of the `Clear` button has been changed for `Slider`, `Checkbox...\"],[\"### Bug Fixes:\\n\\n- Remove target=\\\"\\\\_blank\\\" override on anchor tags with internal targets by [@hannahb...\"],[\"### Other Changes:\\n\\n- When running on Spaces, handler functions will be transformed by the [PySpaces...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.33.0\\n\\n### New Features:\\n\\n- Introduced `gradio ...\"],[\"### Bug Fixes:\\n\\n- Fix bug where Label change event was triggering itself by [@freddyaboulton](https:...\"],[\"### Other Changes:\\n\\n- Remove flicker of loading bar by adding opacity transition, by [@aliabid94](ht...\"],[\"### Bug Fixes:\\n\\n- Fixed Gallery\\u002fAnnotatedImage components not respecting GRADIO_DEFAULT_DIR variable...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.31.0\\n\\n### New Features:\\n\\n- The reloader comman...\"],[\"### Bug Fixes:\\n\\n- Fix \\\"TypeError: issubclass() arg 1 must be a class\\\" When use Optional[Types] by [@...\"],[\"### Other Changes:\\n\\n- Change `gr.Chatbot()` markdown parsing to frontend using `marked` library and ...\"],[\"### Bug Fixes:\\n\\n- Records username when flagging by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR ...\"],[\"- Returning language agnostic types in the `\\u002finfo` route by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffre...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"## 3.28.2\\n\\n### Bug Fixes\\n\\n- Code component visual updates by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\n- `gr.Hugg...\"],[\"- Fix duplicate play commands in full-screen mode of 'video'. by [@tomchang25](https:\\u002f\\u002fgithub.com\\u002fto...\"],[\"- Fix `gr.ChatBot` to handle image url [tye-singwa](https:\\u002f\\u002fgithub.com\\u002ftye-signwa) in [PR 3953](http...\"],[\"### Documentation Changes:\\n\\n- Make use of `gr` consistent across the docs by [@duerrsimon](https:\\u002f\\u002fg...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.27.0\\n\\n### New Features:\\n\\n###### Annotated...\"],[\"```py\\nwith gr.Blocks() as demo:\\n    gr.Video((\\\"video.mp4\\\", \\\"captions.srt\\\"))\\n```\\n\\n### Bug Fixes:\\n\\n- F...\"],[\"![Recording 2023-04-08 at 17 44 39](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f230748572-90a2...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"```python\\n  import gradio as gr\\n  gr.themes.builder()\\n  ```\\n\\n  ![Theme Builder](https:\\u002f\\u002fuser-images....\"],[\"- Fixed bug where text for altair plots was not legible in dark mode by [@freddyaboulton](https:\\u002f\\u002fgi...\"],[\"- Support using an empty list as `gr.Dataframe` value, by [@space-nuko](https:\\u002f\\u002fgithub.com\\u002fspace-nuk...\"],[\"### Documentation Changes:\\n\\n- Makes some fixes to the Theme Guide related to naming of variables, by...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Mobile responsive iframes in...\"],[\"By [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3428](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"###### `elem_classes`\\n\\nAdd keyword argument `elem_classes` to Components to control class names of c...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.21.0\\n\\n### New Features:\\n\\n###### Theme Sha...\"],[\"by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3428](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    gallery = gr.Gallery([\\\"images\\u002f1.jpg\\\", \\\"...\"],[\"### Documentation Changes:\\n\\n- Added a section on security and access when sharing Gradio apps by [@a...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Prevent in-place updates of ...\"],[\"No changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Bre...\"],[\"By [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3297](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"- Ensure `mirror_webcam` is always respected by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 3245](http...\"],[\"- Fix change event listed twice in image docs by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3318](h...\"],[\"### Documentation Changes:\\n\\n- Added the `types` field to the dependency field in the config by [@fre...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 3205](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f3205)\\n\\n...\"],[\"```python\\ngr.Chatbot([(\\\"Hi, I'm DialoGPT. Try asking me a question.\\\", None)])\\n```\\n\\nBy [@dawoodkhan82...\"],[\"### Documentation Changes:\\n\\n- Sort components in docs by alphabetic order by [@aliabd](https:\\u002f\\u002fgithu...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### New Features:\\n\\n###### Revamped Stop Button for Interfaces 🛑\\n\\nIf your Interface function is a gen...\"],[\"- Fixes URL resolution on Windows by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 3108](https:\\u002f\\u002fg...\"],[\"- Fixed bug where the font color of axis labels and titles for native plots did not respond to dark ...\"],[\"### Documentation Changes:\\n\\n- Added a guide on the 4 kinds of Gradio Interfaces by [@yvrjsharma](htt...\"],[\"###### Run on Kaggle kernels 🧪\\n\\nA share link will automatically be created when running on Kaggle ke...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"By [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 3014](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"- Fixes bug where interpretation event was not configured correctly by [@freddyaboulton](https:\\u002f\\u002fgit...\"],[\"- Fix forwarding for guides after SEO renaming by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3017](...\"],[\"- Adding missing embedded components on docs by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3027](ht...\"],[\"- Fixes bug where tabs selected attribute not working if manually change tab by [@tomchang25](https:...\"],[\"### Documentation Changes:\\n\\n- SEO improvements to guides by[@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.16.2\\n\\n### New Features:\\n\\nNo changes to hi...\"],[\"No changes to highlight.\\n\\n### Full Changelog:\\n\\nNo changes to highlight.\\n\\n### Contributors Shoutout:\\n...\"],[\"```python\\ngr.Dropdown([\\\"angola\\\", \\\"pakistan\\\", \\\"canada\\\"], multiselect=True, value=[\\\"angola\\\"])\\n```\\n\\n\\u003cim...\"],[\"### Documentation Changes:\\n\\n- Added a Guide on using Google Sheets to create a real-time dashboard w...\"],[\"For an example of the api see below:\\n\\n```python\\ngr.LinePlot(stocks,\\n            x=\\\"date\\\",\\n          ...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"```py\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    df = gr.DataFrame(run_query, every=60*60)\\n\\n...\"],[\"You don't need to do anything differently, but when you set `share=True` in `launch()`,\\nyou'll get t...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n- Fixed typo in parameter `visible` in classes...\"],[\"```python\\nimport gradio as gr\\nimport altair as alt\\nfrom vega_datasets import data\\n\\ncars = data.cars(...\"],[\"###### Add Brazilian Portuguese translation\\n\\nAdd Brazilian Portuguese translation (pt-BR.json) by [@...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.12.0\\n\\n### New Features:\\n\\n###### The `Chat...\"],[\"```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Accordion(label=\\\"Open for greet...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"demo.launch()\\n```\\n\\n###### Revamped API documentation page\\n\\nNew API Docs page with in-browser playgro...\"],[\"### Documentation Changes:\\n\\n- Updated documentation for embedding Gradio demos on Spaces as web comp...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.10.0\\n\\n- Add support for `'password'` and ...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.9.1\\n\\n### New Features:\\n\\nNo changes to hig...\"],[\"###### Calling functions by api_name in loaded apps\\n\\nWhen you load an upstream app with `gr.Blocks.l...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.8.2\\n\\n### Bug Fixes:\\n\\n- Ensure gradio apps...\"],[\"![live_demo](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f198357377-633ce460-4e31-47bd-8202-14...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\\n\\ndemo.queue()\\ndem...\"],[\"### Documentation Changes:\\n\\n- Added an example interactive dashboard to the \\\"Tabular & Plots\\\" sectio...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.6\\n\\n### New Features:\\n\\n###### Cancelling R...\"],[\"gr.Interface(iteration,\\n             inputs=gr.Slider(minimum=1, maximum=10, step=1, value=5),\\n     ...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"[@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 2459](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f2459)\\n\\n###...\"],[\"- Speeds up Gallery component by using temporary files instead of base64 representation in the front...\"],[\"- Change \\\"return ValueError\\\" to \\\"raise ValueError\\\" by [@vzakharov](https:\\u002f\\u002fgithub.com\\u002fvzakharov) in ...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4.1\\n\\n### New Features:\\n\\n###### 1. See Pas...\"],[\"### Documentation Changes:\\n\\n1. New Guide: Connecting to a Database 🗄️\\n\\n   A new guide by [@freddyabo...\"],[\"- Create a guide on how to connect an app to a database hosted on the cloud by [@freddyaboulton](htt...\"],[\"- Fixed small typos in the docs [@julien-c](https:\\u002f\\u002fgithub.com\\u002fjulien-c) in [PR 2373](https:\\u002f\\u002fgithub...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4\\n\\n### New Features:\\n\\n###### 1. Gallery C...\"],[\"![color-sketch](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192410500-3c8c3e64-a5fd-4df2-a991-...\"],[\"### Documentation Changes:\\n\\n1. Adding a Playground Tab to the Website by [@aliabd](https:\\u002f\\u002fgithub.co...\"],[\"- Website fixes and refactoring by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 2280](https:\\u002f\\u002fgithub....\"],[\"- release 3.4b3 by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 2328](https:\\u002f\\u002fgithub.com\\u002fgradio-a...\"],[\"### Contributors Shoutout:\\n\\n- [@SkyTNT](https:\\u002f\\u002fgithub.com\\u002fSkyTNT) made their first contribution in ...\"],[\"![187936493-5c90c01d-a6dd-400f-aa42-833a096156a1](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f...\"],[\"- safari fixes by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 2138](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- Fix bugs with gr.update by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 2157](https...\"],[\"### Contributors Shoutout:\\n\\n- [@lamhoangtung](https:\\u002f\\u002fgithub.com\\u002flamhoangtung) made their first cont...\"],[\"###### 3. Component Fixes 🧱\\n\\n- Specify the width and height of an image in its style tag (thanks to ...\"],[\"```python\\nimport gradio as gr\\ndemo = gr.Interface(lambda x:x, gr.Slider(0, 10, randomize=True), \\\"num...\"],[\"- Reset components to original state by setting value to None by [@freddyaboulton](https:\\u002f\\u002fgithub.co...\"],[\"- Fix TimeSeries examples not properly displayed in UI by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodk...\"],[\"### Contributors Shoutout:\\n\\n- [@chrisemezue](https:\\u002f\\u002fgithub.com\\u002fchrisemezue) made their first contri...\"],[\"If your demo code is in a script named `app.py`, instead of running `python app.py` you can now run ...\"],[\"gr.Examples takes two required parameters:\\n\\n- `examples` which takes in a nested list\\n- `inputs` whi...\"],[\"- File component: list multiple files and allow for download #1446 by [@dawoodkhan82](https:\\u002f\\u002fgithub...\"],[\"- Remove usage of deprecated gr.inputs and gr.outputs from website by [@freddyaboulton](https:\\u002f\\u002fgith...\"],[\"### Contributors Shoutout:\\n\\n- [@nhankiet](https:\\u002f\\u002fgithub.com\\u002fnhankiet) made their first contribution...\"],[\"![kitchen_sink](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f168686333-7a6e3096-3e23-4309-abf2-...\"],[\"- Gradio dash fe by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 807](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- Fixing external.py in blocks-dev to reflect the new HF Spaces paths by [@abidlabs](https:\\u002f\\u002fgithub....\"],[\"- Restore Interpretation, Live, Auth, Queueing by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR ...\"],[\"- Blocks analytics by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 947](https:\\u002f\\u002fgithub.com\\u002fgradio...\"],[\"- indentation fix by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 993](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"- Model3D + Plot Components by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 1010](https:\\u002f...\"],[\"- Slackbot web tracker fix by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1043](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"- Update PULL_REQUEST_TEMPLATE.md by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 1068](h...\"],[\"- Website Reload: README in demos docker by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1100](https:...\"],[\"- ONNX guide fixes by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1131](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- Issue #768: Support passing none to resize and crop image by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fda...\"],[\"- use secondary buttons in interface by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1173](https:\\u002f\\u002fgith...\"],[\"- Automatic word-break in highlighted text, combine_adjacent support by [@aliabid94](https:\\u002f\\u002fgithub....\"],[\"- Layout bugs by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1246](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"- update logo by [@gary149](https:\\u002f\\u002fgithub.com\\u002fgary149) in [PR 1266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fg...\"],[\"- New meta img by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1289](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"### Contributors Shoutout:\\n\\n- [@JefferyChiang](https:\\u002f\\u002fgithub.com\\u002fJefferyChiang) made their first co...\"],[\"Build a Custom Multimodal Chatbot - Part 1\\n\\nThis is the first in a two part series where we build a ...\"],[\"For our component, each chatbot message will consist of two keys: a `text` key that displays the tex...\"],[\"We can leave the `postprocess` method as is and modify the `_postprocess_chat_messages`\\n\\n```python\\nd...\"],[\"export let value: [\\n    MultimodalMessage | null,\\n    MultimodalMessage | null\\n][] = [];\\n\\nlet _value...\"],[\"Now for the fun part, actually rendering the text and files in the same message!\\n\\nYou should see som...\"],[\"We will modify this code to always display the text message and then loop through the files and disp...\"],[\"The demo code will look like the following:\\n\\n```python\\nimport gradio as gr\\nfrom gradio_multimodalcha...\"],[\"Creating a Real-Time Dashboard from BigQuery Data\\n\\nTags: TABULAR, DASHBOARD, PLOTS\\n\\n[Google BigQuery...\"],[\"1. First, log in to your Google Cloud account and go to the Google Cloud Console (https:\\u002f\\u002fconsole.cl...\"],[\"```bash\\npip install google-cloud-bigquery[pandas]\\n```\\n\\nYou'll notice that we've installed the pandas...\"],[\"```py\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    gr.DataFrame(run_query, every=60*60)\\n\\ndemo....\"],[\"Gradio Demo: sepia_filter\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\nimport gradio a...\"],[\"`@gradio\\u002fimage`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseImageUploader, BaseStaticImage, Webcam, BaseExample ...\"],[\"`@gradio\\u002faudio`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseStaticAudio, BaseInteractiveAudio, BasePlayer, BaseE...\"],[\"Gradio Demo: blocks_multiple_event_triggers\\n\\n\\n```\\n!pip install -q gradio plotly pypistats\\n```\\n\\n\\n```\\n...\"],[\"!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides\\u002f1)getting_start...\"],[\"```bash\\npip install gradio\\n```\\n\\n2\\\\. 用Python脚本或在Jupyter Notebook中运行下面的代码 （或者使用 [Google Colab](https:\\u002f...\"],[\"![`hello_world_3` demo](..\\u002f..\\u002fdemo\\u002fhello_world_3\\u002fscreenshot.gif)\\n\\n您只需将组件包装在列表中。输入列表`inputs`中的每个组件依次对...\"],[\"#### 更多复杂性\\n\\n这里有一个应用程序可以让你感受一下`Blocks`的更多可能：\\n\\n```python\\nimport numpy as np\\nimport gradio as gr\\n\\ndef f...\"],[\"## 引用\\n\\n另外请参阅论文 _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https:\\u002f\\u002farxiv.org...\"],[\"More on Examples\\n\\nIn the [previous Guide](\\u002fmain\\u002fguides\\u002fthe-interface-class), we discussed how to pro...\"],[\"## Caching examples\\n\\nYou may wish to provide some cached examples of your model for users to quickly...\"],[\"Backend Testing Guidelines\\n\\n- All the tests should test Backend functionalities. Frontend functional...\"],[\"Gradio and ONNX on Hugging Face\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fonnx\\u002fEfficientNet-Lit...\"],[\"Get started [here](https:\\u002f\\u002fgradio.app\\u002fgetting_started)\\n\\n### Hugging Face Spaces\\n\\nHugging Face Spaces...\"],[\"ONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live ...\"],[\"# loads ONNX model from ONNX Model Zoo\\nmodel = hub.load(\\\"efficientnet-lite4\\\")\\n# loads the labels tex...\"],[\"title = \\\"EfficientNet-Lite4\\\"\\ndescription = \\\"EfficientNet-Lite 4 is the largest variant and most accu...\"],[\"# 使用 Gradio 进行表格数据科学工作流\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fscikit-learn\\u002fgradio-skops-int...\"],[\"\\u003cgradio-app space=\\\"gradio\\u002ftabular-playground\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\n```python\\nimport gradio as gr\\nimport pa...\"],[\"his translation demo takes in the text, source and target languages, and returns the translation. It...\"],[\"Gradio Demo: model3D\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\n...\"],[\"@gradio\\u002flite\\n\\n## 0.4.4\\n\\n## 0.4.4-beta.0\\n\\n### Features\\n\\n- [#6147](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.3.0\\n\\n### Minor Changes\\n\\n- [#4785](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4785) [`da0e9447`](...\"],[\"## 0.2.0\\n\\n### Minor Changes\\n\\n- [#4732](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4732) [`1dc3c1a9`](...\"],[\"@gradio\\u002fvideo\\n\\n## 0.2.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.1.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.2\\n\\n### Fixes\\n\\n- [#6234](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6234) [`aaa55ce85`](https:\\u002f...\"],[\"## 0.1.0-beta.9\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"### Fixes\\n\\n- [#6067](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6067) [`bf38e5f06`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.0.11\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n  - @gradio\\u002fatoms@0...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"Gradio Demo: outbreak_forecast\\n### Generate a plot based on 5 inputs.\\n        \\n\\n\\n```\\n!pip install -q...\"],[\"demo = gr.Interface(\\n    fn=outbreak,\\n    inputs=inputs,\\n    outputs=outputs,\\n    examples=[\\n       ...\"],[\"@gradio\\u002ffile\\n\\n## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"## 0.2.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`9caddc17b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f2ba14b284f908aa138...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"This component allows you to populate the explorer by passing a glob, but only provides the selected...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"### Fixes\\n\\n- [#5253](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5253) [`ddac7e4d`](https:\\u002f\\u002fgithub.com...\"],[\"Gradio Demo: blocks_flashcards\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport random\\n\\nimport gradio ...\"],[\"def flip_card(card):\\n        return card[1], gr.Column(visible=True)\\n\\n    flip_btn.click(flip_card, ...\"],[\"`@gradio\\u002fjson`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseJSON } from \\\"@gradio\\u002fjson\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nBaseJSON\\n`...\"],[\"使用Gradio JavaScript客户端快速入门\\n\\nTags: CLIENT, API, SPACES\\n\\nGradio JavaScript客户端使得使用任何Gradio应用作为API非常简单。例...\"],[\"`duplicate`与`client`几乎相同，唯一的区别在于底层实现：\\n\\n```js\\nimport { client } from \\\"@gradio\\u002fclient\\\";\\n\\nconst respons...\"],[\"## 进行预测\\n\\n进行预测的最简单方法就是使用适当的参数调用`.predict()`方法：\\n\\n```js\\nimport { client } from \\\"@gradio\\u002fclient\\\";\\n\\nconst...\"],[\"job.on(\\\"status\\\", log_status);\\n```\\n\\n## 取消作业\\n\\n作业实例还具有`.cancel()`方法，用于取消已排队但尚未启动的作业。例如，如果您运行以下命令：\\n\\n```j...\"],[\"@gradio\\u002ftootils\\n\\n## 0.1.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.1.0-beta.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.0.2\\n\\n### Highlights\\n\\n#### Improve startup performance and markdown support ([#5279](https:\\u002f\\u002fgit...\"],[\"Quickstart\\n\\nGradio is an open-source Python package that allows you to quickly **build** a demo or w...\"],[\"$demo_hello_world_4\\n\\nType your name in the textbox on the left, drag the slider, and then press the ...\"],[\"Tip: For the `inputs` and `outputs` arguments, you can pass in the name of these components as a str...\"],[\"## An Overview of Gradio\\n\\nSo far, we've been discussing the `Interface` class, which is a high-level...\"],[\"* [Gradio Python Client](https:\\u002f\\u002fwww.gradio.app\\u002fguides\\u002fgetting-started-with-the-python-client) (`gra...\"],[\"如何创建一个聊天机器人\\n\\nTags: NLP, TEXT, CHAT\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002fchatbot_stre...\"],[\"最后，我们通过运行 `demo.queue()` 启用排队，这对于流式中间输出是必需的。您可以通过滚动到本页面顶部的演示来尝试改进后的聊天机器人。\\n\\n## 添加 Markdown、图片、音频或视频\\n\\n...\"],[\"Gradio Demo: chatbot_streaming\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ra...\"],[\"Gradio Demo: upload_button\\n### A simple demo showcasing the upload button used with its `upload` eve...\"],[\"Gradio Demo: button_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"Gradio Demo: image_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"`@gradio\\u002fcheckbox`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseCheckbox } from \\\"@gradio\\u002fcheckbox\\\";\\n\\u003c\\u002fscript\\u003e\\n...\"],[\"Gradio Demo: dropdown_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith g...\"],[\"Gradio Demo: filter_records\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef filter...\"],[\"@gradio\\u002fdropdown\\n\\n## 0.4.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"### Fixes\\n\\n- [#6065](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6065) [`7d07001e8`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.1\\n\\n### Fixes\\n\\n- [#5525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5525) [`21f1db40`](https:\\u002f\\u002f...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"## 0.0.2\\n\\n### Fixes\\n\\n- [#5062](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5062) [`7d897165`](https:\\u002f\\u002f...\"],[\"Gradio Demo: autocomplete\\n### This text generation demo works like autocomplete. There's only one te...\"],[\"Gradio Demo: generate_english_german\\n\\n\\n```\\n!pip install -q gradio transformers torch\\n```\\n\\n\\n```\\nimpor...\"],[\"Gradio Demo: gallery_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\n...\"],[\"Gradio Demo: theme_builder\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.th...\"],[\"Gradio Demo: blocks_inputs\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: html_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Bl...\"],[\"Gradio Demo: checkboxgroup_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nw...\"],[\"Gradio Demo: state_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.B...\"],[\"`@gradio\\u002ftextbox`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseTextbox, BaseExample } from \\\"@gradio\\u002ftextbox\\\";\\n...\"],[\"Gradio Demo: clear_components\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"images = [\\n    \\\"https:\\u002f\\u002fimages.unsplash.com\\u002fphoto-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=Mnw...\"],[\"components = [\\n    gr.Textbox(value=lambda: datetime.now(), label=\\\"Current Time\\\"),\\n    gr.Number(val...\"],[\"),\\n    ),\\n    gr.JSON(value=lambda: random.choice([{\\\"a\\\": 1}, {\\\"b\\\": 2}])),\\n    gr.HTML(\\n        value...\"],[\"def evaluate_values(*args):\\n    are_false = []\\n    for a in args:\\n        if isinstance(a, (pd.DataF...\"],[\"快速开始\\n\\n**先决条件**：Gradio 需要 Python 3.8 或更高版本，就是这样！\\n\\n## Gradio 是做什么的？\\n\\n与他人分享您的机器学习模型、API 或数据科学流程的*最佳方式之一...\"],[\"$code_sepia_filter\\n$demo_sepia_filter\\n\\n使用 `Image` 组件作为输入时，您的函数将接收到一个形状为`（高度，宽度，3）` 的 NumPy 数组，其中最后一个...\"],[\"@gradio\\u002fstorybook\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6451](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6451)...\"],[\"Sharing Your App\\n\\nHow to share your Gradio app:\\n\\n1. [Sharing demos with the share parameter](#sharin...\"],[\"Tip: Keep in mind that share links are publicly accessible, meaning that anyone can use your model f...\"],[\"\\u003cvideo autoplay muted loop\\u003e\\n  \\u003csource src=\\\"https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fblob\\u002fmain\\u002fguides\\u002fass...\"],[\"```html\\n\\u003cgradio-app\\n\\tsrc=\\\"https:\\u002f\\u002fabidlabs-pytorch-image-classifier.hf.space\\\"\\n\\u003e\\u003c\\u002fgradio-app\\u003e\\n```\\n\\n\\u003cs...\"],[\"Here's an example of how to use these attributes to create a Gradio app that does not lazy load and ...\"],[\"![Use via API](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fblob\\u002fmain\\u002fguides\\u002fassets\\u002fuse_via_api.png?raw=true...\"],[\"```python\\ndef same_auth(username, password):\\n    return username == password\\ndemo.launch(auth=same_a...\"],[\"with gr.Blocks() as demo:\\n    gr.LoginButton()\\n    gr.LogoutButton()\\n    m1 = gr.Markdown()\\n    m2 =...\"],[\"io = gr.Interface(echo, \\\"textbox\\\", \\\"textbox\\\").launch()\\n```\\n\\nNote: if your function is called directl...\"],[\"- **Cached examples created by Gradio.** These are files that are created by Gradio as part of cachi...\"],[\"`@gradio\\u002ftooltip`\\n\\n```javascript\\nimport { Tooltip } from \\\"@gradio\\u002ftooltip\\\";\\n```\\n\\n```javascript\\n\\texpo...\"],[\"enerate a plot based on 5 inputs....\"],[\"Gradio Demo: stock_forecast\\n\\n\\n```\\n!pip install -q gradio numpy matplotlib\\n```\\n\\n\\n```\\nimport matplotli...\"],[\"simple dashboard showing pypi stats for python libraries. Updates on load, and has no buttons!...\"],[\"Gradio Demo: neon-tts-plugin-coqui\\n### This  demo converts text to speech in 14 languages.\\n        \\n...\"],[\"@gradio\\u002ftooltip\\n\\n## 0.1.0\\n\\n## 0.1.0-beta.2\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"`@gradio\\u002fcode`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseCode, BaseCopy, BaseDownload, BaseWidget, BaseExam...\"],[\"Gradio Demo: gif_maker\\n\\n\\n```\\n!pip install -q gradio opencv-python\\n```\\n\\n\\n```\\nimport cv2\\nimport gradio...\"],[\"Gradio Demo: file_explorer_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading fi...\"],[\"fe = gr.FileExplorer(root=str(base_root \\u002f \\\"dir1\\\"),\\n                         glob=\\\"**\\u002f*\\\", interactive...\"],[\"Gradio Demo: stream_audio\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport numpy a...\"],[\"Gradio Demo: gallery_selections\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport n...\"],[\"Gradio Demo: stream_asr\\n\\n\\n```\\n!pip install -q gradio torch torchaudio transformers\\n```\\n\\n\\n```\\nimport ...\"],[\"his demo takes in 12 inputs from the user in dropdowns and sliders and predicts income. It also has ...\"],[\"`@gradio\\u002flabel`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { BaseLabel } from \\\"@gradio\\u002flabel\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nBaseLab...\"],[\"Contributing a Guide\\n\\nWant to help teach Gradio? Consider contributing a Guide! 🤗\\n\\nBroadly speaking,...\"],[\"That's it! We're looking forward to reading your Guide 🥳...\"],[\"alculate taxes using Textbox, Radio, and Dataframe components...\"],[\"Gradio Demo: json_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Bl...\"]],\"hovertemplate\":\"source=gradio\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"gradio, circle\",\"marker\":{\"color\":\"#ab63fa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"gradio, circle\",\"showlegend\":true,\"x\":[-4.8075633,0.54796785,11.869282,11.949554,12.0617075,-3.081097,-2.4863849,-1.7547424,8.481644,1.7459499,1.6835105,1.8336929,-10.297098,-12.994046,-12.339963,1.1214088,1.548983,3.80634,4.1835914,-4.166876,-3.0320845,6.198552,6.2087913,-3.2524762,-1.975966,6.664341,5.877443,7.321871,7.6204486,-8.5394945,6.7063637,0.046440184,-0.24944532,-0.15257534,0.13056742,-1.5484661,-1.0501398,-3.404148,-3.945902,1.6778604,-3.9057548,-4.8299937,-4.8266,-6.5273767,11.765406,5.8114347,10.537763,10.991728,11.784429,11.672529,3.5811958,3.6639245,3.7428927,3.9098418,-3.6594083,9.065106,3.7085366,4.041926,8.914428,-5.9070635,-3.3250463,-3.5184908,-3.4835033,-3.4233413,-3.175463,-3.271536,-3.3535008,-3.4079757,-3.3882167,-3.3901799,-3.3631322,-3.471046,-3.5066504,-3.434856,-3.4744775,-3.280907,-3.4126945,-2.8481553,-2.8250449,-3.480497,-3.5126162,-3.423822,-3.4066744,-4.4341736,-3.0387993,-3.5453763,-14.955377,-13.606264,-17.416315,-17.923586,12.250971,-2.8537424,-3.1224744,-3.6168592,-4.0865726,-3.8768237,8.389507,-3.1434536,-0.20551229,-0.63960904,7.9629564,-4.0291743,-3.4616878,-4.276991,-4.4470186,6.909305,-6.3661995,-4.650132,-3.0421243,1.4243739,-6.4642625,-2.4261217,7.586471,-3.9128716,-6.355386,3.581424,3.039882,3.0128124,4.905187,-7.2294297,-7.176288,-2.4430199,-6.2408123,-5.723722,2.308704,3.0165126,-5.5700755,3.2130187,-5.5442405,-4.5931697,2.3122299,1.3197185,-2.4164615,-2.4410462,-0.05002851,-2.5621095,-2.5976574,0.3188953,0.7744447,-1.1043416,8.434804,8.420407,8.064617,-0.15947373,0.6887214,-0.35811573,-0.010568173,-1.1781359,0.55190265,1.2409245,-5.2632084,4.4077206,-5.0603275,-5.1272645,-4.791133,-2.5813215,-2.5035627,-3.6508112,-10.063045,0.8637842,2.3469987,3.468468,1.0465163,-1.3071704,-2.254509,-1.7573298,0.123008244,0.23590556,0.7865141,0.7627735,-0.21510674,-2.632361,-0.87923634,0.58607554,-0.5882239,-9.62678,-1.1834528,-6.9312716,-6.754517,-5.4574604,-6.733228,-6.021002,-5.5823255,-14.116854,-15.024601,-0.90517163,-21.729332,-12.423479,-12.583701,7.664517,-12.263833,-12.274141,-12.284004,-12.459867,-12.406223,-12.369317,-12.236573,-12.335108,-12.482024,-12.048436,-12.495095,-12.4151,-12.393013,-12.365828,-12.463461,-12.315127,-12.393013,-12.480648,-12.575264,-12.46707,-12.584601,-12.560191,-12.435842,5.270905,5.300744,5.7452106,4.887948,0.04573418,-0.6668357,-1.384557,0.16608879,7.630715,7.409896,8.576006,7.935854,8.272308,7.908546,8.156712,7.910976,7.8822603,17.561195,17.563118,7.293863,7.9379277,8.264794,-2.8647342,8.657502,8.536798,1.1618555,7.2792034,8.220282,-12.803649,-12.855891,-13.220795,-12.86969,-12.815908,-12.811467,-12.82555,-12.72877,-12.714766,-13.031065,-12.8793335,-12.927123,-12.556628,-12.671632,-13.07341,-12.686245,-12.538461,-13.084862,-12.911685,-12.802449,-12.340818,-12.410368,-12.36519,-12.653168,-12.525152,-12.718595,-13.13832,-12.8548355,-12.88019,-12.645943,-12.859949,-12.868974,-12.536785,-12.379782,-12.705943,-12.757135,-12.76206,-12.356801,-12.969021,-12.749123,-12.331596,-12.847789,-12.893823,-12.935407,-13.105091,-12.633522,-12.565818,-13.340542,-12.515255,-12.802217,-12.798053,-12.2740555,-12.6299095,-12.731703,-12.574302,-12.845864,-12.595134,-12.709875,-12.555638,-12.788267,-12.813307,-12.729442,-5.128319,15.402242,17.171646,16.30506,16.461826,16.32547,16.52695,16.438606,15.067254,16.174845,16.342382,16.259237,16.33351,13.621332,16.254105,1.1605209,9.129384,16.441868,16.281338,16.902779,16.76481,16.465738,16.27947,15.791438,17.036217,16.785633,15.552022,15.781345,16.496325,13.741173,16.886751,15.98491,16.295681,16.830952,16.408249,16.46098,16.45279,16.83848,16.88542,12.386803,15.525681,16.820503,15.934579,15.429212,14.536689,16.451443,16.106382,16.174688,16.23299,16.311722,16.148508,15.99665,16.426691,16.14316,15.988749,16.354301,16.45606,15.69523,16.16987,19.412764,7.3885775,4.142837,-5.3712945,-5.747972,-2.2940211,-4.2145286,8.131071,-4.2014422,3.5596795,3.7008233,10.111022,5.3939986,-6.9785476,-2.617613,3.7437904,3.6208527,1.5019659,1.750127,6.535564,-15.014553,-21.73337,-17.41481,-18.368708,-18.771572,-9.068919,6.842921,3.0549908,-9.065198,-2.4342437,-2.6570847,13.012394,12.415824,-5.8751316,-5.4467764,4.051229,3.4965801,4.2249036,6.1415906,3.395744,1.7426045,4.5128107,-6.2512336,-12.630029,-14.922637,-5.6948986,-2.564915,1.7554299,7.201436,-9.525442,-5.698885,-1.9764286,11.777785,-1.3783692,-2.7944782,-3.9799995,-1.4350626,-3.4308252,-5.9298463,4.6073813,0.083794214,3.6709106,-4.6530204,-4.299289,2.9839532,-4.551262,-1.9404061,-4.042217,-3.6674936,-7.1590915,-7.4892645,-7.3764873,-4.6872287,-4.894782,-2.8571649,4.718781,4.4838195,3.6610243,-7.5152674,-7.330869,-1.5761967,11.789219,-4.0376973,11.0397835,-7.3966355,-4.663272,-3.8165324,2.7728071,1.5551699,2.501871,7.797414,-13.297873,-2.6031468,12.049914,2.9934044,2.7209969,0.33896095,0.43608436,-0.4418646,-0.8455575,-0.7951118,-0.53934824,-1.5048758,0.1301388,-10.394085,0.040301975,0.16953205,0.18641336,4.2225676,0.86750114,-12.643493,-12.761585,-12.820233,-12.619457,6.849487,8.020344,-6.094528,-12.434617,-12.595575,-12.825405,-12.705874,-2.1756701,-0.22224285,-8.580268,-6.8000145,-5.598074,9.030961,-8.503032,18.127102,-6.785133,1.5268779,1.8502697,2.7856817,-7.2304983,17.851038,19.428196,19.955463,17.136742,-7.513884,5.864625,-4.1151357,-4.2533407,0.4160218,-4.5652466,-3.4578497,-4.4220386,-3.859944,-7.9121265,-14.912059,1.1642153,-3.728769,-2.399626,-2.6964483,-2.0439494,-4.2995496,-4.5020795,-3.5045073,-5.250969,-9.755739,-9.328566,1.2479192,1.469014,-1.7278787,0.6205613,-2.9989064,5.99224,-4.234882,9.29722,9.295277,9.303065,9.295717,9.298321,9.298054,9.29786,9.297371,9.296368,7.134327,6.9903326,18.037584,18.139181,17.983395,18.505302,19.81328,17.937674,15.621303,0.2778118,0.6935747,1.0512846,2.116179,-4.09942,7.6467505,10.758504,7.1791186,-3.48048,-7.080284,-10.302112,-2.4578485,8.735922,-7.381594,3.172396,-4.004287,10.749778,8.022221,-6.8987393,-2.5739346,-1.3363296,-2.5240223,0.018791173,-0.24390537,-1.2909185,0.8053584,-3.4626741,-3.2072968,-2.8046052,-2.8706698,-4.2552705,-3.3460066,-4.4084177,-3.774558,4.6970444,0.36717588,-0.6874149,-0.7357453,-0.28179768,-1.6826506,-0.5176648,-1.8710591,12.439789,-2.7447019,-3.6054902,-2.9272325,-3.1720192,0.27460486,12.445865,0.16027594,12.684533,12.014108,-18.191902,-18.514133,-18.517086,-3.9088037,13.200201,13.069846,17.974672,18.51942,-3.3245537,-1.1030153,12.236609,-7.8507223,-12.250477,-12.314859,-12.233987,-12.05978,-12.140854,0.018475434,6.5380855,16.534653,16.40404,5.963453,8.483044,8.131992,12.794581,-1.179745,-2.8337853,-7.7288523,3.8502095,3.7339787,-0.35155213,-8.037363,-7.9619503,-7.5831246,-5.3078146,-7.5900455,-7.6242886,-0.3871496,0.54109365,-0.77551824,-6.120598,-5.657438,-5.6436143,-5.7471557,-5.938103,-4.7604766,-2.1826603,-1.7668549,-12.661225,-12.8377495,-12.686253,-12.341746,-12.832975,-12.951639,-12.910689,-13.07913,12.939337,11.995315,13.27577,13.572137,-5.9249983,-4.083209,-6.944673,-7.2289352,-5.880647,0.7624807,3.9149864,0.61599463,10.1331835,1.4924097,2.058014,1.0039423,6.5933914,17.820135,19.443064,-4.104031,3.9556797,9.115473,8.835675,8.529998,7.7799296,8.979395,5.9643617,-6.0141525,-6.1203723,-5.775163,-2.4037817,-6.854963,-2.8833976,-3.8662813,-2.776512,-3.2540426,-2.8379948,1.4346459,-14.0942955,-1.1285131,-0.9429998,-2.2589946,-4.994506,7.9352155,7.8206415,3.0503416,2.9703734,3.7584896,2.8692255,3.8718407,7.71306,7.755896,7.7093363,7.7094836,2.390032,-1.4725456,-18.63745,-4.7258,-3.706534,0.18040535,11.755056,11.513058,-2.4054549,-2.7706525,0.75676847,-4.0100117,-2.9412792,-2.5118833,-2.2432704,3.2483573,-3.3730505,-7.0047593,-6.810283,-7.9163637,0.1249431,-2.9141579,-8.62168,3.1621108,-3.8791308,-3.9492514,-7.9016113,-1.0459881,12.760256,13.304542,-0.26574352,-2.310005,-1.7361597,-1.5554141,-0.26962253,-1.4231589,-0.76907045,12.549295,12.428093,12.412905,12.290841,11.749591,12.760909,-12.789455,-12.613685,2.1233776,-1.7071775,-2.0550296,-2.2328753,-2.4538631,-3.6874208,5.886807,6.4647517,-7.4829288,12.114398,11.884347,12.238524,-1.0249556,-0.9888574,-2.558879,-1.2651219,1.9027994,-0.5664827,-1.2116891,-0.34457737,-1.9346536,0.19108969,-1.8527647,-1.8628538,-6.4503055,-9.628807,-1.3748211,5.474671,-1.4267218,1.0878083,0.15193073,-3.9988263,-4.1682696,5.6794567,-4.039737,3.763082,11.453203,11.852138,-10.586367,16.913424,15.591871,15.778921,15.781825,15.777229,15.78039,15.773277,15.778839,15.780542,15.765672,15.769349,15.751151,19.457735,20.031845,15.386785,18.064205,-3.809673,8.626473,2.5961885,-2.5541332,-1.8331673,-4.507752,-18.841167,13.794641,13.680294,1.0479401,1.0496484,-0.46645138,1.4268737,1.1656449,-1.8658935,0.0050875754,-4.941183,8.594543,-4.7754226,-7.0362754,-5.6136775,-6.1254416,-6.090882,-6.544796,0.20293373,13.283889,7.743269,7.3456364,7.8795576,7.7588224,-11.135235,-2.9833946,-0.18769124,-6.0159874,-5.9485884,-10.192501,-13.818475,-13.41792,-13.872104,-11.409563,-15.083622,-9.234837,-2.5534098,-2.5424588,12.94559,13.165208,12.615059,11.948082,13.534003,12.802303,5.997499,-7.044179,-6.8871994,10.525212,9.9767275,7.570636,9.381272,10.200824,12.934638,13.315741,6.926968,-2.3722787,-0.5511279,-0.65888304,-2.8032749,-18.728657,-3.3390908,-6.1124616,-3.020756,7.6186776,-5.9802976,-5.0561867,-5.212774,-5.0962253,3.509258,2.150566,-4.090768,-7.42704,-7.166675,-7.2426634,-6.387732,-6.5432677,7.8372045,-1.1169394,-3.089729,-0.42129585,-4.342365,-4.461557,-0.51265484,-6.8049445,-3.7977152,3.0937963,-6.726426,-4.0054803,-3.1329975,8.895438,-3.6026902,-3.291845,-3.1631882,-2.684848,-2.0863779,-6.200861,-1.6557994,10.659741,-7.3191266,-2.1209457,-1.9099792,-1.8183148,-2.1203938,-0.9326171,-0.8302396,-2.191579,-1.7476724,-7.7301126,-6.4087267,-10.457937,-9.646901,11.686281,11.681427,-0.56141686,0.22599246,-1.77436,-1.3304778,-0.8045312,13.2406025,13.044947,-7.044765,-6.833534,-6.789127,-4.9790425,-8.201898,2.5071123,-4.6419244,5.46823,0.8117358,1.1122391,2.7360923,3.5088272,-3.9844499,6.6812487,6.2056265,5.5135965,8.566722,5.496746,13.115151,-2.097423,12.850784,-2.7770956,-0.8550177,3.0197515,2.9408689,-3.6245444,11.609706,10.933958,-18.637817,-18.447412,-18.790907,-18.82013,-18.641697,-18.561764,-18.838625,-8.533991,-6.4541373,-6.3341675,-6.073311,11.501896,-3.9232,-3.3661542,2.6778643,8.182801,0.21384618,-1.0134349,-3.3902345,-6.400188,-2.081926,-3.2805395,-0.81247616,-0.9969721,6.702691,2.542539,2.3720133,-8.07911,7.1326027,-8.174327,-1.5925308,11.647074,11.182356,7.2605395,1.819884,11.877604,12.053409,-2.4564524,0.05802457,-0.9711046,11.725999,-18.47686,-3.4099457,-1.5906479,-0.18567947,-6.6317983,-3.4975426,-4.3322096,-5.3606415,12.52726,8.26012,3.3209298,13.535849,13.948794,-7.286999,-1.2459182,1.6474683,0.27107197,-1.7164234,-0.32067817,-0.12068717,-1.199387,-3.456924,2.8127296,2.6272871,-7.945928,-0.14104721,-10.593202,-10.648591,-10.603352,-10.627497,-10.380349,-4.4036875,-2.7121072,-0.3901884,-4.0726633,-4.044739,-3.5177598,-3.9450688,-3.812994,1.7069842,1.476396,1.0894558,1.8853091,-5.056624,-3.7147655,-3.484748,1.273307,-2.7506263,-3.1743915,6.7612696,8.810642,6.628307,-0.5224802,-3.4423804,-9.079836,-0.12794182,-2.8114676,-2.9640195,-2.5582213,-0.175736,0.84487367,0.7456305,-0.14192517,-3.9896896,11.592577,-1.7441257,-0.9254715,-2.4030495,-2.7153087,-3.6122646,4.57464,5.5986056,-4.0063405,1.0476143,-3.7274113,-3.1799407,-2.7006407,-2.718679,-2.9434948,-2.7755876,-1.6583079,-3.011832,-2.497999,0.82754433,12.940913,12.887451,12.918685,13.493562,11.590906,-10.5336895,-10.567276,-10.581048,-5.1458874,-4.9196887,2.414345,7.489011,-3.5102098,-3.7329993,-1.8051264,-1.5286484,-1.981154,-2.0034986,7.7021503,-8.102063,-1.4952389,-0.54527014,-2.9073966,-3.0171208,-2.7274475,-7.1587176,-8.325905,-8.464555,-8.54653,-8.283385,-6.164843,1.115385,-1.7268221,13.246429,-0.24276161,0.3202088,-0.19604638,1.2761434,1.1124495,0.83608705,13.688371,-3.1851428,0.2732855,-0.23991762,-6.4452987,-2.1724026,-1.1102775,8.781685,2.9315474,-2.6722767,-3.5986707,1.0981771,1.0471158,1.5287937,0.9312712,1.2804394,-6.3696947,0.6282999,1.2753732,0.6873483,2.4044976,2.5774713,2.4699557,2.8925552,2.051247,3.5557406,8.686895,8.734286,-5.4823174,-5.306424,-5.4212847,-5.603554,-5.477407,4.823788,-4.8856716,-5.0593495,-14.9688635,-21.732815,-5.816512,-0.44375622,-17.416292,2.9148972,3.1901636,2.9574406,2.0837889,3.9447026,1.4681872,-1.6699346,-1.3231814,-1.6104873,-1.7363211,-1.8442942,-1.7658526,-1.4360651,-2.5570076,-1.2351639,-2.5165656,-10.234879,-9.559814,2.9806495,2.3251684,1.2220612,-9.504543,-1.1346213,-0.0057730353,-9.2772,-9.57574,-2.8938582,0.105854906,-10.967115,-11.280706,3.8217843,4.7336016,-4.2239366,8.101296,-4.0206847,-0.19309925,-0.08499214,-0.08872285,19.930944,18.64065,18.924732,19.505585,-4.9296446,-5.412556,-5.3553033,-5.566292,-5.5446224,-5.5323,-3.7136247,0.5032296,-1.0922647,-0.30298796,-0.0018499512,0.36173382,-0.15644264,-9.48965,-8.527295,-1.2609488,-1.6244949,-7.5729065,-7.4827194,-2.4047084,-6.687977,-4.793479,4.5027304,3.618102,6.4244204,4.4268003,-9.936475,-10.474118,-9.842918,5.3271832,-2.9424856,-1.1356227,-5.515987,-2.9618392,-7.310117,-7.151731,-1.3032838,-6.2863526,-6.1861305,-5.930789,-4.5391455,-2.495237,6.524243,5.5989027,-2.134105,-2.1988492,-1.1247569,-2.3058133,-2.108806,-2.0677686,3.521218,3.7935607,3.7308214,3.5095282,3.6524894,3.747642,3.5219216,3.550664,3.219854,3.373602,2.9205074,3.3462808,-3.4463718,-3.0952916,-2.7619557,-2.549677,-3.2708285,-3.3951185,-3.2370255,-1.935324,-0.31953284,-1.7340435,-1.9080061,-2.783658,-3.354051,-3.1428251,1.7032725,1.6922661,0.49075365,8.507192,8.416922,8.421201,-4.9666934,5.3541207,3.0842016,12.837341,13.143017,-1.3263633,-1.758401,-12.620302,-12.690423,-12.721712,-12.695211,8.271759,8.155411,-9.304017,-9.6061,-4.5543885,0.9547304,-3.4827228,-1.5053439,-2.366754,-1.9974356,-3.8832936,-3.5970628,2.5430787,-2.4245224,-2.0608747,-0.28594333,-0.17195939,0.39933732,-0.45179185,7.001802,-3.762762,-3.3347538,-2.1615448,-2.6376107,4.2345386,-4.6937995,-4.0540776,-0.2682991,0.18876828,7.6532607,3.9866354,3.585736,-3.3923612,-3.0378802,-3.5510075,-2.2358642,-3.2459161,-2.2153516,-2.5135565,-1.9924405,-3.1246812,-1.404085,-1.1763067,-3.4921665,-3.1776237,-0.2888642,-7.202328,8.500593,13.373221,12.908849,11.592569,-3.3873465,-6.1462917,-4.8188453,-0.4787065,-2.0121608,-4.8844094,14.486914,13.625428,12.54471,8.441824,-5.0437937,-10.478097,-10.426189,-8.587483,-9.30733,-9.394608,-2.400987,-10.583075,-10.595036,-10.582386,-1.3924776,-10.014323,-2.402536,11.592908,-5.579228,9.048604,9.085769,8.490581,2.9921038,-2.8453038,-2.366669,-1.4886434,-0.6830752,0.25134408,-1.9236635,-0.34842646,-4.1950197,-3.409961,-2.7909656,-3.6450732,6.198292,6.208352,-1.0335274,-2.3894253,-2.7477634,-1.9085237,-2.3658156,-1.1083778,-1.5865718,-2.2381167,-3.9573236,-7.3644,-6.861385,-1.8286572,-2.1979928,-6.282165,-6.294812,-6.378339,-6.270533,-6.1654005,-6.6502476,-6.4206514,-6.375979,13.237117,-3.409245,8.607879,-3.111798,-4.6503606,-0.94281924,-4.064852,-4.5862813,-3.6832302,-0.75828767,-7.7614613,-7.364743,-7.474493,-6.640447,-7.008042,-7.3580256,12.095977,11.776504,4.0469413,3.8078227,3.78968,11.996149,19.542797,18.604986,19.174145,17.57838,16.873737,5.786145,5.4803457,3.3636813,3.1948497,3.368441,3.9754014,3.7267048,3.8126109,-14.729803,-21.733583,4.674799,4.9770145,-0.1406705,-0.76370394,1.8221493,3.4651046,2.8515031,8.468219,-0.15386821,0.7402522,0.68549484,-8.00758,5.927318,19.033989,18.7813,17.975685,19.623903,17.913704,-4.955266,-2.2717426,-2.5221608,8.798053,-1.2947428,-3.5468817,-1.8133646,-7.3404136,-9.433794,-9.658377,-9.988198,-5.0605807,-2.4442427,8.7911005,8.754315,8.822966,5.082536,5.0568833,10.008158,2.6064153,-5.323634,19.345522,19.631887,-18.75039,-18.7253,-18.64603,-18.665802,-0.16918659,-6.481154,-6.3848877,-6.0882382,-5.605718,-5.5514636,-6.081564,-6.218883,-1.7145127,-3.4781985,8.303266,-2.005674,8.331746,-4.561543,-2.64933,-2.7967677,-4.6029205,-2.8363378,-4.907887,4.9108386,-2.5892096,-0.5755131,-6.260706,-6.8318186,7.6389804,-10.115357,-4.5437083,-4.726073,-6.0286155,8.123237,7.990732,7.7942343,7.9319386,7.7257695,7.6908736,7.822153,-4.84791,-9.990161,-10.526695,14.20254,-3.214742,-6.0789347,-3.4001157,-3.9633617,3.6109824,3.2465918,2.8469584,3.213974,3.430593,-3.465567,-3.8015559,8.366987,6.853808,2.0673904,-2.4871185,-3.546441,-7.7134776,-4.3504033,11.37164,9.197802,-0.7072084,6.102109,11.945204,-3.5116966,-3.2158155,2.378713,-1.7708513,-1.4395825,-1.5805314,-1.5363492,-1.1265465,-0.8432941,-1.6209059,-3.614226,1.8034575,-1.3660532,-5.776773,-5.161807,2.8930671,-2.3640661,-6.6800375,-3.744971,8.84369,-5.3439775,-0.9825433,7.667597,-1.338178,-5.745719,-2.381733,4.5700817,11.918106,11.815216,13.152924,11.271642,-4.1229386],\"xaxis\":\"x\",\"y\":[1.8776364,7.4563518,2.2382545,2.2138507,2.3960648,-0.06182539,1.7786597,3.032441,3.6660345,4.281624,4.2860765,4.2710524,-0.28735694,-18.337503,-18.450468,-0.061609086,0.076036856,1.2103798,2.2657757,-3.545557,-5.7828207,20.722948,20.755339,-3.497248,-7.3147774,0.19029015,0.3297845,0.4269568,0.6099263,2.0426276,-0.5253692,4.3242106,3.929654,4.2426972,4.5232353,1.7406559,-0.5756864,-6.05622,-8.338619,0.9666148,-2.3464382,4.1340046,3.818621,-1.3310779,2.1381073,0.36953926,2.4014974,1.9769638,2.0359926,2.1468754,-1.8568088,-1.8185551,-1.3201977,-1.1252218,3.9656565,3.3655756,-0.77609974,-0.59829426,3.2152145,-0.22341053,5.986482,6.0103927,6.287892,6.3578396,6.037379,5.838416,6.310988,6.2214627,6.039748,6.2496037,6.317555,6.3368587,6.4006896,6.387121,6.419771,6.3161354,6.334604,5.6949315,5.136797,6.276516,6.2129345,6.3445687,5.044079,2.0987883,-0.99849504,-1.0975839,-2.1290367,19.448708,14.8122635,-4.0954657,4.054723,-6.6551647,-5.565713,-7.853204,-2.4116998,-2.3067997,4.103376,-4.104675,9.642988,6.9247866,-9.473547,-6.633584,-6.476625,-6.4762235,-6.48024,-0.8632332,1.6223902,2.5152652,5.821544,0.96451384,1.9335202,6.3010006,3.4035544,4.1568975,4.044194,4.0825987,5.5137253,5.523352,3.503557,6.3987546,6.426319,-11.870958,1.9611616,2.6957114,5.2984486,4.6355844,2.6810348,2.3153324,1.6042281,0.1909204,1.2880661,4.419984,3.5373812,4.476178,4.496438,3.3387241,1.4934683,6.7408347,6.3825145,-0.123418875,4.1881986,4.077215,3.348734,0.32021767,6.3496513,4.5561004,6.172735,5.648041,6.212103,-0.17739452,-0.41264254,-0.99423504,3.2984202,3.385565,3.3322618,3.9839582,4.225565,3.5117362,-0.28102303,0.52382576,1.4539768,2.1394074,4.077216,0.6029686,0.84989333,0.03932549,-6.22894,4.1924796,4.3375516,4.141962,-6.3988733,-0.20356385,0.041762285,5.029539,-0.4341641,-0.8892215,2.521129,2.5408254,2.660087,2.6853006,2.8741026,3.51611,3.7322075,-1.8293613,-2.224392,3.344798,8.552671,-1.5954258,-1.6117833,2.642237,-1.6261876,-1.5389532,-1.524919,-1.6350657,-1.5826434,-1.5513196,-1.4341608,-1.6014452,-1.479328,-1.6275625,-1.5864729,-1.5588263,-1.5785366,-1.5661252,-1.6374032,-1.5340117,-1.6017301,-1.5823195,-1.5896999,-1.5612072,-1.6283647,-1.629462,-1.5538986,0.14346135,2.3898914,2.5300226,2.0542235,2.65541,1.7870779,3.9233496,2.7617261,0.44215244,-0.22276598,2.5926943,-0.31953132,0.14654215,3.4957874,3.6145105,3.2422836,3.420493,-5.399755,-5.3975353,3.252209,0.19363667,0.15236676,3.1884162,-0.13266662,0.528982,0.7547907,-0.2536411,0.07452443,-18.659037,-17.990082,-18.219423,-18.062548,-18.165827,-18.431427,-18.372925,-18.850683,-17.884209,-18.356464,-18.179682,-18.44354,-18.41032,-18.600052,-18.296995,-18.519403,-18.588703,-18.301554,-17.906504,-18.333208,-18.456976,-18.030405,-17.87758,-17.412994,-18.473808,-18.240288,-18.245173,-18.26064,-18.225075,-17.057482,-18.182533,-19.109867,-18.46566,-18.509674,-18.450945,-17.933739,-18.576355,-18.363304,-18.346823,-18.417192,-18.375235,-18.007648,-18.227736,-18.18519,-18.242506,-18.580282,-18.480545,-18.327227,-18.619322,-19.19929,-19.121134,-18.380589,-18.163855,-18.078833,-18.445524,-19.204927,-18.498001,-18.341295,-18.475069,-19.18252,-19.123976,-18.565718,-1.5483607,3.2572348,3.8348114,3.5983803,3.3001685,3.4437146,3.168945,3.797143,3.244144,3.458711,3.0390563,2.9805307,3.1620657,4.4437394,3.3623185,1.880349,2.1319032,2.9172688,3.0056753,2.5003889,2.3620644,2.9003334,3.1200237,3.1459718,2.6004949,2.8651397,3.0245056,3.5372398,2.9253025,3.0239122,2.1218145,3.1237152,3.3949933,1.979693,3.4825642,2.9878135,2.8892474,2.7356744,1.9209177,3.1214864,3.2609146,2.0911777,2.994149,3.29114,3.5620358,2.9722688,3.13141,3.216738,3.0320578,3.0647004,3.07893,3.1457539,3.0789,3.09321,3.025848,3.1212113,3.1454048,3.0277357,3.0519226,2.571022,0.30482662,-1.2491376,3.3697674,4.2937894,2.5818756,4.53394,3.8429523,4.66402,-1.367985,-0.6770471,3.4558468,0.34903622,0.77197963,-2.3542576,3.6405058,1.6919218,4.0744133,3.868239,2.8797746,-2.1421475,8.55496,14.81198,-4.2338285,-4.3260636,-0.54410386,1.5352728,3.1816218,-0.6784021,-11.9880085,-10.255096,4.9126186,3.825772,2.732271,2.7107103,0.41182882,1.6474164,1.6391959,1.9316949,1.0429741,1.7397758,1.6277058,2.979362,-1.4787418,-2.301654,1.7544318,-11.612303,1.0729425,3.0979366,0.01820238,0.6330189,1.8994513,4.7680674,-6.6434584,-6.63028,-6.669534,-6.62861,-7.8477917,1.4271747,2.7350914,-6.291314,-1.8671461,-5.984389,-5.9935102,1.146786,-5.7633467,-6.895221,-0.7467251,-0.6613373,4.5055966,6.401798,6.429945,-6.371752,-6.5841274,-8.462857,1.1341312,2.1311488,-1.8338611,-1.9927258,-1.5872904,-5.142533,4.839731,1.1456668,4.4808445,4.992465,1.7598481,-0.17663638,4.308642,3.552229,1.1269786,0.056746,-18.247757,-2.0611439,3.3014135,4.0670075,3.8796427,5.2028294,4.4803953,3.9592683,3.2085278,-0.086679086,-0.42984253,1.8089571,-6.1884007,-0.83794767,8.111688,8.051625,8.053683,3.5095563,0.8533181,-1.6201777,-1.637395,-1.6713246,-1.5825872,-0.59201044,-0.372276,1.2726719,-18.565464,-18.405203,-19.191761,-18.486464,3.4456637,4.1168475,-0.008183641,2.9959013,2.9170277,0.41941673,2.610981,2.0884607,0.63096535,0.6532473,0.11979652,0.4697166,-0.92195153,4.2647443,3.2416644,2.3475468,2.7370908,-2.3231854,1.3388135,-2.1925044,-2.3861902,3.362656,-1.9454325,-2.4574797,-2.3894892,-2.026611,-0.15662715,-2.0856233,-0.24707748,4.389391,-6.98757,3.3339946,-6.892555,-1.1653888,-1.6336206,-2.1903386,-0.79147685,1.1283493,0.7203622,3.7152002,2.8306777,5.750034,6.3096685,-1.6510442,1.4633809,-4.925342,-20.004639,-20.007233,-20.003849,-20.006227,-20.006775,-20.010267,-20.008808,-20.010376,-20.010216,-9.532457,-9.451285,3.031321,3.9034011,3.9516985,3.3208687,2.3662438,2.4124796,3.5105948,7.681259,8.008892,4.3553357,4.1417737,-7.1754646,-9.700578,3.432275,-9.413086,-1.517804,-1.3618612,0.097882316,-11.8018055,-11.806161,1.5558395,2.8389885,-1.6305974,3.5585537,0.6014162,-0.5322115,-0.82411945,1.1884226,-0.29135665,5.8617916,6.2910676,0.087439604,0.6113552,-6.2986374,-6.6349325,-6.5958834,-4.6979446,0.37076646,5.661829,2.694494,3.8576663,1.8342453,-0.15226743,1.2050292,7.5920506,0.06947915,0.37560996,4.5921917,-0.0317865,4.662959,3.0398371,6.14227,5.1123776,5.858676,7.5966806,3.729594,3.5669918,3.929137,3.5607424,-4.1470537,-4.1920896,-4.118555,4.6856637,5.507002,4.7865176,4.00152,3.75484,-7.0190296,-7.1671605,2.381701,1.3003988,-1.5594369,-1.5964226,-1.4805392,-1.3926976,-1.5170484,8.441268,1.8642814,2.843831,2.8108463,1.9528079,-10.247644,-9.925572,5.1290116,1.6843594,1.5486277,-1.5642596,3.1454692,2.0260832,8.904911,-2.530784,-2.5069509,-2.4193454,-5.410271,-2.350929,-2.386421,8.183557,8.520049,7.901231,2.5100932,2.7143755,3.2837477,3.5101848,3.4692664,3.5303626,-1.5690306,-1.1025311,-17.237469,-18.330063,-18.139475,-18.364443,-18.057186,-18.141232,-18.064856,-18.286106,3.010544,2.2662354,3.22317,3.297527,-0.37460074,-7.730957,0.8929111,-0.52980185,2.9849656,4.0328374,1.3296634,3.8808768,3.3979316,4.1970453,4.2247987,4.3840265,-0.90987366,2.6280065,2.388603,-7.116804,0.9586437,2.931471,2.9977603,3.6528428,3.2004845,2.9945881,1.7171,7.455341,7.366372,6.9957733,-12.1121855,7.41392,-4.556772,-6.838721,-5.610128,-3.9263575,-4.953691,4.541571,-1.8054894,1.9427602,2.040859,-6.9140277,-6.7262444,3.3757138,0.08919828,3.971648,4.3277783,4.4315042,4.5288963,4.435408,3.329488,3.2610278,3.554143,3.507243,3.553313,-7.342347,-3.5321481,-6.884315,-7.0300703,7.5409775,2.330696,2.3416917,-1.9743567,-2.1006734,0.42363685,-5.594293,-6.273677,-4.850316,-6.892281,0.8872626,5.2472234,2.5763383,3.0044618,-0.7751223,-6.1763644,0.37264147,-0.5620176,3.3053677,-6.8810472,-6.8093114,-0.018660085,2.9394748,3.892223,5.6529617,-0.48717937,4.202764,4.2421727,3.6920447,4.315942,4.25366,-0.58191335,2.6882377,2.5902734,2.7088108,2.614661,2.1114936,4.9208302,-1.5976509,-1.4951024,-0.37874073,-2.1729057,-3.7774403,-4.119013,-0.2700256,-5.9333253,-0.219991,2.7547603,-2.3564396,2.460162,2.2518094,2.4714575,7.598305,-6.9210653,-7.211918,-7.101604,3.613552,2.7246978,1.0137345,4.112665,4.67362,6.6134515,0.77044505,-1.5292475,2.5455089,1.0182099,1.5260947,-0.37579384,-0.32295597,0.542427,0.25066316,-7.8189454,-8.259262,1.1641691,-8.516027,4.1033063,2.6185846,2.5834541,0.16042423,1.8812237,-16.19195,-16.035948,-16.035799,-16.03677,-16.033611,-16.039894,-16.037806,-16.035698,-16.045647,-16.042908,-16.056343,2.2979245,2.321211,3.4340463,2.171885,-7.7741466,-10.251672,3.6563444,-8.064626,-6.7648835,-1.0806258,-4.3490524,3.5189905,3.3711967,1.3663157,1.3983309,1.8743535,1.4940974,1.5533624,-1.4753486,3.8637621,-6.6410594,-11.761823,-6.9335265,6.7931013,6.7651525,7.1375036,7.3449583,7.8874764,3.1725729,5.3653755,2.7645035,3.1005988,0.19490105,0.06052504,4.506054,3.562719,9.945795,0.6036258,0.87489015,0.037731797,-1.8762947,-1.6549971,-1.8231379,-0.9525517,-2.3516958,1.6677085,-6.441296,-6.346528,3.7096097,3.7261262,4.148033,4.087369,3.5846217,3.3387315,1.827124,2.7117813,2.8937771,3.3810225,3.3499677,2.7185104,2.6519256,3.2463088,3.8892674,3.5795453,-1.1895307,-6.1207976,0.23557682,0.71771467,2.4152224,-3.669063,3.0866807,1.7183344,2.861229,3.180319,-0.8817405,-1.4700036,-1.6365571,-1.5658718,1.0116878,2.2599065,-7.418209,6.3756866,6.1951604,6.240914,6.7324004,7.1677985,3.2301836,1.9029372,3.7555363,5.817943,4.1206956,3.4538834,7.588427,7.030737,5.1339808,3.0245454,7.124934,-6.749461,3.2614777,-11.3528,0.086604945,0.07150033,-0.048575997,-0.5972603,0.14173205,1.8443912,-1.3277658,2.2360578,-0.8704392,-7.3438754,-6.81859,-6.6553164,-6.7517962,-6.116379,-6.6069865,-4.7830677,-2.2723055,-1.4510933,3.0846767,0.054043602,-0.35148534,2.33716,2.133465,4.1866612,4.22788,-0.6617532,-0.22534193,-0.4930448,3.7533407,4.252481,7.356688,7.4975376,7.5712185,-6.4817214,-2.4965425,2.1277287,0.031055149,1.782594,6.8376255,5.8083515,0.6595007,0.89314026,1.7137924,0.22489229,0.24216525,0.22304203,-11.184989,1.534953,3.4884908,1.7259495,3.7888665,-0.20064107,0.19039103,-2.4364572,-1.5618627,-1.0911838,2.1269264,2.045604,-4.204399,-4.1923327,-4.333336,-4.350509,-3.7201211,-4.213402,-4.364078,2.336242,2.6080828,7.4901032,7.216689,3.698074,-5.6589694,-4.6622386,-0.020310514,-10.386583,-0.53888893,-2.5577588,1.0130081,0.5077004,-6.3217854,-4.566133,1.5789208,1.557756,-0.98759586,4.0992785,3.4522681,2.8448565,1.7975117,2.2485263,3.9345357,2.1558144,2.5437365,-9.605773,2.3138354,2.9860675,3.077625,-11.826257,-6.255622,5.7007923,2.2281477,-3.5037491,6.51026,0.44104806,1.0528536,6.5956726,3.727576,1.1543703,-1.1318725,5.3107305,-9.998455,0.83446354,3.6922605,4.0537615,5.5571957,-0.14022407,0.28665304,-0.013524728,3.7958467,5.150775,1.3654115,5.8779407,-4.2696457,0.70154744,1.1856285,-1.8736675,9.603767,-0.74601525,-0.75968474,-0.7406263,-0.77906585,-0.9658741,-5.919392,-5.639952,-6.462888,-3.1314752,-3.3190882,-2.303736,-2.5940235,-2.1773207,4.0294657,4.293951,3.1773243,1.8838379,-2.1779292,-1.5567276,-1.6226077,2.1332343,-2.3807929,-1.9635332,0.2938282,2.2287214,0.39250976,3.4014232,5.2968016,2.655738,4.057409,2.3308232,-6.125973,-6.374404,10.030787,7.4704413,7.935172,7.1184325,1.9994113,17.956915,2.7073648,4.233577,1.6741843,0.9440641,0.9022201,1.4447353,0.2057769,-7.2118807,0.5592026,-7.044304,-6.8251243,-7.134846,-7.025642,-7.987953,-7.0958138,-6.196669,-7.4130163,-7.068723,4.6288877,3.9562385,4.0489554,3.747716,3.8682258,3.2751765,-0.6709431,-0.5632289,-0.8774339,-0.8139341,-0.40358174,1.8447583,0.23645186,1.5621151,-6.1630793,-6.9451475,-7.1015415,-7.295272,-7.0739083,-9.726111,1.8424453,4.133127,4.277887,-1.855254,-2.3940532,-2.2932737,1.4852318,2.7837214,2.5828507,2.576117,2.4628663,-0.39547336,-1.0520796,0.676296,3.9246073,8.349018,8.057479,8.481207,2.746041,2.2877824,2.1240563,3.6402802,5.755754,6.6105456,5.5078273,6.064812,0.7670163,-0.6291674,-11.6680975,-2.3886516,3.5458093,3.909722,3.9789124,4.1100783,3.9127438,3.9729278,3.493832,7.9437284,3.8303287,3.8639503,4.001434,3.8456597,3.779912,3.7546103,3.4503996,4.133417,2.8527262,-11.759575,-11.713811,-1.364935,-1.524412,-1.3726804,-1.4739223,-1.2532431,1.4928292,-6.2212453,-6.527993,-2.1393926,8.552034,1.5233103,-0.4321354,14.814085,5.4428496,5.187878,0.433708,0.17190081,0.9249075,0.706027,-7.743606,-7.052392,-6.891771,-6.781602,-6.724333,-6.6143336,-6.9211135,-6.761047,-7.020115,-7.421688,-0.13601677,0.19477853,2.9380124,4.056276,4.3695436,-0.57611847,2.6400745,-5.793557,-0.14305156,-0.17500654,-4.097761,-6.0582595,-1.3768624,-1.3872712,3.2926095,1.9119278,-8.205809,-9.88782,-7.101434,8.628812,8.703103,8.566014,2.3079777,2.4388597,3.360238,3.2524238,-1.4201714,-1.490017,-1.2705864,-0.6582475,-0.90005165,-0.5997981,-1.9934751,0.29777196,-0.8518745,9.596522,8.363429,8.030169,8.444002,0.91766745,0.12218121,2.2597995,-0.155777,6.4230595,6.3956833,-12.177017,-1.1122013,2.334046,0.38763782,-1.5677165,1.6815382,0.4682726,1.0057961,0.30743465,0.11645666,1.86513,0.5820837,4.442965,2.0232902,1.7469791,5.486181,4.504909,0.7889765,6.8590565,6.49958,5.827631,2.7983706,-11.531514,-0.2538942,0.03295362,-1.639873,-1.3427519,0.55898225,3.5672755,3.326102,1.8026614,-1.5435753,-0.88809246,-0.8429233,-0.9863303,-0.8605931,-0.8004976,-0.88639295,-0.9366939,-0.8713162,-0.831209,-0.68319875,-0.7995142,6.5251236,6.9454837,6.960244,6.9501224,6.9866276,6.6157784,5.8269296,6.3442597,6.638369,6.4584765,6.3014417,6.306474,6.2457933,6.540735,4.121606,4.273081,4.4440246,-10.498153,-10.44348,-10.352718,-1.5433277,1.3941194,3.1240985,4.632619,5.212532,-2.292441,-3.5865927,-1.5882021,-1.6473699,-1.6235641,-1.637793,-10.213419,-10.292227,0.6319833,0.6701317,2.9363961,0.94194895,-5.7153206,-2.573166,-5.1762524,-5.847401,-4.481779,-5.28797,-0.18774295,-4.6853256,-5.6502976,9.862708,3.7902126,7.865333,8.148214,3.242999,-0.06779872,-6.2278595,-5.1313567,-5.369169,1.0075232,0.87755245,4.699507,10.095306,8.232164,2.6964626,1.19165,1.0282271,0.58301604,1.8736659,1.7716987,4.1136103,1.1225321,1.7869674,1.7766138,2.3532457,1.9001766,4.284707,2.2915828,2.0775995,4.2493734,0.27614212,-1.757784,2.5277956,4.948239,3.9521947,17.957329,0.9324654,2.9390576,1.6535634,-2.366062,5.42101,1.4603562,3.1757917,3.8367903,4.583965,-10.14662,2.59255,0.0387937,-0.03870474,-0.21384683,-0.55575454,-0.7421166,-12.234126,-0.027237203,-0.030527228,-0.55947495,1.5284832,-0.61756504,-12.182312,17.956324,2.5136437,0.3728837,0.43973058,1.0181652,1.1836126,3.8711493,4.7676783,4.845494,4.231107,-1.1090083,0.7051626,0.4946758,2.3669086,3.142202,3.5090992,-5.5370636,20.723856,20.755781,-2.8264785,-5.616172,-8.668613,3.8760448,-5.421966,-3.3069532,-4.129881,-4.3810735,-5.663065,6.1696687,6.1775026,2.2593455,0.917166,7.0166297,6.814686,6.136343,6.8805904,6.02632,6.770325,6.9773493,6.3750677,5.319246,-0.11589234,2.631658,-0.3250739,1.6006955,-0.91397446,-1.2293202,1.5231118,-1.3221033,-1.2423123,-1.4871396,-1.6406302,-2.1120853,1.5698769,-1.5562508,-1.931635,4.0758286,3.7761042,0.5822102,0.84740037,3.147969,4.117803,2.3080783,3.7743583,2.4411871,3.7293448,3.2521477,0.5296555,1.0864844,0.8962684,1.1782022,1.1170045,1.2331061,0.9858187,0.90516704,-2.0220876,8.551396,1.1552657,0.96157205,-0.43654767,-0.42555577,0.8167665,2.317555,3.3082416,-10.412739,9.768744,7.280405,7.9639792,-1.8753955,0.056662533,2.2347171,3.5739036,4.2577057,2.42646,2.614045,-7.0383463,-5.944161,-9.280604,-10.454927,1.5646825,-2.4544306,0.8777956,-1.82693,1.0858696,0.70536155,-0.04583097,-6.8420167,-11.756659,-11.644276,-11.69595,-11.727201,0.21221277,0.19547743,4.028229,4.608426,2.65817,2.3911304,2.4212565,-4.261131,-4.1236863,-4.3049936,-4.1962,9.910053,-0.10904929,0.3873004,-0.041023016,-0.47560865,-0.4492176,-0.12558502,-0.5544881,3.626202,2.799031,4.1599507,3.2714894,4.196547,2.5057836,2.9851887,3.4141357,1.5902139,-1.8707381,1.787236,2.0101836,-9.906502,-0.5116876,3.9795554,5.083402,3.4820569,-0.5624413,-5.0686984,-1.8987317,0.8091294,-10.032804,-9.794204,-9.833267,-9.844468,-10.039624,-10.097791,-10.273934,-7.2159915,-0.12955219,-0.38284054,3.8385923,0.33683914,-0.0779493,3.9410765,3.0819426,-2.17256,-2.3590608,-2.3410943,-2.400859,-2.347638,2.893724,2.1057668,-10.750953,-1.2395382,3.9007812,-1.1863754,-7.6321907,-1.664089,3.3817072,2.0318253,0.50470006,1.1666735,0.1963145,2.9249885,4.844656,4.8448243,3.8708203,0.92502546,1.5361257,1.5045689,2.1186426,1.2699976,0.9727103,0.97097856,-5.670139,-0.435483,-3.284823,0.9654233,-0.14156543,4.074324,3.8254585,6.002251,-0.70738727,-11.596975,-1.3931903,4.5267797,3.3759193,4.3108287,3.286341,-12.375556,1.4765238,2.1423216,2.096316,5.6164384,4.0878453,-2.0157845],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"New users are often very confused by the range of TPUs, and the different ways to access them. The f...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe second way to access a TPU is via a **TPU VM.** When using a TPU VM, you connect directl...\"],[\"Debugging on TPU is generally a bit harder than on CPU\\u002fGPU, so we recommend getting your code runnin...\"],[\"```python\\nif tf.reduce_sum(tensor) \\u003e 10:\\n    tensor = tensor \\u002f 2.0\\n```\\n\\nThis might seem very restric...\"],[\"```python\\nlabel_mask = labels \\u003e= 0\\nmasked_outputs = outputs[label_mask]\\nmasked_labels = labels[label...\"],[\"How can you get around rule #3? The key is **padding** - if you pad all your inputs to the same leng...\"],[\"### Summary\\n\\nThere was a lot in here, so let’s summarize with a quick checklist you can follow when ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"There's also the \\\"Training metrics\\\" [tab](https:\\u002f\\u002fhuggingface.co\\u002fnielsr\\u002flayoutlmv3-finetuned-funsd\\u002ft...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## VisionTextDualEncoderConfig\\n\\n[[autodoc]] VisionTextDualEncoderConfig\\n\\n## VisionTextDualEncoderPro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"With this approach, the model can detect objects based on textual descriptions without prior trainin...\"],[\"```py\\n\\u003e\\u003e\\u003e predictions = detector(\\n...     image,\\n...     candidate_labels=[\\\"human face\\\", \\\"rocket\\\", \\\"...\"],[\"Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](http...\"],[\"\\u003e\\u003e\\u003e draw = ImageDraw.Draw(im)\\n\\n\\u003e\\u003e\\u003e scores = results[\\\"scores\\\"].tolist()\\n\\u003e\\u003e\\u003e labels = results[\\\"labels\\\"...\"],[\"\\u003e\\u003e\\u003e scores = results[image_idx][\\\"scores\\\"].tolist()\\n\\u003e\\u003e\\u003e labels = results[image_idx][\\\"labels\\\"].tolist(...\"],[\"In the preprocessing step, instead of text queries, you now need to use `query_images`:\\n\\n```py\\n\\u003e\\u003e\\u003e i...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fcircleci.com\\u002fgh\\u002fhuggingface\\u002ftransformers\\\"\\u003e\\n        \\u003cimg alt=...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fR...\"],[\"🤗 Transformers предоставляет API для быстрой загрузки и использования предварительно обученных модел...\"],[\"В области NLP ( Обработка текстов на естественном языке ):\\n- [Маскированное заполнение слов с помощь...\"],[\"- [Ответы на вопросы с помощью DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-...\"],[\"В области компьютерного зрения:\\n- [Классификация изображений с помощью ViT](https:\\u002f\\u002fhuggingface.co\\u002fg...\"],[\"## 100 проектов, использующих Transformers\\n\\nTransformers - это не просто набор инструментов для испо...\"],[\"Вторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейером, а ...\"],[\"Подробнее о задачах, поддерживаемых API `pipeline`, можно узнать в [этом учебном пособии](https:\\u002f\\u002fhu...\"],[\"## Почему необходимо использовать transformers?\\n\\n1. Простые в использовании современные модели:\\n    ...\"],[\"## Почему я не должен использовать transformers?\\n\\n- Данная библиотека не является модульным набором ...\"],[\"После установки одного из этих бэкендов 🤗 Transformers может быть установлен с помощью pip следующим...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (from VinAI Research)...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (from Facebook) r...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (from Harbin In...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (from OFA-Sys...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (from MetaAI) rele...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (from Tsinghua University) rele...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (from Microsoft) ...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (from Facebook) released with...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (from Baidu) released wi...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflava)** (from Facebook AI) released...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (from KAIST) released with th...\"],[\"1. **[GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptj)** (from EleutherAI) released i...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (from Microsoft) ...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[LayoutLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlm)** (from Microsoft Resea...\"],[\"1. **[LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flilt)** (from South China University ...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[LUKE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fluke)** (from Studio Ousia) released ...\"],[\"1. **[MarianMT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmarian)** Machine translation mod...\"],[\"1. **[mBART-50](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (from Facebook) released with t...\"],[\"1. **[MPNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmpnet)** (from Microsoft Research) r...\"],[\"1. **[Nezha](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnezha)** (from Huawei Noah’s Ark Lab...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (from Google AI) release...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fmain\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft Research) ...\"],[\"1. **[Pop2Piano](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpop2piano)** released with the p...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (from Google Research...\"],[\"1. **[RoCBert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froc_bert)** (from WeChatAI) releas...\"],[\"1. **[SEW-D](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew_d)** (from ASAPP) released with ...\"],[\"1. **[SwiftFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswiftformer)** (from MBZUAI) r...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (from Google AI) released with th...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[UMT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fumt5)** (from Google Research) releas...\"],[\"1. **[ViLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvilt)** (from NAVER AI Lab\\u002fKakao Ente...\"],[\"1. **[ViTMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_mae)** (from Meta AI) released ...\"],[\"1. **[Wav2Vec2-Conformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2-conformer)** (...\"],[\"1. **[X-MOD](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxmod)** (from Meta AI) released with...\"],[\"1. **[XLM-RoBERTa-XL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-roberta-xl)** (from Fac...\"],[\"1. **[YOLOS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyolos)** (from Huazhong University o...\"],[\"Чтобы проверить, есть ли у каждой модели реализация на Flax, PyTorch или TensorFlow, или связанный с...\"],[\"## Цитирование\\n\\nТеперь у нас есть [статья](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f), ко...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Repository features\\n\\nEach repository on the Model Hub behaves like a typical GitHub repos...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import notebook_login\\n\\n\\u003e\\u003e\\u003e notebook_login()\\n```\\n\\n## Convert a model f...\"],[\"## Push a model during training\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\u003cYoutube id=\\\"Z1-XMy-GNLQ\\\"\\u002f\\u003e\\n\\nSharing a mode...\"],[\"## Use the `push_to_hub` function\\n\\nYou can also call `push_to_hub` directly on your model to upload ...\"],[\"![new_model_repo](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fnew_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present a convolution-free approach to video clas...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale NLP models have been shown to significan...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We re-evaluate the standard practice of sharing weig...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"## TFRemBertForTokenClassification\\n\\n[[autodoc]] TFRemBertForTokenClassification\\n    - call\\n\\n## TFRem...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Extending the forecasting time is a critical demand ...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision-and-language reasoning requires an understand...\"],[\"This model was contributed by [eltoto1219](https:\\u002f\\u002fhuggingface.co\\u002feltoto1219). The original code can...\"],[\"[[autodoc]] LxmertForQuestionAnswering\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFLxmertModel\\n\\n[[autodoc]] TFLx...\"],[\"Author: [@vasudevgupta7](https:\\u002f\\u002fgithub.com\\u002fthevasudevgupta\\u002f)\\n\\n## Intro\\n\\nIn this project, we fine-tu...\"],[\"```shell\\n# download validation-dataset first\\nmkdir natural-questions-validation\\nwget https:\\u002f\\u002fhugging...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [Fine-tuning a pretrained model](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002ftransformers_d...\"],[\"### PyTorch Examples\\n\\n#### Natural Language Processing[[pytorch-nlp]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on language modeling](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"| [How to train a language model from scratch](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnoteboo...\"],[\"| [Reformer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnotebooks\\u002f03_reformer.ipynb)| How Reforme...\"],[\"#### Computer Vision[[pytorch-cv]]...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to fine-tune a model on image classification (Albumentations)](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"| [How to perform zero-shot object detection with OWL-ViT](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002f...\"],[\"| [How to build an image similarity system with Transformers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"| [How to fine-tune a VideoMAE model on video classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"#### Audio[[pytorch-audio]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"#### Biological Sequences[[pytorch-bio]]...\"],[\"| Notebook     | Description                                                                        ...\"],[\"| [How to fine-tune a Nucleotide Transformer model](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"#### Other modalities[[pytorch-other]]\\n\\n| Notebook     | Description                                ...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on language modeling](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"#### Computer Vision[[tensorflow-cv]]\\n\\n| Notebook                                                   ...\"],[\"#### Biological Sequences[[tensorflow-bio]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on text classification with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"## Community notebooks:\\n\\nMore notebooks developed by the community are available [here](https:\\u002f\\u002fhf.c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nThe [`Trainer`] class is optimized for 🤗 Transformers models and can have surp...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers-based models, such as BERT, have been o...\"],[\"This model was contributed by [vasudevgupta](https:\\u002f\\u002fhuggingface.co\\u002fvasudevgupta). The original code...\"],[\"[[autodoc]] BigBirdForPreTraining\\n    - forward\\n\\n## BigBirdForCausalLM\\n\\n[[autodoc]] BigBirdForCausal...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In terms of model details, the work outlines the architecture and training methodology of Persimmon-...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\nTips:\\n\\n- To convert the model, you need to clone the original repository using `git clone h...\"],[\"*TEMPLATE**\\n=====================================\\n\\n*search & replace the following keywords, e.g.:*\\n...\"],[\"To start, let's try to get a general overview of the Transformers\\nlibrary.\\n\\nGeneral overview of 🤗 Tr...\"],[\"Let's take a look:\\n\\n![image](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolv...\"],[\"```python\\n# assuming that `brand_new_bert` belongs to the organization `brandy`\\nmodel = BrandNewBert...\"],[\"From experience, we can tell you that the most important things to keep\\nin mind when adding a model ...\"],[\"6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfu...\"],[\"-   What type of model is *[camelcase name of model]*? BERT-like encoder-only\\n    model? GPT2-like d...\"],[\"2.  Clone your `transformers` fork to your local disk, and add the base\\n    repository as a remote:\\n...\"],[\"You should start thereby by diving into the [original repository]([link to original repo]).\\n\\nSuccess...\"],[\"At this point, it is really up to you which debugging environment and\\nstrategy you prefer to use to ...\"],[\"Next, regarding the debugging strategy, there are generally a few from\\nwhich to choose from:\\n\\n-   De...\"],[\"[Lysandre's](https:\\u002f\\u002fgist.github.com\\u002fLysandreJik\\u002fdb4c948f6b4483960de5cbac598ad4ed)\\nintegration check...\"],[\"We expect that every model added to 🤗 Transformers passes a couple of\\nintegration tests, meaning tha...\"],[\"-   Find the best way of debugging intermediate results. Is the original\\n    repository written in P...\"],[\"where in the forward call the string input is changed to input ids\\n    and start from this point. Th...\"],[\"#### More details on how to create a debugging environment for [camelcase name of model] \\n\\n[TODO FIL...\"],[\"You should do the following:\\n\\n1.  Create a branch with a descriptive name from your main branch\\n\\n```...\"],[\"In the same way, [name of mentor] will open comments when reviewing\\nyour code. We recommend asking m...\"],[\"**Note** that at this point, you don't have to be very sure that your\\ncode is fully correct or clean...\"],[\"[TODO FILL: Here the mentor should add very specific information on what exactly has to be changed f...\"],[\"Now we can create an instance of this model definition which will fill\\nall weights: `dense`, `interm...\"],[\"In the conversion script, you should fill those randomly initialized\\nweights with the exact weights ...\"],[\"Finally, you should also check that **all** required weights are\\ninitialized and print out all check...\"],[\"[TODO FILL: Here the model name might have to be adapted, *e.g.*, maybe [camelcase name of model]For...\"],[\"-   Some layers were not added, *i.e.* an activation layer\\n    was not added, or the residual connec...\"],[\"When you're confident that both implementations yield the same output,\\nverifying the outputs with\\n`t...\"],[\"```python\\nRUN_SLOW=1 pytest -sv tests\\u002ftest_modeling_[lowercase name of model].py::[camelcase name of...\"],[\"```bash\\ninput_str = \\\"This is a long example input string containing special characters .$?-, numbers...\"],[\"[TODO FILL: Here mentor should again point to an existing similar test of another model that the stu...\"],[\"Next, make sure that the docstring added to\\n`src\\u002ftransformers\\u002fmodels\\u002f[lowercase name of model]\\u002fmodel...\"],[\"It is worth spending some time to create fitting model cards for each\\ncheckpoint. The model cards sh...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"For Chinese models, we need to generate a reference files (which requires the ltp library), because ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Implementation Notes\\n\\n- Each model is about 298 MB on disk, there are more than 1,000 models.\\n- T...\"],[\"## Examples\\n\\n- Since Marian models are smaller than many other translation models available in the l...\"],[\"Here is the code to see all available pretrained models on the hub:\\n\\n```python\\nfrom huggingface_hub ...\"],[\"Example of translating english to many romance languages, using old-style 2 character language codes...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Att...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recent progress in language model pre-training has a...\"],[\"## Usage tips\\n\\n- TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPE...\"],[\"\\u003e\\u003e\\u003e # decode back to text\\n\\u003e\\u003e\\u003e predicted_answer = tokenizer.batch_decode(outputs, skip_special_tokens...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"microsoft\\u002ftapex-large-finetuned-tabfact\\\")\\n\\u003e\\u003e\\u003e model =...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"The (`sorted`) video paths appear like so:\\n\\n```bash\\n...\\n'UCF101_subset\\u002ftrain\\u002fApplyEyeMakeup\\u002fv_ApplyE...\"],[\"There are 10 unique classes. For each class, there are 30 videos in the training set.\\n\\n## Load a mod...\"],[\"While the model is loading, you might notice the following warning:\\n\\n```bash\\nSome weights of the mod...\"],[\"**Note** that [this checkpoint](https:\\u002f\\u002fhuggingface.co\\u002fMCG-NJU\\u002fvideomae-base-finetuned-kinetics) lea...\"],[\"* Image mean and standard deviation with which the video frame pixels will be normalized.\\n* Spatial ...\"],[\"The same sequence of workflow can be applied to the validation and evaluation sets: \\n\\n```py \\n\\u003e\\u003e\\u003e val...\"],[\"**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https:\\u002f\\u002fpyt...\"],[\"\\u003e\\u003e\\u003e def unnormalize_img(img):\\n...     \\\"\\\"\\\"Un-normalizes the image pixels.\\\"\\\"\\\"\\n...     img = (img * std...\"],[\"Most of the training arguments are self-explanatory, but one that is quite important here is `remove...\"],[\"**A note on evaluation**:\\n\\nIn the [VideoMAE paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2203.12602), the authors us...\"],[\"## Inference\\n\\nGreat, now that you have fine-tuned a model, you can use it for inference!\\n\\nLoad a vid...\"],[\"...     device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n...     inputs = {k: v...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Natural language understanding comprises a wide rang...\"],[\"Note:\\n\\nIf you want to reproduce the original tokenization process of the *OpenAI GPT* paper, you wil...\"],[\"- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https:\\u002f\\u002fwww.philschmid.de\\u002f...\"],[\"- [`TFOpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https:\\u002f\\u002f...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- A course material on [Byte-Pair Encoding tokenizat...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"这里是一些例子：\\n- [用 BERT 做掩码填词](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+F...\"],[\"- [用 DistilBERT 做问答](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**，由抱抱脸团队打造，是一个文本生成的官方 demo。\\n\\n## 如果你在寻...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e model = TFAutoModel.from_pret...\"],[\"当这些后端之一安装成功后， 🤗 Transformers 可依此安装：\\n\\n```bash\\npip install transformers\\n```\\n\\n如果你想要试试用例或者想在正式发布前使用最新的开发...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (来自 Google Research and t...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (来自 VinAI Research) 伴...\"],[\"1. **[BlenderbotSmall](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot-small)** (来自 Fa...\"],[\"1. **[BROS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbros)** (来自 NAVER CLOVA) 伴随论文 [BROS: ...\"],[\"1. **[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)** (来自 OpenAI) 伴随论文 [Learning Tr...\"],[\"1. **[Conditional DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconditional_detr)** (来自 M...\"],[\"1. **[CPM-Ant](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpmant)** (from OpenBMB) released ...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (来自 Sen...\"],[\"1. **[DiNAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinat)** (来自 SHI Labs) 伴随论文 [Dilated...\"],[\"1. **[DPR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdpr)** (来自 Facebook) 伴随论文 [Dense Passa...\"],[\"1. **[ERNIE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie)** (来自 Baidu) 伴随论文 [ERNIE: Enh...\"],[\"1. **[Falcon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffalcon)** (from Technology Innovati...\"],[\"1. **[FlauBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflaubert)** (来自 CNRS) 伴随论文 [FlauB...\"],[\"1. **[GIT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgit)** (来自 Microsoft Research) 伴随论文 [G...\"],[\"1. **[GPT NeoX Japanese](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox_japanese)** (来自...\"],[\"1. **[GPTSAN-japanese](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptsan-japanese)** release...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[KOSMOS-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fkosmos-2)** (from Microsoft Resea...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (来自 Meta AI) 伴随论文 [LeViT: A...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (来自 The FAIR team of Meta...\"],[\"1. **[LongT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flongt5)** (来自 Google AI) released 伴...\"],[\"1. **[MarianMT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmarian)** 用 [OPUS](http:\\u002f\\u002fopus.nl...\"],[\"1. **[mBART-50](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (来自 Facebook) 伴随论文 [Mult...\"],[\"1. **[Mixtral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmixtral)** (from Mistral AI) by Th...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (来自 Google Inc...\"],[\"1. **[MusicGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmusicgen)** (from Meta) released ...\"],[\"1. **[Nyströmformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (来自 the Uni...\"],[\"1. **[PatchTSMixer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtsmixer)** (来自  IBM Rese...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[Pop2Piano](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpop2piano)** released with the p...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (来自 Google Research) ...\"],[\"1. **[RoFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froformer)** (来自 ZhuiyiTechnology)...\"],[\"1. **[SEW](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew)** (来自 ASAPP) 伴随论文 [Performance-Ef...\"],[\"1. **[SqueezeBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsqueezebert)** (来自 Berkeley) 伴...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (来自 Google AI) 伴随论文 [Exploring th...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (from Google Research) released...\"],[\"1. **[VAN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvan)** (来自 Tsinghua University and Nan...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (来自 Google AI) 伴随...\"],[\"1. **[ViViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvivit)** (来自 Google Research) releas...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (来自 Microsoft Research) 伴随...\"],[\"1. **[XLM-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-roberta)** (来自 Facebook AI...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (来自 Faceboo...\"],[\"要检查某个模型是否已有 Flax、PyTorch 或 TensorFlow 的实现，或其是否在 🤗 Tokenizers 库中有对应词符化器（tokenizer），敬请参阅[此表](https:\\u002f\\u002fh...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## AWQ\\n\\n\\u003cTip\\u003e\\n\\nTry AWQ quantization with this [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1Hz...\"],[\"```py\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_id = \\\"TheBloke\\u002fzephyr-7B-a...\"],[\"```python\\nimport torch\\nfrom transformers import AwqConfig, AutoModelForCausalLM\\n\\nmodel_id = \\\"TheBlok...\"],[\"model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(0...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe [AutoGPTQ](https:\\u002f\\u002fgithub.com\\u002fPanQiWei\\u002fAutoGPTQ) library implements the GPTQ algorithm, ...\"],[\"```py\\nquantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\\\"auto\\\", quantizati...\"],[\"```py\\nfrom transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\\"{...\"],[\"## bitsandbytes\\n\\n[bitsandbytes](https:\\u002f\\u002fgithub.com\\u002fTimDettmers\\u002fbitsandbytes) is the easiest option f...\"],[\"model_8bit = AutoModelForCausalLM.from_pretrained(\\\"facebook\\u002fopt-350m\\\", load_in_8bit=True, torch_dtyp...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nTraining with 8-bit and 4-bit weights are only supported for training *extra* ...\"],[\"```py\\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\\n    \\\"bigscience\\u002fbloom-1b7\\\",\\n    device_map=...\"],[\"model_8bit = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    device_map=\\\"auto\\\",\\n    quantiza...\"],[\"```py\\nfrom transformers import BitsAndBytesConfig\\n\\nnf4_config = BitsAndBytesConfig(\\n    load_in_4bit...\"],[\"## Benchmarks\\n\\nTo compare the speed, throughput, and latency of each quantization scheme, check the ...\"],[\"The benchmarks indicate AWQ quantization is the fastest for inference, text generation, and has the ...\"],[\"\\u003cfigcaption class=\\\"text-center text-gray-500 text-lg\\\"\\u003eFused module\\u003c\\u002ffigcaption\\u003e\\n\\n|   Batch Size |   ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The model is trained end-to-end with a combination of losses derived from variational lower bound an...\"],[\"This model was contributed by [Matthijs](https:\\u002f\\u002fhuggingface.co\\u002fMatthijs) and [sanchit-gandhi](https...\"],[\"```python\\nfrom transformers import VitsTokenizer\\n\\ntokenizer = VitsTokenizer.from_pretrained(\\\"faceboo...\"],[\"inputs = tokenizer(text=uromaized_text, return_tensors=\\\"pt\\\")\\n\\nset_seed(555)  # make deterministic\\nwi...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper shows that pretraining multilingual langu...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForTokenClassification`] is supported ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForMaskedLM`] is supported by this [example scrip...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForQuestionAnswering`] is supported by t...\"],[\"## XLMRobertaConfig\\n\\n[[autodoc]] XLMRobertaConfig\\n\\n## XLMRobertaTokenizer\\n\\n[[autodoc]] XLMRobertaTok...\"],[\"[[autodoc]] FlaxXLMRobertaForSequenceClassification\\n    - __call__\\n\\n## FlaxXLMRobertaForMultipleChoi...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Inspired by progress in unsupervised representation ...\"],[\"- ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that a different activati...\"],[\"- Alternatively, one can further fine-tune the entire model on a downstream dataset, similar to BERT...\"],[\"| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size** | **Params (M)** | **I...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Piano covers of pop music are enjoyed by many people...\"],[\"## Examples\\n\\n- Example using HuggingFace Dataset:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\u003e...\"],[\"- Example of processing multiple audio files in batch:\\n\\n```python\\n\\u003e\\u003e\\u003e import librosa\\n\\u003e\\u003e\\u003e from transf...\"],[\"\\u003e\\u003e\\u003e inputs = feature_extractor(\\n...     audio=[audio1, audio2], \\n...     sampling_rate=[sr1, sr2], \\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdeformable_d...\"],[\"### Fine-tuning BERT on SQuAD1.0 with relative position embeddings\\n\\nThe following examples show how ...\"],[\"```bash\\n'exact': 83.6802270577105, 'f1': 90.54772098174814\\n```\\n\\nThe change of `max_seq_length` from ...\"],[\"Training with the previously defined hyper-parameters yields the following results:\\n\\n```bash\\nf1 = 93...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Detection Transformer (DETR) directly transforms que...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large Transformer models routinely achieve state-of-...\"],[\"### Axial Positional Encodings\\n\\nAxial Positional Encodings were first implemented in Google's [trax ...\"],[\"Using the above example again, axial position encoding with \\\\\\\\(d^1 = 2^9, d^2 = 2^9, n_s^1 = 2^9, n_...\"],[\"For more information, see the [original Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2001.04451) or this great [blog...\"],[\"### Training\\n\\nDuring training, we must ensure that the sequence length is set to a value that can be...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] {{cookiecutter.camelcase_modelname}}ForCausalLM\\n    - forward\\n\\n\\n## {{cookiecutter.camelc...\"],[\"## TF{{cookiecutter.camelcase_modelname}}ForCausalLM\\n\\n[[autodoc]] TF{{cookiecutter.camelcase_modelna...\"],[\"[[autodoc]] Flax{{cookiecutter.camelcase_modelname}}ForTokenClassification\\n    - call\\n\\n\\n## Flax{{coo...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained language models have attracted increasin...\"],[\"This model was contributed by [kamalkraj](https:\\u002f\\u002fhuggingface.co\\u002fkamalkraj). The original code can b...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, plain vision Transformers (ViTs) have show...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, Speec...\"],[\"To perform inference, one uses the [`generate`] method, which allows to autoregressively generate te...\"],[\"\\u003e\\u003e\\u003e encoder_id = \\\"facebook\\u002fwav2vec2-base-960h\\\"  # acoustic model encoder\\n\\u003e\\u003e\\u003e decoder_id = \\\"bert-base...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\n\\u003e\\u003e\\u003e from accelerate import Accelerator\\n\\n\\u003e\\u003e\\u003e accelerator = Accelerator()\\n```\\n\\n## Prepare to acc...\"],[\"progress_bar = tqdm(range(num_training_steps))\\n\\n  model.train()\\n  for epoch in range(num_epochs):\\n  ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We propose focal modulation networks (FocalNets in s...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| **Task**                     | **Description**                                                    ...\"],[\"| Automatic speech recognition | transcribe speech into text                                        ...\"],[\"Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this ...\"],[\"You need to make sure the sampling rate of the dataset matches the sampling \\nrate [`facebook\\u002fwav2vec...\"],[\"```py\\n\\u003e\\u003e\\u003e model_name = \\\"nlptown\\u002fbert-base-multilingual-uncased-sentiment\\\"\\n```\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cp...\"],[\"## AutoClass\\n\\n\\u003cYoutube id=\\\"AhChOFRegn4\\\"\\u002f\\u003e\\n\\nUnder the hood, the [`AutoModelForSequenceClassification`...\"],[\"* [input_ids](.\\u002fglossary#input-ids): numerical representations of your tokens.\\n* [attention_mask](.g...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nNow pass your preprocessed batch of inputs directly to the model. You just have to unpack th...\"],[\"```py\\n\\u003e\\u003e\\u003e import tensorflow as tf\\n\\n\\u003e\\u003e\\u003e tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\\n\\u003e\\u003e...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import AutoModel\\n\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer...\"],[\"Take a look at the [Create a custom architecture](.\\u002fcreate_a_model) guide for more information about...\"],[\"```py\\n   \\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n   \\u003e\\u003e\\u003e dataset = load_dataset(\\\"rotten_tomatoes\\\")  # ...\"],[\"## Train with TensorFlow\\n\\nAll models are a standard [`tf.keras.Model`](https:\\u002f\\u002fwww.tensorflow.org\\u002fap...\"],[\"```py\\n   \\u003e\\u003e\\u003e from tensorflow.keras.optimizers import Adam\\n\\n   \\u003e\\u003e\\u003e model.compile(optimizer=Adam(3e-5)...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Randomly initializing `EncoderDecoderModel` from model configurations.\\n\\n[`EncoderDecoderModel`] c...\"],[\"\\u003e\\u003e\\u003e tokenizer = BertTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e model = EncoderDecoderModel.f...\"],[\"## Loading a PyTorch checkpoint into `TFEncoderDecoderModel`.\\n\\n[`TFEncoderDecoderModel.from_pretrain...\"],[\"\\u003e\\u003e\\u003e model.config.decoder_start_token_id = tokenizer.cls_token_id\\n\\u003e\\u003e\\u003e model.config.pad_token_id = tok...\"],[\"[[autodoc]] EncoderDecoderConfig\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## EncoderDecoderModel\\n\\n[[autodoc]] Encod...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-attention has become a defacto choice for captu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to M...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we develop and release Llama 2, a coll...\"],[\"Training the model in `float16` is not recommended and is known to produce `nan`; as such, the model...\"],[\"- After conversion, the model and tokenizer can be loaded via:\\n\\n```python\\nfrom transformers import L...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\"\\u002f\\u003e\\n\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1P...\"],[\"🚀 Deploy\\n- [Fine-tune LLaMA 2 (7-70B) on Amazon SageMaker](https:\\u002f\\u002fwww.philschmid.de\\u002fsagemaker-llama...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we present a new network design paradi...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised learning (SSL) achieves great succes...\"],[\"## Resources\\n\\n- [Audio classification task guide](..\\u002ftasks\\u002faudio_classification)\\n- [Automatic speech...\"],[\"Performer fine-tuning\\n\\nExample authors: @TevenLeScao, @Patrickvonplaten\\n\\nPaper authors: Krzysztof Ch...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Deeper neural networks are more difficult to train. ...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"End-to-End finetuning of RAG (including DPR retriever) for Question Answering.\\n\\nThis finetuning scri...\"],[\"# Note\\n\\n⚠️ This project should be run with pytorch-lightning==1.3.1 which has a potential security v...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"This model was contributed by [shangz](https:\\u002f\\u002fhuggingface.co\\u002fshangz).\\n\\n## Usage tips\\n\\n- QDQBERT mod...\"],[\"Example:\\n\\n```python\\n\\u003e\\u003e\\u003e import pytorch_quantization.nn as quant_nn\\n\\u003e\\u003e\\u003e from pytorch_quantization.ten...\"],[\"```python\\n\\u003e\\u003e\\u003e from pytorch_quantization.nn import TensorQuantizer\\n\\n\\u003e\\u003e\\u003e TensorQuantizer.use_fb_fake_q...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this paper, we propose a unified pre-training app...\"],[\"## UniSpeech specific outputs\\n\\n[[autodoc]] models.unispeech.modeling_unispeech.UniSpeechForPreTraini...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before you begin, make sure you have all the necessary libraries installed:\\n\\n```bash\\npip install -q ...\"],[\"## Depth estimation inference by hand\\n\\nNow that you've seen how to use the depth estimation pipeline...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Most neural vocoders employ band-limited mel-spectro...\"],[\"Tips:\\n\\n- The `noise_sequence` argument for [`UnivNetModel.forward`] should be standard Gaussian nois...\"],[\"with torch.no_grad():\\n    audio = model(**inputs)\\n\\n# Remove the extra padding at the end of the outp...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Understanding document images (e.g., invoices) is a ...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr). The original code can be foun...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(image, return_tensors=\\\"pt\\\").pixel_values\\n\\n\\u003e\\u003e\\u003e outputs = model.generate(...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(image, return_tensors=\\\"pt\\\").pixel_values\\n\\n\\u003e\\u003e\\u003e outputs = model.generate(...\"],[\"\\u003e\\u003e\\u003e # load document image from the DocVQA dataset\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\\"hf-internal-testing\\u002fex...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer based models, like BERT and RoBERTa, hav...\"],[\"## IBertConfig\\n\\n[[autodoc]] IBertConfig\\n\\n## IBertModel\\n\\n[[autodoc]] IBertModel\\n    - forward\\n\\n## IBe...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While the Transformer architecture has become the de...\"],[\"- [BEiT](beit) (BERT pre-training of Image Transformers) by Microsoft Research. BEiT models outperfo...\"],[\"- To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-o...\"],[\"an experiment with a self-supervised pre-training objective, namely masked patched prediction (inspi...\"],[\"## Resources\\n\\nDemo notebooks regarding inference as well as fine-tuning ViT on custom data can be fo...\"],[\"⚗️ Optimization\\n\\n- A blog post on how to [Accelerate Vision Transformer (ViT) with Quantization usin...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002fpytorch\\u002ffairseq\\u002fblob\\u002f1f7ef9ed1e1061f8c7f88f...\"],[\"\\u003e\\u003e\\u003e ds = load_dataset(\\\"hf-internal-testing\\u002flibrispeech_asr_dummy\\\", \\\"clean\\\", split=\\\"validation\\\")\\n\\u003e\\u003e\\u003e ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The `padding` argument controls padding. It can be a boolean or a string:\\n\\n  - `True` or `'longest'`...\"],[\"The following table summarizes the recommended way to setup padding and truncation. If you use pairs...\"],[\"| Truncation                           | Padding                           | Instruction            ...\"],[\"|                                      | padding to max sequence in batch  | `tokenizer(batch_senten...\"],[\"|                                      | padding to specific length        | `tokenizer(batch_senten...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"where task name can be one of cola, sst2, mrpc, stsb, qqp, mnli, qnli, rte, wnli.\\n\\nWe get the follow...\"],[\"\\u003e If your model classification head dimensions do not fit the number of labels in the dataset, you c...\"],[\"The following is a multi-label classification example. It fine-tunes BERT on the `reuters21578` data...\"],[\"Using mixed precision training usually results in 2x-speedup for training with the same final result...\"],[\"It offers less options than the script with `Trainer` (for instance you can easily change the option...\"],[\"## XNLI\\n\\nBased on the script [`run_xnli.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fe...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision Transformers (ViT) have shown rapid progress ...\"],[\"This model was contributed by [novice03](https:\\u002f\\u002fhuggingface.co\\u002fnovice03) and [Bearnardd](https:\\u002f\\u002fhu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Fine-tuning ViLT\\n\\nViLT model incorporates text embeddings into a Vision Transformer (ViT), allowi...\"],[\"Let's load the first 200 examples from the validation split and explore the dataset's features:  \\n\\n`...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumen...\"],[\"## Preprocessing data\\n\\nThe next step is to load a ViLT processor to prepare the image and text data ...\"],[\"...     encoding[\\\"labels\\\"] = targets\\n    \\n...     return encoding\\n```\\n\\nTo apply the preprocessing fu...\"],[\"2. Pass the training arguments to [`Trainer`] along with the model, dataset, processor, and data col...\"],[\"```py\\n\\u003e\\u003e\\u003e processor = ViltProcessor.from_pretrained(\\\"MariaK\\u002fvilt_finetuned_200\\\")\\n\\n\\u003e\\u003e\\u003e image = Image....\"],[\"```py \\n\\u003e\\u003e\\u003e example = dataset[0]\\n\\u003e\\u003e\\u003e image = Image.open(example['image_id'])\\n\\u003e\\u003e\\u003e question = example['...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"where task name can be one of cola, mnli, mnli_mismatched, mnli_matched, mrpc, qnli, qqp, rte, sst2,...\"],[\"| Task  | Metric                       | Acc (best run) | Acc (avg\\u002f5runs) | Stdev     | Metrics     ...\"],[\"Some of these results are significantly different from the ones reported on the test set of GLUE ben...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"![model](https:\\u002f\\u002fgithub.com\\u002fnamctin\\u002ftransformers\\u002fassets\\u002f8100\\u002f150af169-29de-419a-8d98-eb78251c21fa)\\n\\n...\"],[\"## PatchTSTConfig\\n\\n[[autodoc]] PatchTSTConfig\\n\\n## PatchTSTModel\\n\\n[[autodoc]] PatchTSTModel\\n    - for...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"예시:\\n- [BERT로 마스킹된 단어 완성하기](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+...\"],[\"- [DistilBERT를 이용한 질문 답변](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+...\"],[\"**[Transformer와 글쓰기](https:\\u002f\\u002ftransformer.huggingface.co)** 는 이 저장소의 텍스트 생성 능력에 관한 Hugging Face 팀의 공식...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e model = AutoModel.from_pretra...\"],[\"1. 필요한 대로 모델이나 예시를 커스터마이즈하세요:\\n    - 우리는 저자가 공개한 결과를 재현하기 위해 각 모델 구조의 예시를 제공합니다.\\n    - 모델 내부 구조는 가능한 ...\"],[\"현재 사용 가능한 모델 체크포인트의 개수: ![](https:\\u002f\\u002fimg.shields.io\\u002fendpoint?url=https:\\u002f\\u002fhuggingface.co\\u002fapi\\u002fshields\\u002fm...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (from VinAI Research)...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (from Facebook) r...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (from Harbin In...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (OFA-Sys 에서) ...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (MetaAI 에서 제공)은 Ba...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (Tsinghua University 에서) Zhengy...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (Microsoft 에서) Pe...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (Facebook 에서) Nicolas Carion,...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (HuggingFace 에서) ...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (Baidu 에서 제공)은 Xuan Ouya...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflava)** (from Facebook AI) released...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (from KAIST) released with th...\"],[\"1. **[GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptj)** (from EleutherAI) released i...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (from Microsoft) ...\"],[\"1. **[ImageGPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fimagegpt)** (OpenAI 에서) Mark Chen...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (Microsoft Resear...\"],[\"1. **[LLaMA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama)** (The FAIR team of Meta AI 에...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Microsoft Research & Unive...\"],[\"1. **[M2M100](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fm2m_100)** (Facebook 에서) Angela Fan...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (Meta and UIUC 에서...\"],[\"1. **[Megatron-GPT2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron_gpt2)** (NVIDIA 에서)...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (Facebook 에서 제공)은 Vineel Pratap...\"],[\"1. **[MPNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmpnet)** (Microsoft Research 에서) Kai...\"],[\"1. **[Nezha](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnezha)** (Huawei Noah’s Ark Lab 에서) ...\"],[\"1. **[OpenLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopen-llama)** (from [s-JoL](http...\"],[\"1. **[PEGASUS-X](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus_x)** (Google 에서) Jason P...\"],[\"1. **[Pix2Struct](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpix2struct)** (Google 에서 제공)은 K...\"],[\"1. **[QDQBert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fqdqbert)** (NVIDIA 에서) Hao Wu, Pat...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (Facebook 에서) Yinhan Li...\"],[\"1. **[SeamlessM4Tv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t_v2)** (from Met...\"],[\"1. **[SpeechToTextTransformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text)** ...\"],[\"1. **[Swin Transformer V2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswinv2)** (Microsoft 에...\"],[\"1. **[Table Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftable-transformer)** (Mi...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (Microsoft 에서) Minghao Li, ...\"],[\"1. **[UniSpeechSat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech-sat)** (Microsoft R...\"],[\"1. **[VipLlava](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvipllava)** (University of Wiscon...\"],[\"1. **[ViTMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_mae)** (Meta AI 에서) Kaiming He,...\"],[\"1. **[Wav2Vec2-Conformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2-conformer)** (...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (Facebook AI 에서 제공) Xi Victor...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (Meta AI 에서) Davis Liang, H...\"],[\"1. **[YOSO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyoso)** (the University of Wisconsin ...\"],[\"각 모델이 Flax, PyTorch, TensorFlow으로 구현되었는지 또는 🤗 Tokenizers 라이브러리가 지원하는 토크나이저를 사용하는지 확인하려면, [이 표](https...\"],[\"## 인용\\n\\n🤗 Transformers 라이브러리를 인용하고 싶다면, 이 [논문](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)을...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Intel® oneCCL Bindings for PyTorch installation\\n\\nWheel files are available for the following Pyt...\"],[\"for Intel® oneCCL \\u003e= 1.12.0\\n```\\noneccl_bindings_for_pytorch_path=$(python -c \\\"from oneccl_bindings_f...\"],[\"Let's see an example with the [question-answering example](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransforme...\"],[\"## Usage with Kubernetes\\n\\nThe same distributed training job from the previous section can be deploye...\"],[\"### PyTorchJob Specification File\\n\\nThe [Kubeflow PyTorchJob](https:\\u002f\\u002fwww.kubeflow.org\\u002fdocs\\u002fcomponent...\"],[\"The snippet below is an example of a yaml file for a PyTorchJob with 4 workers running the\\n[question...\"],[\"- name: TRANSFORMERS_CACHE\\n                value: \\\"\\u002ftmp\\u002fpvc-mount\\u002ftransformers_cache\\\"\\n              ...\"],[\"\\u003cTip\\u003e\\n\\nThe CPU resource limits\\u002frequests in the yaml are defined in [cpu units](https:\\u002f\\u002fkubernetes.io...\"],[\"The logs for worker can be viewed using `kubectl logs -n kubeflow \\u003cpod name\\u003e`. Add `-f` to stream th...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper states the following:\\n\\n*Visual language data such as plots, charts, and in...\"],[\"## Usage\\n\\nCurrently 6 checkpoints are available for MatCha:\\n\\n- `google\\u002fmatcha`: the base MatCha mode...\"],[\"## Fine-tuning\\n\\nTo fine-tune MatCha, refer to the pix2struct [fine-tuning notebook](https:\\u002f\\u002fgithub.c...\"],[\"!--Copyright 2022 The HuggingFace Team and The OpenBMB Team. All rights reserved.\\n\\nLicensed under th...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recent breakthroughs in natural language process...\"],[\"url = 'http:\\u002f\\u002fimages.cocodataset.org\\u002fval2017\\u002f000000039769.jpg'\\nimage = Image.open(requests.get(url, ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[IDEFICS](..\\u002fmodel_doc\\u002fidefics) is an open-access vision and language model based on [Flamingo](http...\"],[\"Before you begin, make sure you have all the necessary libraries installed. \\n\\n```bash\\npip install -q...\"],[\"\\u003e\\u003e\\u003e model = IdeficsForVisionText2Text.from_pretrained(\\n...     checkpoint,\\n...     quantization_conf...\"],[\"\\u003cTip\\u003e\\n\\nIt is a good idea to include the `bad_words_ids` in the call to `generate` to avoid errors ar...\"],[\"## Few-shot prompting\\n\\nWhile IDEFICS demonstrates great zero-shot results, your task may require a c...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"Photo by [Jarritos Mexican Soda](https:\\u002f\\u002funsplash.com\\u002f@jarritos). \\n\\nYou can steer the model from ima...\"],[\"Photo by [Peter Wendt](https:\\u002f\\u002funsplash.com\\u002f@peterwendt).\\n\\nWe can instruct the model to classify the...\"],[\"Photo by [Craig Tidball](https:\\u002f\\u002funsplash.com\\u002f@devonshiremedia).\\n  \\n```py\\n\\u003e\\u003e\\u003e prompt = [\\\"Instruction...\"],[\"## Running inference in batch mode\\n\\nAll of the earlier sections illustrated IDEFICS for a single exa...\"],[\"The use and prompting for the conversational use is very similar to using the base models: \\n\\n```py\\n\\u003e...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fcircleci.com\\u002fgh\\u002fhuggingface\\u002ftransformers\\\"\\u003e\\n        \\u003cimg alt=...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003cb\\u003eEnglish\\u003c\\u002fb\\u003e |\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fi...\"],[\"In Natural Language Processing:\\n- [Masked word completion with BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-bas...\"],[\"- [Question answering with DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squa...\"],[\"In Computer Vision:\\n- [Image classification with ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16...\"],[\"In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on the...\"],[\"``` python\\n\\u003e\\u003e\\u003e import requests\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n# Do...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e model = AutoModel.from_pretra...\"],[\"1. Lower compute costs, smaller carbon footprint:\\n    - Researchers can share trained models instead...\"],[\"## Installation\\n\\n### With pip\\n\\nThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+,...\"],[\"## Model architectures\\n\\n**[All the model checkpoints](https:\\u002f\\u002fhuggingface.co\\u002fmodels)** provided by 🤗...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (from VinAI Research)...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (from Facebook) r...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (from Harbin In...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (from OFA-Sys...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (from MetaAI) rele...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (from Tsinghua University) rele...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (from Microsoft) ...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (from Facebook) released with...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (from Baidu) released wi...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflava)** (from Facebook AI) released...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (from KAIST) released with th...\"],[\"1. **[GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptj)** (from EleutherAI) released i...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (from Microsoft) ...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[KOSMOS-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fkosmos-2)** (from Microsoft Resea...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (from Meta AI) released wit...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[LongT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flongt5)** (from Google AI) released...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released wi...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MobileNetV1](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v1)** (from Google I...\"],[\"1. **[MT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmt5)** (from Google AI) released with ...\"],[\"1. **[Nougat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnougat)** (from Meta AI) released w...\"],[\"1. **[OWLv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlv2)** (from Google AI) released w...\"],[\"1. **[Persimmon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpersimmon)** (from ADEPT) releas...\"],[\"1. **[PoolFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpoolformer)** (from Sea AI Labs...\"],[\"1. **[RAG](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frag)** (from Facebook) released with t...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (from Facebook), releas...\"],[\"1. **[SeamlessM4Tv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t_v2)** (from Met...\"],[\"1. **[SpeechT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeecht5)** (from Microsoft Resea...\"],[\"1. **[SwiftFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswiftformer)** (from MBZUAI) r...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (from Google AI) released with th...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (from Google Research) released...\"],[\"1. **[UPerNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fupernet)** (from Peking University...\"],[\"1. **[VisualBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvisual_bert)** (from UCLA NLP) ...\"],[\"1. **[VITS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvits)** (from Kakao Enterprise) relea...\"],[\"1. **[Whisper](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwhisper)** (from OpenAI) released ...\"],[\"1. **[XLM-ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-prophetnet)** (from Mic...\"],[\"1. **[XLS-R](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxls_r)** (from Facebook AI) released...\"],[\"To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated to...\"],[\"## Citation\\n\\nWe now have a [paper](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f) you can cit...\"],[\"CodeParrot 🦜\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flvwerra\\u002frepo-images\\u002fra...\"],[\"```bash\\nhuggingface-cli login\\n```\\n\\nAdditionally, sure you have git-lfs installed. You can find instr...\"],[\"The script to process the full dataset can be found in `scripts\\u002fpreprocessing.py`. Executing the scr...\"],[\"_Note:_ We originally trained the tokenizer on the unprocessed train split of the dataset `transform...\"],[\"Recall that you can see the full set of possible options with descriptions (for all scripts) by runn...\"],[\"The results as well as reference values are shown in the following table:\\n\\n| Model | pass@1 | pass@1...\"],[\"With the following Docker command you can run the container (`xx.xx` denotes your Docker version), a...\"],[\"### Training\\nYou can configure the model architecture and training parameters as shown below, or put...\"],[\"### Convert model to `transformers`\\nAfter training we want to use the model in `transformers` e.g. t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Low precision data type BFloat16 has been natively supported on the 3rd Generation Xeon® Scalable Pr...\"],[\"Take an example of the use cases on [Transformers question-answering](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"To print summary statistics for the GPU utilization and the training run with the [`Trainer`] we def...\"],[\"\\u003e\\u003e\\u003e model = AutoModelForSequenceClassification.from_pretrained(\\\"bert-large-uncased\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e ...\"],[\"+-----------------------------------------------------------------------------+\\n| Processes:        ...\"],[\"```\\nTime: 57.82\\nSamples\\u002fsecond: 8.86\\nGPU memory occupied: 14949 MB.\\n```\\n\\nWe see that already a relat...\"],[\"1. model weights\\n2. optimizer states\\n3. gradients\\n4. forward activations saved for gradient computat...\"],[\"**Functionality-specific memory**\\n\\nThen, your software could have special memory needs. For example,...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmo...\"],[\"..\\u002fmodel_doc\\u002fopenai-gpt), [OPT](..\\u002fmodel_doc\\u002fopt), [Perceiver](..\\u002fmodel_doc\\u002fperceiver), [Persimmon](...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"Start by loading the IMDb dataset from the 🤗 Datasets library:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets import load_...\"],[\"## Preprocess\\n\\nThe next step is to load a DistilBERT tokenizer to preprocess the `text` field:\\n\\n```p...\"],[\"\\u003e\\u003e\\u003e accuracy = evaluate.load(\\\"accuracy\\\")\\n```\\n\\nThen create a function that passes your predictions an...\"],[\"```py\\n\\u003e\\u003e\\u003e training_args = TrainingArguments(\\n...     output_dir=\\\"my_awesome_model\\\",\\n...     learning...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TFAutoModelForSequenceClassification\\n\\n\\u003e\\u003e\\u003e model = TFAutoModelForS...\"],[\"Then bundle your callbacks together:\\n\\n```py\\n\\u003e\\u003e\\u003e callbacks = [metric_callback, push_to_hub_callback]\\n...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"ste...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n *The design choices in the Transformer attention mec...\"],[\"## Implementation Notes\\n\\n- The original implementation of MEGA had an inconsistent expectation of at...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \\\"ngram\\\" language mode...\"],[\"## XLMProphetNetTokenizer\\n\\n[[autodoc]] XLMProphetNetTokenizer\\n\\n## XLMProphetNetModel\\n\\n[[autodoc]] XL...\"],[\"!--Copyright 2022 NVIDIA and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache L...\"],[\"The abstract from the paper is the following:\\n\\n*Grouping and recognition are important components of...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"root\\u002fcat\\u002f123.png\\nroot\\u002fcat\\u002fnsdf3.png\\nroot\\u002fcat\\u002f[...]\\u002fasd932_.png\\n```\\n\\n## Train the model\\n\\nNext we can ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PreTrainedModel\\n\\n[[autodoc]] PreTrainedModel\\n    - push_to_hub\\n    - all\\n\\n\\u003ca id='from_pretrained-...\"],[\"t0pp = AutoModelForSeq2SeqLM.from_pretrained(\\\"bigscience\\u002fT0pp\\\", device_map=\\\"auto\\\")\\n```\\n\\nYou can insp...\"],[\"```python\\nmodel = T5ForConditionalGeneration.from_pretrained(\\\"t5\\\", torch_dtype=torch.float16)\\n```\\n\\no...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The \\\"Roaring 20s\\\" of visual recognition began with t...\"],[\"\\u003csmall\\u003e ConvNeXT architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2201.03545\\\"\\u003eoriginal pa...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"In this example we will use the vision model from [CLIP](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=clip)\\n...\"],[\"```python\\nfrom modeling_hybrid_clip import FlaxHybridCLIP\\n\\nmodel = FlaxHybridCLIP.from_text_vision_p...\"],[\"mkdir coco_dataset\\nmv train2014 coco_dataset\\u002f\\nmv annotations coco_dataset\\u002f\\n```\\n\\n### Prepare dataset ...\"],[\"## Train the model\\nNext we can run the example script to train the model:\\n\\n```bash\\npython run_hybrid...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision-and-Language Pre-training (VLP) has improved ...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr). The original code can be foun...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In this guide you'll learn how to:\\n\\n* create a zero-shot image classification pipeline\\n* run zero-sh...\"],[\"## Zero-shot image classification by hand\\n\\nNow that you've seen how to use the zero-shot image class...\"],[\"\\u003e\\u003e\\u003e result = [\\n...     {\\\"score\\\": score, \\\"label\\\": candidate_label}\\n...     for score, candidate_label...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recently-proposed Perceiver model obtains good r...\"],[\"Internally, [`PerceiverModel`] will create the latents, which is a tensor of shape `(batch_size, num...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fperceiver_ar...\"],[\"## PerceiverTokenizer\\n\\n[[autodoc]] PerceiverTokenizer\\n    - __call__\\n\\n## PerceiverFeatureExtractor\\n\\n...\"],[\"## PerceiverMultimodalPostprocessor\\n\\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverMultim...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Development of the model was led by [Shinya Otani](https:\\u002f\\u002fgithub.com\\u002fSO0529), [Takayoshi Makabe](ht...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"Community leaders have the right and responsibility to remove, edit, or reject\\ncomments, commits, co...\"],[\"### 2. Warning\\n\\n**Community Impact**: A violation through a single incident or series of\\nactions.\\n\\n*...\"],[\"[homepage]: https:\\u002f\\u002fwww.contributor-covenant.org\\n[v2.1]: https:\\u002f\\u002fwww.contributor-covenant.org\\u002fversio...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"For custom datasets in `jsonlines` format please see: https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002floading_d...\"],[\"The task of summarization supports custom CSV and JSONLINES formats.\\n\\n#### Custom CSV Files\\n\\nIf it's...\"],[\"```bash\\n    --text_column text \\\\\\n    --summary_column summary \\\\\\n```\\n\\n#### Custom JSONLINES Files\\n\\nTh...\"],[\"## With Accelerate\\n\\nBased on the script [`run_summarization_no_trainer.py`](https:\\u002f\\u002fgithub.com\\u002fhuggi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Many real-world applications require the prediction ...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Having downloaded COCO dataset manually you should be able to load with the `ydshieh\\u002fcoc_dataset_scr...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"However, if the preferred batch size fits into memory, there's no reason to apply memory-optimizing ...\"],[\"Note: when using mixed precision with a small model and a large batch size, there will be some memor...\"],[\"[Tensor Core Requirements](https:\\u002f\\u002fdocs.nvidia.com\\u002fdeeplearning\\u002fperformance\\u002fdl-performance-matrix-mu...\"],[\"Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate e...\"],[\"**Gradient checkpointing** offers a compromise between these two approaches and saves strategically ...\"],[\"Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some...\"],[\"You can enable BF16 in the 🤗 Trainer with:\\n\\n```python\\ntraining_args = TrainingArguments(bf16=True, *...\"],[\"## Optimizer choice\\n\\nThe most common optimizer used to train transformer models is Adam or AdamW (Ad...\"],[\"You can switch to Adafactor by setting `optim=\\\"adafactor\\\"` in [`TrainingArguments`]:\\n\\n```py\\ntraining...\"],[\"```py\\nimport bitsandbytes as bnb\\nfrom torch import nn\\nfrom transformers.trainer_pt_utils import get_...\"],[\"## Data preloading\\n\\nOne of the important requirements to reach great training speed is the ability t...\"],[\"If your model fits onto a single GPU and you have enough space to fit a small batch size, you don't ...\"],[\"`torch.compile` has a growing list of backends, which can be found in by calling `torchdynamo.list_b...\"],[\"**Inference-only backend**s:\\n* `dynamo.optimize(\\\"ofi\\\")` -  Uses Torchscript optimize_for_inference. ...\"],[\"accelerator = Accelerator(fp16=training_args.fp16)\\nmodel, optimizer, dataloader = accelerator.prepar...\"],[\"## Efficient Software Prebuilds\\n\\nPyTorch's [pip and conda builds](https:\\u002f\\u002fpytorch.org\\u002fget-started\\u002flo...\"],[\"Since it has been discovered that more parameters lead to better performance, this technique allows ...\"],[\"And for Pytorch DeepSpeed has built one as well: [DeepSpeed-MoE: Advancing Mixture-of-Experts Infere...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nCheck out this [blogpost](https:\\u002f\\u002fpytorch.org\\u002fblog\\u002fout-of-the-box-acceleration\\u002f) to learn mo...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Motivation\\nWithout processing, english-\\u003e romanian mbart-large-en-ro gets BLEU score 26.8 on the W...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"In particular all \\\"Please explain\\\" questions or objectively very user-specific feature requests belo...\"],[\"Let's look at some examples:\\n\\n    The error message, often referred to as an assertion, tells us wha...\"],[\"If the error includes any messages that include bits unique to your filesystem, always remove those ...\"],[\"3. If there is a software failure, always provide the full traceback, for example:\\n\\n   ```python\\n   ...\"],[\"If it's a command line with a long argument list, please consider breaking it down using backslashes...\"],[\"6. If this is an issue in your code, do try to reduce that code to a minimal example that still demo...\"],[\"8. Before reporting an issue, first, always try to update your environment to the latest official ve...\"],[\"If you got helped by one of the developers in the past please don't tag them in future issues, unles...\"],[\"For example, the very first comment is the most important one. If while the thread unfolds you reali...\"],[\"13. If you are replying to a last comment, it's totally fine to make your reply with just your comme...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*With the success of language pretraining, it is high...\"],[\"This model was contributed by [sgugger](https:\\u002f\\u002fhuggingface.co\\u002fsgugger). The original code can be fo...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"## TFFunnelForMultipleChoice\\n\\n[[autodoc]] TFFunnelForMultipleChoice\\n    - call\\n\\n## TFFunnelForTokenC...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\npip install transformers[quality]\\n```\\n\\nor for an editable install:\\n\\n```bash\\npip install -e ....\"],[\"```bash\\npython -m pytest -n 8 --dist=loadfile -rA -s $(cat test_list.txt)\\n```\\n\\nJust in case anything...\"],[\"```bash\\nmake fixup\\n```\\n\\nThis last command will also run all the additional checks for the repository...\"],[\"```bash\\nmake fix-copies\\n```\\n\\nAdditional checks concern PRs that add new models, mainly that:\\n\\n- All ...\"],[\"```py\\n# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\\n```\\n\\nNote that instead of ...\"],[\"```py\\n# Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM with Roberta-\\u003eCa...\"],[\"Movement Pruning: Adaptive Sparsity by Fine-Tuning\\n\\nAuthor: @VictorSanh\\n\\n*Magnitude pruning is a wid...\"],[\"This page contains information on how to fine-prune pre-trained models such as `BERT` to obtain extr...\"],[\"While movement pruning does not directly optimize for memory footprint (but rather the number of non...\"],[\"Note that we built our experiments on top of a stabilized version of the library (commit https:\\u002f\\u002fgit...\"],[\"### Fine-pruning with other methods\\n\\nWe can also explore other fine-pruning methods by changing the ...\"],[\"Iterative Magnitude Pruning\\n```bash\\npython examples\\u002fmovement-pruning\\u002fmasked_run_squad.py \\\\\\n    --out...\"],[\"## Inference speed\\n\\nEarly experiments show that even though models fine-pruned with (soft) movement ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n    \\u003ciframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembed\\u002fH39Z_72...\"],[\"### Encoder[[cv-encoder]]\\n\\nThe [Vision Transformer (ViT)](model_doc\\u002fvit) opened the door to computer...\"],[\"Other vision models, like BeIT and ViTMAE, drew inspiration from BERT's pretraining objective. [BeIT...\"],[\"## Natural language processing\\n\\n\\u003ciframe style=\\\"border: 1px solid rgba(0, 0, 0, 0.1);\\\" width=\\\"1000\\\" h...\"],[\"However, most Transformer models continued to trend towards more parameters, leading to new models f...\"],[\"After GPT-2, language models grew even bigger and are now known as *large language models (LLMs)*. L...\"],[\"### Encoder[[audio-encoder]]\\n\\n[Wav2Vec2](model_doc\\u002fwav2vec2) uses a Transformer encoder to learn spe...\"],[\"### Encoder[[mm-encoder]]\\n\\n[VisualBERT](model_doc\\u002fvisual_bert) is a multimodal model for vision-lang...\"],[\"### Encoder-decoder[[mm-encoder-decoder]]\\n\\nOptical character recognition (OCR) is a long-standing te...\"],[\"### Decoder[[rl-decoder]]\\n\\nThe Decision and Trajectory Transformer casts the state, action, and rewa...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"## Setup\\n\\nGet started by installing 🤗 PEFT:\\n\\n```bash\\npip install peft\\n```\\n\\nIf you want to try out th...\"],[\"model_id = \\\"facebook\\u002fopt-350m\\\"\\npeft_model_id = \\\"ybelkada\\u002fopt-350m-lora\\\"\\n\\nmodel = AutoModelForCausalL...\"],[\"# use adapter_2\\nmodel.set_adapter(\\\"adapter_2\\\")\\noutput_enabled = model.generate(**inputs)\\nprint(token...\"],[\"```py\\nmodel.add_adapter(peft_config)\\n```\\n\\n3. Now you can pass the model to [`Trainer`]!\\n\\n```py\\ntrain...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [jegormeister](https:\\u002f\\u002fhuggingface.co\\u002fjegormeister). The original code...\"],[\"!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apac...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\u003e\\u003e\\u003e device = \\\"cuda\\\" # the...\"],[\"First, make sure to install the latest version of Flash Attention 2 to include the sliding window at...\"],[\"The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommend...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This page provides an overview of the different speech and audio, computer vision, and NLP tasks tha...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n\\u003e\\u003e\\u003e classifier = pipeline(task=\\\"audio-classification\\\", ...\"],[\"Two general ways computer vision tasks can be solved are:\\n\\n1. Use convolutions to learn the hierarch...\"],[\"* self-driving vehicles: detect everyday traffic objects such as other vehicles, pedestrians, and tr...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n\\u003e\\u003e\\u003e segmenter = pipeline(task=\\\"image-segmentation\\\")\\n\\u003e\\u003e\\u003e...\"],[\"## Natural language processing\\n\\nNLP tasks are among the most common types of tasks because text is s...\"],[\"Two common types of token classification are:\\n\\n* named entity recognition (NER): label a token accor...\"],[\"### Question answering\\n\\nQuestion answering is another token-level task that returns an answer to a q...\"],[\"Like question answering, there are two types of summarization:\\n\\n* extractive: identify and extract t...\"],[\"In the early days, translation models were mostly monolingual, but recently, there has been increasi...\"],[\"* masked: the model's objective is to predict a masked token in a sequence with full access to the t...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e import requests\\n\\n\\u003e\\u003e\\u003e url =...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the paper is the following:\\n\\n*Recent work in language modeling demonstrates that t...\"],[\"This model was contributed by [jdemouth](https:\\u002f\\u002fhuggingface.co\\u002fjdemouth). The original code can be ...\"],[\"Summarization (Seq2Seq model) training examples\\n\\nThe following example showcases how to finetune a s...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## GPTSAN Features\\n\\nGPTSAN has some unique features. It has a model structure of Prefix-LM. It works...\"],[\"\\u003e\\u003e\\u003e x_token = tokenizer(\\\"ｳｴ\\\", prefix_text=\\\"ｱｲ\\\")\\ninput_ids:      | SOT | ｱ | ｲ | SEG | ｳ | ｴ |\\ntoken_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Multimodal pre-training with text, layout, and image...\"],[\"\\u003csmall\\u003e MarkupLM architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2110.08518\\\"\\u003eoriginal pa...\"],[\"In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note t...\"],[\"**Use case 3: token classification (training), parse_html=False**\\n\\nFor token classification tasks (s...\"],[\"\\u003e\\u003e\\u003e question = \\\"What's his name?\\\"\\n\\u003e\\u003e\\u003e encoding = processor(html_string, questions=question, return_t...\"],[\"## MarkupLMTokenizerFast\\n\\n[[autodoc]] MarkupLMTokenizerFast\\n    - all\\n\\n## MarkupLMProcessor\\n\\n[[autod...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [andreasmaden](https:\\u002f\\u002fhuggingface.co\\u002fandreasmadsen).\\nThe original cod...\"],[\"\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFRobertaPreLayerNormModel\\n\\n[[autodoc]] TFRobertaPreLayerNormModel\\n    - call\\n\\n## TFR...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Summary\\nIn Phi-1 and Phi-1.5 papers, the authors showed how important the quality of the data is...\"],[\"The abstract from the Phi-1.5 paper is the following:\\n\\n*We continue the investigation into the power...\"],[\"### Example :\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import PhiForCausalLM, AutoTokenizer\\n\\n\\u003e\\u003e\\u003e # define th...\"],[\"\\u003e\\u003e\\u003e # use the model to generate new tokens.\\n\\u003e\\u003e\\u003e generated_output = model.generate(**tokens, use_cach...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"The preferred input format is either a CSV or newline-delimited JSON file that contains a `sentence1...\"],[\"### Multi-GPU and TPU usage\\n\\nBy default, the script uses a `MirroredStrategy` and will use multiple ...\"],[\"### Usage notes\\n\\nThe `--do_train`, `--do_eval` and `--do_predict` arguments control whether training...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*State-of-the-art computer vision systems are trained...\"],[\"This model was contributed by [valhalla](https:\\u002f\\u002fhuggingface.co\\u002fvalhalla). The original code can be ...\"],[\"\\u003e\\u003e\\u003e inputs = processor(text=[\\\"a photo of a cat\\\", \\\"a photo of a dog\\\"], images=image, return_tensors=\\\"...\"],[\"**Image retrieval**\\n\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1bLVwVKpAndpEDHqjzxVPr_9...\"],[\"[[autodoc]] CLIPImageProcessor\\n    - preprocess\\n\\n## CLIPFeatureExtractor\\n\\n[[autodoc]] CLIPFeatureExt...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"For something slightly more challenging, you can also take a look at the [Good Second Issue](https:\\u002f...\"],[\"To get the OS and software versions automatically, run the following command:\\n\\n```bash\\ntransformers-...\"],[\"## Do you want to add documentation?\\n\\nWe're always looking for improvements to the documentation tha...\"],[\"3. Create a new branch to hold your development changes:\\n\\n   ```bash\\n   git checkout -b a-descriptiv...\"],[\"```bash\\n   make quality\\n   ```\\n\\n   Finally, we have a lot of scripts to make sure we don't forget to...\"],[\"If you've already opened a pull request, you'll need to force push with the `--force` flag. Otherwis...\"],[\"☐ All public methods must have informative docstrings (see\\n[`modeling_bert.py`](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"```bash\\npip install -r examples\\u002fxxx\\u002frequirements.txt  # only needed the first time\\npython -m pytest ...\"],[\"```bash\\npython -m unittest discover -s tests -t . -v\\npython -m unittest discover -s examples -t exam...\"],[\"!--Copyright 2023 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace I...\"],[\"This paper has been accepted to the [AAAI'23](https:\\u002f\\u002faaai.org\\u002fConferences\\u002fAAAI-23\\u002f) conference. \\n\\nT...\"],[\"\\u003csmall\\u003e BridgeTower architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2206.08657\\\"\\u003eoriginal...\"],[\"The following example shows how to run image-text retrieval using [`BridgeTowerProcessor`] and [`Bri...\"],[\"\\u003e\\u003e\\u003e # forward pass\\n\\u003e\\u003e\\u003e outputs = model(**encoding)\\n\\n\\u003e\\u003e\\u003e results = processor.decode(outputs.logits.ar...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2021 NVIDIA Corporation. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"## Quantization Aware Training (QAT)\\n\\nCalibrate the pretrained model and finetune with quantization ...\"],[\"```\\npython3 ort-infer-benchmark.py\\n```\\n\\n### Evaluate the INT8 QAT ONNX model inference with TensorRT...\"],[\"### Quantization options\\n\\nSome useful options to support different implementations and optimizations...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Contrastive learning has shown remarkable success in...\"],[\"## ClapModel\\n\\n[[autodoc]] ClapModel\\n    - forward\\n    - get_text_features\\n    - get_audio_features\\n\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### XLM with language embeddings\\n\\nThe following XLM models use language embeddings to specify the la...\"],[\"\\u003e\\u003e\\u003e # We reshape it to be of size (batch_size, sequence_length)\\n\\u003e\\u003e\\u003e langs = langs.view(1, -1)  # is ...\"],[\"## M2M100\\n\\nThe following M2M100 models can be used for multilingual translation:\\n\\n- `facebook\\u002fm2m100...\"],[\"In this example, load the `facebook\\u002fmbart-large-50-many-to-many-mmt` checkpoint to translate Finnish...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"A language model trained for [causal language modeling](tasks\\u002flanguage_modeling) takes a sequence of...\"],[\"Properly setting up the token selection step and the stopping condition is essential to make your mo...\"],[\"After tokenizing the inputs, you can call the [`~generation.GenerationMixin.generate`] method to ret...\"],[\"### Generated output is too short\\u002flong\\n\\nIf not specified in the [`~generation.GenerationConfig`] fil...\"],[\"\\u003e\\u003e\\u003e model_inputs = tokenizer([\\\"I am a cat.\\\"], return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\n\\u003e\\u003e\\u003e # LLM + greedy de...\"],[\"### Wrong prompt\\n\\nSome models and tasks expect a certain input prompt format to work properly. When ...\"],[\"\\u003e\\u003e\\u003e set_seed(0)\\n\\u003e\\u003e\\u003e messages = [\\n...     {\\n...         \\\"role\\\": \\\"system\\\",\\n...         \\\"content\\\": \\\"You...\"],[\"### Latency, throughput and memory utilization\\n\\n1. [Guide](llm_tutorial_optimization) on how to opti...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] data.data_collator.DataCollatorForLanguageModeling\\n    - numpy_mask_tokens\\n    - tf_mask...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We release Code Llama, a family of large language mo...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nThe `Llama2` family models, on which Code Llama is based, were trained using `...\"],[\"Here is a sample usage:\\n\\n```bash\\npython src\\u002ftransformers\\u002fmodels\\u002fllama\\u002fconvert_llama_weights_to_hf.py...\"],[\"Under the hood, the tokenizer [automatically splits by `\\u003cFILL_ME\\u003e`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftran...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"If you get a terrible BLEU score, make sure that you didn't forget to use the `--source_prefix` argu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Illustrating feature maps of the first stage looks like below.\\n\\u003cdiv style=\\\"text-align: center\\\"\\u003e\\n\\u003cimg...\"],[\"## Initializing Backbone Configuration\\n\\nIn computer vision, models consist of backbone, neck, and a ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Arthur Zucker](https:\\u002f\\u002fhuggingface.co\\u002fArthurZ), [Younes Belkada](http...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\" \\u002f\\u003e\\n\\n- A notebook on [fine-tuning OPT with PEFT, bitsandbytes...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\" \\u002f\\u003e\\n\\n- [`OPTForQuestionAnswering`] is supported by this [q...\"],[\"\\u003e\\u003e\\u003e model_inputs = tokenizer([prompt], return_tensors=\\\"pt\\\").to(device)\\n\\u003e\\u003e\\u003e model.to(device)\\n\\n\\u003e\\u003e\\u003e gen...\"],[\"[[autodoc]] TFOPTForCausalLM\\n    - call\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003cjax\\u003e\\n\\n## FlaxOPTModel\\n\\n[[autodoc]] FlaxOPTModel\\n    ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Existing work in translation demonstrated the potent...\"],[\"The [`M2M100Tokenizer`] depends on `sentencepiece` so be sure to install it before running the\\nexamp...\"],[\"\\u003e\\u003e\\u003e # translate Chinese to English\\n\\u003e\\u003e\\u003e tokenizer.src_lang = \\\"zh\\\"\\n\\u003e\\u003e\\u003e encoded_zh = tokenizer(chinese_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- [google\\u002fflan-t5-base](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fflan-t5-base)\\n\\n- [google\\u002fflan-t5-large](https:...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Language model pretraining has led to significant pe...\"],[\"* dynamic masking: tokens are masked differently at each epoch, whereas BERT does it once and for al...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog on [Getting Started with Sentiment Analysis ...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`RobertaForTokenClassification`] is supported by ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- A blog on [How to train a new language model from scratch usi...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- A blog on [Accelerated Inference with Optimum and Tr...\"],[\"## RobertaTokenizer\\n\\n[[autodoc]] RobertaTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_s...\"],[\"[[autodoc]] FlaxRobertaForMultipleChoice\\n    - __call__\\n\\n## FlaxRobertaForTokenClassification\\n\\n[[aut...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"Please discuss on the [forum](https:\\u002f\\u002fdiscuss.huggingface.co\\u002f) or in an [issue](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eExamples for older versions of 🤗 Transformers\\u003c\\u002fsummary\\u003e\\n\\t\\u003cul\\u003e\\n\\t    \\u003cli\\u003e\\u003ca href=...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv4.4.2\\u002fexamples\\\"\\u003ev4.4.2\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv2.4.0\\u002fexamples\\\"\\u003ev2.4.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"Alternatively, you can switch your cloned 🤗 Transformers to a specific version (for instance with v3...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large multimodal models trained on natural documents...\"],[\"!---\\nCopyright 2022 The Microsoft Inc. and The HuggingFace Inc. Team. All rights reserved.\\n\\nLicensed...\"],[\"### What Questions Can be Answered\\n\\nBenefiting from the powerfulness of generative models, TAPEX can...\"],[\"```bash\\nexport EXP_NAME=wikisql_tapex_base\\n\\npython run_wikisql_with_tapex.py \\\\\\n  --do_train \\\\\\n  --do...\"],[\"#### TAPEX-Base on WikiTableQuestions\\n\\nHere is how to run the script on the WikiTableQuestions with ...\"],[\"```bash\\nexport EXP_NAME=wikitablequestions_tapex_large\\n\\npython run_wikitablequestions_with_tapex.py ...\"],[\"#### TAPEX-Base on TabFact\\n\\nWe provide a fine-tuning script of tapex for TableFV on the TabFact benc...\"],[\"```bash\\nexport EXP_NAME=tabfact_tapex_large\\n\\npython run_tabfact_with_tapex.py \\\\\\n  --do_train \\\\\\n  --d...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"🤗 Transformers aporta APIs para descargar rápidamente y usar estos modelos preentrenados en un texto...\"],[\"En procesamiento del lenguaje natural:\\n- [Terminación de palabras enmascaradas con BERT](https:\\u002f\\u002fhug...\"],[\"- [Responder a preguntas con DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-sq...\"],[\"En visión de ordenador:\\n- [Clasificación de imágenes con ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n# Allocate a pipeline for sentiment-analysis\\n\\u003e\\u003e\\u003e cl...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003ca\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-image...\"],[\"El modelo en si es un [Pytorch `nn.Module`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fnn.html#torch.nn.Module)...\"],[\"## ¿Por qué no debería usar transformers?\\n\\n- Esta biblioteca no es una caja de herramientas modular ...\"],[\"Primero, crea un entorno virtual con la versión de Python que vas a usar y actívalo.\\n\\nLuego, deberás...\"],[\"Número actual de puntos de control: ![](https:\\u002f\\u002fimg.shields.io\\u002fendpoint?url=https:\\u002f\\u002fhuggingface.co\\u002fa...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (from VinAI Research)...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (from Facebook) r...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (from Harbin In...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (from OFA-Sys...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (from MetaAI) rele...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (from Tsinghua University) rele...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (from Microsoft) ...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (from Facebook) released with...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (from Baidu) released wi...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflava)** (from Facebook AI) released...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (from KAIST) released with th...\"],[\"1. **[GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptj)** (from EleutherAI) released i...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (from Microsoft) ...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[KOSMOS-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fkosmos-2)** (from Microsoft Resea...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (from Meta AI) released wit...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[LongT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flongt5)** (from Google AI) released...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released wi...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MobileNetV1](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v1)** (from Google I...\"],[\"1. **[MT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmt5)** (from Google AI) released with ...\"],[\"1. **[Nougat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnougat)** (from Meta AI) released w...\"],[\"1. **[OWLv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlv2)** (from Google AI) released w...\"],[\"1. **[Persimmon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpersimmon)** (from ADEPT) releas...\"],[\"1. **[PoolFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpoolformer)** (from Sea AI Labs...\"],[\"1. **[RAG](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frag)** (from Facebook) released with t...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (from Facebook), releas...\"],[\"1. **[SeamlessM4Tv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t_v2)** (from Met...\"],[\"1. **[SpeechT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeecht5)** (from Microsoft Resea...\"],[\"1. **[SwiftFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswiftformer)** (from MBZUAI) r...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (from Google AI) released with th...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (from Google Research) released...\"],[\"1. **[UPerNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fupernet)** (from Peking University...\"],[\"1. **[VisualBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvisual_bert)** (from UCLA NLP) ...\"],[\"1. **[VITS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvits)** (from Kakao Enterprise) relea...\"],[\"1. **[Whisper](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwhisper)** (from OpenAI) released ...\"],[\"1. **[XLM-ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-prophetnet)** (from Mic...\"],[\"1. **[XLS-R](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxls_r)** (from Facebook AI) released...\"],[\"Para comprobar si cada modelo tiene una implementación en Flax, PyTorch o TensorFlow, o tiene un tok...\"],[\"## Citación\\n\\nAhora nosotros tenemos un [papel](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"GPT-NeoX-20B was trained with fp16, thus it is recommended to initialize the model as follows:\\n\\n```p...\"],[\"```bash\\npip install -U flash-attn --no-build-isolation\\n```\\n\\n### Usage\\n\\nTo load a model using Flash A...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide offers an in-depth overview of individual types of parallelism, as well as guidance on wa...\"],[\"If your model is too large for a single GPU, you have several alternatives to consider:\\n\\n1. Pipeline...\"],[\"In the following sections of this guide we dig deeper into how these different parallelism methods w...\"],[\"Key differences include:\\n1. DDP performs only a single communication per batch - sending gradients, ...\"],[\"{'train_runtime': 110.5948, 'train_samples_per_second': 1.808, 'epoch': 0.69}\\n```\\n\\n**DDP w\\u002f NVlink**...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumen...\"],[\"This way each of the 3 GPUs gets the full tensors reconstructed and makes a forward pass with its ow...\"],[\"## From Naive Model Parallelism to Pipeline Parallelism\\n\\nTo explain Pipeline parallelism, we'll firs...\"],[\"Naive Model Parallelism comes several shortcomings:\\n- **All but one GPU are idle at any given moment...\"],[\"PP introduces a new hyperparameter to tune - `chunks`, which determines how many data chunks are sen...\"],[\"Pipeline API solutions have been implemented in:\\n- PyTorch\\n- DeepSpeed\\n- Megatron-LM\\n\\nThese come wit...\"],[\"We have not experimented with Varuna and SageMaker but their papers report that they have overcome t...\"],[\"Here the bubble (idle time) is further minimized by prioritizing backward passes. Varuna further att...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumen...\"],[\"Alternative names:\\n- DeepSpeed calls it [tensor slicing](https:\\u002f\\u002fwww.deepspeed.ai\\u002ftraining\\u002f#model-pa...\"],[\"Since each dimension requires at least 2 GPUs, here you'd need at least 4 GPUs.\\n\\nImplementations:\\n- ...\"],[\"When ZeRO-DP is combined with PP (and optionally TP) it typically enables only ZeRO stage 1 (optimiz...\"],[\"Paper: [\\\"Beyond Data and Model Parallelism for Deep Neural Networks\\\" by Zhihao Jia, Matei Zaharia, A...\"],[\"So the promise is very attractive - it runs a 30min simulation on the cluster of choice and it comes...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"DeepSpeed\\\"\\u003e\\n\\nUse `--num_gpus` to select how many GPUs to use.\\n\\n```bash\\ndee...\"],[\"```bash\\nexport CUDA_DEVICE_ORDER=PCI_BUS_ID\\n```\\n\\n2. GPU compute ability\\n\\n```bash\\nexport CUDA_DEVICE_...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Memory usage and data loading\\n\\nOne thing to note is that all data is loaded into memory in this ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this paper, we design and train a Generative Imag...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Unsupervised pretraining of large neural models has ...\"],[\"\\u003e\\u003e\\u003e # create tokenizer...\\n\\u003e\\u003e\\u003e tokenizer = BertTokenizer.from_pretrained(\\\"bert-large-uncased\\\")\\n\\n\\u003e\\u003e\\u003e i...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained Language Models (PLMs) have proven to be...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recently-developed DETR approach applies the tra...\"],[\"## Resources\\n\\n- [Object detection task guide](..\\u002ftasks\\u002fobject_detection)\\n\\n## ConditionalDetrConfig\\n\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Convolutional Neural Networks (ConvNets) are commonl...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import PreTrainedTokenizerFast\\n\\n\\u003e\\u003e\\u003e fast_tokenizer = PreTrainedToken...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\npython run_audio_classification.py \\\\\\n    --model_name_or_path facebook\\u002fwav2vec2-base \\\\\\n    -...\"],[\"```bash\\npython run_audio_classification.py \\\\\\n    --model_name_or_path facebook\\u002fwav2vec2-base \\\\\\n    -...\"],[\"- [SUPERB Keyword Spotting](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fsuperb#ks)\\n- [Common Language](https:\\u002f\\u002fh...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"*Recent advancements in automatic speech translation have dramatically expanded language coverage, i...\"],[\". For human evaluations, we adapted existing protocols tailored for measuring the most relevant attr...\"],[\"## Usage\\n\\nIn the following example, we'll load an Arabic audio sample and an English text sample and...\"],[\"```python \\n\\u003e\\u003e\\u003e # from audio\\n\\u003e\\u003e\\u003e output_tokens = model.generate(**audio_inputs, tgt_lang=\\\"fra\\\", gener...\"],[\"#### 3. Change the generation strategy\\n\\nYou can use different [generation strategies](..\\u002fgeneration_...\"],[\"#### Difference in the speech encoder\\n\\nThe speech encoder, which is used during the first-pass gener...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten).\\n\\n## Usage...\"],[\"\\u003cPipelineTag pipeline=\\\"automatic-speech-recognition\\\"\\u002f\\u003e\\n\\n- A blog post on [boosting Wav2Vec2 with n-g...\"],[\"## Wav2Vec2ProcessorWithLM\\n\\n[[autodoc]] Wav2Vec2ProcessorWithLM\\n    - __call__\\n    - pad\\n    - from_...\"],[\"...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text\\n...     batch[\\\"trans...\"],[\"## TFWav2Vec2ForSequenceClassification\\n\\n[[autodoc]] TFWav2Vec2ForSequenceClassification\\n    - call\\n\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This is also equivalent to the exponentiation of the cross-entropy between the data and model predic...\"],[\"\\u003cimg width=\\\"600\\\" alt=\\\"Suboptimal PPL not taking advantage of full available context\\\" src=\\\"https:\\u002f\\u002fhu...\"],[\"```python\\nfrom datasets import load_dataset\\n\\ntest = load_dataset(\\\"wikitext\\\", \\\"wikitext-2-raw-v1\\\", sp...\"],[\"nlls.append(neg_log_likelihood)\\n\\n    prev_end_loc = end_loc\\n    if end_loc == seq_len:\\n        break...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"| Task | Example model | Example dataset | 🤗 Datasets | Colab\\n|---|---|---|:---:|:---:|\\n| [**`causal...\"],[\"[Flax](https:\\u002f\\u002fgithub.com\\u002fgoogle\\u002fflax) builds on top of JAX with an ergonomic\\nmodule abstraction usi...\"],[\"For a complete overview of models that are supported in JAX\\u002fFlax, please have a look at [this](https...\"],[\"# 🔥 Model cards now live inside each huggingface.co model repo 🔥\\n\\n\\nFor consistency, ease of use and ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"## ऑनलाइन डेमो\\n\\nआप सबसे सीधे मॉडल पृष्ठ पर परीक्षण कर सकते हैं [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmo...\"],[\"यहाँ कुछ उदाहरण हैं：\\n- [शब्द को भरने के लिए मास्क के रूप में BERT का प्रयोग करें](https:\\u002f\\u002fhuggingfac...\"],[\"- [डिस्टिलबर्ट के साथ प्रश्नोत्तर](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?te...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**，हगिंग फेस टीम द्वारा बनाया गया, यह ...\"],[\"```\\n\\nउत्तर देने के अलावा, पूर्व-प्रशिक्षित मॉडल संगत आत्मविश्वास स्कोर भी देता है, जहां उत्तर टोकनयु...\"],[\"## ट्रांसफार्मर का उपयोग क्यों करें?\\n\\n1. उपयोग में आसानी के लिए उन्नत मॉडल:\\n    - एनएलयू और एनएलजी प...\"],[\"## मुझे ट्रांसफॉर्मर का उपयोग कब नहीं करना चाहिए?\\n\\n- यह लाइब्रेरी मॉड्यूलर न्यूरल नेटवर्क टूलबॉक्स न...\"],[\"जब इनमें से कोई एक बैकएंड सफलतापूर्वक स्थापित हो जाता है, तो ट्रांसफॉर्मर निम्नानुसार स्थापित किए जा...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (Google Research and the ...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (VinAI Research से) स...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (फेसबुक से) साथ म...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (हरबिन इंस्टिट्...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (from OFA-Sys...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (MetaAI से) Baptis...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (सिंघुआ यूनिवर्सिटी से) साथ में...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (Microsoft से) सा...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (फेसबुक से) साथ में कागज [ट्र...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (हगिंगफेस से), सा...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (Baidu से) Xuan Ouyang, ...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffnet)** (गूगल रिसर्च से) साथ वाला पेप...\"],[\"1. **[GPT Neo](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neo)** (EleutherAI से) रिपॉजिट...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (BigCode से) Lou...\"],[\"1. **[HerBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fherbert)** (Allegro.pl, AGH Univer...\"],[\"1. **[Informer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finformer)** (from Beihang Univers...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (from Microsoft R...\"],[\"1. **[LLaMA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama)** (The FAIR team of Meta AI स...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Microsoft Research & Unive...\"],[\"1. **[M2M100](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fm2m_100)** (फेसबुक से) साथ देने वाल...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (मेटा और UIUC से)...\"],[\"1. **[Megatron-BERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron-bert)** (NVIDIA से)...\"],[\"1. **[mLUKE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmluke)** (फ्रॉम Studio Ousia) साथ मे...\"],[\"1. **[MobileViTV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevitv2)** (Apple से) Sach...\"],[\"1. **[NAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnat)** (from SHI Labs) released with t...\"],[\"1. **[OneFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002foneformer)** (SHI Labs से) पेपर ...\"],[\"1. **[PatchTST](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtst)** (IBM से) Yuqi Nie, Na...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[Pop2Piano](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpop2piano)** released with the p...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (from Google Research...\"],[\"1. **[RoCBert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froc_bert)** (from WeChatAI) releas...\"],[\"1. **[Segment Anything](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsam)** (Meta AI से) Alexa...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[Swin2SR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin2sr)** (from University of Wür...\"],[\"1. **[TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapas)** (Google AI से) साथ में कागज...\"],[\"1. **[TVLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftvlt)** (from UNC Chapel Hill) releas...\"],[\"1. **[UnivNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funivnet)** (from Kakao Corporation...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (गूगल एआई ...\"],[\"1. **[ViTMatte](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitmatte)** (HUST-VL से) Jingfeng...\"],[\"1. **[Wav2Vec2Phoneme](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2_phoneme)** (Faceb...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (From Facebook AI) released w...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (from Meta AI) released wit...\"],[\"1. **[YOSO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyoso)** (विस्कॉन्सिन विश्वविद्यालय - ...\"],[\"यह जांचने के लिए कि क्या किसी मॉडल में पहले से ही Flax, PyTorch या TensorFlow का कार्यान्वयन है, या ...\"],[\"## उद्धरण\\n\\nहमने आधिकारिक तौर पर इस लाइब्रेरी का [पेपर](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-d...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Language models have become a key step to achieve st...\"],[\"## FlaubertConfig\\n\\n[[autodoc]] FlaubertConfig\\n\\n## FlaubertTokenizer\\n\\n[[autodoc]] FlaubertTokenizer\\n\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"🤗 Transformers integrates [DeepSpeed](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fDeepSpeed) via 2 options:\\n\\n1. Int...\"],[\"If you're still struggling with the build, first make sure to read [CUDA Extension Installation Note...\"],[\"Again, remember to ensure to adjust `TORCH_CUDA_ARCH_LIST` to the target architectures.\\n\\nYou can fin...\"],[\"You can use a launcher of your choice here. You can continue using the pytorch launcher:\\n\\n```bash\\nto...\"],[\"\\u003ca id='deepspeed-one-gpu'\\u003e\\u003c\\u002fa\\u003e\\n\\n### Deployment with one GPU\\n\\nTo deploy DeepSpeed with one GPU adjust...\"],[\"which enables optimizer offload and some other important features. You may experiment with the buffe...\"],[\"```bash\\npython -m torch.distributed.run --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hos...\"],[\"export GPUS_PER_NODE=8\\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)...\"],[\"# Now proceed as normal, plus pass the deepspeed config file\\ntraining_args = TrainingArguments(..., ...\"],[\"\\\"gradient_accumulation_steps\\\": \\\"auto\\\",\\n    \\\"gradient_clipping\\\": \\\"auto\\\",\\n    \\\"steps_per_print\\\": 2000,...\"],[\"```bash\\ngrep -i Lamb $(find . -name '*json')\\n```\\n\\nSome more examples are to be found in the [main re...\"],[\"\\u003ca id='deepspeed-config-passing'\\u003e\\u003c\\u002fa\\u003e\\n\\n### Passing Configuration\\n\\nAs discussed in this document norm...\"],[\"There are multiple other values that are specific to DeepSpeed-only and those you will have to set m...\"],[\"\\u003ca id='deepspeed-zero2-config'\\u003e\\u003c\\u002fa\\u003e\\n\\n#### ZeRO-2 Config\\n\\nThe following is an example of configuratio...\"],[\"\\u003ca id='deepspeed-zero3-config'\\u003e\\u003c\\u002fa\\u003e\\n\\n#### ZeRO-3 Config\\n\\nThe following is an example of configuratio...\"],[\"`stage3_max_live_parameters` is the upper limit on how many full parameters you want to keep on the ...\"],[\"- `sub_group_size`: `1e9`\\n\\n`sub_group_size` controls the granularity in which parameters are updated...\"],[\"The following configuration example enables NVMe to offload both optimizer states and the params:\\n\\n`...\"],[\"In order to figure out the optimal `aio` configuration block you must run a benchmark on your target...\"],[\"\\\"optimizer\\\": {\\n        \\\"type\\\": \\\"AdamW\\\",\\n        \\\"params\\\": {\\n            \\\"lr\\\": \\\"auto\\\",\\n            \\\"b...\"],[\"\\\"scheduler\\\": {\\n        \\\"type\\\": \\\"WarmupLR\\\",\\n        \\\"params\\\": {\\n            \\\"warmup_min_lr\\\": 0,\\n     ...\"],[\"\\\"gradient_accumulation_steps\\\": \\\"auto\\\",\\n    \\\"gradient_clipping\\\": \\\"auto\\\",\\n    \\\"steps_per_print\\\": 2000,...\"],[\"So now you know there are all these different stages. How to decide which of them to use? This secti...\"],[\"You can, of course, work through these steps in reverse by starting with the most GPU memory efficie...\"],[\"Other quick related performance notes:\\n- if you are training something from scratch always try to ha...\"],[\"### Optimizer and Scheduler\\n\\nAs long as you don't enable `offload_optimizer` you can mix and match D...\"],[\"- `lr` with the value of `--learning_rate`\\n- `betas` with the value of `--adam_beta1 --adam_beta2`\\n-...\"],[\"If you don't configure the `scheduler` entry in the configuration file, the [`Trainer`] will use\\nthe...\"],[\"But then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\\nc...\"],[\"With the 🤗 Trainer you can use `--tf32` to enable it, or disable it with `--tf32 0` or `--no_tf32`. ...\"],[\"You can also enable\\u002fdisable this mode explicitly:\\n\\n```json\\n{\\n    \\\"bf16\\\": {\\n        \\\"enabled\\\": true\\n ...\"],[\"In order to override the default you simply add a new configuration entry:\\n\\n```json\\n{\\n    \\\"communica...\"],[\"```json\\n{\\n    \\\"gradient_accumulation_steps\\\": \\\"auto\\\"\\n}\\n```\\n\\nand the [`Trainer`] will automatically se...\"],[\"```json\\n{\\n    \\\"zero_optimization\\\": {\\n        \\\"stage3_gather_16bit_weights_on_model_save\\\": true\\n    }...\"],[\"\\u003cTip\\u003e\\n\\nNote, that once `load_state_dict_from_zero_checkpoint` was run, the `model` will no longer be...\"],[\"Let's say your checkpoint folder looks like this:\\n\\n```bash\\n$ ls -l output_dir\\u002fcheckpoint-1\\u002f\\n-rw-rw-r...\"],[\"#### Constructing Massive Models\\n\\nDeepSpeed\\u002fZeRO-3 can handle models with Trillions of parameters wh...\"],[\"For full details on this method and other related features please refer to [Constructing Massive Mod...\"],[\"Otherwise you just need to pass the usual [`TrainingArguments`] arguments. For example:\\n\\n```bash\\ndee...\"],[\"Let's estimate how much memory is needed to finetune \\\"bigscience\\u002fT0_3B\\\" on a single GPU:\\n\\n```bash\\n$ ...\"],[\"If you have enough GPU memory make sure to disable the CPU\\u002fNVMe offload as it'll make everything fas...\"],[\"3. Output of:\\n\\n    ```bash\\n    python -c 'import torch; print(f\\\"torch: {torch.__version__}\\\")'\\n    py...\"],[\"### Troubleshooting\\n\\n#### the `deepspeed` process gets killed at startup without a traceback\\n\\nIf the...\"],[\"and you see in your log that Deepspeed reports `OVERFLOW!` as follows:\\n\\n```\\n0%|                     ...\"],[\"In this case you usually need to raise the value of `initial_scale_power`. Setting it to `\\\"initial_s...\"],[\"For example for a pretrained model:\\n\\n```python\\nfrom transformers.integrations import HfDeepSpeedConf...\"],[\"The example has copious notes and is self-documenting.\\n\\nMake sure to:\\n\\n1. disable CPU offload if you...\"],[\"from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\\nfrom transformers.integrat...\"],[\"# keeping the same format as json for consistency, except it uses lower case for true\\u002ffalse\\n# fmt: o...\"],[\"# initialise Deepspeed ZeRO and store only the engine object\\nds_engine = deepspeed.initialize(model=...\"],[\"Starting from `transformers\\u003e=4.28`, if `synced_gpus` isn't explicitly specified, it'll be set to `Tr...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Model checkpoints\\n\\n|     Model Name      | Language |           Description           |\\n|:------...\"],[\"[[autodoc]] ErnieModel\\n    - forward\\n\\n## ErnieForPreTraining\\n\\n[[autodoc]] ErnieForPreTraining\\n    - ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Compression plays an important role on the efficient...\"],[\"## Resources\\n\\nDemo notebooks for Swin2SR can be found [here](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransform...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Multi-GPU and TPU usage\\n\\nBy default, these scripts use a `MirroredStrategy` and will use multipl...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Natural Language Processing (NLP) has recently achie...\"],[\"## Usage tips\\n\\n- MobileBERT is a model with absolute position embeddings so it's usually advised to ...\"],[\"## MobileBertForQuestionAnswering\\n\\n[[autodoc]] MobileBertForQuestionAnswering\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recent \\\"Text-to-Text Transfer Transformer\\\" (T5) ...\"],[\"## MT5Config\\n\\n[[autodoc]] MT5Config\\n\\n## MT5Tokenizer\\n\\n[[autodoc]] MT5Tokenizer\\n\\nSee [`T5Tokenizer`] ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recent studies have demonstrated the efficiency of g...\"],[\"* Causal language modeling (CLM) which is the traditional autoregressive training (so this model cou...\"],[\"## XLMForMultipleChoice\\n\\n[[autodoc]] XLMForMultipleChoice\\n    - forward\\n\\n## XLMForTokenClassificatio...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e tokenizer = NllbTokenizer.from_pretrained(\\\"facebook\\u002fnllb-200-distilled-600M\\\")\\n\\u003e\\u003e\\u003e tokenizer(\\\"How...\"],[\"The abstract of the paper is the following:\\n\\n*Driven by the goal of eradicating language barriers on...\"],[\"This model was contributed by [Lysandre](https:\\u002f\\u002fhuggingface.co\\u002flysandre). The authors' code can be ...\"],[\"\\u003e\\u003e\\u003e article = \\\"Şeful ONU spune că nu există o soluţie militară în Siria\\\"\\n\\u003e\\u003e\\u003e inputs = tokenizer(arti...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [zphang](https:\\u002f\\u002fhuggingface.co\\u002fzphang) with contributions from [Black...\"],[\"This model was contributed by [zphang](https:\\u002f\\u002fhuggingface.co\\u002fzphang) with contributions from [Black...\"],[\"⚡️ Inference\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fDominguesM\\u002falpaca-lora-ptbr-7b\\u002f...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper shows that masked autoencoders (MAE) are ...\"],[\"## Usage tips\\n\\n- MAE (masked auto encoding) is a method for self-supervised pre-training of Vision T...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"ESM-2 outperforms all tested single-sequence protein language models across a range of structure pre...\"],[\"The abstract from \\n\\\"Biological structure and function emerge from scaling unsupervised learning to 2...\"],[\"The abstract from\\n\\\"Language models of protein sequences at the scale of evolution enable accurate st...\"],[\"## Usage tips\\n\\n- ESM models are trained with a masked language modeling (MLM) objective.\\n- The Huggi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We design a family of image classification architect...\"],[\"- Compared to ViT, LeViT models use an additional distillation head to effectively learn from a teac...\"],[\"pre-training.\\n- The authors of LeViT released 5 trained LeViT models, which you can directly plug in...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- Similar to other models in the library, [`TimeSeriesTransformerModel`] is the raw Transformer with...\"],[\"e.g. if a given time-series value was obtained on the 11th of August, then one could have [11, 8] as...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```python\\nfrom datasets import Dataset, DatasetDict, Image\\n\\n# your images can of course have a diffe...\"],[\"```python\\nimport json\\n# simple example\\nid2label = {0: 'cat', 1: 'dog'}\\nwith open('id2label.json', 'w...\"],[\"\\u003e We trained the models using AdamW optimizer for 160K iterations on ADE20K, Cityscapes, and 80K ite...\"],[\"With the default settings, the script fine-tunes a [SegFormer]((https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransfor...\"],[\"## Important notes\\n\\nSome datasets, like [`scene_parse_150`](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fscene_pa...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\u003cdiv class=\\\"flex flex-wrap space-x-1\\\"\\u003e\\n\\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=transfo...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have a potential of learning longer-ter...\"],[\"This model was contributed by [thomwolf](https:\\u002f\\u002fhuggingface.co\\u002fthomwolf). The original code can be ...\"],[\"## TransfoXLConfig\\n\\n[[autodoc]] TransfoXLConfig\\n\\n## TransfoXLTokenizer\\n\\n[[autodoc]] TransfoXLTokeniz...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] modeling_tf_utils.TFCausalLanguageModelingLoss\\n\\n[[autodoc]] modeling_tf_utils.TFMaskedLa...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For any feature you'd like to implement in an example script, please discuss it on the [forum](https...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eExamples for older versions of 🤗 Transformers\\u003c\\u002fsummary\\u003e\\n\\t\\u003cul\\u003e\\n\\t\\t\\u003cli\\u003e\\u003ca href=\\\"ht...\"],[\"\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fv2.6.0\\u002fexamples\\\"\\u003ev2.6.0\\u003c\\u002fa\\u003e\\u003c\\u002fli\\u003e\\n\\t\\t\\u003cli...\"],[\"Then switch your current clone of 🤗 Transformers to a specific version, like v3.5.1 for example:\\n\\n``...\"],[\"```bash\\npython examples\\u002ftensorflow\\u002fsummarization\\u002frun_summarization.py  \\\\\\n    --model_name_or_path t5...\"],[\"```bash\\npython xla_spawn.py --num_cores 8 \\\\\\n    summarization\\u002frun_summarization.py \\\\\\n    --model_nam...\"],[\"```bash\\naccelerate config\\n```\\n\\nTest your setup to make sure it is configured correctly:\\n\\n```bash\\nacc...\"],[\"- `max_train_samples`\\n- `max_eval_samples`\\n- `max_predict_samples`\\n\\n```bash\\npython examples\\u002fpytorch\\u002f...\"],[\"The second method uses the `resume_from_checkpoint path_to_specific_checkpoint` argument to resume t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Get a closer look at [DistilBERT](model_doc\\u002fdistilbert) by accessing [`DistilBertConfig`] to inspect...\"],[\"Once you are satisfied with your model configuration, you can save it with [`~PretrainedConfig.save_...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import DistilBertModel\\n\\n\\u003e\\u003e\\u003e my_config = DistilBertConfig.from_pretrained...\"],[\"Create a pretrained model with [`~TFPreTrainedModel.from_pretrained`]:\\n\\n```py\\n\\u003e\\u003e\\u003e tf_model = TFDisti...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TFDistilBertForSequenceClassification\\n\\n\\u003e\\u003e\\u003e tf_model = TFDistilBer...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import DistilBertTokenizer\\n\\n\\u003e\\u003e\\u003e my_tokenizer = DistilBertTokenizer(vocab...\"],[\"\\u003cTip\\u003e\\n\\nIf you aren't looking for any customization, just use the `from_pretrained` method to load a ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nModify any of the [`Wav2Vec2FeatureExtractor`] parameters to create your custom feature extr...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [stas](https:\\u002f\\u002fhuggingface.co\\u002fstas). The original code can be found\\n[h...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the...\"],[\"New in v2:\\n\\n- **Vocabulary** In v2 the tokenizer is changed to use a new vocabulary of size 128K bui...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## DebertaV2Model\\n\\n[[autodoc]] DebertaV2Model\\n    - forward\\n\\n## DebertaV2Pr...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Multimodal pre-training with text, layout, and image...\"],[\"\\u003cTip\\u003e\\n\\nAs LayoutXLM's architecture is equivalent to that of LayoutLMv2, one can refer to [LayoutLMv2...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present in this paper a new architecture, named C...\"],[\"This model was contributed by [anugunj](https:\\u002f\\u002fhuggingface.co\\u002fanugunj). The original code can be fo...\"],[\"## CvtConfig\\n\\n[[autodoc]] CvtConfig\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## CvtModel\\n\\n[[autodoc]] CvtModel\\n    ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"#### WMT16 English-Romanian Translation Data\\n\\ndownload with this command:\\n```bash\\nwget https:\\u002f\\u002fcdn-d...\"],[\"### Tips and Tricks\\n\\nGeneral Tips:\\n- since you need to run from `examples\\u002flegacy\\u002fseq2seq`, and likel...\"],[\"Summarization Tips:\\n- (summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB...\"],[\"With PyTorch 1.6+ it'll automatically use `native AMP` when `--fp16` is set.\\n\\nTo see all the possibl...\"],[\"For t5, you need to specify --task translation_{src}_to_{tgt} as follows:\\n```bash\\nexport DATA_DIR=wm...\"],[\"Contributions that implement this command for other distributed hardware setups are welcome!\\n\\n#### S...\"],[\"The script accepts the exact same arguments as `run_eval.py`, plus an additional argument `--search`...\"],[\"### Contributing\\n- follow the standard contributing guidelines and code of conduct.\\n- add tests to `...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Driven by the goal of eradicating language barriers on...\"],[\"This model was contributed by [Arthur Zucker](https:\\u002f\\u002fhuggingface.co\\u002fArthurZ).\\nThe original code can...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"facebook\\u002fnllb-moe-54b\\\")\\n\\u003e\\u003e\\u003e model = AutoModelForSeq2S...\"],[\"## NllbMoeConfig\\n\\n[[autodoc]] NllbMoeConfig\\n\\n## NllbMoeTop2Router\\n\\n[[autodoc]] NllbMoeTop2Router\\n   ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by repl...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We tackle the task of conditional music generation. ...\"],[\"```bash\\npython src\\u002ftransformers\\u002fmodels\\u002fmusicgen\\u002fconvert_musicgen_transformers.py \\\\\\n    --checkpoint ...\"],[\"\\u003e\\u003e\\u003e audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)\\n```\\n\\nT...\"],[\"### Audio-Prompted Generation\\n\\nThe same [`MusicgenProcessor`] can be used to pre-process an audio pr...\"],[\"\\u003e\\u003e\\u003e # take the first half of the audio sample\\n\\u003e\\u003e\\u003e sample_2 = sample[\\\"array\\\"][: len(sample[\\\"array\\\"]) ...\"],[\"## Model Structure\\n\\nThe MusicGen model can be de-composed into three distinct stages:\\n1. Text encode...\"],[\"Tips:\\n* MusicGen is trained on the 32kHz checkpoint of Encodec. You should ensure you use a compatib...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## GenerationConfig\\n\\n[[autodoc]] generation.GenerationConfig\\n\\t- from_pretrained\\n\\t- from_model_config...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIn this tutorial, learn to:\\n\\n* Load a pretrained tokenizer.\\n* Load a pretrained image proces...\"],[\"Load a processor with [`AutoProcessor.from_pretrained`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import AutoPro...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nGenerally, we recommend using the `AutoTokenizer` class and the `AutoModelFor` class to load...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoImageProcessor, AutoBackbone\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from PIL im...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--This tip is automatically generated by `make fix-copies`, do not fill manually!--\\u003e\\n\\n[ALBERT](..\\u002f...\"],[\"We encourage you to log in to your Hugging Face account so you can upload and share your model with ...\"],[\"```py\\n\\u003e\\u003e\\u003e eli5 = eli5.train_test_split(test_size=0.2)\\n```\\n\\nThen take a look at an example:\\n\\n```py\\n\\u003e\\u003e...\"],[\"## Preprocess\\n\\n\\u003cYoutube id=\\\"8PmhEIXhBvI\\\"\\u002f\\u003e\\n\\nFor masked language modeling, the next step is to load a...\"],[\"```py\\n\\u003e\\u003e\\u003e eli5 = eli5.flatten()\\n\\u003e\\u003e\\u003e eli5[\\\"train\\\"][0]\\n{'answers.a_id': ['c3d1aib', 'c3d4lya'],\\n 'answ...\"],[\"```py\\n\\u003e\\u003e\\u003e def preprocess_function(examples):\\n...     return tokenizer([\\\" \\\".join(x) for x in examples...\"],[\"Apply the `group_texts` function over the entire dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e lm_dataset = tokenized_eli5.map...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoModelForMaskedLM\\n\\n\\u003e\\u003e\\u003e model = AutoModelForMaskedLM.from_pretr...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import create_optimizer, AdamWeightDecay\\n\\n\\u003e\\u003e\\u003e optimizer = AdamWeightDeca...\"],[\"Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\\n...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"ste...\"],[\"Then return the three masked tokens with the highest probability and print them out:\\n\\n```py\\n\\u003e\\u003e\\u003e top_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Existing pre-trained models are generally geared tow...\"],[\"## Usage tips\\n\\n- UL2 is an encoder-decoder model pre-trained on a mixture of denoising functions as ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- [torch hub integration](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002f.github\\u002fworkflows\\u002fgi...\"],[\"### Run a specific test module\\n\\nTo run an individual test module:\\n\\n```bash\\npytest tests\\u002futils\\u002ftest_l...\"],[\"If you want to include only tests that include both patterns, `and` is to be used:\\n\\n```bash\\npytest -...\"],[\"### Run only modified tests\\n\\nYou can run the tests related to the unstaged files or the current bran...\"],[\"```bash\\npytest *ls -1 tests\\u002f*py | grep -v test_modeling*\\n```\\n\\n### Clearing state\\n\\nCI builds and when...\"],[\"```bash\\npip install pytest-random-order\\n```\\n\\nImportant: the presence of `pytest-random-order` will a...\"],[\"### Look and feel variations\\n\\n#### pytest-sugar\\n\\n[pytest-sugar](https:\\u002f\\u002fgithub.com\\u002fFrozenball\\u002fpytest...\"],[\"Let's depict the GPU requirements in the following table:\\n\\n\\n| n gpus | decorator                    ...\"],[\"```bash\\nTRANSFORMERS_TEST_DEVICE=\\\"cpu\\\" pytest tests\\u002futils\\u002ftest_logging.py\\n```\\n\\nThis variable is usef...\"],[\"Currently, only `MANUAL_SEED_FN`, `EMPTY_CACHE_FN` and `DEVICE_COUNT_FN` are supported for device-sp...\"],[\"Creating a URL for a whole test session log:\\n\\n```bash\\npytest --pastebin=all tests\\u002futils\\u002ftest_logging...\"],[\"```bash\\ntest_this1.py::TestMathUnitTest::test_floor_0_negative\\ntest_this1.py::TestMathUnitTest::test...\"],[\"```bash\\npytest test_this2.py::test_floor[negative--1.5--2.0] test_this2.py::test_floor[integer-1-1.0...\"],[\"```python\\nfrom transformers.testing_utils import TestCasePlus\\n\\n\\nclass PathExampleTest(TestCasePlus):...\"],[\"- You can override the default behavior by directly overriding the `before` and `after` args, leadin...\"],[\"-  A **xfail** means that you expect a test to fail for some reason. A common example is a test for ...\"],[\"or skip the whole module:\\n\\n```python no-style\\n@pytest.mark.skipif(sys.platform == 'win32', reason=\\\"d...\"],[\"- All tests that need to download a heavy set of weights or a dataset that is larger than ~50MB (e.g...\"],[\"That report is also useful to find slow outliers that aren't marked as such, or which need to be re-...\"],[\"An important potential issue with capturing stdout is that it may contain `\\\\r` characters that in no...\"],[\"If you need to validate the output of a logger, you can use `CaptureLogger`:\\n\\n```python\\nfrom transfo...\"],[\"```python\\nseed = 42\\n\\n# python RNG\\nimport random\\n\\nrandom.seed(seed)\\n\\n# pytorch RNGs\\nimport torch\\n\\ntor...\"],[\"Testing CI features can be potentially problematic as it can interfere with the normal CI functionin...\"],[\"For simple commands you could also do:\\n\\n```bash\\ncmd_that_may_fail || true\\n```\\n\\nOf course, once satis...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce dense vision transformers, an architect...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr). The original code can be foun...\"],[\"## DPTModel\\n\\n[[autodoc]] DPTModel\\n    - forward\\n\\n## DPTForDepthEstimation\\n\\n[[autodoc]] DPTForDepthEs...\"],[\"Awesome projects built with Transformers\\n\\nThis page lists awesome projects built on top of Transform...\"],[\"Keywords: inpainting, SD, Stable Diffusion\\n\\n## [flair](https:\\u002f\\u002fgithub.com\\u002fflairNLP\\u002fflair)\\n\\nFLAIR is ...\"],[\"Keywords: Dialogue, Chatbots, VQA, Datasets, Agents\\n\\n## [sentence-transformers](https:\\u002f\\u002fgithub.com\\u002fU...\"],[\"Keywords: NLP, Chinese, Research, Industry\\n\\n## [stanza](https:\\u002f\\u002fgithub.com\\u002fstanfordnlp\\u002fstanza)\\n\\nThe ...\"],[\"Keywords: Adapters, LoRA, Parameter-efficient fine-tuning, Hub\\n\\n## [NeMo](https:\\u002f\\u002fgithub.com\\u002fNVIDIA\\u002f...\"],[\"Keywords: Healthcare imaging, Training, Evaluation\\n\\n## [simpletransformers](https:\\u002f\\u002fgithub.com\\u002fThili...\"],[\"Keywords: NLP, Framework, LLM\\n\\n## [spaCy](https:\\u002f\\u002fgithub.com\\u002fexplosion\\u002fspaCy)\\n\\n[spaCy](https:\\u002f\\u002fgithu...\"],[\"Keywords: Haiku, Model parallelism, LLM, TPU\\n\\n## [deepchem](https:\\u002f\\u002fgithub.com\\u002fdeepchem\\u002fdeepchem)\\n\\nD...\"],[\"Keywords: Stable-Diffusion, Blender\\n\\n## [seldon-core](https:\\u002f\\u002fgithub.com\\u002fSeldonIO\\u002fseldon-core)\\n\\nSeld...\"],[\"Keywords: Semantic search, LLM\\n\\n## [djl](https:\\u002f\\u002fgithub.com\\u002fdeepjavalibrary\\u002fdjl)\\n\\nDeep Java Library ...\"],[\"Keywords: Music understanding, Music generation\\n\\n## [dalle-flow](https:\\u002f\\u002fgithub.com\\u002fjina-ai\\u002fdalle-fl...\"],[\"Keywords: CLIP, Open-source, Contrastive, Image-text\\n\\n## [dalle-playground](https:\\u002f\\u002fgithub.com\\u002fsahar...\"],[\"## [text-generation-webui](https:\\u002f\\u002fgithub.com\\u002foobabooga\\u002ftext-generation-webui\\u002f)\\n\\n[text-generation-we...\"],[\"Keywords: Deployment, BERT, XLNet\\n\\n## [towhee](https:\\u002f\\u002fgithub.com\\u002ftowhee-io\\u002ftowhee)\\n\\nTowhee makes it...\"],[\"Keywords: Training, Generation\\n\\n## [diffgram](https:\\u002f\\u002fgithub.com\\u002fdiffgram\\u002fdiffgram)\\n\\nDiffgram aims t...\"],[\"Keywords: Knowledge Extraction, Knowledge Graphs\\n\\n## [Nebuly](https:\\u002f\\u002fgithub.com\\u002fnebuly-ai\\u002fnebuly)\\n\\n...\"],[\"Keywords: Differential privacy\\n\\n## [LAVIS](https:\\u002f\\u002fgithub.com\\u002fsalesforce\\u002fLAVIS)\\n\\n[LAVIS](https:\\u002f\\u002fgit...\"],[\"Keywords: NLP, Knowledge distillation, Few-shot learning, Multi-modality, Training, Inference, Deplo...\"],[\"Keywords: Code Generation Model\\n\\n## [ktrain](https:\\u002f\\u002fgithub.com\\u002famaiya\\u002fktrain)\\n\\n[ktrain](https:\\u002f\\u002fgit...\"],[\"Keywords: Vietnamese, NLP\\n\\n## [hasktorch](https:\\u002f\\u002fgithub.com\\u002fhasktorch\\u002fhasktorch)\\n\\nHasktorch is a li...\"],[\"Keywords: MLOps\\n\\n## [FederatedScope](https:\\u002f\\u002fgithub.com\\u002falibaba\\u002fFederatedScope)\\n\\n[FederatedScope](ht...\"],[\"Keywords: IR, Information Retrieval, Dense, Sparse\\n\\n## [baal](https:\\u002f\\u002fgithub.com\\u002fbaal-org\\u002fbaal)\\n\\n[ba...\"],[\"Keywords: BentoML, Framework, Deployment, AI Applications\\n\\n## [LLaMA-Efficient-Tuning](https:\\u002f\\u002fgithu...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fcircleci.com\\u002fgh\\u002fhuggingface\\u002ftransformers\\\"\\u003e\\n        \\u003cimg alt=...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003cb\\u003eEnglish\\u003c\\u002fb\\u003e |\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"Os modelos Transformer também podem executar tarefas em diversas modalidades combinadas, como respon...\"],[\"- [Completar palavra mascarada com BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+...\"],[\"- [Resposta a perguntas com DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squ...\"],[\"Em Visão Computacional:\\n- [Classificação de Imagens com ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-...\"],[\"Para celebrar as 100.000 estrelas do Transformers, decidimos destacar a comunidade e criamos a págin...\"],[\"``` python\\n\\u003e\\u003e\\u003e import requests\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n# Do...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, AutoModel\\n\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_...\"],[\"1. Menores custos de computação, menor pegada de carbono:\\n    - Pesquisadores podem compartilhar mod...\"],[\"## Por que não devo usar transformers?\\n\\n- Esta biblioteca não é uma caixa de ferramentas modular par...\"],[\"Quando um desses back-ends estiver instalado, o 🤗 Transformers pode ser instalado usando pip da segu...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (from VinAI Research)...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (from Facebook) r...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (from Harbin In...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (from OFA-Sys...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (from MetaAI) rele...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (from Tsinghua University) rele...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (from Microsoft) ...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (from Facebook) released with...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (from Baidu) released wi...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflava)** (from Facebook AI) released...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (from OpenAI) released w...\"],[\"1. **[GPT-Sw3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt-sw3)** (from AI-Sweden) releas...\"],[\"1. **[GroupViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgroupvit)** (from UCSD, NVIDIA) r...\"],[\"1. **[ImageGPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fimagegpt)** (from OpenAI) release...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (from Microsoft R...\"],[\"1. **[LLaMA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama)** (from The FAIR team of Meta...\"],[\"1. **[Longformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flongformer)** (from AllenAI) re...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released wi...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (from Google I...\"],[\"1. **[MusicGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmusicgen)** (from Meta) released ...\"],[\"1. **[Nyströmformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (from the U...\"],[\"1. **[PEGASUS-X](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus_x)** (from Google) relea...\"],[\"1. **[PoolFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpoolformer)** (from Sea AI Labs...\"],[\"1. **[RAG](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frag)** (from Facebook) released with t...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (from Facebook), releas...\"],[\"1. **[Segment Anything](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsam)** (from Meta AI) rel...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[Swin Transformer V2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswinv2)** (from Micros...\"],[\"1. **[Table Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftable-transformer)** (fr...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (from Microsoft), released ...\"],[\"1. **[UniSpeechSat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech-sat)** (from Micros...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (from Goog...\"],[\"1. **[ViTMatte](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitmatte)** (from HUST-VL) rrelea...\"],[\"1. **[Wav2Vec2Phoneme](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2_phoneme)** (from ...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (From Facebook AI) released w...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (from Meta AI) released wit...\"],[\"1. **[YOSO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyoso)** (from the University of Wisco...\"],[\"1. Quer contribuir com um novo modelo? Adicionamos um **guia detalhado e modelos de exemplo** para o...\"],[\"## Citação\\n\\nAgora temos um [artigo](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f) que você p...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We propose Masked Siamese Networks (MSN), a self-sup...\"],[\"This model was contributed by [sayakpaul](https:\\u002f\\u002fhuggingface.co\\u002fsayakpaul). The original code can b...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Although convolutional neural networks (CNNs) have a...\"],[\"This model was contributed by [Xrenya](\\u003chttps:\\u002f\\u002fhuggingface.co\\u002fXrenya). The original code can be fou...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--This tip is automatically generated by `make fix-copies`, do not fill manually!--\\u003e...\"],[\"[BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [Bert Generation](..\\u002fmodel_doc\\u002fbert-generation...\"],[\"..\\u002fmodel_doc\\u002frwkv), [Speech2Text2](..\\u002fmodel_doc\\u002fspeech_to_text_2), [Transformer-XL](..\\u002fmodel_doc\\u002ftra...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```py\\n\\u003e\\u003e\\u003e eli5 = eli5.train_test_split(test_size=0.2)\\n```\\n\\nThen take a look at an example:\\n\\n```py\\n\\u003e\\u003e...\"],[\"## Preprocess\\n\\n\\u003cYoutube id=\\\"ma1TrR7gE7I\\\"\\u002f\\u003e\\n\\nThe next step is to load a DistilGPT2 tokenizer to proce...\"],[\"```py\\n\\u003e\\u003e\\u003e eli5 = eli5.flatten()\\n\\u003e\\u003e\\u003e eli5[\\\"train\\\"][0]\\n{'answers.a_id': ['c3d1aib', 'c3d4lya'],\\n 'answ...\"],[\"```py\\n\\u003e\\u003e\\u003e def preprocess_function(examples):\\n...     return tokenizer([\\\" \\\".join(x) for x in examples...\"],[\"Apply the `group_texts` function over the entire dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e lm_dataset = tokenized_eli5.map...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\\n\\n\\u003e\\u003e\\u003e model = Aut...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import create_optimizer, AdamWeightDecay\\n\\n\\u003e\\u003e\\u003e optimizer = AdamWeightDeca...\"],[\"Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\\n...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoModelForCausalLM\\n\\n\\u003e\\u003e\\u003e model = AutoModelForCausalLM.from_pretr...\"],[\"Decode the generated token ids back into text:\\n\\n```py\\n\\u003e\\u003e\\u003e tokenizer.batch_decode(outputs, skip_speci...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Semi-supervised learning through pseudo-labeling has...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"The example script uses the 🤗 Datasets library. You can easily customize them to your needs if you n...\"],[\"tokenizer.save_pretrained(model_dir)\\nconfig.save_pretrained(model_dir)\\n```\\n\\n### Train model\\n\\nNext we...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer-based models are widely used in natural ...\"],[\"This model was contributed by [novice03](https:\\u002f\\u002fhuggingface.co\\u002fnovice03). The original code can be ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Susnato Dhar](https:\\u002f\\u002fhuggingface.co\\u002fsusnato).\\nThe original code can ...\"],[\"\\u003e\\u003e\\u003e # Define processor and model.\\n\\u003e\\u003e\\u003e processor = ClvpProcessor.from_pretrained(\\\"susnato\\u002fclvp_dev\\\")\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The tremendous success of CLIP (Radford et al., 2021...\"],[\"\\u003e\\u003e\\u003e # compute image feature\\n\\u003e\\u003e\\u003e inputs = processor(images=image, return_tensors=\\\"pt\\\")\\n\\u003e\\u003e\\u003e image_feat...\"],[\"## ChineseCLIPModel\\n\\n[[autodoc]] ChineseCLIPModel\\n    - forward\\n    - get_text_features\\n    - get_im...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pretrained multilingual large language models have t...\"],[\"## Usage tips \\n\\n- UMT5 was only pre-trained on [mC4](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmc4) excluding ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"ProphetNet is an encoder-decoder model and can predict n-future tokens for \\\"ngram\\\" language modeling...\"],[\"## ProphetNetConfig\\n\\n[[autodoc]] ProphetNetConfig\\n\\n## ProphetNetTokenizer\\n\\n[[autodoc]] ProphetNetTok...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [sshleifer](https:\\u002f\\u002fhuggingface.co\\u002fsshleifer). The Authors' code can b...\"],[\"## Implementation Notes\\n\\n- All models are transformer encoder-decoders with 16 layers in each compon...\"],[\"## Resources\\n\\n- [Script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fresearch_pro...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [anton-l](https:\\u002f\\u002fhuggingface.co\\u002fanton-l).\\n\\n## Usage tips\\n\\n- SEW-D is ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While the general idea of self-supervised learning i...\"],[\"## Usage tips\\n\\n- Data2VecAudio, Data2VecText, and Data2VecVision have all been trained using the sam...\"],[\"**Data2VecVision documentation resources**\\n- [Image classification](..\\u002ftasks\\u002fimage_classification)\\n-...\"],[\"## Data2VecVisionForSemanticSegmentation\\n\\n[[autodoc]] Data2VecVisionForSemanticSegmentation\\n    - fo...\"],[\"Token classification\\n\\n## PyTorch version, no Trainer\\n\\nFine-tuning (m)LUKE for token classification t...\"],[\"This command is the same and will work for:\\n\\n- a CPU-only setup\\n- a setup with one GPU\\n- a distribut...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Building open-domain chatbots is a challenging area fo...\"],[\"## BlenderbotSmallTokenizerFast\\n\\n[[autodoc]] BlenderbotSmallTokenizerFast\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e books = load_dataset(\\\"opus_books\\\", \\\"en-fr\\\")\\n```\\n\\nSp...\"],[\"To apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`...\"],[\"\\u003e\\u003e\\u003e def postprocess_text(preds, labels):\\n...     preds = [pred.strip() for pred in preds]\\n...     la...\"],[\"\\u003e\\u003e\\u003e model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\\n```\\n\\nAt this point, only three steps r...\"],[\"\\u003c\\u002fTip\\u003e\\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate s...\"],[\"Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\\n\\n```py\\n\\u003e\\u003e...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n\\u003e\\u003e\\u003e translator = pipeline(\\\"translation\\\", model=\\\"my_awes...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TFAutoModelForSeq2SeqLM\\n\\n\\u003e\\u003e\\u003e model = TFAutoModelForSeq2SeqLM.from...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nagent.run(\\\"Caption the following image\\\", image=image)\\n```\\n\\n| **Input**                        ...\"],[\"## Quickstart\\n\\nBefore being able to use `agent.run`, you will need to instantiate an agent, which is...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nYou're now good to go! Let's dive into the two APIs that you now have at your disposal.\\n\\n###...\"],[\"\\u003cTip\\u003e\\n\\nThis can be helpful when the model is unable to understand your request and mixes tools. An e...\"],[\"We have turned these off for now, but in order to see how to set up remote executors tools yourself,...\"],[\"Then, we don't allow any attribute lookup or imports (which shouldn't be needed anyway for passing a...\"],[\"These tools have an integration in transformers, and can be used manually as well, for example:\\n\\n```...\"],[\"### Code generation\\n\\nSo far we have shown how to use the agents to perform actions for you. However,...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The paper aims at creating a single unified foundation model which can work across vision, language\\n...\"],[\"[[autodoc]] FlavaImageCodebook\\n    - forward\\n    - get_codebook_indices\\n    - get_codebook_probs\\n\\n##...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale language models show promising text gene...\"],[\"This model was contributed by [keskarnitishr](https:\\u002f\\u002fhuggingface.co\\u002fkeskarnitishr). The original co...\"],[\"[[autodoc]] TFCTRLModel\\n    - call\\n\\n## TFCTRLLMHeadModel\\n\\n[[autodoc]] TFCTRLLMHeadModel\\n    - call\\n\\n...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- Pre-trained on C4 only without mixing in the downstream tasks.\\n\\n- No parameter sharing between the...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In the first case, the list of IDs will be extended by the padding indices. We can pass a list to th...\"],[\"## C\\n\\n### causal language modeling\\n\\nA pretraining task where the model reads the texts in order and ...\"],[\"Learn more about how DataParallel works [here](perf_train_gpu_many#dataparallel-vs-distributeddatapa...\"],[\"### feed forward chunking\\n\\nIn each residual attention block in transformers the self-attention layer...\"],[\"See the [Fine-tune a pretrained model](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002ftraining) tutorial f...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import BertTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer = BertTokenizer.from_pretrained(...\"],[\"we will see\\n\\n```python\\n\\u003e\\u003e\\u003e print(decoded_sequence)\\n[CLS] A Titan RTX has 24GB of VRAM [SEP]\\n```\\n\\nbec...\"],[\"- For sequence classification models, ([`BertForSequenceClassification`]), the model expects a tenso...\"],[\"`class_labels` and `boxes` key where each value of the batch corresponds to the expected label and n...\"],[\"Each model's labels may be different, so be sure to always check the documentation of each model for...\"],[\"For more details, see [Pipelines for inference](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fpipeline_tu...\"],[\"### preprocessing\\n\\nThe task of preparing raw data into a format that can be easily consumed by machi...\"],[\"### self-attention\\n\\nEach element of the input finds out which other elements of the input they shoul...\"],[\"### supervised learning\\n\\nA form of model training that directly uses labeled data to correct and ins...\"],[\"\\u003e\\u003e\\u003e tokenizer = BertTokenizer.from_pretrained(\\\"bert-base-cased\\\")\\n\\u003e\\u003e\\u003e sequence_a = \\\"HuggingFace is ba...\"],[\"## Z\\n\\n### Zero Redundancy Optimizer (ZeRO)\\n\\nParallelism technique which performs sharding of the ten...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Based on the script [`run_image_classification.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob...\"],[\"\\u003e If your model classification head dimensions do not fit the number of labels in the dataset, you c...\"],[\"```python\\nfrom datasets import load_dataset\\n\\n# example 1: local folder\\ndataset = load_dataset(\\\"image...\"],[\"2. Log in with your HuggingFace account credentials using `huggingface-cli`:\\n\\n```bash\\n$ huggingface-...\"],[\"- single\\u002fmultiple CPUs\\n- single\\u002fmultiple GPUs\\n- TPUs\\n\\nNote that this library is in alpha release so ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Here's a code snippet you can use to listen to the resulting audio in a notebook: \\n\\n```python\\n\\u003e\\u003e\\u003e fr...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import notebook_login\\n\\n\\u003e\\u003e\\u003e notebook_login()\\n```\\n\\n## Load the dataset\\n...\"],[\"### Text cleanup for SpeechT5 tokenization \\n\\nStart by cleaning up the text data. You'll need the tok...\"],[\"\\u003e\\u003e\\u003e dataset_vocab = set(vocabs[\\\"vocab\\\"][0])\\n\\u003e\\u003e\\u003e tokenizer_vocab = {k for k, _ in tokenizer.get_vocab...\"],[\"\\u003e\\u003e\\u003e for speaker_id in dataset[\\\"speaker_id\\\"]:\\n...     speaker_counts[speaker_id] += 1\\n```\\n\\nBy plottin...\"],[\"### Speaker embeddings\\n\\nTo enable the TTS model to differentiate between multiple speakers, you'll n...\"],[\"### Processing the dataset\\n\\nFinally, let's process the data into the format the model expects. Creat...\"],[\"Now apply the processing function to the entire dataset. This will take between 5 and 10 minutes.\\n\\n`...\"],[\"...         # collate the inputs and targets into a batch\\n...         batch = processor.pad(input_id...\"],[\"The `use_cache=True` option is incompatible with gradient checkpointing. Disable it for training.\\n\\n`...\"],[\"To be able to use your checkpoint with a pipeline, make sure to save the processor with the checkpoi...\"],[\"```py\\n\\u003e\\u003e\\u003e model = SpeechT5ForTextToSpeech.from_pretrained(\\\"YOUR_ACCOUNT\\u002fspeecht5_finetuned_voxpopuli...\"],[\"Finally, it is essential to consider ethical considerations. Although TTS technology has numerous us...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised learning (SSL) is a long-standing go...\"],[\"## Usage tips\\n\\n- UniSpeechSat is a speech model that accepts a float array corresponding to the raw ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It achieves state-of-the-art on both SQA and WTQ, while having comparable performance to SOTA on Wik...\"],[\"In addition, the authors have further pre-trained TAPAS to recognize **table entailment**, by creati...\"],[\"- TAPAS is a model that uses relative position embeddings by default (restarting the position embedd...\"],[\"- TAPAS is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It ...\"],[\"## Usage: fine-tuning\\n\\nHere we explain how you can fine-tune [`TapasForQuestionAnswering`] on your o...\"],[\"To summarize:\\n\\n| **Task**                            | **Example dataset** | **Description**        ...\"],[\"Of course, you don't necessarily have to follow one of these three ways in which TAPAS was fine-tune...\"],[\"Of course, you don't necessarily have to follow one of these three ways in which TAPAS was fine-tune...\"],[\"- `id`: optional, id of the table-question pair, for bookkeeping purposes.\\n- `annotator`: optional, ...\"],[\"**STEP 3: Convert your data into tensors using TapasTokenizer**\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\nThird, give...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TapasTokenizer\\n\\u003e\\u003e\\u003e import pandas as pd\\n\\n\\u003e\\u003e\\u003e model_name = \\\"google\\u002f...\"],[\"\\u003e\\u003e\\u003e class TableDataset(torch.utils.data.Dataset):\\n...     def __init__(self, data, tokenizer):\\n...  ...\"],[\"| **Task**                           | **Required inputs**                                          ...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TapasTokenizer\\n\\u003e\\u003e\\u003e import pandas as pd\\n\\n\\u003e\\u003e\\u003e model_name = \\\"google\\u002f...\"],[\"\\u003e\\u003e\\u003e class TableDataset:\\n...     def __init__(self, data, tokenizer):\\n...         self.data = data\\n.....\"],[\"...     def __len__(self):\\n...         return len(self.data)\\n\\n\\n\\u003e\\u003e\\u003e data = pd.read_csv(tsv_path, sep=...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TapasConfig, TapasForQuestionAnswering, AdamW\\n\\n\\u003e\\u003e\\u003e # this is the ...\"],[\"```py\\n\\u003e\\u003e\\u003e import tensorflow as tf\\n\\u003e\\u003e\\u003e from transformers import TapasConfig, TFTapasForQuestionAnswer...\"],[\"## Usage: inference\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\nHere we explain how you can use [`TapasForQuestionAnswe...\"],[\"\\u003e\\u003e\\u003e answers = []\\n\\u003e\\u003e\\u003e for coordinates in predicted_answer_coordinates:\\n...     if len(coordinates) ==...\"],[\"\\u003e\\u003e\\u003e model_name = \\\"google\\u002ftapas-base-finetuned-wtq\\\"\\n\\u003e\\u003e\\u003e model = TFTapasForQuestionAnswering.from_pret...\"],[\"In case of a conversational set-up, then each table-question pair must be provided **sequentially** ...\"],[\"## Translating the Transformers documentation into your language\\n\\nAs part of our mission to democrat...\"],[\"**✍️ Start translating**\\n\\nThe fun part comes - translating the text!\\n\\nThe first thing we recommend i...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import notebook_login\\n\\n\\u003e\\u003e\\u003e notebook_login()\\n```\\n\\n## Load SWAG dataset...\"],[\"```py\\n\\u003e\\u003e\\u003e ending_names = [\\\"ending0\\\", \\\"ending1\\\", \\\"ending2\\\", \\\"ending3\\\"]\\n\\n\\n\\u003e\\u003e\\u003e def preprocess_function(...\"],[\"\\u003e\\u003e\\u003e @dataclass\\n... class DataCollatorForMultipleChoice:\\n...     \\\"\\\"\\\"\\n...     Data collator that will ...\"],[\"...     tokenizer: PreTrainedTokenizerBase\\n...     padding: Union[bool, str, PaddingStrategy] = True...\"],[\"Then create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compu...\"],[\"\\u003e\\u003e\\u003e trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     train_dataset=tokeni...\"],[\"\\u003e\\u003e\\u003e tf_validation_set = model.prepare_tf_dataset(\\n...     tokenized_swag[\\\"validation\\\"],\\n...     shuf...\"],[\"Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\\n...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"my_...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"**Note:** This script only works with models that have a fast tokenizer (backed by the 🤗 Tokenizers ...\"],[\"#### Command for SQuAD1.0:\\n\\n```bash\\npython run_qa_beam_search.py \\\\\\n    --model_name_or_path xlnet-la...\"],[\"## Accelerate-based scripts\\n\\nBased on the scripts `run_qa_no_trainer.py` and `run_qa_beam_search_no_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale pretrained language models have achieved...\"],[\"## RoCBertForPreTraining\\n\\n[[autodoc]] RoCBertForPreTraining\\n    - forward\\n\\n## RoCBertForCausalLM\\n\\n[[...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transfer of pre-trained representations improves sam...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Local attention\\n\\n[Longformer](#longformer) uses local attention: often, the local context (e.g., ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import notebook_login\\n\\n\\u003e\\u003e\\u003e notebook_login()\\n```\\n\\n## Types of Segmenta...\"],[\"```python\\nsemantic_segmentation = pipeline(\\\"image-segmentation\\\", \\\"nvidia\\u002fsegformer-b1-finetuned-city...\"],[\"```python\\ninstance_segmentation = pipeline(\\\"image-segmentation\\\", \\\"facebook\\u002fmask2former-swin-large-ci...\"],[\"```bash\\n[{'score': 0.999981,\\n  'label': 'car',\\n  'mask': \\u003cPIL.Image.Image image mode=L size=612x415\\u003e...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```py\\n\\u003e\\u003e\\u003e import json\\n\\u003e\\u003e\\u003e from huggingface_hub import cached_download, hf_hub_url\\n\\n\\u003e\\u003e\\u003e repo_id = \\\"hu...\"],[\"# step 2: create DatasetDict\\n     dataset = DatasetDict({\\n          \\\"train\\\": train_dataset,\\n        ...\"],[\"```py\\n\\u003e\\u003e\\u003e from torchvision.transforms import ColorJitter\\n\\n\\u003e\\u003e\\u003e jitter = ColorJitter(brightness=0.25, ...\"],[\"```py\\n\\u003e\\u003e\\u003e import tensorflow as tf\\n\\n\\n\\u003e\\u003e\\u003e def aug_transforms(image):\\n...     image = tf.keras.utils.im...\"],[\"```py\\n\\u003e\\u003e\\u003e train_ds.set_transform(train_transforms)\\n\\u003e\\u003e\\u003e test_ds.set_transform(val_transforms)\\n```\\n\\u003c\\u002ft...\"],[\"\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003ctf\\u003e\\n\\n```py\\n\\u003e\\u003e\\u003e def compute_metrics(eval_pred):\\n...  ...\"],[\"\\u003e\\u003e\\u003e model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id...\"],[\"```py\\n\\u003e\\u003e\\u003e trainer.push_to_hub()\\n```\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003ctf\\u003e\\n\\u003cTip\\u003e\\n\\nIf you...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import DefaultDataCollator\\n\\n\\u003e\\u003e\\u003e data_collator = DefaultDataCollator(retu...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\nWe will now see how to infer without a pipeline. Process the image with an ...\"],[\"\\u003e\\u003e\\u003e pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]\\n```\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nTo visua...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Scene text recognition (STR) has been an active rese...\"],[\"\\u003csmall\\u003e MGP-STR architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2209.03592\\\"\\u003eoriginal pap...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(images=image, return_tensors=\\\"pt\\\").pixel_values\\n\\u003e\\u003e\\u003e outputs = model(pix...\"],[\"Zero-shot classifier distillation\\n\\nAuthor: @joeddav \\n\\nThis script provides a way to improve the spee...\"],[\"`\\u003cunlabeled_data.txt\\u003e` should be a text file with a single unlabeled example per line. `\\u003cclass_names...\"],[\"Any of the arguments in the 🤗 Trainer's\\n[`TrainingArguments`](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002fma...\"],[\"Unfortunately, inference is slow since each of our 4 class names must be fed through the large model...\"],[\"\\u003e Tip: pass `device=0` when constructing a pipeline to run on a GPU\\n\\nAs we can see, the results of t...\"],[\"!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apac...\"],[\"This model was contributed by [Younes Belkada](https:\\u002f\\u002fhuggingface.co\\u002fybelkada) and [Arthur Zucker](...\"],[\"\\u003e\\u003e\\u003e model = AutoModelForCausalLM.from_pretrained(\\\"mistralai\\u002fMixtral-8x7B-v0.1\\\")\\n\\u003e\\u003e\\u003e tokenizer = Auto...\"],[\"\\u003e\\u003e\\u003e prompt = \\\"My favourite condiment is\\\"\\n\\n\\u003e\\u003e\\u003e model_inputs = tokenizer([prompt], return_tensors=\\\"pt\\\"...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- How to customize the prompt\\n- How to use custom tools\\n- How to create custom tools\\n\\n## Customizing...\"],[\"Tools:\\n- document_qa: This is a tool that answers a question about a document (pdf). It takes an inp...\"],[\"The second part (the bullet points below *\\\"Tools\\\"*) is dynamically added upon calling `run` or `chat...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe third part includes a set of curated examples that show the agent exactly what code it s...\"],[\"```py\\nagent.run(\\\"Draw me a picture of rivers and lakes\\\")\\n```\\n\\nThe user input - *a.k.a* the task: *\\\"D...\"],[\"Upon running `.chat`, the user's input or *task* is cast into an unfinished example of the form:\\n```...\"],[\"```text\\n'This is a tool that creates an image according to a prompt, which is a text description. It...\"],[\"==Code generated by the agent==\\nhouse_image = image_generator(prompt=\\\"A house\\\")\\ncar_image = image_ge...\"],[\"==Code generated by the agent==\\nimage = image_generator(prompt=\\\"A house and car\\\")\\n```\\n\\n\\u003cTip warning=...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nPlease make sure to have the `\\u003c\\u003call_tools\\u003e\\u003e` string defined somewhere in the `...\"],[\"```py\\nprint(f\\\"Description: '{controlnet_transformer.description}'\\\")\\nprint(f\\\"Name: '{controlnet_trans...\"],[\"```py\\nprint(\\\"\\\\n\\\".join([f\\\"- {a}\\\" for a in agent.toolbox.keys()]))\\n```\\n\\n```text\\n- document_qa\\n- image_...\"],[\"```text\\n==Explanation from the agent==\\nI will use the following tool: `image_upscaler` to upscale th...\"],[\"```python\\nfrom transformers import Tool\\n\\n\\nclass HFModelDownloadsTool(Tool):\\n    pass\\n```\\n\\nThis class...\"],[\"```python\\ntool.push_to_hub(\\\"hf-model-downloads\\\")\\n```\\n\\nYou now have your code on the Hub! Let's take ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Replacing existing tools\\n\\nReplacing existing tools can be done simply by assigning a new...\"],[\"agent.run(\\\"Generate an image of the `prompt` after improving it.\\\", prompt=\\\"A rabbit wearing a space ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nHereby, _inference_ is defined by a single forward pass, and _training_ is defined by a singl...\"],[\"Here, three arguments are given to the benchmark argument data classes, namely `models`, `batch_size...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"An instantiated benchmark object can then simply be run by calling `benchmark.run()`.\\n\\n```py\\n\\u003e\\u003e\\u003e res...\"],[\"====================        ENVIRONMENT INFORMATION         ====================\\n\\n- transformers_ver...\"],[\"\\u003e\\u003e\\u003e benchmark = PyTorchBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])\\n\\u003e\\u003e\\u003e benc...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"\\u003e\\u003e\\u003e args = TensorFlowBenchmarkArguments(\\n...     models=[\\\"bert-base\\\", \\\"bert-384-hid\\\", \\\"bert-6-lay\\\"],...\"],[\"====================      INFERENCE - MEMORY - RESULT       ====================\\n-------------------...\"],[\"## Benchmark best practices\\n\\nThis section lists a couple of best practices one should be aware of wh...\"],[\"!--Copyright 2023 IBM and HuggingFace Inc. team. All rights reserved.\\n\\nLicensed under the Apache Lic...\"],[\"The abstract from the paper is the following:\\n\\n*TSMixer is a lightweight neural architecture exclusi...\"],[\"## Sample usage \\n```python\\n\\nfrom transformers import PatchTSMixerConfig, PatchTSMixerForPrediction\\nf...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*GPT-2 is a large transformer-based language model wi...\"],[\"This model was contributed by [thomwolf](https:\\u002f\\u002fhuggingface.co\\u002fthomwolf). The original code can be ...\"],[\"- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https:\\u002f\\u002fwww.philschmid.de\\u002f...\"],[\"- [`TFGPT2LMHeadModel`] is supported by this [causal language modeling example script](https:\\u002f\\u002fgithu...\"],[\"## GPT2Config\\n\\n[[autodoc]] GPT2Config\\n\\n## GPT2Tokenizer\\n\\n[[autodoc]] GPT2Tokenizer\\n    - save_vocabu...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\npython -m venv .env\\n```\\n\\nActivate the virtual environment. On Linux and MacOs:\\n\\n```bash\\nsour...\"],[\"Check if 🤗 Transformers has been properly installed by running the following command:\\n\\n```bash\\npytho...\"],[\"1. Shell environment variable (default): `HUGGINGFACE_HUB_CACHE` or `TRANSFORMERS_CACHE`.\\n2. Shell e...\"],[\"![download-icon](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdownl...\"],[\"Once your file is downloaded and locally cached, specify it's local path to load and use it:\\n\\n```py\\n...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e input_ids = tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids\\n\\n\\u003e\\u003e\\u003e gen_tokens = model.generate(\\n....\"],[\"### Expected speedups\\n\\nBelow is an expected speedup diagram that compares pure inference time betwee...\"],[\"DeeBERT: Early Exiting for *BERT\\n\\nThis is the code base for the paper [DeeBERT: Dynamic Early Exitin...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"You'll see that this dataset already comes with a training set containing 1000 images and a test set...\"],[\"\\u003e\\u003e\\u003e image = cppe5[\\\"train\\\"][0][\\\"image\\\"]\\n\\u003e\\u003e\\u003e annotations = cppe5[\\\"train\\\"][0][\\\"objects\\\"]\\n\\u003e\\u003e\\u003e draw = Ima...\"],[\"## Preprocess the data\\n\\nTo finetune a model, you must preprocess the data you plan to use to match p...\"],[\"```py\\n\\u003e\\u003e\\u003e import albumentations\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e transform = albumentati...\"],[\"...     targets = [\\n...         {\\\"image_id\\\": id_, \\\"annotations\\\": formatted_anns(id_, cat_, ar_, box_...\"],[\"[[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\\n          [ 1.4200,  1.4200,  1.4200,...\"],[\"## Training the DETR model\\nYou have done most of the heavy lifting in the previous sections, so now ...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TrainingArguments\\n\\n\\u003e\\u003e\\u003e training_args = TrainingArguments(\\n...    ...\"],[\"The evaluation step requires a bit of work, but it can be split in three major steps.\\nFirst, prepare...\"],[\"...     with open(path_anno, \\\"w\\\") as file:\\n...         json.dump(output_json, file, ensure_ascii=Fal...\"],[\"Finally, load the metrics and run the evaluation.\\n\\n```py\\n\\u003e\\u003e\\u003e import evaluate\\n\\u003e\\u003e\\u003e from tqdm import tq...\"],[\"...         module.add(prediction=results, reference=labels)\\n...         del batch\\n\\n\\u003e\\u003e\\u003e results = mo...\"],[\"You can also manually replicate the results of the pipeline if you'd like:\\n\\n```py\\n\\u003e\\u003e\\u003e image_processo...\"],[\"Fine-Tuning week of XLSR-Wav2Vec2 on 60 languages 🌍\\n\\nWelcome to the fine-tuning week! The goal of th...\"],[\"**Please keep in mind:**\\nThe spirit of the fine-tuning week is to provide state-of-the-art speech re...\"],[\"## Table of Contents\\n\\n- [Organization of the fine tuning week](#organization-of-the-fine-tuning-week...\"],[\"## Organization of the fine tuning week\\n\\nThe week officially starts on 22.03.2021 and ends on 29.03....\"],[\"The other option is to run a script locally. While this can be more difficult to set up, it also mea...\"],[\"**3.**: Now it is highly recommended to carefully read the google colab without running the cells ye...\"],[\"- Data Processing. You should adapt the data processing to your specific language. In data processin...\"],[\"Also, make sure that you uncomment the cells corresponding to save the preprocessing files and train...\"],[\"Awesome you have successfully trained a XLSR-Wav2Vec2 model 😎. Now you can jump to the section [\\\"How...\"],[\"3. The following examples show how you can launch fine-tuning for the common voice dataset. \\nHere we...\"],[\"Once the training is finished, the model and checkpoints will be saved under the directory specified...\"],[\"The next **very important** step is to create the model card. For people to use your fine-tuned \\nmod...\"],[\"\\u003c======================Copy **raw** version from here=========================\\n---\\nlanguage: {lang_i...\"],[\"# Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, *e.g.* French\\n\\nFin...\"],[\"test_dataset = test_dataset.map(speech_file_to_array_fn)\\ninputs = processor(test_dataset[:2][\\\"speech...\"],[\"test_dataset = test_dataset.map(speech_file_to_array_fn)\\n\\n# Preprocessing the datasets.\\n# We need to...\"],[\"## Rules of training and evaluation\\n\\nIn this section, we will quickly go over what data is allowed t...\"],[\"## Tips and tricks\\n\\nThis section summarizes a couple of tips and tricks across various topics. It wi...\"],[\"- How was XLSR-Wav2Vec2 pretrained? -\\u003e Feature vectors were masked and had to be predicted by the mo...\"],[\"## FAQ\\n\\n- Can a participant fine-tune models for more than one language? \\nYes! A participant can fin...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Read more about it [in the release blogpost](https:\\u002f\\u002fwww.mosaicml.com\\u002fblog\\u002fmpt-7b)\\n\\n## Usage tips\\n\\n-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Open-vocabulary object detection has benefited great...\"],[\"## Usage example\\n\\nOWLv2 is, just like its predecessor [OWL-ViT](owlvit), a zero-shot text-conditione...\"],[\"\\u003e\\u003e\\u003e # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\\n\\u003e\\u003e\\u003e target_sizes...\"],[\"## Owlv2Config\\n\\n[[autodoc]] Owlv2Config\\n    - from_text_vision_configs\\n\\n## Owlv2TextConfig\\n\\n[[autodo...\"],[\"Flax\\u002fJAX community week 🤗\\n\\nWelcome to the Flax\\u002fJAX community week! The goal of this week is to make ...\"],[\"## Organization\\n\\nParticipants can propose ideas for an interesting NLP and\\u002for CV project. Teams of 3...\"],[\"## Important dates\\n\\n- **23.06.** Official announcement of the community week. Make sure to sign-up i...\"],[\"For issues with Flax\\u002fJAX, Transformers, Datasets or for questions that are specific to your project ...\"],[\"All officially defined projects can be seen [here](https:\\u002f\\u002fdocs.google.com\\u002fspreadsheets\\u002fd\\u002f1GpHebL7qr...\"],[\"Once created, the team can start refining their project:\\n\\n- What is the goal of the project? *E.g.*,...\"],[\"**Important**:\\n\\n- For project ideas that see a lot of interest, we are more than happy to create mor...\"],[\"Once your team is defined, you can start working on the project as soon as possible. \\n\\n\\n### Communic...\"],[\"To give an example, a well-defined project would be the following:\\n\\n- task: summarization\\n- model: [...\"],[\"It is recommended that the motivated and experienced team members take the lead in dividing the work...\"],[\"### Other tips\\n\\nHere is a collection of some more tips:\\n\\n- We strongly recommend to work as publicly...\"],[\"You should install the above libraries in a [virtual environment](https:\\u002f\\u002fdocs.python.org\\u002f3\\u002flibrary\\u002f...\"],[\"```bash\\n   $ git checkout -b a-descriptive-name-for-my-project\\n   ```\\n\\n4. Set up a flax environment ...\"],[\"model = FlaxRobertaModel.from_pretrained(\\\"julien-c\\u002fdummy-unknown\\\")\\n\\n# run a forward pass, should ret...\"],[\"```bash\\nsource ~\\u002f\\u003cyour-venv-name\\u003e\\u002fbin\\u002factivate\\n```\\n\\nNext you should install JAX's TPU version on TPU...\"],[\"```python\\nimport jax\\njax.device_count()\\n```\\n\\nThis should display the number of TPU cores, which shou...\"],[\"If you have already cloned that repo, you might need to `git pull` to get the most recent changes in...\"],[\"model = FlaxRobertaModel.from_pretrained(\\\"julien-c\\u002fdummy-unknown\\\")\\n\\n# run a forward pass, should ret...\"],[\"## Quickstart flax and jax in transformers\\n\\nCurrently, we support the following models in Flax. \\nNot...\"],[\"- [Causal language modeling (GPT2)](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fexamples\\u002ff...\"],[\"This section will explain how Flax models are implemented in Transformers and how the design differs...\"],[\"Instantiating an object `model_pytorch` of the class `ModelPyTorch` would actually allocate memory f...\"],[\"At first glance the linear layer class `flax.linen.Dense` looks very similar to PyTorch's `torch.nn....\"],[\"Another term which is often used to describe the design difference between Flax\\u002fJAX and PyTorch is *...\"],[\"model = FlaxBertModel(config)\\n\\n- The `__call__` method defines forward pass. It takes all necessary ...\"],[\"class FlaxMLPPreTrainedModel(FlaxPreTrainedModel):\\n   config_class = MLPConfig\\n   base_model_prefix ...\"],[\"Now the `FlaxMLPModel` will have a similar interface as PyTorch or Tensorflow models and allows us t...\"],[\"Another significant difference between Flax and PyTorch models is that, we can pass the `labels` dir...\"],[\"outputs = run_model(**inputs)\\n```\\n\\nWe use `jax.jit` to compile the function to get maximum performan...\"],[\"```python\\nfrom flax.training.common_utils import onehot\\n\\ndef cross_entropy(logits, labels):\\n   retur...\"],[\"We can now save the model with the trained parameters using\\n\\n```python\\nmodel.save_pretrained(\\\"awesom...\"],[\"### Thursday, July 1st\\n- [Watch the talks on YouTube](https:\\u002f\\u002fwww.youtube.com\\u002fwatch?v=__eG63ZP_5g)\\n-...\"],[\"Speaker        | Topic                           | Time                  |  Video |\\n|-------------|-...\"],[\"#### Marc van Zee, Research SWE, Google Brain (Flax team)\\n- Talk: Introduction to Flax\\n- Abstract: I...\"],[\"#### Suraj Patil & Patrick von Platen, Machine Learning Engineers at Hugging Face\\n- Talk: How to use...\"],[\"#### Sabrina J. Mielke, PhD student at The Johns Hopkins University & Part-time research intern at H...\"],[\"#### Mostafa Dehghani, Research Scientist, Google Brain\\n- Talk: Long Range Arena: Benchmarking Effic...\"],[\"#### Rohan Anil, Senior Staff Software Engineer, Google Research, Brain Team\\n- Talk: Scalable Second...\"],[\"#### Lucas Beyer, Senior Research Engineer, Google Brain\\n- Talk: Vision Transformer\\n- Abstract: This...\"],[\"#### Iurii Kemaev, Research Engineer, Soňa Mokrá, Research Engineer, and Junhyuk Oh, Research Scient...\"],[\"#### Siddhartha Kamalakara, Joanna Yoo, João G M Araújo, MLE at Cohere\\n- Talk: Training large scale ...\"],[\"We highly recommend each team to make use of the 🤗 hub during the event.\\nTo better understand how th...\"],[\"Awesome! Now, let's first go over a simple example where most of the required we'll pre-train a RoBE...\"],[\"At first we should log in:\\n\\n```bash\\n$ huggingface-cli login\\n```\\n\\nNext we can clone the repo:\\n\\n```bas...\"],[\"# Instantiate tokenizer\\ntokenizer = ByteLevelBPETokenizer()\\n\\ndef batch_iterator(batch_size=1000):\\n  ...\"],[\"As you can see, it is pretty simple to upload model weights and training logs to the model hub. Sinc...\"],[\"# Repo is created and available here: https:\\u002f\\u002fhuggingface.co\\u002fflax-community\\u002fflax-model-dummy\\n```\\n\\n**...\"],[\"```bash\\n$ gcloud config set account \\u003cyour-email-adress\\u003e\\n```\\n\\n3. Let's also make sure the correct pro...\"],[\"All the widgets are open sourced in the `huggingface_hub` [repo](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhugg...\"],[\"**Image**\\n* **Image Classification:** Given an image, predict its class. [Example](https:\\u002f\\u002fhuggingfa...\"],[\"```\\nfrom huggingface_hub import snapshot_download\\nlocal_path = snapshot_download(\\\"flax-community\\u002frob...\"],[\"### Jury\\n\\n* [Niki Parmar](https:\\u002f\\u002fresearch.google\\u002fpeople\\u002fNikiParmar\\u002f): Staff Research Scientist at G...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The cost of vision-and-language pre-training has bec...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"Text Summarization with Pretrained Encoders\\n\\nThis folder contains part of the code necessary to repr...\"],[\"The scripts executes on GPU if one is available and if `no_cuda` is not set to `true`. Inference on ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper presents a new vision Transformer, called...\"],[\"\\u003csmall\\u003e Swin Transformer architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.03334\\\"\\u003eori...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## SwinModel\\n\\n[[autodoc]] SwinModel\\n    - forward\\n\\n## SwinForMaskedImageMod...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The Intel Team Authors and HuggingFace Inc. team. All rights reserved.\\n\\nLicensed u...\"],[\"The abstract from the paper is the following:\\n\\n*In this paper, we study the problem of temporal vide...\"],[\"This research addresses temporal video grounding (TVG), which is the process of pinpointing the star...\"],[\"The goal of this model is to incorporate trainable prompts into both visual inputs and textual featu...\"],[\"def pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\\n    '''\\n    ...\"],[\"def decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\\n    '''\\n    Decod...\"],[\"model_inputs[\\\"pixel_values\\\"] = model_inputs[\\\"pixel_values\\\"].to(model.dtype)\\noutput = model(**model_i...\"],[\"# Adversarial evaluation of model performances\\n\\nHere is an example on evaluating a model using adver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original checkpoints can be found [here](https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002ft5x\\u002fblob\\u002fmain\\u002fdocs\\u002fm...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"1. additionally parallelizing the attention computation over sequence length\\n2. partitioning the wor...\"],[\"Before you begin, make sure you have FlashAttention-2 installed.\\n\\n\\u003chfoptions id=\\\"install\\\"\\u003e\\n\\u003chfoption...\"],[\"```py\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\\n\\nm...\"],[\"\\u003cdiv style=\\\"text-align: center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fybelkada\\u002fdocumentation-im...\"],[\"For now, Transformers supports SDPA inference and training for the following architectures:\\n* [Bart]...\"],[\"```bash\\nRuntimeError: No available kernel. Aborting execution.\\n\\n# install PyTorch nightly\\npip3 insta...\"],[\"Then you can enable BetterTransformer with the [`PreTrainedModel.to_bettertransformer`] method:\\n\\n```...\"],[\"### 8-bit\\n\\n\\u003cTip\\u003e\\n\\nIf you're curious and interested in learning more about the concepts underlying 8-...\"],[\"```py\\nmax_memory_mapping = {0: \\\"1GB\\\", 1: \\\"2GB\\\"}\\nmodel_name = \\\"bigscience\\u002fbloom-3b\\\"\\nmodel_8bit = Auto...\"],[\"ORT is supported by 🤗 Optimum which can be used in 🤗 Transformers. You'll need to use an [`~optimum....\"],[\"# load model in 4-bit\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cYoutube id=\\\"nhJxYji1aho\\\"\\u002f\\u003e\\n\\nA simple way of tokenizing this text is to split it by spaces, which wo...\"],[\"```\\n[\\\"Do\\\", \\\"n't\\\", \\\"you\\\", \\\"love\\\", \\\"🤗\\\", \\\"Transformers\\\", \\\"?\\\", \\\"We\\\", \\\"sure\\\", \\\"do\\\", \\\".\\\"]\\n```\\n\\nAs can be s...\"],[\"## Subword tokenization\\n\\n\\u003cYoutube id=\\\"zHvTiHr506c\\\"\\u002f\\u003e\\n\\nSubword tokenization algorithms rely on the pr...\"],[\"As another example, [`~transformers.XLNetTokenizer`] tokenizes our previously exemplary text as foll...\"],[\"After pre-tokenization, a set of unique words has been created and the frequency with which each wor...\"],[\"At this stage, the vocabulary is `[\\\"b\\\", \\\"g\\\", \\\"h\\\", \\\"n\\\", \\\"p\\\", \\\"s\\\", \\\"u\\\", \\\"ug\\\", \\\"un\\\", \\\"hug\\\"]` and our se...\"],[\"\\u003ca id='wordpiece'\\u003e\\u003c\\u002fa\\u003e\\n\\n### WordPiece\\n\\nWordPiece is the subword tokenization algorithm used for [BER...\"],[\"At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) ov...\"],[\"$$\\\\mathcal{L} = -\\\\sum_{i=1}^{N} \\\\log \\\\left ( \\\\sum_{x \\\\in S(x_{i})} p(x) \\\\right )$$\\n\\n\\u003ca id='sentencep...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained language models like BERT and its varian...\"],[\"## ConvBertConfig\\n\\n[[autodoc]] ConvBertConfig\\n\\n## ConvBertTokenizer\\n\\n[[autodoc]] ConvBertTokenizer\\n ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Randomly initializing `VisionEncoderDecoderModel` from model configurations.\\n\\n[`VisionEncoderDeco...\"],[\"\\u003e\\u003e\\u003e model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\\n...     \\\"microsoft\\u002fswin-base-...\"],[\"## Loading a PyTorch checkpoint into `TFVisionEncoderDecoderModel`.\\n\\n[`TFVisionEncoderDecoderModel.f...\"],[\"\\u003e\\u003e\\u003e model.config.decoder_start_token_id = tokenizer.cls_token_id\\n\\u003e\\u003e\\u003e model.config.pad_token_id = tok...\"],[\"Distil*\\n\\nAuthor: @VictorSanh\\n\\nThis folder contains the original code used to train Distil* as well a...\"],[\"## What is Distil*\\n\\nDistil* is a class of compressed models that started with DistilBERT. DistilBERT...\"],[\"For more information on DistilBERT, please refer to our [NeurIPS workshop paper](https:\\u002f\\u002farxiv.org\\u002fa...\"],[\"| Model                        | English | Spanish | Chinese | German | Arabic  | Urdu |\\n| :---:    ...\"],[\"- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to p...\"],[\"- `distilgpt2`: DistilGPT2 English language model pretrained with the supervision of `gpt2` (the sma...\"],[\"Using DistilBERT is very similar to using BERT. DistilBERT share the same tokenizer as BERT's `bert-...\"],[\"```bash\\npython scripts\\u002fbinarized_data.py \\\\\\n    --file_path data\\u002fdump.txt \\\\\\n    --tokenizer_type bert...\"],[\"```bash\\nexport NODE_RANK=0\\nexport N_NODES=1\\n\\nexport N_GPU_NODE=4\\nexport WORLD_SIZE=4\\nexport MASTER_P...\"],[\"How to propose a Flax\\u002fJAX + Transformers project \\n\\nGreat that you've opened this document! \\nWhile we...\"],[\"## How to submit a project proposal\\n\\nFirst, you should make sure that you are [logged in](https:\\u002f\\u002fhu...\"],[\"1. *A clear description of the project*\\n2. *In which language should the project be conducted?* Engl...\"],[\"6. *(Optionally) What are possible challenges?* List possible difficulties with your project. *E.g.*...\"],[\"Feel free to copy-paste the following format for your project proposal and fill out the respective s...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Tianyi Tang](https:\\u002f\\u002fhuggingface.co\\u002fStevenTang). The detailed informa...\"],[\"For data-to-text generation, it is an example to use MVP and multi-task pre-trained variants.\\n```pyt...\"],[\"\\u003e\\u003e\\u003e # lightweight tuning with task-specific prompts\\n\\u003e\\u003e\\u003e model = MvpForConditionalGeneration.from_pre...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers-based models, such as BERT, have been o...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002fbigbird).\\n\\n## Usage tips\\n\\n-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```py\\n\\u003e\\u003e\\u003e minds = minds.remove_columns([\\\"english_transcription\\\", \\\"intent_class\\\", \\\"lang_id\\\"])\\n```\\n\\nTa...\"],[\"```py\\n\\u003e\\u003e\\u003e minds = minds.cast_column(\\\"audio\\\", Audio(sampling_rate=16_000))\\n\\u003e\\u003e\\u003e minds[\\\"train\\\"][0]\\n{'au...\"],[\"```py\\n\\u003e\\u003e\\u003e encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names[\\\"train\\\"], num...\"],[\"...         return batch\\n```\\n\\nNow instantiate your `DataCollatorForCTCWithPadding`:\\n\\n```py\\n\\u003e\\u003e\\u003e data_...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoModelForCTC, TrainingArguments, Trainer\\n\\n\\u003e\\u003e\\u003e model = AutoMode...\"],[\"```py\\n\\u003e\\u003e\\u003e trainer.push_to_hub()\\n```\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003cTip\\u003e\\n\\nFor a more in-depth example of...\"],[\"\\u003e\\u003e\\u003e processor = AutoProcessor.from_pretrained(\\\"stevhliu\\u002fmy_awesome_asr_mind_model\\\")\\n\\u003e\\u003e\\u003e inputs = pro...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Position encoding in transformer architecture provid...\"],[\"## RoFormerConfig\\n\\n[[autodoc]] RoFormerConfig\\n\\n## RoFormerTokenizer\\n\\n[[autodoc]] RoFormerTokenizer\\n ...\"],[\"## FlaxRoFormerForMultipleChoice\\n\\n[[autodoc]] FlaxRoFormerForMultipleChoice\\n    - __call__\\n\\n## FlaxR...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper presents XLSR which learns cross-lingual ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Program synthesis strives to generate a computer pro...\"],[\"This model was contributed by [Hiroaki Hayashi](https:\\u002f\\u002fhuggingface.co\\u002frooa).\\nThe original code can ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before you dive deeper, it is recommended that you check the following resources if you're new to 🤗 ...\"],[\"- Don't reinvent the wheel! More often than not, there are at least two reference implementations yo...\"],[\"For simplicity, the remainder of this guide assumes you've decided to contribute with the TensorFlow...\"],[\"```bash\\ngit checkout -b add_tf_brand_new_bert\\n```\\n\\n5. Fetch and rebase to current main\\n\\n```bash\\ngit ...\"],[\"It's perfectly natural that you feel overwhelmed with the amount of information that you've just abs...\"],[\"Sadly, there is no prescription to convert a PyTorch model into TensorFlow. You can, however, follow...\"],[\"- If the PyTorch model has a `#copied from ...` on top of a function, the odds are that your TensorF...\"],[\"In addition to the model file itself, you will also need to add the pointers to the model classes an...\"],[\"### 5. Add model tests\\n\\nHurray, you've implemented a TensorFlow model! Now it's time to add tests to...\"],[\"It's now time to convert your draft pull request into a real pull request. To do so, click on the \\\"R...\"],[\"## Adding TensorFlow weights to 🤗 Hub\\n\\nAssuming that the TensorFlow model architecture is available ...\"],[\"As in other numerical problems, the devil is in the details. And as in any detail-oriented craft, th...\"],[\"Image Captioning (vision-encoder-text-decoder model) training example\\n\\nThe following example showcas...\"],[\"### Create a model from a vision encoder model and a text decoder model\\nNext, we create a [FlaxVisio...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003csmall\\u003e SimMIM framework. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2111.09886\\\"\\u003eoriginal paper\\u003c\\u002f...\"],[\"We can also for instance replicate the pre-training of a Swin Transformer using the same architectur...\"],[\"```bash\\npython run_mim.py \\\\\\n    --model_type vit \\\\\\n    --dataset_name nateraw\\u002fimage-folder \\\\\\n    --t...\"],[\"### Using datasets from 🤗 `datasets`\\n\\nOne can use the following command to pre-train a `ViTMAEForPre...\"],[\"\\u003csmall\\u003e Original hyperparameters. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2111.06377\\\"\\u003eoriginal...\"],[\"2. Log in with your HuggingFace account credentials using `huggingface-cli`\\n\\n```bash\\n$ huggingface-c...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The pre-trained language models have achieved great ...\"],[\"## NezhaForMaskedLM\\n\\n[[autodoc]] NezhaForMaskedLM\\n    - forward\\n\\n## NezhaForNextSentencePrediction\\n\\n...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [Fine-tuning a pretrained model](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002ftransformers_d...\"],[\"### PyTorch Examples\\n\\n#### Natural Language Processing[[pytorch-nlp]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on language modeling](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"| [How to train a language model from scratch](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnoteboo...\"],[\"| [Reformer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnotebooks\\u002f03_reformer.ipynb)| How Reforme...\"],[\"#### Computer Vision[[pytorch-cv]]...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to fine-tune a model on image classification (Albumentations)](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"| [How to perform zero-shot object detection with OWL-ViT](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002f...\"],[\"| [How to build an image similarity system with Transformers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"| [How to fine-tune a VideoMAE model on video classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"#### Audio[[pytorch-audio]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"#### Biological Sequences[[pytorch-bio]]...\"],[\"| Notebook     | Description                                                                        ...\"],[\"| [How to fine-tune a Nucleotide Transformer model](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"#### Other modalities[[pytorch-other]]\\n\\n| Notebook     | Description                                ...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on language modeling](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"#### Computer Vision[[tensorflow-cv]]\\n\\n| Notebook                                                   ...\"],[\"#### Biological Sequences[[tensorflow-bio]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on text classification with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"## Community notebooks:\\n\\nMore notebooks developed by the community are available [here](https:\\u002f\\u002fhf.c...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In deep learning, models typically reuse the same pa...\"],[\"## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](..\\u002ftasks...\"],[\"Simple VQGAN CLIP\\n\\nAuthor: @ErwannMillon \\n\\nThis is a very simple VQGAN-CLIP implementation that was ...\"],[\"### Edit an image\\nTo get a test image, run \\n`git clone https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ferwann\\u002fvqgan-...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recent work has shown that either (1) increasing the...\"],[\"- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging th...\"],[\"a few new parameters -- global relative position biases and a layer normalization for global token's...\"],[\"```python\\n\\u003e\\u003e\\u003e import evaluate\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\u003e\\u003e\\u003e from transformers import Aut...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have emerged as a preferred model for m...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note tha...\"],[\"```py\\n\\u003e\\u003e\\u003e with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir, max_...\"],[\"\\u003e\\u003e\\u003e with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir, max_shard_...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [matthijs](https:\\u002f\\u002fhuggingface.co\\u002fMatthijs). The original code and wei...\"],[\"- The original TensorFlow checkpoints include quantized models. We do not support these models as th...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"## BloomForQuestionAnswering\\n\\n[[autodoc]] BloomForQuestionAnswering\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003cjax\\u003e\\n\\n## F...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Language model pre-training has been shown to captur...\"],[\"[[autodoc]] RealmTokenizerFast\\n    - batch_encode_candidates\\n\\n## RealmRetriever\\n\\n[[autodoc]] RealmRe...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fcircleci.com\\u002fgh\\u002fhuggingface\\u002ftransformers\\\"\\u003e\\n        \\u003cimg alt=...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"🤗 ట్రాన్స్‌ఫార్మర్లు అందించిన టెక్స్ట్‌లో ప్రీట్రైన్డ్ మోడల్‌లను త్వరగా డౌన్‌లోడ్ చేయడానికి మరియు ఉప...\"],[\"సహజ భాషా ప్రాసెసింగ్‌లో:\\n- [BERT తో మాస్క్‌డ్ వర్డ్ కంప్లీషన్](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-unca...\"],[\"- [DistilBERT తో ప్రశ్న సమాధానం](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text...\"],[\"కంప్యూటర్ దృష్టిలో:\\n- [VIT తో చిత్ర వర్గీకరణ](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224)\\n- ...\"],[\"ట్రాన్స్‌ఫార్మర్‌ల 100,000 నక్షత్రాలను జరుపుకోవడానికి, మేము స్పాట్‌లైట్‌ని ఉంచాలని నిర్ణయించుకున్నామ...\"],[\"``` python\\n\\u003e\\u003e\\u003e import requests\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n# Do...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e model = AutoModel.from_pretra...\"],[\"## నేను ట్రాన్స్‌ఫార్మర్‌లను ఎందుకు ఉపయోగించాలి?\\n\\n1. ఉపయోగించడానికి సులభమైన స్టేట్ ఆఫ్ ది ఆర్ట్ మోడల...\"],[\"## నేను ట్రాన్స్‌ఫార్మర్‌లను ఎందుకు ఉపయోగించకూడదు?\\n\\n- ఈ లైబ్రరీ న్యూరల్ నెట్‌ల కోసం బిల్డింగ్ బ్లాక్...\"],[\"ఆ బ్యాకెండ్‌లలో ఒకటి ఇన్‌స్టాల్ చేయబడినప్పుడు, 🤗 ట్రాన్స్‌ఫార్మర్‌లను ఈ క్రింది విధంగా పిప్‌ని ఉపయోగ...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (from VinAI Research)...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (from Facebook) r...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (from Harbin In...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (from OFA-Sys...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (from MetaAI) rele...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (from Tsinghua University) rele...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (from Microsoft) ...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (from Facebook) released with...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (from Baidu) released wi...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflava)** (from Facebook AI) released...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (from KAIST) released with th...\"],[\"1. **[GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptj)** (from EleutherAI) released i...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (from Microsoft) ...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[LayoutLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlm)** (from Microsoft Resea...\"],[\"1. **[LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flilt)** (from South China University ...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[LUKE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fluke)** (from Studio Ousia) released ...\"],[\"1. **[MarianMT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmarian)** Machine translation mod...\"],[\"1. **[mBART-50](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (from Google I...\"],[\"1. **[MusicGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmusicgen)** (from Meta) released ...\"],[\"1. **[Nyströmformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (from the U...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (from Google) released ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (from UCLA NLP) released ...\"],[\"1. **[RAG](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frag)** (from Facebook) released with t...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (from Facebook), releas...\"],[\"1. **[SegFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsegformer)** (from NVIDIA) relea...\"],[\"1. **[SpeechToTextTransformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text)** ...\"],[\"1. **[Swin Transformer V2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswinv2)** (from Micros...\"],[\"1. **[Table Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftable-transformer)** (fr...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (from Microsoft), released ...\"],[\"1. **[UniSpeechSat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech-sat)** (from Micros...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (from Goog...\"],[\"1. **[ViTMatte](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitmatte)** (from HUST-VL) releas...\"],[\"1. **[Wav2Vec2Phoneme](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2_phoneme)** (from ...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (From Facebook AI) released w...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (from Meta AI) released wit...\"],[\"1. **[YOSO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyoso)** (from the University of Wisco...\"],[\"ప్రతి మోడల్ ఫ్లాక్స్, పైటార్చ్ లేదా టెన్సర్‌ఫ్లోలో అమలు చేయబడిందా లేదా 🤗 Tokenizers లైబ్రరీ ద్వారా అ...\"],[\"## అనులేఖనం\\n\\n🤗 ట్రాన్స్‌ఫార్మర్స్ లైబ్రరీ కోసం మీరు ఉదహరించగల [పేపర్](https:\\u002f\\u002fwww.aclweb.org\\u002fantholo...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Most widely-used pre-trained language models operate...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nSince ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix ...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e to...\"],[\"\\u003e\\u003e\\u003e # ^- Note how 258 descends to 257, 256, 255\\n\\n\\u003e\\u003e\\u003e # Now we need to split on the sentinel tokens, ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage example\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import AutoModel, AutoTokenizer\\n\\n...\"],[\"- This implementation is only for tokenization: \\\"monolingual_vocab_file\\\" consists of Vietnamese-spec...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"這裡是一些範例：\\n- [用 BERT 做遮蓋填詞](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+F...\"],[\"- [用 DistilBERT 做問答](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**，由 Hugging Face 團隊所打造，是一個文本生成的官方 dem...\"],[\"\\u003e\\u003e\\u003e inputs = tokenizer(\\\"Hello world!\\\", return_tensors=\\\"pt\\\")\\n\\u003e\\u003e\\u003e outputs = model(**inputs)\\n```\\n這裡是對應的...\"],[\"首先，用你打算使用的版本的 Python 創建一個虛擬環境並進入。\\n\\n然後，你需要安裝 Flax、PyTorch 或 TensorFlow 其中之一。對於該如何在你使用的平台上安裝這些框架，請參閱 [...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (from VinAI Research)...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (from Facebook) r...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (from Harbin In...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (from OFA-Sys...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (from MetaAI) rele...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (from Tsinghua University) rele...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (from Microsoft) ...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (from Facebook) released with...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (from Baidu) released wi...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflava)** (from Facebook AI) released...\"],[\"1. **[GLPN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fglpn)** (from KAIST) released with th...\"],[\"1. **[GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptj)** (from EleutherAI) released w...\"],[\"1. **[Graphormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgraphormer)** (from Microsoft) ...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[KOSMOS-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fkosmos-2)** (from Microsoft Resea...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (from Meta AI) released wit...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[LongT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flongt5)** (from Google AI) released...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released wi...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MobileNetV1](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v1)** (from Google I...\"],[\"1. **[MT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmt5)** (from Google AI) released with ...\"],[\"1. **[Nougat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnougat)** (from Meta AI) released w...\"],[\"1. **[OWLv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlv2)** (from Google AI) released w...\"],[\"1. **[Persimmon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpersimmon)** (from ADEPT) releas...\"],[\"1. **[PoolFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpoolformer)** (from Sea AI Labs...\"],[\"1. **[RAG](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frag)** (from Facebook) released with t...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (from Facebook), releas...\"],[\"1. **[SeamlessM4Tv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t_v2)** (from Met...\"],[\"1. **[SpeechT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeecht5)** (from Microsoft Resea...\"],[\"1. **[SwiftFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswiftformer)** (from MBZUAI) r...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (from Google AI) released with th...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (from Google Research) released...\"],[\"1. **[UPerNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fupernet)** (from Peking University...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (from Goog...\"],[\"1. **[ViTMatte](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitmatte)** (from HUST-VL) releas...\"],[\"1. **[Wav2Vec2Phoneme](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2_phoneme)** (from ...\"],[\"1. **[XGLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxglm)** (From Facebook AI) released w...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (from Meta AI) released wit...\"],[\"1. **[YOSO](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fyoso)** (from the University of Wisco...\"],[\"要檢查某個模型是否已有 Flax、PyTorch 或 TensorFlow 的實作，或其是否在🤗 Tokenizers 函式庫中有對應的 tokenizer，敬請參閱[此表](https:\\u002f\\u002fhugg...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-training techniques have been verified successfu...\"],[\"```python\\ndef normalize_bbox(bbox, width, height):\\n    return [\\n        int(1000 * (bbox[0] \\u002f width)...\"],[\"- See also: [Document question answering task guide](..\\u002ftasks\\u002fdocument_question_answering)\\n\\n\\u003cPipelin...\"],[\"Wav2Vec2 Contrastive Loss PreTraining examples\\n\\nThe following example showcases how to pretrain a wa...\"],[\"Next, let's add a symbolic link to the `run_wav2vec2_pretrain_flax`.\\n\\n```bash\\nexport MODEL_DIR=\\\".\\u002fwa...\"],[\"from transformers import Wav2Vec2FeatureExtractor\\nconfig = Wav2Vec2FeatureExtractor.from_pretrained(...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-training video transformers on extra large-scale...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fDialoGPT).\\n\\n## Usage tips\\n\\n- Dial...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Humans read and write hundreds of billions of messag...\"],[\"This model was contributed by [forresti](https:\\u002f\\u002fhuggingface.co\\u002fforresti).\\n\\n## Usage tips\\n\\n- Squeeze...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## OpenLlamaConfig\\n\\n[[autodoc]] OpenLlamaConfig\\n\\n## OpenLlamaModel\\n\\n[[autodoc]] OpenLlamaModel\\n    -...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Tool\\n\\n[[autodoc]] Tool\\n\\n### PipelineTool\\n\\n[[autodoc]] PipelineTool\\n\\n### RemoteTool\\n\\n[[autodoc]] ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Let's install the libraries needed for distillation and evaluating the process. \\n\\n```bash\\npip instal...\"],[\"```python\\nfrom transformers import TrainingArguments, Trainer\\nimport torch\\nimport torch.nn as nn\\nimp...\"],[\"Let's set the `TrainingArguments`, the teacher model and the student model. \\n\\n```python\\nfrom transfo...\"],[\"Let's initialize the `Trainer` with the training arguments we defined. We will also initialize our d...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Connectionist Temporal Classification\\n\\nThe script [`run_speech_recognition_ctc.py`](https:\\u002f\\u002fgithu...\"],[\"If the environment variable is not set, the training script might freeze, *i.e.* see: https:\\u002f\\u002fgithub...\"],[\"```bash\\ntorchrun \\\\\\n\\t--nproc_per_node 8 run_speech_recognition_ctc.py \\\\\\n\\t--dataset_name=\\\"common_voice...\"],[\"```bash\\n**torchrun \\\\\\n\\t--nproc_per_node 4 run_speech_recognition_ctc_streaming.py \\\\\\n\\t--dataset_name=\\\"...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [TIMIT](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftimit_asr)| -  | [ntu-spml\\u002fdistilhubert](https:\\u002f\\u002fhuggingfa...\"],[\"#### Librispeech CTC\\n\\n- [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr)| `\\\"clean\\\"` - `\\\"train.100\\\"` |  [face...\"],[\"#### Common Voice CTC\\n\\n- [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)...\"],[\"| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)| `\\\"tr\\\"`  | [facebook\\u002fwav2vec2-large-x...\"],[\"| [Common Voice](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcommon_voice)| `\\\"tr\\\"` in streaming mode  | [faceboo...\"],[\"#### Multilingual Librispeech CTC\\n\\n- [Multilingual Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmult...\"],[\"### MMS Model\\n\\nThe [Massive Multilingual Speech (MMS) model](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fmms-1b-...\"],[\"Now, let's run an example and upload it to the Hub under `wav2vec2-common_voice-tr-mms-demo`.\\n\\n```sh...\"],[\"```sh\\npython run_speech_recognition_ctc.py \\\\\\n\\t--dataset_name=\\\"common_voice\\\" \\\\\\n\\t--model_name_or_path=...\"],[\"#### Single GPU Whisper Training\\nThe following example shows how to fine-tune the [Whisper small](ht...\"],[\"If training on a different language, you should be sure to change the `language` argument. The `lang...\"],[\"### Warm-Started Speech-Encoder-Decoder Model\\nA very common use case is to leverage a pretrained spe...\"],[\"# checkpoints to leverage\\nencoder_id = \\\"facebook\\u002fwav2vec2-base\\\"\\ndecoder_id = \\\"facebook\\u002fbart-base\\\"\\n\\n#...\"],[\"```bash\\nln -s $(realpath \\u003cpath\\u002fto\\u002ftransformers\\u003e\\u002fexamples\\u002fpytorch\\u002fspeech-recognition\\u002frun_speech_recog...\"],[\"Having warm-started the speech-encoder-decoder model under `\\u003cyour-user-name\\u003e\\u002fwav2vec2-2-bart`, we ca...\"],[\"```bash\\npython run_speech_recognition_seq2seq.py \\\\\\n\\t--dataset_name=\\\"librispeech_asr\\\" \\\\\\n\\t--model_name...\"],[\"```bash\\ntorchrun \\\\\\n \\t--nproc_per_node 8 run_speech_recognition_seq2seq.py \\\\\\n\\t--dataset_name=\\\"librisp...\"],[\"| Dataset                                                        | Dataset Config            | Pretr...\"],[\"| [Librispeech](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flibrispeech_asr) | `\\\"clean\\\"` - `\\\"train.100\\\"` | [face...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"MobileViTV2 is the second version of MobileViT, constructed by replacing the multi-headed self-atten...\"],[\"This model was contributed by [shehan97](https:\\u002f\\u002fhuggingface.co\\u002fshehan97).\\nThe original code can be ...\"],[\"LXMERT DEMO\\n\\n1. make a virtualenv: ``virtualenv venv`` and activate ``source venv\\u002fbin\\u002factivate``\\n2. ...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"To setup all relevant files for training, let's create a directory.\\n\\n```bash\\nmkdir .\\u002fnorwegian-rober...\"],[\"### Train model\\n\\nNext we can run the example script to pretrain the model:\\n\\n```bash\\npython run_mlm_f...\"],[\"To setup all relevant files for training, let's create a directory.\\n\\n```bash\\nmkdir .\\u002fnorwegian-gpt2\\n...\"],[\"### Train model\\n\\nFinally, we can run the example script to pretrain the model:\\n\\n```bash\\npython run_c...\"],[\"The example script uses the 🤗 Datasets library. You can easily customize them to your needs if you n...\"],[\"# Train tokenizer\\ntokenizer.train_from_iterator(\\n    iterator=batch_iterator(input_sentence_size=inp...\"],[\"## BART: Denoising language modeling\\n\\nIn the following, we demonstrate how to train a BART model \\nus...\"],[\"# Save files to disk\\ntokenizer.save(\\\".\\u002fnorwegian-bart-base\\u002ftokenizer.json\\\")\\n```\\n\\n### Create configur...\"],[\"| Task  | [TPU v3-8 (Flax)](https:\\u002f\\u002ftensorboard.dev\\u002fexperiment\\u002fGdYmdak2TWeVz0DDRYOrrg\\u002f)  | [TPU v3-8...\"],[\"export NUM_TPUS=8\\nexport TOKENIZERS_PARALLELISM=0\\nexport MODEL_DIR=\\\".\\u002fnorwegian-roberta-base\\\"\\nmkdir ...\"],[\"```bash\\nexport NUM_GPUS=8\\nexport TOKENIZERS_PARALLELISM=0\\nexport MODEL_DIR=\\\".\\u002fnorwegian-roberta-base...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large pre-trained language models have been shown to...\"],[\"This model was contributed by [ola13](https:\\u002f\\u002fhuggingface.co\\u002fola13).\\n\\n## Usage tips\\n\\nRetrieval-augme...\"],[\"Robust Speech Challenge 🤗\\n\\nWelcome to the robust speech recognition challenge 🎙️ !\\n\\nThe goal of this...\"],[\"Participants can make use of whatever data they think is useful to build a \\nspeech recognition syste...\"],[\"During the event, the speech recognition system will be evaluated on both the Common Voice `\\\"test\\\"` ...\"],[\"## Important dates\\n\\n![timeline](https:\\u002f\\u002fgithub.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fraw\\u002fmaster\\u002fRob...\"],[\"In addition, participants can also make use of their audio data. Here, please make sure that you **a...\"],[\"Next, let's talk about preprocessing. Audio data and transcriptions have to be brought into the corr...\"],[\"we should not remove the single quotation mark `'` since it would change the meaning of the word `\\\"i...\"],[\"Since those choices are not always obvious when in doubt feel free to ask on Discord or even better ...\"],[\"1. Fork the [repository](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers) by\\n   clicking on the 'Fork' b...\"],[\"To verify that all libraries are correctly installed, you can run the following command in a Python ...\"],[\"Next, we recommended that you get familiar with the XLS-R model and its capabilities.\\nIn collaborati...\"],[\"Assuming that we want to call our model repository *xls-r-ab-test*, we can run the \\nfollowing comman...\"],[\"Alright, finally we can define the training script. We'll simply use some \\ndummy hyper-parameters an...\"],[\"5. **Tips for real model training**\\n\\nThe above steps illustrate how a model can technically be fine-...\"],[\", clone it locally (assuming the `\\u003cusername\\u003e` is `hf-test`)\\n\\n```bash\\ngit clone hf-test\\u002fxls-r-300m-sv...\"],[\"### Creating an OVHCloud account\\n*TIP*: If you haven't created a project on OVHcloud yet, make sure ...\"],[\"### Setting up an AI notebook\\n1. Go to the `Public Cloud` page and select `Project Management` -\\u003e `U...\"],[\"For more quick tutorials about OVHcloud AI products, check out the showcase https:\\u002f\\u002fvimeo.com\\u002fshowca...\"],[\"## Evaluation\\n\\nFinally, we have arrived at the most fun part of the challenge - sitting back and\\nwat...\"],[\"- 1. The following input arguments should not be changed and keep their original functionality\\u002fmeani...\"],[\"- a. Somehow giving the model access to the target transcriptions to improve performance. The model ...\"],[\"Uff, that was a lot of text describing how to make sure your `eval.py` script \\nis in the correct for...\"],[\"In the case of `xls-r-300m-sv`, the following command can be run:\\n\\n```bash \\ncd xls-r-300m-sv\\n.\\u002feval....\"],[\"The dataset `WER_REAL_AUDIO_TEST` is hidden and will only be published \\nat the end of the robust spe...\"],[\"The following table summarizes what platform to use for which problem.\\n\\n- Problem\\u002fquestion\\u002fbug with ...\"],[\"## Talks\\n\\nWe are very excited to be hosting 2 days of talks from Kensho-Technologies, Mozilla's Comm...\"],[\"### Talks & Speakers\\n\\n#### Patrick von Platen, Research Engineer, Hugging Face\\n- Talk: Introduction ...\"],[\"#### Gabriel Habayeb, Data Engineer, Common Voice @ Mozilla\\n- Talk: Unlocking global speech with Moz...\"],[\"## General Tips and Tricks\\n\\n- Memory efficient training:\\n\\nIn case, you are getting out-of-memory err...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [AI Sweden](https:\\u002f\\u002fhuggingface.co\\u002fAI-Sweden).\\n\\n## Usage example\\n\\n```p...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"Testing mixed int8 quantization\\n\\n![HFxbitsandbytes.png](https:\\u002f\\u002fcdn-uploads.huggingface.co\\u002fproductio...\"],[\"### `To use the type as a Parameter, please correct the detach() semantics defined by __torch_dispat...\"],[\"```bash\\nls -l $path\\u002flibcudart.so\\n```\\n\\nOn each path (`$path`) separated by `:`.\\nIf not, simply run\\n``...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Driven by improved architectures and better represen...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pipelined NLP systems have largely been superseded b...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr). The original code can be foun...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import CanineModel\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e model = CanineModel.from_pr...\"],[\"## CanineForSequenceClassification\\n\\n[[autodoc]] CanineForSequenceClassification\\n    - forward\\n\\n## Ca...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Image segmentation groups pixels with different sema...\"],[\"## Usage tips\\n\\n- Mask2Former uses the same preprocessing and postprocessing steps as [MaskFormer](ma...\"],[\"Plug and Play Language Models: a Simple Approach to Controlled Text Generation\\n\\nAuthors: [Sumanth Da...\"],[\"## PPLM-Discrim\\n\\n### Example command for discriminator based sentiment control\\n\\n```bash\\npython run_p...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### GPT-2\\u002fGPT and causal language modeling\\n\\nThe following example fine-tunes GPT-2 on WikiText-2. We...\"],[\"In accordance to the RoBERTa paper, we use dynamic masking rather than static masking. The model may...\"],[\"### Whole word masking\\n\\nThis part was moved to `examples\\u002fresearch_projects\\u002fmlm_wwm`.\\n\\n### XLNet and ...\"],[\"## Low Cpu Memory Usage\\n\\nTo use low cpu memory mode which can be very useful for LLM, add `--low_cpu...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Reinforcement learning (RL) is typically concerned w...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\npip install watchdog\\n```\\n\\nThen run the following command:\\n\\n```bash\\ndoc-builder preview {pack...\"],[\"```\\nSections that were moved:\\n\\n[ \\u003ca href=\\\"..\\u002fnew-file#section-b\\\"\\u003eSection A\\u003c\\u002fa\\u003e\\u003ca id=\\\"section-a\\\"\\u003e\\u003c\\u002fa\\u003e...\"],[\"When translating, refer to the guide at [.\\u002fTRANSLATING.md](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransforme...\"],[\"Values that should be put in `code` should either be surrounded by backticks: \\\\`like so\\\\`. Note that...\"],[\"[What are input IDs?](..\\u002fglossary#input-ids)\\n```\\n\\nFor optional arguments or arguments with defaults ...\"],[\"```\\n    Returns:\\n        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special tok...\"],[\"This script may have some weird failures if you made a syntax mistake or if you uncover a bug. There...\"],[\"The docstring should give a minimal, clear example of how the respective model \\nis to be used in inf...\"],[\"### Writing doctests\\n\\nHere are a few tips to help you debug the doctests and make them pass:\\n\\n- The ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] data.processors.utils.DataProcessor\\n\\n[[autodoc]] data.processors.utils.InputExample\\n\\n[[a...\"],[\"This library hosts the processor to load the XNLI data:\\n\\n- [`~data.processors.utils.XnliProcessor`]\\n...\"],[\"```python\\n# Loading a V2 processor\\nprocessor = SquadV2Processor()\\nexamples = processor.get_dev_examp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"`BrosForTokenClassification` has a simple linear layer on top of BrosModel. It predicts the label of...\"],[\"The abstract from the paper is the following:\\n\\n*Key information extraction (KIE) from document image...\"],[\"# Normalize bbox -\\u003e 0 ~ 1\\n    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] \\u002f width\\n    bboxes[:, [1, 3]] = ...\"],[\"## BrosProcessor\\n\\n[[autodoc]] BrosProcessor\\n    - __call__\\n\\n## BrosModel\\n\\n[[autodoc]] BrosModel\\n    ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While large pretrained Transformer models have prove...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"outputs = model(inputs[\\\"input_ids\\\"][:, :2])\\noutput_one = outputs.last_hidden_state\\n\\n# Using the stat...\"],[\"Replacing the softmax by its value gives:\\n\\n$$O_{i} = \\\\frac{\\\\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} \\u002f \\\\sqrt...\"],[\"$$\\\\frac{e^{x_{i}}}{\\\\sum_{j=1}^{n} e^{x_{j}}} = \\\\frac{e^{x_{i} - M}}{\\\\sum_{j=1}^{n} e^{x_{j} - M}}$$\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg width=\\\"600\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002f...\"],[\"The figure below illustrates the architecture of OneFormer. Taken from the [original paper](https:\\u002f\\u002f...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## With Accelerate\\n\\nBased on the script [run_swag_no_trainer.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftran...\"],[\"This command is the same and will work for:\\n\\n- a CPU-only setup\\n- a setup with one GPU\\n- a distribut...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"...\"],[\"## How do I use chat templates?\\n\\nAs you can see in the example above, chat templates are easy to use...\"],[\"```python\\noutputs = model.generate(tokenized_chat, max_new_tokens=128) \\nprint(tokenizer.decode(outpu...\"],[\"[`ConversationalPipeline`] will take care of all the details of tokenization and calling `apply_chat...\"],[\"Not all models require generation prompts. Some models, like BlenderBot and LLaMA, don't have any\\nsp...\"],[\"```python\\n\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(...\"],[\"This is a pretty simple template - it doesn't add any control tokens, and it doesn't support \\\"system...\"],[\"```\\n{% for message in messages %}\\n    {% if message['role'] == 'user' %}\\n        {{ bos_token + '[IN...\"],[\"This is something we do purely for backward compatibility reasons, to avoid breaking any existing wo...\"],[\"```\\n{% for message in messages %}\\n    {{'\\u003c|im_start|\\u003e' + message['role'] + '\\\\n' + message['content']...\"],[\"### I want to add some chat templates! How should I get started?\\n\\nIf you have any chat models, you s...\"],[\"You can also use the following tips to convert your code to Jinja:\\n\\n### For loops\\n\\nFor loops in Jinj...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"自然言語処理にて:\\n- [BERTによるマスクドワード補完](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D...\"],[\"- [DistilBERTによる質問応答](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name...\"],[\"コンピュータビジョンにて:\\n- [ViTによる画像分類](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16-224)\\n- [DETRによる物体検出](htt...\"],[\"自然言語処理だけでなく、コンピュータビジョンや音声処理においても、多くのタスクにはあらかじめ訓練された`pipeline`が用意されている。例えば、画像から検出された物体を簡単に抽出することができる:...\"],[\"\\u003e\\u003e\\u003e inputs = tokenizer(\\\"Hello world!\\\", return_tensors=\\\"pt\\\")\\n\\u003e\\u003e\\u003e outputs = model(**inputs)\\n```\\n\\nそしてこち...\"],[\"1. モデルやサンプルをニーズに合わせて簡単にカスタマイズ可能:\\n    - 原著者が発表した結果を再現するために、各アーキテクチャの例を提供しています。\\n    - モデル内部は可能な限り一貫して公...\"],[\"🤗Transformersは以下のようにcondaを使って設置することができます:\\n\\n```shell script\\nconda install -c huggingface transformers...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (Google Research and the ...\"],[\"1. **[Bark](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbark)** (from Suno) released in the r...\"],[\"1. **[BERTweet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbertweet)** (VinAI Research から) D...\"],[\"1. **[Blenderbot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblenderbot)** (Facebook から) Ste...\"],[\"1. **[BridgeTower](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbridgetower)** (Harbin Institu...\"],[\"1. **[CLAP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclap)** (LAION-AI から) Yusong Wu, Ke C...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (MetaAI から) Baptis...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (Tsinghua University から) Zhengy...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (Microsoft から) Pe...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (Facebook から) Nicolas Carion,...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (HuggingFace から),...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (Snap R...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (Baidu から) Xuan Ouyang, ...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (Google AI から) Hyung Wo...\"],[\"1. **[FNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffnet)** (Google Research から) James Le...\"],[\"1. **[GPT Neo](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neo)** (EleutherAI から) Sid Bla...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (BigCode から) Lou...\"],[\"1. **[Hubert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fhubert)** (Facebook から) Wei-Ning Hs...\"],[\"1. **[InstructBLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finstructblip)** (Salesforce か...\"],[\"1. **[LayoutXLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutxlm)** (Microsoft Research...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (The FAIR team of Meta AI...\"],[\"1. **[LongT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flongt5)** (Google AI から) Mandy Guo,...\"],[\"1. **[MarianMT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmarian)** Jörg Tiedemann から. [OPU...\"],[\"1. **[mBART-50](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (Facebook から) Yuqing Tan...\"],[\"1. **[Mixtral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmixtral)** (from Mistral AI) by Th...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (Google Inc. か...\"],[\"1. **[MusicGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmusicgen)** (from Meta) released ...\"],[\"1. **[Nyströmformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (the Univer...\"],[\"1. **[PatchTSMixer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtsmixer)** ( IBM Researc...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[Pop2Piano](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpop2piano)** released with the p...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (Google Research から) ...\"],[\"1. **[RoFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froformer)** (ZhuiyiTechnology から)...\"],[\"1. **[SEW](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew)** (ASAPP から) Felix Wu, Kwangyoun ...\"],[\"1. **[SqueezeBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsqueezebert)** (Berkeley から) F...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (Google AI から) Colin Raffel and N...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (Google Research から) Yi Tay, Mo...\"],[\"1. **[VAN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvan)** (Tsinghua University and Nankai...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (Google AI から) Al...\"],[\"1. **[ViViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvivit)** (from Google Research) rele...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (Microsoft Research から) Bo...\"],[\"1. **[XLM-RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-roberta)** (Facebook AI から...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (Facebook A...\"],[\"各モデルがFlax、PyTorch、TensorFlowで実装されているか、🤗Tokenizersライブラリに支えられた関連トークナイザを持っているかは、[この表](https:\\u002f\\u002fhuggingfa...\"],[\"## 引用\\n\\n🤗 トランスフォーマーライブラリに引用できる[論文](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)が出来ました:\\n```bi...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### Optimizing Bark\\n\\nBark can be optimized with just a few extra lines of code, which **significantl...\"],[\"#### Using Flash Attention 2\\n\\nFlash Attention 2 is an even faster, optimized version of the previous...\"],[\"\\u003cdiv style=\\\"text-align: center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fylacombe\\u002fbenchmark-compar...\"],[\"\\u003e\\u003e\\u003e voice_preset = \\\"v2\\u002fen_speaker_6\\\"\\n\\n\\u003e\\u003e\\u003e inputs = processor(\\\"Hello, my dog is cute\\\", voice_preset=v...\"],[\"[[autodoc]] BarkSemanticModel\\n    - forward\\n\\n## BarkCoarseModel\\n\\n[[autodoc]] BarkCoarseModel\\n    - f...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*BERT adopts masked language modeling (MLM) for pre-t...\"],[\"## MPNetConfig\\n\\n[[autodoc]] MPNetConfig\\n\\n## MPNetTokenizer\\n\\n[[autodoc]] MPNetTokenizer\\n    - build_i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision-Language Pre-training (VLP) has advanced the ...\"],[\"[[autodoc]] BlipImageProcessor\\n    - preprocess\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## BlipModel\\n\\n[[autodoc]] ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recent studies have shown that multilingual pretrain...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Building open-domain chatbots is a challenging area fo...\"],[\"An example:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import BlenderbotTokenizer, BlenderbotForConditionalGen...\"],[\"[[autodoc]] BlenderbotForConditionalGeneration\\n    - forward\\n\\n## BlenderbotForCausalLM\\n\\n[[autodoc]] ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"A Hugging Face team member will be available to help you along the way so you'll never be alone. 🤗 ❤...\"],[\"In our opinion, the library's code is not just a means to provide a product, *e.g.* the ability to u...\"],[\"Let's take a look:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresol...\"],[\"```python\\nmodel = BrandNewBertModel.from_pretrained(\\\"brandy\\u002fbrand_new_bert\\\")\\nmodel.config  # model h...\"],[\"### Overview of tokenizers\\n\\nNot quite ready yet :-( This section will be added soon!\\n\\n## Step-by-ste...\"],[\"In the following, we try to give you a general recipe that we found most useful when porting a model...\"],[\"### 1. (Optional) Theoretical aspects of BrandNewBert\\n\\nYou should take some time to read *BrandNewBe...\"],[\"2. Clone your `transformers` fork to your local disk, and add the base repository as a remote:\\n\\n```b...\"],[\"Now you have set up a development environment to port *brand_new_bert* to 🤗 Transformers.\\n\\n### 3.-4....\"],[\"- Where to find the pretrained weights?\\n- How to load the pretrained weights into the corresponding ...\"],[\"-  [Jupyter notebooks](https:\\u002f\\u002fjupyter.org\\u002f) \\u002f [google colab](https:\\u002f\\u002fcolab.research.google.com\\u002fnote...\"],[\"Again, it is up to you which strategy to choose. Often, one or the other is advantageous depending o...\"],[\"No matter which strategy you choose, the recommended procedure is often the same that you should sta...\"],[\"We expect that every model added to 🤗 Transformers passes a couple of integration tests, meaning tha...\"],[\"- Find the best way of debugging intermediate results. Is the original repository written in PyTorch...\"],[\"and start from this point. This might mean that you have to possibly write a small script yourself o...\"],[\"The following section gives you more specific details\\u002ftips on how you can do this for *brand_new_ber...\"],[\"```bash\\ngit fetch upstream\\ngit rebase upstream\\u002fmain\\n```\\n\\n4. Push the changes to your account using:\\n...\"],[\"**5. Adapt the generated models code for brand_new_bert**\\n\\nAt first, we will focus only on the model...\"],[\"```python\\nfrom transformers import BrandNewBertModel, BrandNewBertConfig\\n\\nmodel = BrandNewBertModel(...\"],[\"```py\\ndef _init_weights(self, module):\\n    \\\"\\\"\\\"Initialize the weights\\\"\\\"\\\"\\n    if isinstnace(module, Wa...\"],[\"- If you are porting a model from TensorFlow to PyTorch, a good starting point might be BERT's conve...\"],[\"```python\\nprint(model.dense.weight.data)\\n```\\n\\nto see that the weights were randomly initialized\\n\\n```...\"],[\"```python\\nassert (\\n    model_pointer.weight.shape == pretrained_weight.shape\\n), f\\\"Pointer shape of r...\"],[\"```python\\nmodel.save_pretrained(\\\"\\u002fpath\\u002fto\\u002fconverted\\u002fcheckpoint\\u002ffolder\\\")\\n```\\n\\n**7. Implement the forw...\"],[\"- Some layers were not added, *i.e.* an *activation* layer was not added, or the residual connection...\"],[\"**8. Adding all necessary model tests**\\n\\nAt this point, you have successfully added a new model. How...\"],[\"- It helps to transfer the knowledge you have acquired during the model addition to the community by...\"],[\"input_ids = tokenizer(input_str).input_ids\\n```\\n\\nWhen both `input_ids` yield the same values, as a fi...\"],[\"Next, make sure that the docstring added to `src\\u002ftransformers\\u002fmodels\\u002fbrand_new_bert\\u002fmodeling_brand_n...\"],[\"You have now finished the coding part, congratulation! 🎉 You are Awesome! 😎\\n\\n**12. Upload the models...\"],[\"### Share your work!!\\n\\nNow, it's time to get some credit from the community for your work! Having co...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Self-training\\n\\nThis is an implementation of the self-training algorithm (without task augmentation) ...\"],[\"```sh\\npip install -r STraTA\\u002fselftraining\\u002frequirements.txt\\n```\\nThis will install PyTorch as a backend...\"],[\"- `finetune_on_labeled_data`: If set to `True`, the resulting model from each self-training iteratio...\"],[\"3. Run your script with the following command:\\n\\n```sh\\ntorchrun --nnodes=\\\"{$NUM_NODES}\\\" --nproc_per_n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"1. Start by creating a [`pipeline`] and specify the inference task:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers imp...\"],[\"Now this result looks more accurate! For a deep-dive comparison on Wav2Vec2 vs Whisper, refer to the...\"],[\"### Device\\n\\nIf you use `device=n`, the pipeline automatically puts the model on the specified device...\"],[\"Pipelines can also alleviate some of the complexities of batching because, for some pipelines, a sin...\"],[\"```python\\n\\u003e\\u003e\\u003e transcriber = pipeline(model=\\\"openai\\u002fwhisper-large-v2\\\", chunk_length_s=30, return_time...\"],[\"```py\\n# KeyDataset is a util that will just output the item we're interested in.\\nfrom transformers.p...\"],[\"## Text pipeline\\n\\nUsing a [`pipeline`] for NLP tasks is practically identical.\\n\\n```py\\n\\u003e\\u003e\\u003e from trans...\"],[\"You can easily run `pipeline` on large models using 🤗 `accelerate`! First make sure you have install...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"From the abstract of the XLM-V paper:\\n\\n*Large multilingual language models typically rely on a singl...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Combining simple architectures with large-scale pre-...\"],[\"## Usage tips\\n\\nOWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses [CLIP](c...\"],[\"\\u003e\\u003e\\u003e # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\\n\\u003e\\u003e\\u003e target_sizes...\"],[\"## OwlViTTextModel\\n\\n[[autodoc]] OwlViTTextModel\\n    - forward\\n\\n## OwlViTVisionModel\\n\\n[[autodoc]] Owl...\"],[\"Security Policy\\n\\n## Reporting a Vulnerability\\n\\n🤗 We have our bug bounty program set up with HackerOn...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"*The MobileNetV2 architecture is based on an inverted residual structure where the input and output ...\"],[\"- One can use [`MobileNetV2ImageProcessor`] to prepare images for the model.\\n\\n- The available image ...\"],[\"- The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"According to the abstract\\n\\n*Code summarization and generation empower conversion between programming...\"],[\"In cases where the language code is needed, the regular [`~PLBartTokenizer.__call__`] will encode so...\"],[\"## PLBartConfig\\n\\n[[autodoc]] PLBartConfig\\n\\n## PLBartTokenizer\\n\\n[[autodoc]] PLBartTokenizer\\n    - bui...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce a self-supervised vision representation...\"],[\"- BEiT models are regular Vision Transformers, but pre-trained in a self-supervised way rather than ...\"],[\"images and 1,000 classes).\\n- BEiT uses relative position embeddings, inspired by the T5 model. Durin...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"## BeitForSemanticSegmentation\\n\\n[[autodoc]] BeitForSemanticSegmentation\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003cjax\\u003e\\n\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce a framework that abstracts Reinforcemen...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"For something slightly more challenging, you can also take a look at the [Good Second Issue](https:\\u002f...\"],[\"To get the OS and software versions automatically, run the following command:\\n\\n```bash\\ntransformers-...\"],[\"## Do you want to add documentation?\\n\\nWe're always looking for improvements to the documentation tha...\"],[\"3. Create a new branch to hold your development changes:\\n\\n   ```bash\\n   git checkout -b a-descriptiv...\"],[\"```bash\\n   make quality\\n   ```\\n\\n   Finally, we have a lot of scripts to make sure we don't forget to...\"],[\"If you've already opened a pull request, you'll need to force push with the `--force` flag. Otherwis...\"],[\"☐ All public methods must have informative docstrings (see\\n[`modeling_bert.py`](https:\\u002f\\u002fgithub.com\\u002fh...\"],[\"```bash\\npip install -r examples\\u002fxxx\\u002frequirements.txt  # only needed the first time\\npython -m pytest ...\"],[\"```bash\\npython -m unittest discover -s tests -t . -v\\npython -m unittest discover -s examples -t exam...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*This paper presents XLS-R, a large-scale model for c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e # INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\\n\\u003e\\u003e\\u003e line = \\\"Tôi là sinh_viên trường đại_học Công_ng...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Text recognition is a long-standing research problem...\"],[\"## Usage tips\\n\\n- The quickest way to get started with TrOCR is by checking the [tutorial\\n  notebooks...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog post on [Accelerating Document AI](https:\\u002f\\u002fh...\"],[\"## Inference\\n\\nTrOCR's [`VisionEncoderDecoder`] model accepts images as input and makes use of\\n[`~gen...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```diff\\nfrom transformers import AutoModelForImageClassification\\n\\nmodel = AutoModelForImageClassific...\"],[\"processor = AutoImageProcessor.from_pretrained(\\\"facebook\\u002fdetr-resnet-50\\\")\\nmodel = AutoModelForObject...\"],[\"**Object Detection** \\n- [google\\u002fowlvit-base-patch32](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fowlvit-base-patch...\"],[\"Below you can find inference durations in milliseconds for each model with and without `compile()`. ...\"],[\"### A100 (batch size: 16)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecom...\"],[\"### V100 (batch size: 4)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecomp...\"],[\"### T4 (batch size: 1)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompil...\"],[\"### T4 (batch size: 16)\\n\\n| **Task\\u002fModel** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecompi...\"],[\"### V100\\n\\n| **Task\\u002fModel** | **Batch Size** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch 2.0 - \\u003cbr\\u003ecom...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Get started by making sure you have PyTorch installed. MPS acceleration is supported on macOS 12.3+....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Scientific knowledge is predominantly stored in book...\"],[\"## Inference\\n\\nNougat's [`VisionEncoderDecoder`] model accepts images as input and makes use of\\n[`~ge...\"],[\"\\u003e\\u003e\\u003e sequence = processor.batch_decode(outputs, skip_special_tokens=True)[0]\\n\\u003e\\u003e\\u003e sequence = processor...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised approaches for speech representation...\"],[\"## HubertConfig\\n\\n[[autodoc]] HubertConfig\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## HubertModel\\n\\n[[autodoc]] Hube...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers are quickly becoming one of the most he...\"],[\"\\u003cimg\\nsrc=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdilated-neig...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*What does it take to create the Babel Fish, a tool t...\"],[\"## Usage\\n\\nFirst, load the processor and a checkpoint of the model:\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers ...\"],[\"\\u003e\\u003e\\u003e # from text\\n\\u003e\\u003e\\u003e output_tokens = model.generate(**text_inputs, tgt_lang=\\\"fra\\\", generate_speech=Fa...\"],[\"#### 4. Generate speech and text at the same time\\n\\nUse `return_intermediate_token_ids=True` with [`S...\"],[\"## SeamlessM4TForSpeechToText\\n\\n[[autodoc]] transformers.SeamlessM4TForSpeechToText\\n    - forward\\n   ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"3. Check the [Migration](migration) guide if you use an older version of 🤗 Transformers since some i...\"],[\"\\u003cTip\\u003e\\n\\nRefer to the Performance [guide](performance) for more details about memory-saving techniques...\"],[\"## CUDA error: device-side assert triggered\\n\\nSometimes you may run into a generic CUDA error about a...\"],[\"\\u003cTip\\u003e\\n\\nBy default, the tokenizer creates an `attention_mask` for you based on your specific tokenize...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n\\u003e Visually-situated language is ubiquitous -- sources...\"],[\"Tips:\\n\\nPix2Struct has been fine tuned on a variety of tasks and datasets, ranging from image caption...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"## The Big Table of Tasks\\n\\nHere is the list of all our examples:\\n\\n| Task | Example datasets |\\n|---|-...\"],[\"Patience-based Early Exit\\n\\nPatience-based Early Exit (PABEE) is a plug-and-play inference method for...\"],[\"When evaluating on a regression task (STS-B), you may add `--regression_threshold 0.1` to define the...\"],[\"Long Form Question Answering\\n\\nAuthor: @yjernite\\n\\nThis folder contains the code for the Long Form Que...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, neural networks purely based on attention ...\"],[\"- Compared to ViT, DeiT models use a so-called distillation token to effectively learn from a teache...\"],[\"pre-training.\\n- The authors of DeiT also released more efficiently trained ViT models, which you can...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Light-weight convolutional neural networks (CNNs) ar...\"],[\"## Usage tips\\n\\n- MobileViT is more like a CNN than a Transformer model. It does not work on sequence...\"],[\"The resulting model will be just **about an MB** making it a good fit for mobile applications where ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmo...\"],[\"..\\u002fmodel_doc\\u002fsqueezebert), [T5](..\\u002fmodel_doc\\u002ft5), [UMT5](..\\u002fmodel_doc\\u002fumt5), [XLM](..\\u002fmodel_doc\\u002fxlm)...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"There are several important fields here:\\n\\n- `answers`: the starting location of the answer token and...\"],[\"...     offset_mapping = inputs.pop(\\\"offset_mapping\\\")\\n...     answers = examples[\\\"answers\\\"]\\n...     ...\"],[\"```py\\n\\u003e\\u003e\\u003e tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\\\"train...\"],[\"\\u003e\\u003e\\u003e trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     train_dataset=tokeni...\"],[\"Configure the model for training with [`compile`](https:\\u002f\\u002fkeras.io\\u002fapi\\u002fmodels\\u002fmodel_training_apis\\u002f#c...\"],[\"If have more time and you're interested in how to evaluate your model for question answering, take a...\"],[\"```py\\n\\u003e\\u003e\\u003e answer_start_index = outputs.start_logits.argmax()\\n\\u003e\\u003e\\u003e answer_end_index = outputs.end_logi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Expanding the language coverage of speech technology...\"],[\"Tips:\\n\\n- All ASR models accept a float array corresponding to the raw waveform of the speech signal....\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIf you want to use the ASR pipeline, you can load your chosen target language as such:\\n\\n```p...\"],[\"ids = torch.argmax(outputs, dim=-1)[0]\\ntranscription = processor.decode(ids)\\n# 'joe keton disapprove...\"],[\"```bash\\npip install --upgrade transformers accelerate\\n```\\n\\nSince the flow-based model in VITS is non...\"],[\"To do this, first clone the uroman repository to your local machine and set the bash variable `UROMA...\"],[\"set_seed(555)  # make deterministic\\nwith torch.no_grad():\\n   outputs = model(inputs[\\\"input_ids\\\"])\\n\\nw...\"],[\"Next, we load a couple of audio samples via `datasets`. Make sure that the audio data is sampled to ...\"],[\"To see all the supported languages of a checkpoint, you can print out the language ids as follows:\\n`...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"BetterTransformer accelerates inference with its fastpath (native PyTorch specialized implementation...\"],[\"For a gentle introduction to TorchScript, see the [Introduction to PyTorch TorchScript](https:\\u002f\\u002fpyto...\"],[\"```bash\\npip install intel_extension_for_pytorch\\n```\\n\\nSet the `--use_ipex` and `--jit_mode_eval` flag...\"],[\"onnx_qa = pipeline(\\\"question-answering\\\", model=model, tokenizer=tokenizer)\\n\\nquestion = \\\"What's my na...\"],[\"!--Copyright 2023 The HuggingFace and Baidu Team. All rights reserved.\\n\\nLicensed under the Apache Li...\"],[\"The abstract from the paper is the following:\\n\\n*Recent studies have demonstrated that pre-trained cr...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Contrastive language-image pretraining has shown gre...\"],[\"\\u003csmall\\u003e X-CLIP architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.02816\\\"\\u003eoriginal pape...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Having downloaded COCO dataset manually you should be able to load with the `ydshieh\\u002fcoc_dataset_scr...\"],[\"### Train the model\\nFinally, we can run the example script to train the model:\\n\\n```bash\\npython examp...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"To upload all converted models, \\n\\n1. Install [git-lfs](https:\\u002f\\u002fgit-lfs.github.com\\u002f).\\n\\n2. Login to `h...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`] thus implement the main\\nmethods for using al...\"],[\"## PreTrainedTokenizerFast\\n\\nThe [`PreTrainedTokenizerFast`] depend on the [tokenizers](https:\\u002f\\u002fhuggi...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```py\\nfrom starlette.applications import Starlette\\nfrom starlette.responses import JSONResponse\\nfrom...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nThe code sample below is intentionally written like pseudo-code for readabilit...\"],[\"### Circuit breaking\\n\\nWebservers usually look better when they do circuit breaking. It means they \\nr...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Inference\\n\\nSpeech2Text is a speech model that accepts a float tensor of log-mel filter-bank featu...\"],[\"\\u003e\\u003e\\u003e transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\\n\\u003e\\u003e\\u003e transcriptio...\"],[\"See the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=speech_to_text) to look for Speech2Text che...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While originally designed for natural language proce...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, significant progress has been made applyin...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr). The original code can be\\nfoun...\"],[\"# Sequence to Sequence Training and Evaluation\\n\\nThis directory contains examples for finetuning and ...\"],[\"#### Pegasus (multiple datasets)\\n\\nMultiple eval datasets are available for download from:\\nhttps:\\u002f\\u002fgi...\"],[\"Summarization Tips:\\n- (summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB...\"],[\"```bash\\n.\\u002ffinetune.sh \\\\\\n    [...]\\n    --encoder_layerdrop 0.1 \\\\\\n    --decoder_layerdrop 0.1 \\\\\\n    --...\"],[\"```bash\\noutput_dir\\n├── best_tfmr  # this is a huggingface checkpoint generated by save_pretrained. I...\"],[\"# Experimental Features\\nThese features are harder to use and not always useful.\\n\\n###  Dynamic Batch ...\"],[\"![DBART](https:\\u002f\\u002fhuggingface.co\\u002ffront\\u002fthumbnails\\u002fdistilbart_large.png)\\n\\n+ For the CNN\\u002fDailyMail data...\"],[\"`distill-pegasus`:\\n```bash\\ndeval 1 sshleifer\\u002fdistill-pegasus-cnn-16-4 cnn_dm dpx_cnn_eval\\ndeval 1 ss...\"],[\"#### Initialization\\nWe use [make_student.py](.\\u002fmake_student.py) to copy alternating layers from the ...\"],[\"+ Note: The command that produced `sshleifer\\u002fdistilbart-cnn-12-6` is at [train_distilbart_cnn.sh](.\\u002f...\"],[\"To combine datasets, as in Section 6.2, try something like:\\n```bash\\ncurl -S https:\\u002f\\u002fcdn-datasets.hug...\"],[\"\\u003c!--- runtime: 13H on V-100 16GB GPU. --\\u003e\\n\\n### Citation\\n\\n```bibtex\\n@misc{shleifer2020pretrained,\\n   ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\\\"auto\\\"` wh...\"],[\"Inputs need to be passed through a specific Processor to have the correct formats.\\nA processor requi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cbr\\u003e\\n\\nFeel free to check out the [API reference](.\\u002fmain_classes\\u002ftrainer) for these other [`Trainer`]...\"],[\"```py\\nfrom transformers import TrainingArguments\\n\\ntraining_args = TrainingArguments(\\n    output_dir=...\"],[\"* `hub_strategy=\\\"checkpoint\\\"` pushes the latest checkpoint to a subfolder named \\\"last-checkpoint\\\" fr...\"],[\"* [`~Trainer.get_train_dataloader`] creates a training DataLoader\\n* [`~Trainer.get_eval_dataloader`]...\"],[\"### Callbacks\\n\\nAnother option for customizing the [`Trainer`] is to use [callbacks](callbacks). Call...\"],[\"## Logging\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [logging](.\\u002fmain_classes\\u002flogging) API reference for more informatio...\"],[\"Use different combinations of `log_level` and `log_level_replica` to configure what gets logged on e...\"],[\"\\u003cTip\\u003e\\n\\nLearn more about FSDP sharding strategies, CPU offloading, and more with the [`Trainer`] in t...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"DeepSpeed\\\"\\u003e\\n\\n```yml\\ncompute_environment: LOCAL_MACHINE\\ndeepspeed_config:\\n ...\"],[\"```bash\\naccelerate launch \\\\\\n    .\\u002fexamples\\u002fpytorch\\u002ftext-classification\\u002frun_glue.py \\\\\\n    --model_nam...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"---\\n\\n---\\n**NOTE 2**\\n\\nWhen training a model on large datasets it is recommended to run the data prepr...\"],[\"```bash\\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\\\\\n\\t--dataset_name=librispeech_asr \\\\...\"],[\"```bash\\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\\\ \\n\\t--dataset_name=librispeech_asr ...\"],[\"p align=\\\"center\\\"\\u003e \\u003cimg src=\\\"http:\\u002f\\u002fsayef.tech:8082\\u002fuploads\\u002fFSNER-LOGO-2.png\\\" alt=\\\"FSNER LOGO\\\"\\u003e \\u003c\\u002fp\\u003e\\n...\"],[\"```python\\nfrom fsner import FSNERModel, FSNERTokenizerUtils\\n\\nmodel = FSNERModel(\\\"sayef\\u002ffsner-bert-ba...\"],[\"!--⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar t...\"],[\"| Notebook     |      Description      |      Author      |      |\\n|:----------|:-------------|:----...\"],[\"| [Fine-tune DialoGPT on New Datasets and Languages](https:\\u002f\\u002fgithub.com\\u002fncoop57\\u002fi-am-a-nerd\\u002fblob\\u002fmas...\"],[\"| [Optimize 🤗 Hugging Face models with Weights & Biases](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fwa...\"],[\"| [Fine-tune T5 for Sentiment Span Extraction](https:\\u002f\\u002fgithub.com\\u002fenzoampil\\u002ft5-intro\\u002fblob\\u002fmaster\\u002ft5_...\"],[\"|[Speed up Fine-Tuning in Transformers with Dynamic Padding \\u002f Bucketing](https:\\u002f\\u002fgithub.com\\u002fELS-RD\\u002ft...\"],[\"|[Fine-tune Electra and interpret with Integrated Gradients](https:\\u002f\\u002fgithub.com\\u002felsanns\\u002fxai-nlp-note...\"],[\"|[Fine-tune ALBERT for sentence-pair classification](https:\\u002f\\u002fgithub.com\\u002fNadirEM\\u002fnlp-notebooks\\u002fblob\\u002fm...\"],[\"|[Leverage BERT for Encoder-Decoder Summarization on CNN\\u002fDailymail](https:\\u002f\\u002fgithub.com\\u002fpatrickvonpla...\"],[\"|[Evaluate TAPAS on Table Fact Checking (TabFact)](https:\\u002f\\u002fgithub.com\\u002fNielsRogge\\u002fTransformers-Tutori...\"],[\"|[Fine-Tune DistilGPT2 and Generate Text](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002ftripathiaakash\\u002fDi...\"],[\"|[Fine-tune LayoutLM on RVL-CDIP (a document image classification dataset)](https:\\u002f\\u002fgithub.com\\u002fNiels...\"],[\"| [Create video captions using Wav2Vec2](https:\\u002f\\u002fgithub.com\\u002fMuennighoff\\u002fytclipcc\\u002fblob\\u002fmain\\u002fwav2vec_y...\"],[\"| [Evaluate LUKE on Open Entity, an entity typing dataset](https:\\u002f\\u002fgithub.com\\u002fstudio-ousia\\u002fluke\\u002fblob...\"],[\"| [Speech Emotion Classification with Wav2Vec2](https:\\u002f\\u002fgithub\\u002fm3hrdadfi\\u002fsoxan\\u002fblob\\u002fmain\\u002fnotebooks\\u002fE...\"],[\"| [Finetune T5 for Named Entity Recognition](https:\\u002f\\u002fgithub.com\\u002fToluClassics\\u002fNotebooks\\u002fblob\\u002fmain\\u002fT5_...\"],[\"\\u003c!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ve...\"],[\"🤗 Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provide...\"],[\"## Supported models and frameworks\\n\\nThe table below represents the current support in the library fo...\"],[\"|                                  Model                                   | PyTorch support | Tenso...\"],[\"|                 [BertJapanese](model_doc\\u002fbert-japanese)                  |       ✅        |       ...\"],[\"|                          [ByT5](model_doc\\u002fbyt5)                          |       ✅        |       ...\"],[\"|                           [CPM](model_doc\\u002fcpm)                           |       ✅        |       ...\"],[\"|                          [DETR](model_doc\\u002fdetr)                          |       ✅        |       ...\"],[\"|                         [ERNIE](model_doc\\u002fernie)                         |       ✅        |       ...\"],[\"|                          [GLPN](model_doc\\u002fglpn)                          |       ✅        |       ...\"],[\"|                      [ImageGPT](model_doc\\u002fimagegpt)                      |       ✅        |       ...\"],[\"|                         [LLaVa](model_doc\\u002fllava)                         |       ✅        |       ...\"],[\"|                      [mBART-50](model_doc\\u002fmbart50)                       |       ✅        |       ...\"],[\"|                         [MPNet](model_doc\\u002fmpnet)                         |       ✅        |       ...\"],[\"|                      [OpenAI GPT-2](model_doc\\u002fgpt2)                      |       ✅        |       ...\"],[\"|                        [PLBart](model_doc\\u002fplbart)                        |       ✅        |       ...\"],[\"|          [RoBERTa-PreLayerNorm](model_doc\\u002froberta-prelayernorm)          |       ✅        |       ...\"],[\"|                   [SqueezeBERT](model_doc\\u002fsqueezebert)                   |       ✅        |       ...\"],[\"|                  [Transformer-XL](model_doc\\u002ftransfo-xl)                  |       ✅        |       ...\"],[\"|        [Vision Encoder decoder](model_doc\\u002fvision-encoder-decoder)        |       ✅        |       ...\"],[\"|                         [WavLM](model_doc\\u002fwavlm)                         |       ✅        |       ...\"],[\"\\u003c!-- End table--\\u003e...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"By exposing a graph with standardized operators and data types, ONNX makes it easy to\\nswitch between...\"],[\"```bash\\noptimum-cli export onnx --help\\n```\\n\\nTo export a model's checkpoint from the 🤗 Hub, for examp...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e from optimum.onnxruntime import ORTModelFor...\"],[\"### Exporting a model with `transformers.onnx`\\n\\n\\u003cTip warning={true}\\u003e\\n\\n`tranformers.onnx` is no longe...\"],[\"```bash\\npython -m transformers.onnx --model=keras-io\\u002ftransformers-qa onnx\\u002f\\n```\\n\\nTo export a model th...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"To explain how tasks are solved, we'll walk through what goes on inside the model to output useful p...\"],[\"1. A *feature encoder* takes the raw audio waveform, normalizes it to zero mean and unit variance, a...\"],[\"Ready to try your hand at audio classification? Check out our complete [audio classification guide](...\"],[\"### Image classification\\n\\nViT and ConvNeXT can both be used for image classification; the main diffe...\"],[\"4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer percep...\"],[\"\\u003csmall\\u003eA basic convolution without padding or stride, taken from \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1603...\"],[\"The output from the convolution blocks is passed to a classification head which converts the outputs...\"],[\"3. DETR uses a *bipartite matching loss* during training to compare a fixed number of predictions wi...\"],[\"There are three main components to Mask2Former:\\n\\n1. A [Swin](model_doc\\u002fswin) backbone accepts an ima...\"],[\"### Depth estimation\\n\\n[GLPN](model_doc\\u002fglpn), *Global-Local Path Network*, is a Transformer for dept...\"],[\"## Natural language processing\\n\\nThe Transformer was initially designed for machine translation, and ...\"],[\"The second pretraining object is next-sentence prediction. The model must predict whether sentence B...\"],[\"### Question answering\\n\\nTo use BERT for question answering, add a span classification head on top of...\"],[\"2. The output from the decoder is passed to a language modeling head, which performs a linear transf...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"\\u003cTip\\u003e\\n\\nFor more information about text generation, check out the [text generation strategies](genera...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\nXLM-RoBERTa-XL is a multilingual model trained on 100 different languages. Unlike som...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```bash\\noptimum-cli export tflite --help\\n```\\n\\nTo export a model's checkpoint from the 🤗 Hub, for exa...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present a new method that views object detection ...\"],[\"## How DETR works\\n\\nHere's a TLDR explaining how [`~transformers.DetrForObjectDetection`] works:\\n\\nFir...\"],[\"Next, this is sent through the encoder, outputting `encoder_hidden_states` of the same shape (you ca...\"],[\"DETR can be naturally extended to perform panoptic segmentation (which unifies semantic segmentation...\"],[\"- DETR uses so-called **object queries** to detect objects in an image. The number of queries determ...\"],[\"- [`~transformers.DetrForObjectDetection`] and [`~transformers.DetrForSegmentation`] can be initiali...\"],[\"There are three ways to instantiate a DETR model (depending on what you prefer):\\n\\nOption 1: Instanti...\"],[\"As a summary, consider the following table:\\n\\n| Task | Object detection | Instance segmentation | Pan...\"],[\"In short, one should prepare the data either in COCO detection or COCO panoptic format, then use\\n[`~...\"],[\"## DETR specific outputs\\n\\n[[autodoc]] models.detr.modeling_detr.DetrModelOutput\\n\\n[[autodoc]] models....\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In our example, we will take a couple of arguments of the ResNet class that we might want to tweak. ...\"],[\"The inheritance is to make sure you get all the functionality from the 🤗 Transformers library, while...\"],[\"```py\\nfrom transformers import PreTrainedModel\\nfrom timm.models.resnet import BasicBlock, Bottleneck...\"],[\"In both cases, notice how we inherit from `PreTrainedModel` and call the superclass initialization w...\"],[\"## Registering a model with custom code to the auto classes\\n\\nIf you are writing a library that exten...\"],[\"```\\n.\\n└── resnet_model\\n    ├── __init__.py\\n    ├── configuration_resnet.py\\n    └── modeling_resnet.p...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nNext, let's create the config and models as we did before:\\n\\n```py\\nresnet50d_config = ResnetC...\"],[\"```py\\nfrom transformers import AutoModelForImageClassification\\n\\nmodel = AutoModelForImageClassificat...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Open-domain question answering relies on efficient p...\"],[\"[[autodoc]] models.dpr.modeling_dpr.DPRReaderOutput\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## DPRContextEncoder\\n\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"Split the dataset into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\\n...\"],[\"```py\\n\\u003e\\u003e\\u003e billsum[\\\"train\\\"][0]\\n{'summary': 'Existing law authorizes state agencies to enter into cont...\"],[\"'text': 'The people of the State of California do enact as follows:\\\\n\\\\n\\\\nSECTION 1.\\\\nSection 10295.3...\"],[\".\\\\n(c) After taking all reasonable measures to find a contractor that complies with this section, as...\"],[\".\\\\n(e) (1) Every contract subject to this chapter shall contain a statement by which the contractor ...\"],[\".35 of the Public Contract Code shall not be construed to create any new enforcement authority or re...\"],[\"'title': 'An act to add Section 10295.35 to the Public Contract Code, relating to public contracts.'...\"],[\"There are two fields that you'll want to use:\\n\\n- `text`: the text of the bill which'll be the input ...\"],[\"\\u003e\\u003e\\u003e data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\\n```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n``...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\u003cTip\\u003e\\n\\nIf you aren't familiar with finetuning a model with the [`Trainer`], ...\"],[\"\\u003e\\u003e\\u003e trainer.train()\\n```\\n\\nOnce training is completed, share your model to the Hub with the [`~transfo...\"],[\"Pass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from trans...\"],[\"Come up with some text you'd like to summarize. For T5, you need to prefix your input depending on t...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoModelForSeq2SeqLM\\n\\n\\u003e\\u003e\\u003e model = AutoModelForSeq2SeqLM.from_pre...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Use this document as your starting point to navigate further to the methods that match your scenario...\"],[\"## Training and inference\\n\\nHere you'll find techniques, tips and tricks that apply whether you are t...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the paper is the following:\\n\\n*Recent work in language modeling demonstrates that t...\"],[\"This model was contributed by [jdemouth](https:\\u002f\\u002fhuggingface.co\\u002fjdemouth). The original code can be ...\"],[\"```bash\\npython3 $PATH_TO_TRANSFORMERS\\u002fmodels\\u002fmegatron_bert\\u002fconvert_megatron_bert_checkpoint.py megat...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## With Trainer\\n\\nHere is an example of a translation fine-tuning with a MarianMT model:\\n\\n```bash\\npyt...\"],[\"```bash\\npython examples\\u002fpytorch\\u002ftranslation\\u002frun_translation.py \\\\\\n    --model_name_or_path facebook\\u002fm...\"],[\"If you want to use a pre-processed dataset that leads to high BLEU scores, but for the `en-de` langu...\"],[\"You can then use your usual launchers to run in it in a distributed environment, but the easiest way...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Using datasets from Hub\\n\\nHere we show how to fine-tune a Vision Transformer (`ViT`) on the [bean...\"],[\"Below, we explain both in more detail.\\n\\n#### Provide them as folders\\n\\nIf you provide your own folder...\"],[\"# example 4: providing several splits\\ndataset = load_dataset(\\\"imagefolder\\\", data_files={\\\"train\\\": [\\\"p...\"],[\"ere is how to convert a GPT2 model generated outside of `transformers`\\n\\n* [Megatron-LM](https:\\u002f\\u002fgith...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [valhalla](https:\\u002f\\u002fhuggingface.co\\u002fvalhalla). The Authors' code can be ...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import MBartForConditionalGeneration, MBartTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer ...\"],[\"MBart-50 has its own tokenizer [`MBart50Tokenizer`].\\n\\n-  Supervised training\\n\\n```python\\nfrom transfo...\"],[\"# translate Arabic to English\\ntokenizer.src_lang = \\\"ar_AR\\\"\\nencoded_ar = tokenizer(article_ar, return...\"],[\"## FlaxMBartForSequenceClassification\\n\\n[[autodoc]] FlaxMBartForSequenceClassification\\n    - __call__...\"],[\"# MM-IMDb\\n\\nBased on the script [`run_mmimdb.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fma...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Paper: [XTREME-S: Evaluating Cross-lingual Speech Representations](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2203.10752)...\"],[\"We get the following results on the test set of the benchmark's datasets. \\nThe corresponding trainin...\"],[\"```bash\\npython -m torch.distributed.launch \\\\\\n    --nproc_per_node=8 \\\\\\n    run_xtreme_s.py \\\\\\n    --ta...\"],[\"```bash\\npython -m torch.distributed.launch \\\\\\n    --nproc_per_node=2 \\\\\\n    run_xtreme_s.py \\\\\\n    --ta...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e print(tokenizer.decode(inputs[\\\"input_ids\\\"][0]))\\n[CLS] 吾輩 は 猫 で ある 。 [SEP]\\n\\n\\u003e\\u003e\\u003e outputs = bertjap...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Structured document understanding has attracted cons...\"],[\"```\\nfrom transformers import LiltModel\\n\\nmodel = LiltModel.from_pretrained(\\\"path_to_your_files\\\")\\nmode...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Begin by loading the [Yelp Reviews](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyelp_review_full) dataset:\\n\\n```p...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"ber...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoModelForSequenceClassification\\n\\n\\u003e\\u003e\\u003e model = AutoModelForSeque...\"],[\"```py\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e import evaluate\\n\\n\\u003e\\u003e\\u003e metric = evaluate.load(\\\"accuracy\\\")\\n```\\n\\nCall [...\"],[\"First, load a dataset. We'll use the CoLA dataset from the [GLUE benchmark](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThis approach works great for smaller datasets, but for larger datasets, you might find it s...\"],[\"```py\\n\\u003e\\u003e\\u003e tf_dataset = model.prepare_tf_dataset(dataset[\\\"train\\\"], batch_size=16, shuffle=True, token...\"],[\"```py\\n    \\u003e\\u003e\\u003e tokenized_datasets = tokenized_datasets.remove_columns([\\\"text\\\"])\\n    ```\\n\\n2. Rename th...\"],[\"Lastly, specify `device` to use a GPU if you have access to one. Otherwise, training on a CPU may ta...\"],[\"\\u003e\\u003e\\u003e metric.compute()\\n```\\n\\u003c\\u002fpt\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003ca id='additional-resources'\\u003e\\u003c\\u002fa\\u003e\\n\\n## Additional...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"According to the abstract,\\n\\n- Bart uses a standard seq2seq\\u002fmachine translation architecture with a b...\"],[\"## Implementation Notes\\n\\n- Bart doesn't use `token_type_ids` for sequence classification. Use [`Bart...\"],[\"\\u003cPipelineTag pipeline=\\\"summarization\\\"\\u002f\\u003e\\n\\n- A blog post on [Distributed Training: Train BART\\u002fT5 for S...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`BartForConditionalGeneration`] is supported by this [exampl...\"],[\"\\u003cPipelineTag pipeline=\\\"translation\\\"\\u002f\\u003e\\n\\n- A notebook on how to [finetune mBART using Seq2SeqTrainer f...\"],[\"\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFBartModel\\n\\n[[autodoc]] TFBartModel\\n    - call\\n\\n## TFBartForConditionalGeneration\\n\\n[...\"],[\"Intro\\n\\nAuthors: @patrickvonplaten and @lhoestq\\n\\nAimed at tackling the knowledge-intensive NLP tasks ...\"],[\"The `base` models initialize the question encoder with [`facebook\\u002fdpr-question_encoder-single-nq-bas...\"],[\"For the Ray implementation, the index is loaded in *separate* process(es). The training workers rand...\"],[\"## Retrieval evaluation\\nFor `retrieval` evaluation, we expect a gold data file where each line will ...\"],[\"2. Parse the unziped file using the `parse_dpr_relevance_data.py`\\n    ```bash\\n    mkdir output # or ...\"],[\"Predictions of the model for the samples from the `evaluation_set` will be saved under the path spec...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The exact location may vary from system to system, but `usr\\u002flocal\\u002fcuda-10.2` is the most common loca...\"],[\"You could also install an older version of the compiler in addition to the one you're currently usin...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\u003cTip\\u003e\\n\\nFor multi-GPU training it requires DDP (`torch.distributed.launch`).\\n\\n\\u003c\\u002fTip\\u003e\\n\\n\\u003cTip\\u003e\\n\\n...\"],[\"```\\nDetected inf\\u002fnan during batch_number=0\\nLast 21 forward frames:\\nabs min  abs max  metadata\\n      ...\"],[\"At the very start of the trace you can discover at which batch number the problem occurred (here `De...\"],[\"You can see here, that `T5DenseGatedGeluDense.forward` resulted in output activations, whose absolut...\"],[\"```python\\ndef _forward(self, hidden_states):\\n    hidden_gelu = self.gelu_act(self.wi_0(hidden_states...\"],[\"debug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=100)\\n```\\n\\n### Specific batch absol...\"],[\"You can also specify the batch number after which to stop the training, with:\\n\\n```python\\ndebug_overf...\"],[\"# Information Gain Filtration(IGF)\\n\\nAuthors @Tuko @mraunak\\n\\nThis folder contains the code how to imp...\"],[\"Paper [Selecting Informative Contexts Improves Language Model Finetuning](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2005...\"],[\"## How to use this project?\\n\\nTo fine-tune a transformer model with IGF on a language modeling task, ...\"],[\"## Citation\\n\\nIf you find the resource useful, please cite the following paper\\n\\n```\\n@inproceedings{an...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transfer learning, where a model is first pre-traine...\"],[\"This model was contributed by [thomwolf](https:\\u002f\\u002fhuggingface.co\\u002fthomwolf). The original code can be ...\"],[\"- [t5-3b](https:\\u002f\\u002fhuggingface.co\\u002ft5-3b)\\n\\n- [t5-11b](https:\\u002f\\u002fhuggingface.co\\u002ft5-11b).\\n\\nBased on the or...\"],[\"## Training\\n\\nT5 is an encoder-decoder model and converts all NLP problems into a text-to-text format...\"],[\"\\u003e\\u003e\\u003e # the forward function automatically creates the correct decoder_input_ids\\n\\u003e\\u003e\\u003e loss = model(inpu...\"],[\"\\u003e\\u003e\\u003e # the forward function automatically creates the correct decoder_input_ids\\n\\u003e\\u003e\\u003e loss = model(inpu...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import T5Tokenizer, T5ForConditionalGeneration\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e...\"],[\"According to [this forum post](https:\\u002f\\u002fdiscuss.huggingface.co\\u002ft\\u002ft5-finetuning-tips\\u002f684), task prefix...\"],[\"Note that T5 uses the `pad_token_id` as the `decoder_start_token_id`, so when doing generation witho...\"],[\"## Performance\\n\\nIf you'd like a faster training and inference performance, install [NVIDIA APEX](htt...\"],[\"\\u003cPipelineTag pipeline=\\\"summarization\\\"\\u002f\\u003e\\n\\n- A notebook to [Finetune T5-base-dutch to perform Dutch ab...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`FlaxT5ForConditionalGeneration`] is supported by this [exam...\"],[\"## T5Tokenizer\\n\\n[[autodoc]] T5Tokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tok...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"The following example shows how to fine-tune the [Whisper small](https:\\u002f\\u002fhuggingface.co\\u002fopenai\\u002fwhisp...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"inputs = tokenizer(\\\"Hello, my dog is cute and \\\", return_tensors=\\\"pt\\\")\\ngeneration_output = model.gene...\"],[\"[[autodoc]] generation.SampleEncoderDecoderOutput\\n\\n[[autodoc]] generation.SampleDecoderOnlyOutput\\n\\n[...\"],[\"[[autodoc]] ForceTokensLogitsProcessor\\n    - __call__\\n\\n[[autodoc]] HammingDiversityLogitsProcessor\\n ...\"],[\"[[autodoc]] TFSuppressTokensLogitsProcessor\\n    - __call__\\n\\n[[autodoc]] TFTemperatureLogitsWarper\\n  ...\"],[\"[[autodoc]] ConstrainedBeamSearchScorer\\n    - process\\n    - finalize\\n\\n## Utilities\\n\\n[[autodoc]] top_...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*With the capability of modeling bidirectional contex...\"],[\"This model was contributed by [thomwolf](https:\\u002f\\u002fhuggingface.co\\u002fthomwolf). The original code can be ...\"],[\"## XLNetTokenizerFast\\n\\n[[autodoc]] XLNetTokenizerFast\\n\\n## XLNet specific outputs\\n\\n[[autodoc]] models...\"],[\"[[autodoc]] TFXLNetForSequenceClassification\\n    - call\\n\\n## TFLNetForMultipleChoice\\n\\n[[autodoc]] TFX...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[LayoutLM](..\\u002fmodel_doc\\u002flayoutlm), [LayoutLMv2](..\\u002fmodel_doc\\u002flayoutlmv2), [LayoutLMv3](..\\u002fmodel_doc\\u002f...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\\"nielsr\\u002fdocvqa_1200_examples...\"],[\"Note that the LayoutLMv2 checkpoint that we use in this guide has been trained with `max_position_em...\"],[\"## Preprocess the data\\n\\nThe Document Question Answering task is a multimodal task, and you need to m...\"],[\"```py\\n\\u003e\\u003e\\u003e tokenizer = processor.tokenizer\\n```\\n\\nOn top of the preprocessing mentioned above, we also ...\"],[\"```py\\n\\u003e\\u003e\\u003e example = dataset_with_ocr[\\\"train\\\"][1]\\n\\u003e\\u003e\\u003e words = [word.lower() for word in example[\\\"word...\"],[\"Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'develo...\"],[\".', 'emulti-plug', 'te', 'build', 'yaur', 'awn', 'cigarette.', 'eswitchable', 'menthol', 'or', 'non'...\"],[\"Answer:  T.F. Riehl\\nstart_index 17\\nend_index 18\\n```...\"],[\"Once examples are encoded, however, they will look like this:\\n\\n```py\\n\\u003e\\u003e\\u003e encoding = tokenizer(exampl...\"],[\"...         if match:\\n...             # if match is found, use `token_type_ids` to find where words ...\"],[\"...     return encoding\\n```\\n\\nNow that we have this preprocessing function, we can encode the entire ...\"],[\"## Train\\n\\nCongratulations! You've successfully navigated the toughest part of this guide and now you...\"],[\"\\u003e\\u003e\\u003e data_collator = DefaultDataCollator()\\n```\\n\\nFinally, bring everything together, and call [`~Train...\"],[\"You can also manually replicate the results of the pipeline if you'd like:\\n1. Take an image and a qu...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We extract an optimal subset of architectural parame...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- Splitting the embedding matrix into two smaller matrices.\\n- Using repeating layers split among gro...\"],[\"## Usage tips\\n\\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad ...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n\\n- [`AlbertForSequenceClassification`] is supported b...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`AlbertForMaskedLM`] is supported by this [example script](h...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`AlbertForQuestionAnswering`] is supported by this ...\"],[\"## AlbertTokenizerFast\\n\\n[[autodoc]] AlbertTokenizerFast\\n\\n## Albert specific outputs\\n\\n[[autodoc]] mod...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c!--This tip is automatically generated by `make fix-copies`, do not fill manually!--\\u003e\\n\\n[ALBERT](..\\u002f...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"## Preprocess\\n\\n\\u003cYoutube id=\\\"iY2AZYdZAr0\\\"\\u002f\\u003e\\n\\nThe next step is to load a DistilBERT tokenizer to prepr...\"],[\"Here is how you can create a function to realign the tokens and labels, and truncate sequences to be...\"],[\"\\u003e\\u003e\\u003e data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\\n```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n```py\\n\\u003e\\u003e\\u003e...\"],[\"Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your trai...\"],[\"\\u003e\\u003e\\u003e model = AutoModelForTokenClassification.from_pretrained(\\n...     \\\"distilbert-base-uncased\\\", num_...\"],[\"\\u003c\\u002fTip\\u003e\\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate s...\"],[\"Pass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from trans...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n\\u003e\\u003e\\u003e classifier = pipeline(\\\"ner\\\", model=\\\"stevhliu\\u002fmy_awe...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"ste...\"],[\"## Saved Pseudo-Labels\\nThese are the generations of various large models on various large **training...\"],[\"### Available Pseudo-labels\\n| Dataset | Model                       | Link                          ...\"],[\"| CNN\\u002fDM  | `google\\u002fpegasus-xsum`         | [download](https:\\u002f\\u002fcdn-datasets.huggingface.co\\u002fpseudo\\u002fcn...\"],[\"(EN_RO = WMT 2016 English-Romanian).\\n\\nExample Download Command:\\n```bash\\ncurl -S https:\\u002f\\u002fcdn-datasets...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In the past decade, convolutional neural networks (C...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"## ASTConfig\\n\\n[[autodoc]] ASTConfig\\n\\n## ASTFeatureExtractor\\n\\n[[autodoc]] ASTFeatureExtractor\\n    - _...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## How to enable Hyperparameter search in example\\n\\nDefine the hyperparameter search space, different...\"],[\"For raytune, see raytune [object_parameter](https:\\u002f\\u002fdocs.ray.io\\u002fen\\u002flatest\\u002ftune\\u002fapi\\u002fsearch_space.html...\"],[\"You could define your own compute_objective function, if not defined, the default compute_objective ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Image Transformer has recently achieved significant ...\"],[\"```python\\nfrom transformers import BeitForMaskedImageModeling\\n\\nmodel = BeitForMaskedImageModeling.fr...\"],[\"urrently the following model proposals are available:\\n\\n- \\u003cs\\u003e[BigBird (Google)](.\\u002fADD_BIG_BIRD.md)\\u003c\\u002fs...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present SegFormer, a simple, efficient yet powerf...\"],[\"- SegFormer consists of a hierarchical Transformer encoder, and a lightweight all-MLP decoder head.\\n...\"],[\"for the model. Note that this image processor is fairly basic and does not include all data augmenta...\"],[\"| **Model variant** | **Depths**    | **Hidden sizes**    | **Decoder hidden size** | **Params (M)**...\"],[\"Semantic segmentation:\\n\\n- [`SegformerForSemanticSegmentation`] is supported by this [example script]...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Motivated by the success of T5 (Text-To-Text Transfe...\"],[\"## SpeechT5FeatureExtractor\\n\\n[[autodoc]] SpeechT5FeatureExtractor\\n    - __call__\\n\\n## SpeechT5Process...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\npipe = pipeline(task=\\\"image-to...\"],[\"```python\\nimport torch\\n\\nwith torch.no_grad():\\n  outputs = model(pixel_values)\\n```\\nOutput is an objec...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Entity representations are useful in natural languag...\"],[\"## Usage tips\\n\\n- This implementation is the same as [`RobertaModel`] with the addition of entity emb...\"],[\"- There are three head models for the former use case:\\n\\n  - [`LukeForEntityClassification`], for tas...\"],[\"\\u003e\\u003e\\u003e text = \\\"Beyoncé lives in Los Angeles.\\\"\\n\\u003e\\u003e\\u003e entity_spans = [(0, 7)]  # character-based entity spa...\"],[\"## Resources\\n\\n- [A demo notebook on how to fine-tune [`LukeForEntityPairClassification`] for relatio...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e food = load_dataset(\\\"food101\\\", split=\\\"train[:5000]\\\"...\"],[\"Crop a random part of the image, resize it, and normalize it with the image mean and standard deviat...\"],[\"```py\\n\\u003e\\u003e\\u003e from tensorflow import keras\\n\\u003e\\u003e\\u003e from tensorflow.keras import layers\\n\\n\\u003e\\u003e\\u003e size = (image_pr...\"],[\"Use 🤗 Datasets [`~datasets.Dataset.set_transform`] to apply the transformations on the fly:\\n\\n```py\\nf...\"],[\"You're ready to start training your model now! Load ViT with [`AutoModelForImageClassification`]. Sp...\"],[\"\\u003e\\u003e\\u003e trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     data_collator=data_c...\"],[\"Convert your datasets to the `tf.data.Dataset` format using the [`~datasets.Dataset.to_tf_dataset`] ...\"],[\"Finally, you are ready to train your model! Call `fit()` with your training and validation datasets,...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n\\u003e\\u003e\\u003e classifier = pipeline(\\\"image-classification\\\", model...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import TFAutoModelForImageClassification\\n\\n\\u003e\\u003e\\u003e model = TFAutoModelForImag...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- The model usually performs well without requiring any finetuning.\\n- The architectur...\"],[\"\\u003e\\u003e\\u003e # Generate token ids\\n\\u003e\\u003e\\u003e predicted_ids = model.generate(input_features)\\n\\n\\u003e\\u003e\\u003e # Decode token ids ...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## WhisperModel\\n\\n[[autodoc]] WhisperModel\\n    - forward\\n    - _mask_input_f...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e device = \\\"cuda\\\"\\n\\u003e\\u003e\\u003e model = GPTJForCausalLM.from_pretrained(\\n...     \\\"EleutherAI\\u002fgpt-j-6B\\\",\\n... ...\"],[\"\\u003e\\u003e\\u003e model = AutoModelForCausalLM.from_pretrained(\\\"EleutherAI\\u002fgpt-j-6B\\\")\\n\\u003e\\u003e\\u003e tokenizer = AutoTokenize...\"],[\"- Description of [GPT-J](https:\\u002f\\u002fhuggingface.co\\u002fEleutherAI\\u002fgpt-j-6B).\\n- A blog on how to [Deploy GPT...\"],[\"- [`FlaxGPTJForCausalLM`] is supported by this [causal language modeling example script](https:\\u002f\\u002fgit...\"],[\"**Documentation resources**\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nWhen passing `output_hidden_states=True` you may expect the `outputs.hidden_states[-1]` to ma...\"],[\"## Seq2SeqModelOutput\\n\\n[[autodoc]] modeling_outputs.Seq2SeqModelOutput\\n\\n## CausalLMOutput\\n\\n[[autodoc...\"],[\"## SampleTSPredictionOutput\\n\\n[[autodoc]] modeling_outputs.SampleTSPredictionOutput\\n\\n## TFBaseModelOu...\"],[\"[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutput\\n\\n## FlaxBaseModelOutputWithPast\\n\\n[[autodoc]] m...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\" \\u002f\\u003e\\n\\n- [`DebertaForTokenClassification`] is supported by...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`DebertaForQuestionAnswering`] is supported by this...\"],[\"## TFDebertaForMaskedLM\\n\\n[[autodoc]] TFDebertaForMaskedLM\\n    - call\\n\\n## TFDebertaForSequenceClassif...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce a state-of-the-art real-time, high-fide...\"],[\"\\u003e\\u003e\\u003e model = EncodecModel.from_pretrained(\\\"facebook\\u002fencodec_24khz\\\")\\n\\u003e\\u003e\\u003e processor = AutoProcessor.fro...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"# initialize a random model with the right vocab_size\\nconfig = GPTNeoConfig.from_pretrained(\\\"Eleuthe...\"],[\"VisualBERT Demo\\n\\nThis demo shows usage of VisualBERT VQA model and is adapted from LXMERT demo prese...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [gchhablani](https:\\u002f\\u002fhuggingface.co\\u002fgchhablani). The original code can...\"],[\"The [`BertTokenizer`] is used to encode the text. A custom detector\\u002fimage processor must be used\\nto ...\"],[\"## VisualBertForQuestionAnswering\\n\\n[[autodoc]] VisualBertForQuestionAnswering\\n    - forward\\n\\n## Visu...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have emerged as a powerful tool for a b...\"],[\"## NystromformerConfig\\n\\n[[autodoc]] NystromformerConfig\\n\\n## NystromformerModel\\n\\n[[autodoc]] Nystromf...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"![example image](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftrans...\"],[\"Below is an example on how to run mask generation given an image and a 2D point:\\n\\n```python\\nimport t...\"],[\"## SamPromptEncoderConfig\\n\\n[[autodoc]] SamPromptEncoderConfig\\n\\n\\n## SamProcessor\\n\\n[[autodoc]] SamProc...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002ftransformers...\"],[\"Note that this will randomly initialize all the weights of the model.\\n\\n## Resources\\n\\nA list of offic...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers have shown great potential in computer ...\"],[\"\\u003cimg width=\\\"600\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f15921929\\u002f142746124-1ab7635d-2536-4a0...\"],[\"If you're interested in submitting a resource to be included here, please feel free to open a Pull R...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\\n\\n```bash\\npi...\"],[\"### Wrapping policy\\n\\nFSDP is applied by wrapping each layer in the network. The wrapping is usually ...\"],[\"```yaml\\nxla: True # must be set to True to enable PyTorch\\u002fXLA\\nxla_fsdp_settings: # XLA-specific FSDP...\"],[\"* Follow along with the more in-depth Accelerate guide for [FSDP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002faccele...\"],[\"Testing new Hugging Face Deep Learning Container.\\n\\nThis document explains the testing strategy for r...\"],[\"After we have released the Release Candidate we need to create a PR at the [Deep Learning Container ...\"],[\"images:\\n  BuildHuggingFacePytorchGpuPy37Cu110TrainingDockerImage:\\n    \\u003c\\u003c: *TRAINING_REPOSITORY\\n    b...\"],[\"### Run Tests:\\n\\nBefore we can run the tests we need to adjust the `requirements.txt` for Pytorch und...\"],[\"repository_info:\\n  training_repository: &TRAINING_REPOSITORY\\n    image_type: &TRAINING_IMAGE_TYPE tr...\"],[\"## Current Tests\\n\\n| ID                                  | Description                               ...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Relevant checkpoints can be found under https:\\u002f\\u002fhuggingface.co\\u002fmodels?other=phoneme-recognition.\\n\\nTh...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Converting custom checkpoints \\n\\n\\u003cTip\\u003e\\n\\nFalcon models were initially added to the Hugging Face Hub...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Depth estimation from a single image is an important...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you g...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained representations are becoming crucial for...\"],[\"## Usage example\\n\\nALIGN uses EfficientNet to get visual features and BERT to get the text features. ...\"],[\"## AlignConfig\\n\\n[[autodoc]] AlignConfig\\n    - from_text_vision_configs\\n\\n## AlignTextConfig\\n\\n[[autodo...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The library was designed with two strong goals in mind:\\n\\n1. Be as easy and fast to use as possible:\\n...\"],[\"2. Provide state-of-the-art models with performances as close as possible to the original models:\\n\\n ...\"],[\"## Main concepts\\n\\nThe library is built around three types of classes for each model:\\n\\n- **Model clas...\"],[\"All these classes can be instantiated from pretrained instances, saved locally, and shared on the Hu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, install 🤗 Datasets so you can load some datasets to experiment with:\\n\\n```b...\"],[\"Return your input by decoding the `input_ids`:\\n\\n```py\\n\\u003e\\u003e\\u003e tokenizer.decode(encoded_input[\\\"input_ids\\\"...\"],[\"Set the `padding` parameter to `True` to pad the shorter sequences in the batch to match the longest...\"],[\"Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by th...\"],[\"```py\\n\\u003e\\u003e\\u003e batch_sentences = [\\n...     \\\"But what about second breakfast?\\\",\\n...     \\\"Don't think he kn...\"],[\"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]...\"],[\"## Audio\\n\\nFor audio tasks, you'll need a [feature extractor](main_classes\\u002ffeature_extractor) to prep...\"],[\"1. Use 🤗 Datasets' [`~datasets.Dataset.cast_column`] method to upsample the sampling rate to 16kHz:\\n...\"],[\"```py\\n\\u003e\\u003e\\u003e dataset[0][\\\"audio\\\"][\\\"array\\\"].shape\\n(173398,)\\n\\n\\u003e\\u003e\\u003e dataset[1][\\\"audio\\\"][\\\"array\\\"].shape\\n(1064...\"],[\"\\u003cTip\\u003e\\n\\nImage preprocessing often follows some form of image augmentation. Both image preprocessing a...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoImageProcessor\\n\\n\\u003e\\u003e\\u003e image_processor = AutoImageProcessor.from...\"],[\"\\u003e\\u003e\\u003e _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])\\n```\\n\\n2. T...\"],[\"```py\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e import matplotlib.pyplot as plt\\n\\n\\u003e\\u003e\\u003e img = dataset[0][\\\"pixel_values...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e lj_speech = load_dataset(\\\"lj_speech\\\", split=\\\"train\\\"...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"While there is no exact recipe for creating prompts to match all cases, researchers have worked out ...\"],[\"When using a pipeline to generate text with an LLM, it's important to know what type of LLM you are ...\"],[\"### NLP tasks \\n\\nFirst, let's set up the environment: \\n\\n```bash\\npip install -q transformers accelerat...\"],[\"\\u003e\\u003e\\u003e sequences = pipe(\\n...     prompt,\\n...     max_new_tokens=10,\\n... )\\n\\n\\u003e\\u003e\\u003e for seq in sequences:\\n.....\"],[\"As you can see, the model correctly identified two named entities from the given text.\\n\\n#### Transla...\"],[\"```python\\n\\u003e\\u003e\\u003e torch.manual_seed(3) # doctest: +IGNORE_RESULT\\n\\u003e\\u003e\\u003e prompt = \\\"\\\"\\\"Permaculture is a desig...\"],[\"```python\\n\\u003e\\u003e\\u003e torch.manual_seed(4) # doctest: +IGNORE_RESULT\\n\\u003e\\u003e\\u003e prompt = \\\"\\\"\\\"Answer the question usi...\"],[\"Correct! Let's increase the complexity a little and see if we can still get away with a basic prompt...\"],[\"## Best practices of LLM prompting\\n\\nIn this section of the guide we have compiled a list of best pra...\"],[\"In few-shot prompting, we provide examples in the prompt giving the model more context to improve th...\"],[\"### Chain-of-thought\\n\\nChain-of-thought (CoT) prompting is a technique that nudges a model to produce...\"],[\"In all of the above examples, you will need to make sure that you either already have or can easily ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Several TensorFlow methods in 🤗 Transformers have been rewritten to be XLA-compatible, including tex...\"],[\"```py\\nmy_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)\\n```\\n\\n## Running a TF text generatio...\"],[\"You might notice that the generation time is not fast. Successive calls of `xla_generate()` (or any ...\"],[\"xla_generate = tf.function(model.generate, jit_compile=True)\\n\\nfor input_string in [\\\"TensorFlow is\\\", ...\"],[\"We didn’t cover all the text generation options 🤗 Transformers provides in this document. We encoura...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"| Task | Example datasets | Trainer support | 🤗 Accelerate | 🤗 Datasets | Colab\\n|---|---|:---:|:---:...\"],[\"| [**`text-generation`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorch\\u002ftex...\"],[\"| [**`audio-classification`**](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002ftree\\u002fmain\\u002fexamples\\u002fpytorc...\"],[\"## Running quick tests\\n\\nMost examples are equipped with a mechanism to truncate the number of datase...\"],[\"### Upload the trained\\u002ffine-tuned model to the Hub\\n\\nAll the example scripts support automatic upload...\"],[\"As an example, here is how you would fine-tune the BERT large model (with whole word masking) on the...\"],[\"```bash\\npython xla_spawn.py --num_cores num_tpu_you_have \\\\\\n    path_to_script.py \\\\\\n\\t--all_arguments_...\"],[\"### Weights & Biases\\n\\nTo use Weights & Biases, install the wandb package with:\\n\\n```bash\\npip install ...\"],[\"`conda`:\\n\\n```bash\\nconda install -c conda-forge neptune\\n```\\n\\nNext, in your model training script, imp...\"],[\"**Note:** Although you can pass your **Neptune API token** and **project name** as arguments when cr...\"],[\"Additional configuration options are available through generic [clearml environment variables](https...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Each PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU side and can supply up t...\"],[\"The report includes this legend:\\n\\n```\\n  X    = Self\\n  SYS  = Connection traversing PCIe as well as t...\"],[\"\\u003e Third-Generation NVLink®\\n\\u003e GA102 GPUs utilize NVIDIA’s third-generation NVLink interface, which in...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Modern approaches typically formulate semantic segme...\"],[\"This model was contributed by [francesco](https:\\u002f\\u002fhuggingface.co\\u002ffrancesco). The original code can b...\"],[\"## MaskFormer specific outputs\\n\\n[[autodoc]] models.maskformer.modeling_maskformer.MaskFormerModelOut...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While the Transformer architecture has become the de...\"],[\"## ViTHybridConfig\\n\\n[[autodoc]] ViTHybridConfig\\n\\n## ViTHybridImageProcessor\\n\\n[[autodoc]] ViTHybridIm...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised pre-training techniques have achieve...\"],[\"## Usage tips\\n\\n- In terms of data processing, LayoutLMv3 is identical to its predecessor [LayoutLMv2...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- [`LayoutLMv2ForSequenceClassification`] is supporte...\"],[\"**Document question answering**\\n- [Document question answering task guide](..\\u002ftasks\\u002fdocument_questio...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale autoregressive language models such as G...\"],[\"## Resources\\n\\n- [Causal language modeling task guide](..\\u002ftasks\\u002flanguage_modeling)\\n\\n## XGLMConfig\\n\\n[[...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was added by [Juarez Bochi](https:\\u002f\\u002fhuggingface.co\\u002fjbochi). The original checkpoints can ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"def preprocess(self, inputs, maybe_arg=2):\\n        model_input = Tensor(inputs[\\\"input_ids\\\"])\\n       ...\"],[\"A classic example would be a `top_k` argument in the post processing in classification tasks.\\n\\n```py...\"],[\"You can specify a default model if you want, in which case it should come with a specific revision (...\"],[\"The implementation is framework agnostic, and will work for PyTorch and TensorFlow models. If we hav...\"],[\"```py\\nfrom transformers import pipeline\\n\\nclassifier = pipeline(model=\\\"{your_username}\\u002ftest-dynamic-p...\"],[\"You also *need* to implement 2 (ideally 4) tests.\\n\\n- `test_small_model_pt` : Define 1 small model fo...\"],[\"Training a masked language model end-to-end from scratch on TPUs\\n\\nIn this example, we're going to de...\"],[\"\\u003e 💡 **Note**: You don't need a TPU-enabled hardware for tokenizer training and TFRecord shard prepar...\"],[\"**Notes**:\\n\\n* While running the above script, you need to specify the `split` accordingly. The examp...\"],[\"model_id = \\\"tf-tpu\\u002froberta-base-epochs-500-no-wd\\\"\\nunmasker = pipeline(\\\"fill-mask\\\", model=model_id, f...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom datasets import load_dataset\\n\\nds = load_dataset(\\\"lambdalabs\\u002fpokemon-blip-captions\\\")\\nd...\"],[\"```python\\nfrom transformers import AutoProcessor\\n\\ncheckpoint = \\\"microsoft\\u002fgit-base\\\"\\nprocessor = Auto...\"],[\"## Train!\\n\\nNow, you are ready to start fine-tuning the model. You will use the 🤗 [`Trainer`] for thi...\"],[\"```python\\ndevice = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n\\ninputs = processor(images=image, ...\"],[\"!--Copyright 2022 The HuggingFace Team and Microsoft. All rights reserved.\\n\\nLicensed under the MIT L...\"],[\"The abstract from the paper is the following:\\n\\n*The Transformer architecture has become a dominant c...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Can Transformer perform 2D object- and region-level ...\"],[\"\\u003cTip\\u003e\\n\\nUse [`YolosImageProcessor`] for preparing images (and optional targets) for the model. Contra...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Masked language modeling (MLM) pretraining methods s...\"],[\"This model was contributed by [lysandre](https:\\u002f\\u002fhuggingface.co\\u002flysandre). The original code can be ...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"## TFElectraForTokenClassification\\n\\n[[autodoc]] TFElectraForTokenClassification\\n    - call\\n\\n## TFEle...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we present a conceptually simple and e...\"],[\"To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-ove...\"],[\"## AltCLIPModel\\n\\n[[autodoc]] AltCLIPModel\\n    - forward\\n    - get_text_features\\n    - get_image_feat...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While existing large vision-language multimodal mode...\"],[\"For multiple turns conversation:\\n\\n```bash\\nA chat between a curious human and an artificial intellige...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\npip install -e \\\".[quality]\\\"\\n```\\n\\nOnce the installation is done, you can use the CLI command ...\"],[\"Once the command has finished, you should have a total of 7 new files spread across the repository:\\n...\"],[\"```shell script\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\ncd transformers\\npip install -e...\"],[\"Next will be the name of the config class to use for this model:\\n\\n```\\nWhat will be the name of the c...\"],[\"If you answer yes, the new model will have files for all the frameworks implemented by the model you...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg alt=\\\"\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fwarmu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce Kosmos-2, a Multimodal Large Language M...\"],[\"\\u003e\\u003e\\u003e model = Kosmos2ForConditionalGeneration.from_pretrained(\\\"microsoft\\u002fkosmos-2-patch14-224\\\")\\n\\u003e\\u003e\\u003e pr...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- information extraction from scanned documents: the [FUNSD](https:\\u002f\\u002fguillaumejaume.github.io\\u002fFUNSD\\u002f...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-training of text and layout has proved effective...\"],[\"- The main difference between LayoutLMv1 and LayoutLMv2 is that the latter incorporates visual embed...\"],[\"wrapper](https:\\u002f\\u002fpypi.org\\u002fproject\\u002fpytesseract\\u002f) available). Each bounding box should be in (x0, y0, ...\"],[\"```python\\ndef normalize_bbox(bbox, width, height):\\n    return [\\n        int(1000 * (bbox[0] \\u002f width)...\"],[\"- Internally, [`~transformers.LayoutLMv2Model`] will send the `image` input through its visual backb...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A notebook on how to [finetune LayoutLMv2 for text-...\"],[\"## Usage: LayoutLMv2Processor\\n\\nThe easiest way to prepare data for the model is to use [`LayoutLMv2P...\"],[\"In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note t...\"],[\"```python\\nfrom transformers import LayoutLMv2Processor\\nfrom PIL import Image\\n\\nprocessor = LayoutLMv2...\"],[\"**Use case 4: visual question answering (inference), apply_ocr=True**\\n\\nFor visual question answering...\"],[\"## LayoutLMv2FeatureExtractor\\n\\n[[autodoc]] LayoutLMv2FeatureExtractor\\n    - __call__\\n\\n## LayoutLMv2I...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"**Note:** This script only works with models that have a fast tokenizer (backed by the 🤗 Tokenizers ...\"],[\"```bash\\npip install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002faccelerate\\n```\\n\\nthen\\n\\n```bash\\nexport TASK_NAM...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper:\\n\\n\\n*Inductive transfer learning, enabled by self-supervised learning, have...\"],[\"## BarthezTokenizer\\n\\n[[autodoc]] BarthezTokenizer\\n\\n## BarthezTokenizerFast\\n\\n[[autodoc]] BarthezToken...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In recent years, a series of Transformer-based model...\"],[\"\\u003e\\u003e\\u003e encoded_input = tokenizer.encode(\\\"Kto ma lepszą sztukę, ma lepszy rząd – to jasne.\\\", return_tens...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [anton-l](https:\\u002f\\u002fhuggingface.co\\u002fanton-l).\\n\\n## Usage tips\\n\\n- SEW is a ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- accessing all the hidden-states of BERT\\u002fGPT\\u002fGPT-2,\\n- accessing all the attention weights for each ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer-based models are unable to process long ...\"],[\"## Longformer Self Attention\\n\\nLongformer self attention employs self attention on both a \\\"local\\\" con...\"],[\"## Training\\n\\n[`LongformerForMaskedLM`] is trained the exact same way [`RobertaForMaskedLM`] is\\ntrain...\"],[\"[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput\\n\\n[[aut...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"```bash\\n$ tensorboard --logdir .\\n```\\n\\nor directly on the hub under *Training metrics*.\\n\\nTraining wit...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we present the Textless Vision-Languag...\"],[\"The original code can be found [here](https:\\u002f\\u002fgithub.com\\u002fzinengtang\\u002fTVLT). This model was contribute...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*As Transfer Learning from large-scale pre-trained mo...\"],[\"## Usage tips\\n\\n- DistilBERT doesn't have `token_type_ids`, you don't need to indicate which token be...\"],[\"- A blog post on [Getting Started with Sentiment Analysis using Python](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002f...\"],[\"- [`FlaxDistilBertForSequenceClassification`] is supported by this [example script](https:\\u002f\\u002fgithub.c...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`DistilBertForTokenClassification`] is supported ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`DistilBertForMaskedLM`] is supported by this [example scrip...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`DistilBertForQuestionAnswering`] is supported by t...\"],[\"⚗️ Optimization\\n\\n- A blog post on how to [quantize DistilBERT with 🤗 Optimum and Intel](https:\\u002f\\u002fhugg...\"],[\"```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, AutoModel\\n\\n\\u003e\\u003e\\u003e device = \\\"cuda...\"],[\"## FlaxDistilBertModel\\n\\n[[autodoc]] FlaxDistilBertModel\\n    - __call__\\n\\n## FlaxDistilBertForMaskedLM...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"In this guide, we will go over the effective techniques for efficient LLM deployment:\\n\\n1.  **Lower P...\"],[\"At the time of writing this guide, LLMs consist of at least a couple billion parameters. Each parame...\"],[\"As of writing this document, the largest GPU chip on the market is the A100 & H100 offering 80GB of ...\"],[\"By using `device_map=\\\"auto\\\"` the attention layers would be equally distributed over all available GP...\"],[\"```python\\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\\n```\\n\\n**Output**:\\n```bash\\n29.0260648...\"],[\"```python\\nfrom accelerate.utils import release_memory\\n# ...\\n\\nrelease_memory(model)\\n```\\n\\nNow what if ...\"],[\"$$ Y = X * \\\\text{dequantize}(W) $$\\n\\nfor every matrix multiplication. Dequantization and re-quantizat...\"],[\"We delete the models and flush the memory again.\\n```python\\ndel model\\ndel pipe\\n```\\n\\n```python\\nflush()...\"],[\"4-bit quantization allows the model to be run on GPUs such as RTX3090, V100, and T4 which are quite ...\"],[\"Let's take a closer look. The formula to compute the output \\\\\\\\( \\\\mathbf{O} \\\\\\\\) of a self-attention l...\"],[\"In a nutshell, Flash Attention breaks the  \\\\\\\\(\\\\mathbf{V} \\\\times \\\\text{Softmax}(\\\\mathbf{QK}^T\\\\\\\\)) com...\"],[\"Essentially, Flash Attention makes sure that all intermediate write and read operations can be done ...\"],[\"-----\\n\\nQuestion: Write a function that takes two lists and returns a list that has alternating eleme...\"],[\"Let's now run the model just like before *without Flash Attention* and measure the peak GPU memory r...\"],[\"```python\\nmodel.to_bettertransformer()\\n```\\n\\nNow we run the exact same code snippet as before and und...\"],[\"Note that *chat* not only requires the LLM to handle long text inputs, but it also necessitates that...\"],[\"For the LLM to understand sentence order, an additional *cue* is needed and is usually applied in th...\"],[\"Sinusoidal and learned position embeddings used to be the predominant methods to encode sentence ord...\"],[\"$$ \\\\mathbf{\\\\hat{q}}_i^T \\\\mathbf{\\\\hat{x}}_j = \\\\mathbf{{q}}_i^T \\\\mathbf{R}_{\\\\theta, i -j} \\\\mathbf{{x}}...\"],[\"*ALiBi* is used in multiple of today's most important LLMs, such as:\\n\\n-   [**MPT**](https:\\u002f\\u002fhuggingf...\"],[\"In conclusion, LLMs that are intended to be deployed in tasks that require handling large text input...\"],[\"As we can see every time we increase the text input tokens by the just sampled token.\\n\\nWith very few...\"],[\"print(\\\"shape of input_ids\\\", next_token_id.shape)\\n  print(\\\"length of key-value cache\\\", len(past_key_v...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nNote that, despite our advice to use key-value caches, your LLM output may be ...\"],[\"Two things should be noted here:\\n  1. Keeping all the context is crucial for LLMs deployed in chat s...\"],[\"def bytes_to_megabytes(bytes):\\n   return bytes \\u002f 1024 \\u002f 1024\\n\\nAnswer: The function takes a number of...\"],[\"#### 3.2.2 Multi-Query-Attention (MQA)\\n\\n[Multi-Query-Attention](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1911.02150) wa...\"],[\"The important part to understand here is that reducing the number of key-value attention heads to 1 ...\"],[\"GQA was only recently proposed which is why there is less adoption at the time of writing this noteb...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer-based models are unable to process long ...\"],[\"## Usage tips\\n\\n- [`LEDForConditionalGeneration`] is an extension of\\n  [`BartForConditionalGeneration...\"],[\"## LEDConfig\\n\\n[[autodoc]] LEDConfig\\n\\n## LEDTokenizer\\n\\n[[autodoc]] LEDTokenizer\\n    - build_inputs_wi...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper states the following:\\n\\n*Visual language such as charts and plots is ubiqui...\"],[\"- `google\\u002fdeplot`: DePlot fine-tuned on ChartQA dataset \\n\\n\\n```python\\nfrom transformers import AutoPr...\"],[\"*NOTE**: This example is outdated and is not longer actively maintained. Please \\nfollow the new inst...\"],[\"```python\\nOrthography(\\n    do_lower_case=True,\\n    # break compounds like \\\"quarter-century-old\\\" and ...\"],[\"First, let's understand how this dataset represents Arabic text; it uses a format called\\n[Buckwalter...\"],[\"`--target_feature_extractor_sampling_rate` resamples audio to target feature extractor's sampling ra...\"],[\"ZeRO-3:\\n\\n```\\nPYTHONPATH=..\\u002f..\\u002f..\\u002fsrc deepspeed --num_gpus 2 \\\\\\nrun_asr.py \\\\\\n--output_dir=output_dir -...\"],[\"### Forced Alignment\\n\\nCharacter level forced alignment for audio and text pairs with wav2vec2 models...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Multilingual pre-trained models are known to suffer ...\"],[\"```python\\nfrom transformers import XmodModel\\n\\nmodel = XmodModel.from_pretrained(\\\"facebook\\u002fxmod-base\\\"...\"],[\"## XmodForMultipleChoice\\n\\n[[autodoc]] XmodForMultipleChoice\\n    - forward\\n\\n## XmodForTokenClassifica...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom transformers.utils import logging\\n\\nlogging.set_verbosity_info()\\nlogger = logging.get_...\"],[\"We use both in the `transformers` library. We leverage and adapt `logging`'s `captureWarning` method...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"By default, `TrainingArguments.report_to` is set to `\\\"all\\\"`, so a [`Trainer`] will use the following...\"],[\"## Available Callbacks\\n\\nHere is the list of the available [`TrainerCallback`] in the library:\\n\\n[[aut...\"],[\"ow to add BigBird to 🤗 Transformers?\\n=====================================\\n\\nMentor: [Patrick](https:...\"],[\"A good first starting point to better understand the library is to read\\nthe [documentation of our ph...\"],[\"Let's take a look:\\n\\n![image](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolv...\"],[\"```python\\n# assuming that `brand_new_bert` belongs to the organization `brandy`\\nmodel = BrandNewBert...\"],[\"From experience, we can tell you that the most important things to keep\\nin mind when adding a model ...\"],[\"6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfu...\"],[\"-   What type of model is *BigBird*? BERT-like encoder-only\\n    model? GPT2-like decoder-only model?...\"],[\"Alright, now you should be ready to take a closer look into the actual code of BigBird.\\nYou should h...\"],[\"3.  Set up a development environment, for instance by running the\\n    following command:\\n\\n    ```bas...\"],[\"-   Where to find the pretrained weights?\\n-   How to load the pretrained weights into the correspond...\"],[\"In general, there are two possible debugging environments for running\\nthe original model\\n\\n-   [Jupyt...\"],[\"```\\n\\nNext, regarding the debugging strategy, there are generally a few from\\nwhich to choose from:\\n\\n-...\"],[\"[Lysandre's](https:\\u002f\\u002fgist.github.com\\u002fLysandreJik\\u002fdb4c948f6b4483960de5cbac598ad4ed)\\nintegration check...\"],[\"We expect that every model added to 🤗 Transformers passes a couple of\\nintegration tests, meaning tha...\"],[\"-   Find the best way of debugging intermediate results. Is the original\\n    repository written in P...\"],[\"where in the forward call the string input is changed to input ids\\n    and start from this point. Th...\"],[\"#### (Important) More details on how to create a debugging environment for BigBird \\n\\n- BigBird has m...\"],[\"Otherwise, let's start generating a new model with the amazing\\nCookiecutter!\\n\\n**Use the Cookiecutter...\"],[\"6.  Change the PR into a draft by clicking on \\\"Convert to draft\\\" on the\\n    right of the GitHub pull...\"],[\"Now you can finally start coding :). The generated code in\\n`src\\u002ftransformers\\u002fmodels\\u002fbig_bird\\u002fmodelin...\"],[\"The above command will create a model according to the default\\nparameters as defined in `BigBirdConf...\"],[\"You can copy paste the conversion function into `modeling_big_bird.py` and then adapt it \\nto your ne...\"],[\"```python\\nprint(model.dense.weight.data)\\n```\\n\\nto see that the weights were randomly initialized\\n\\n```...\"],[\"```python\\nassert (\\n     model_pointer.weight.shape == pretrained_weight.shape\\n), f\\\"Pointer shape of ...\"],[\"```python\\nmodel.save_pretrained(\\\"\\u002fpath\\u002fto\\u002fconverted\\u002fcheckpoint\\u002ffolder\\\")\\n```\\n\\n**7. Implement the forw...\"],[\"The final part to make sure the 🤗 Transformers implementation works\\ncorrectly is to ensure that the ...\"],[\"The best way to fix the problem is usually to look at the forward pass\\nof the original implementatio...\"],[\"```python\\npytest tests\\u002ftest_modeling_big_bird.py\\n```\\n\\nHaving fixed all common tests, it is now cruci...\"],[\"**9. Implement the tokenizer**\\n\\nNext, we should add the tokenizer of *BigBird*. Usually, the\\ntokeniz...\"],[\"```python\\nfrom transformers import BertGenerationTokenizer\\ninput_str = \\\"This is a long example input...\"],[\"**11. Add Docstring**\\n\\nNow, all the necessary functionality for *BigBird* is added -\\nyou're almost d...\"],[\"You have now finished the coding part, congratulation! 🎉 You are\\nAwesome! 😎\\n\\n**12. Upload the models...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The BigCode project is an open-scientific collaborat...\"],[\"The model is an optimized [GPT2 model](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2) with...\"],[\"To load and run a model using Flash Attention 2, refer to the snippet below:\\n\\n```python\\n\\u003e\\u003e\\u003e import t...\"],[\"Examples\\nIn this folder we showcase some examples to use code models for downstream tasks.\\n\\n## Compl...\"],[\"```python\\naccelerate launch scripts\\u002fcodeparrot_training.py \\\\\\n    --model_ckpt codeparrot\\u002fcodeparrot-...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\nIn several question answering benchmarks, pretrained ...\"],[\"## Usage tips\\n\\n- Splinter was trained to predict answers spans conditioned on a special [QUESTION] t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"As shown on the following figure, Jukebox is made of 3 `priors` which are decoder only models. They ...\"],[\"This model was contributed by [Arthur Zucker](https:\\u002f\\u002fhuggingface.co\\u002fArthurZ).\\nThe original code can...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self atten...\"],[\"## Usage tips\\n\\n- One can use the [`AutoImageProcessor`] API to prepare images for the model.\\n- NAT c...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large multimodal models (LMM) have recently shown en...\"],[\"For multiple turns conversation:\\n\\n```bash\\n\\\"USER: \\u003cimage\\u003e\\\\n\\u003cprompt1\\u003eASSISTANT: \\u003canswer1\\u003eUSER: \\u003cprompt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*General-purpose language models that can solve vario...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"AutoConfig.register(\\\"new-model\\\", NewModelConfig)\\nAutoModel.register(NewModelConfig, NewModel)\\n```\\n\\nY...\"],[\"[[autodoc]] AutoModelForMaskedLM\\n\\n### TFAutoModelForMaskedLM\\n\\n[[autodoc]] TFAutoModelForMaskedLM\\n\\n##...\"],[\"### AutoModelForTextEncoding\\n\\n[[autodoc]] AutoModelForTextEncoding\\n\\n### TFAutoModelForTextEncoding\\n\\n...\"],[\"[[autodoc]] AutoModelForAudioClassification\\n\\n### AutoModelForAudioFrameClassification\\n\\n[[autodoc]] T...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pretrained language models are now ubiquitous in Nat...\"],[\"## CamembertTokenizerFast\\n\\n[[autodoc]] CamembertTokenizerFast\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## Camembert...\"],[\"# Token classification\\n\\nBased on the scripts [`run_ner.py`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransform...\"],[\"```bash\\nexport MAX_LENGTH=128\\nexport BERT_MODEL=bert-base-multilingual-cased\\n```\\n\\nRun the pre-proces...\"],[\"It must be saved with a `.json` extension and can be used by running `python3 run_ner.py config.json...\"],[\"#### Evaluation\\n\\nEvaluation on development dataset outputs the following for our example:\\n```bash\\n  ...\"],[\"### Emerging and Rare Entities task: WNUT’17 (English NER) dataset\\n\\nDescription of the WNUT’17 task ...\"],[\"Here we use the English BERT large model for fine-tuning.\\nThe `preprocess.py` scripts splits longer ...\"],[\"#### Evaluation\\n\\nEvaluation on development dataset outputs the following:\\n\\n```bash\\n05\\u002f29\\u002f2020 23:33:...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"While the dataset contains a lot of useful information, like `lang_id` and `english_transcription`, ...\"],[\"```py\\n\\u003e\\u003e\\u003e minds = minds.cast_column(\\\"audio\\\", Audio(sampling_rate=16_000))\\n\\u003e\\u003e\\u003e minds[\\\"train\\\"][0]\\n{'au...\"],[\"```py\\n\\u003e\\u003e\\u003e encoded_minds = minds.map(preprocess_function, remove_columns=\\\"audio\\\", batched=True)\\n\\u003e\\u003e\\u003e e...\"],[\"At this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingArgume...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Inference\\n\\nGreat, now that you've finetuned a model, you can use it for inference!\\n\\nLoad ...\"],[\"\\u003e\\u003e\\u003e model = AutoModelForAudioClassification.from_pretrained(\\\"stevhliu\\u002fmy_awesome_minds_model\\\")\\n\\u003e\\u003e\\u003e w...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage example\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import AutoModel, AutoTokenizer\\n\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The process of selecting output tokens to generate text is known as decoding, and you can customize ...\"],[\"Printing out the `model.generation_config` reveals only the values that are different from the defau...\"],[\"Even if the default decoding strategy mostly works for your task, you can still tweak a few things. ...\"],[\"## Save a custom decoding strategy with your model\\n\\nIf you would like to share your fine-tuned model...\"],[\"\\u003e\\u003e\\u003e # Tip: add `push_to_hub=True` to push to the Hub\\n\\u003e\\u003e\\u003e translation_generation_config.save_pretrain...\"],[\"## Decoding strategies\\n\\nCertain combinations of the `generate()` parameters, and ultimately `generat...\"],[\"\\u003e\\u003e\\u003e prompt = \\\"Hugging Face Company is\\\"\\n\\u003e\\u003e\\u003e inputs = tokenizer(prompt, return_tensors=\\\"pt\\\")\\n\\n\\u003e\\u003e\\u003e outp...\"],[\"### Beam-search decoding\\n\\nUnlike greedy search, beam-search decoding keeps several hypotheses at eac...\"],[\"\\u003e\\u003e\\u003e outputs = model.generate(**inputs, num_beams=5, do_sample=True)\\n\\u003e\\u003e\\u003e tokenizer.decode(outputs[0],...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\n\\n\\u003e\\u003e\\u003e checkpoint = \\\"googl...\"],[\"This guide illustrates the main parameters that enable various decoding strategies. More advanced pa...\"],[\"When using assisted decoding with sampling methods, you can use the `temperature` argument to contro...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Simple call on one item:\\n\\n```python\\n\\u003e\\u003e\\u003e pipe = pipeline(\\\"text-classification\\\")\\n\\u003e\\u003e\\u003e pipe(\\\"This restau...\"],[\"For ease of use, a generator is also possible:\\n\\n\\n```python\\nfrom transformers import pipeline\\n\\npipe =...\"],[\"class MyDataset(Dataset):\\n    def __len__(self):\\n        return 5000\\n\\n    def __getitem__(self, i):\\n...\"],[\"```\\n------------------------------\\nStreaming no batching\\n100%|██████████████████████████████████████...\"],[\"- If you have no clue about the size of the sequence_length (\\\"natural\\\" data), by default don't batch...\"],[\"Don't hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to u...\"],[\"[[autodoc]] VideoClassificationPipeline\\n    - __call__\\n    - all\\n\\n### ZeroShotImageClassificationPip...\"],[\"[[autodoc]] ImageToTextPipeline\\n    - __call__\\n    - all\\n\\n### MaskGenerationPipeline\\n\\n[[autodoc]] Ma...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Image segmentation is usually addressed by training ...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We introduce a new language representation model cal...\"],[\"* a special mask token with probability 0.8\\n    * a random token different from the one masked with ...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog post on [BERT Text Classification in a diffe...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- A blog post on how to use [Hugging Face Transforme...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`BertForMaskedLM`] is supported by this [example script](htt...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`BertForQuestionAnswering`] is supported by this [e...\"],[\"⚙️ **Pretraining**\\n- A blog post on [Pre-Training BERT with Hugging Face Transformers and Habana Gau...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## BertModel\\n\\n[[autodoc]] BertModel\\n    - forward\\n\\n## BertForPreTraining\\n\\n[...\"],[\"## FlaxBertForSequenceClassification\\n\\n[[autodoc]] FlaxBertForSequenceClassification\\n    - __call__\\n\\n...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"- model instantiation with the `torchscript` flag\\n- a forward pass with dummy inputs\\n\\nThese necessit...\"],[\"Be careful of the total number of operations done on each input and follow the\\nperformance closely w...\"],[\"# Creating the trace\\ntraced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\\ntorch....\"],[\"### Implications\\n\\nTransformers models based on the [BERT (Bidirectional Encoder Representations from...\"],[\"You only need to modify the following line:\\n\\n```diff\\n- torch.jit.trace(model, [tokens_tensor, segmen...\"]],\"hovertemplate\":\"source=transformers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"transformers, circle\",\"marker\":{\"color\":\"#FFA15A\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"transformers, circle\",\"showlegend\":true,\"x\":[11.625388,11.573503,11.496497,11.751707,11.404818,12.415136,11.958741,11.960131,0.31040406,-2.8602335,11.563521,11.67556,3.0878594,3.762811,3.112987,3.4722114,3.4012036,3.2195234,0.6501619,-0.4450552,-0.083740056,-0.6277618,0.15023735,-0.4589025,0.14505447,-9.944975,-0.21896078,-9.927435,8.635925,8.570957,8.669306,4.6564274,13.074726,8.631736,6.6021066,13.404358,12.639527,11.620516,11.382352,-14.862023,-21.73235,-18.403643,-18.828655,-18.76427,-18.80662,-18.503464,1.0016958,0.58378583,12.493056,-4.0411553,-1.3698746,6.8150644,7.453824,8.01293,7.871994,8.05989,8.128713,7.9273853,8.170533,8.343961,7.8849354,17.562422,17.562563,7.3773236,7.958169,-7.8825874,7.99589,8.070436,7.733855,-2.948997,7.801731,-0.738545,7.939585,-2.8649344,-2.876193,8.391941,8.592119,8.114711,8.177399,8.064548,8.157189,7.6528,8.039679,1.1513884,-12.612991,-13.271724,-12.48435,-12.872091,-12.348697,-12.3526535,-12.615553,-12.668929,-12.8274765,-11.118853,2.771153,3.3731687,0.15186106,0.36777198,-0.8236611,-7.55693,0.550853,0.25460115,-6.791281,-6.031408,-2.3831086,-3.9847412,-3.5991004,-2.6899827,-2.990977,-1.2274979,-2.760029,-2.9138968,-1.2557311,-2.6847634,-1.5360376,-2.8436983,-1.5441049,-2.2581131,-3.1835334,-1.1854496,-3.9633565,-3.454403,-1.5602822,-1.0710084,-3.7019246,-1.1023713,-9.605655,-9.624407,-9.60069,-7.5452447,3.1585615,-2.7408545,-2.4399116,0.05595586,0.19050722,-0.11582739,-0.059856743,0.0151028065,-0.40898487,-1.9495022,3.6571581,3.5051625,3.6332574,3.6409888,3.5099578,3.3377817,3.637066,3.595331,-2.3170502,-1.6561534,13.32356,13.325695,-3.6501582,-3.6713905,-3.6017935,-3.6668391,-3.5675423,-3.3404605,7.616779,1.3936826,16.368979,19.462648,18.659954,17.14815,19.398685,10.37787,16.247503,4.2412634,3.4970484,13.557387,-14.889391,12.307081,5.5135355,-18.74284,-18.843233,-18.489817,-14.46209,-0.98608655,-18.77711,-18.833065,-14.924668,-13.604817,-17.414951,-18.369987,-18.691887,-6.8041134,-4.743563,-1.3143098,-4.6394186,13.129018,13.372337,-4.098578,-4.9856954,0.08326449,3.7377515,2.2170885,0.7469384,0.9280386,6.011387,0.09994816,1.6214055,0.7627456,-3.4093177,0.26473877,-2.6421442,-2.0464168,-1.9789187,-1.356344,-2.6171155,-2.955099,-1.9967042,-0.085906416,1.7797208,-3.4031677,-3.9833527,-8.0088,-7.661324,-7.5965996,-5.647291,-7.873521,-8.333404,1.9144284,-8.032166,-8.287978,-8.376946,-8.315089,-8.0662365,-8.33804,7.659899,-7.9464307,-7.6177983,3.2741961,3.2920945,1.5593128,3.02923,2.1339695,1.9231533,1.5524229,0.89598626,-1.7965832,-2.1934083,-3.3793468,-2.016783,-6.074728,-6.164053,-5.767406,-4.968459,-3.5355158,-6.0042996,-0.9972402,-5.850487,-2.3123927,-3.9091072,-5.536086,-0.599474,-3.9843237,-5.385434,-5.5886183,-6.364887,-4.8374195,2.5202227,1.362567,-6.1384225,-4.1273727,-0.22788972,5.999986,5.837847,5.944614,5.8285074,4.743567,5.8444242,11.855271,11.581638,11.693149,11.61069,12.016155,11.877237,11.100717,11.29977,11.2550335,-4.227132,-3.939251,-4.675543,-4.8917265,-3.6949303,-5.3592114,-4.178327,-4.317067,15.979901,16.546019,16.860268,9.058974,15.988067,16.482536,16.021233,16.599169,16.347094,15.974642,16.24258,8.926886,16.812603,16.287683,15.930911,8.214367,15.548999,16.058214,16.322971,16.263088,16.446384,16.155893,16.368391,16.30811,14.201642,13.746243,8.024778,15.816714,16.605679,16.168793,15.642895,13.222161,12.899272,16.258736,16.894629,16.901611,16.478407,13.806985,16.497305,16.41576,6.503614,16.766537,16.900436,16.778137,16.133436,16.642086,16.585995,15.957434,14.312998,14.75378,16.237959,11.717666,12.431969,13.161785,15.953097,16.808424,16.638733,13.022452,13.495293,14.576968,13.059928,12.74814,12.906644,15.0864315,14.712208,8.754426,15.393135,16.367586,16.188005,15.738758,15.765504,16.4176,16.450357,16.133612,19.38498,17.629866,17.567566,-7.898289,-7.31524,-7.5190306,-3.8190572,-17.416004,-18.278797,-18.469877,5.4922276,-7.303971,-5.4646063,-5.6413336,-6.209578,-6.036928,-5.952775,7.5347,12.7364855,-2.3378983,-2.8082042,-2.70029,3.5903883,0.6706177,-1.3084488,0.31207034,-8.838991,-7.081196,4.9487753,4.4506726,4.007527,4.300583,4.1509557,3.8707004,3.3455093,19.175186,16.329172,16.43757,16.181957,2.5555577,0.94337606,7.5005703,17.926058,19.494316,19.937141,-13.45871,-13.666982,-13.509442,-13.6147175,-13.721364,-13.654292,-0.5455893,-1.3776304,-1.1513441,-1.1264135,-0.8947557,-0.085095845,-0.5917875,14.172032,14.18829,14.346313,13.860029,13.732109,13.649055,9.4404545,13.933582,13.588687,12.607361,14.11264,-4.8566356,3.0006144,-0.2238164,-1.334397,-0.23148435,0.40994635,0.19111529,-0.3121976,-9.302745,-3.165007,-3.1598883,-5.795847,-2.6433537,-3.206578,-2.836534,-3.142528,-2.67397,-1.1865189,-1.1415666,-1.451056,-1.5033218,-1.3913236,-0.6285678,-1.9665197,-1.5864478,-1.7617491,-4.446462,8.685725,-3.4992728,-3.7850246,-4.0735316,-1.1274283,-3.161834,0.9883319,0.7376947,14.196802,2.0210254,3.203636,3.1405466,3.8212936,3.031134,3.4884613,8.408378,2.4395504,3.5025036,7.088577,3.1153889,3.4742544,3.3533535,-4.2294097,-0.24052924,-0.18707994,0.112840176,0.3271684,-0.08266544,-0.07203756,-1.2710104,-3.5689292,-3.5470223,-3.3984334,-6.7975254,-0.092357025,-0.84804255,-0.8724291,-0.27014887,-0.02979914,1.2217362,-6.205944,-6.1743274,-6.2629147,-6.324952,-6.1213417,-5.662991,-6.165444,7.6921577,7.711457,-6.329656,-3.4455264,-2.696699,-2.2737112,-7.917367,-7.439379,-10.532984,-10.510319,13.352906,18.940838,16.894371,18.1214,16.617973,17.843695,18.854097,18.965017,17.913618,16.409122,18.204782,16.407976,17.946758,16.85557,12.31949,12.148766,6.8626075,-6.0725117,-6.326296,-1.7287945,-4.8704243,0.7788892,-2.7952302,-5.723297,-5.549954,-5.728037,12.068484,-0.73259753,12.3708315,-0.4250621,0.40774685,-0.8680057,-0.23702076,0.3536465,-0.28895685,7.7690754,7.797045,-5.5228324,7.308284,13.337598,-7.49017,-8.769475,-2.3738356,-4.6517835,-3.5905123,-4.7600107,-3.7517161,-2.8241415,-2.3316734,-2.4007685,-1.927928,-0.42720667,-3.4535282,7.207091,7.1086593,6.9422393,7.0174932,7.436668,-14.840051,-21.732449,-17.41578,-18.476173,-18.560331,-18.689835,-18.79928,-18.676176,5.9513226,13.176685,-4.801434,-3.3686233,17.977879,18.194382,18.526775,17.356379,-4.734146,-0.79624444,-1.9314674,-0.6378764,-1.5934501,-9.876176,-9.899385,-9.909363,-4.15063,-6.553266,9.571732,-7.044402,-0.12333984,13.169531,4.382327,-1.3851535,-1.7114443,-1.2321702,-4.1251464,-5.544317,-2.7669828,-4.6675944,3.7133775,3.697559,-1.5824206,-2.0644472,-6.5975323,-6.3274775,-6.5654964,11.540828,8.93448,9.085995,-4.9007945,-4.51035,-4.48993,9.083279,17.163158,-11.3184595,13.025492,1.875252,2.2543654,13.248712,-4.6551623,-4.063845,-4.1464562,7.589033,-2.358366,-4.507541,19.527695,19.786272,18.854992,17.9523,-18.391403,-18.757727,-18.825745,-18.483921,-1.5322294,-2.071434,-5.51034,-5.683641,-1.184866,-5.19472,-2.3221185,-7.374227,-6.8651013,-5.9937673,-6.38013,-7.756165,-7.5728445,-7.6150675,-4.1890297,-4.081676,-4.1051917,-3.4575877,-4.12782,-2.9520857,-1.1711264,-3.2367473,0.3584868,0.28931302,0.5609773,-0.49114174,-14.451123,-13.604294,-17.415285,-17.886536,-0.36015078,-1.0838475,0.52091455,-0.5976212,-14.20211,-1.04786,-21.733051,-11.33426,-18.411987,-18.73604,-18.622763,-18.770893,1.7445824,8.7159815,8.232066,8.263931,8.117116,1.1271147,1.1353977,7.3894486,8.134603,-12.81292,-12.844812,-12.835153,-12.856629,-12.549337,-12.538722,-12.801889,-12.72511,-12.87622,-12.52492,-12.6618595,-12.831241,-12.392714,-12.233815,-12.603088,-12.637394,-9.389991,-5.141098,-1.3020554,-3.9648225,-6.258148,2.9960191,0.9811438,-0.13186747,-2.9080446,-6.8070555,-2.5190897,-10.380517,-9.807422,-1.1918571,-9.389828,-9.327896,-8.012212,-3.0151956,-3.6107373,-1.3573388,-2.8402188,-5.044601,-2.7106352,-3.2607472,1.2622186,7.571989,-2.3706284,-11.120639,-4.238585,-3.956722,-3.4688525,-2.311988,-2.0819173,-1.126823,-3.4774203,-9.368192,-9.2390375,-7.722963,-7.7055736,-8.238492,-9.165169,-7.9676104,-8.492201,-8.449368,-8.630062,-7.866335,-5.421069,2.804896,1.3704913,9.1482725,8.972169,9.02592,9.001959,8.861602,-9.153251,-9.139381,1.6114333,1.680542,3.175861,-1.2273016,-8.984162,-9.183319,-7.482979,-7.592545,-2.8719752,-2.1533675,-3.096479,-2.4758587,11.742475,17.300623,19.284664,-7.6194224,5.463496,8.048876,8.104404,-4.0706253,-3.6594753,-3.8594964,-6.442223,-6.3823028,-3.6390717,-3.359542,-4.9349265,-7.131189,-4.8173246,3.303181,4.6420174,7.8571243,3.7286136,9.323014,-1.0598501,-1.9550614,-4.311164,-1.1330583,-4.0750937,-3.8685813,-9.481069,-9.665848,6.6990256,6.876878,6.945415,13.190402,-3.3318777,-2.2746925,13.030046,-6.688797,-6.245537,-6.769602,-6.1423597,1.7648787,-3.6762397,-0.1930766,-0.1957691,-3.107382,-3.09869,-2.4305973,-1.5063971,-1.4336323,-3.0318463,-2.5857399,-4.1709256,-4.911679,-3.2042723,-2.5465794,-3.2166903,4.386796,3.525664,2.524993,6.854933,9.294827,9.296918,3.35461,-3.9062915,-4.1861315,-1.8634739,4.9437056,6.254769,-3.1349022,6.933074,7.0566497,-7.6263084,-8.413238,-9.541563,-9.802252,-9.450802,-1.7690213,-9.866915,-10.082009,-9.944214,-9.50209,-9.581781,-6.1557927,-5.7599154,-2.416734,-2.3960729,5.849322,1.4313424,2.5874825,1.6581888,-1.9629759,-2.5358953,-2.1576798,-2.972293,-6.6840897,-6.1481075,1.0837314,8.012065,-0.7383441,-5.910492,-0.93974924,0.3406414,0.4562694,-0.59446955,0.89772487,-0.09562409,0.13465673,-0.027991308,0.9790277,1.398506,-4.3792996,0.73822016,1.2188907,-5.4293427,-5.5365577,-6.4587007,-6.596954,-6.168481,12.996087,4.7001657,4.890059,-10.032307,1.4272369,0.27224594,0.24875657,0.7782551,0.37807533,-1.1913527,-1.3476957,-1.8950206,-1.3805395,-1.958124,-1.5830643,-0.31815624,0.16644175,-9.715836,13.479218,13.820146,13.559182,-4.298334,-4.003391,-5.5950704,-5.0918207,-5.708733,-5.203489,-5.21412,-0.7852059,-4.6139317,-4.5526147,-4.885524,-6.3981647,18.986702,18.857286,10.268602,13.513657,-2.8323858,0.5636293,0.06209478,13.150964,12.523427,6.0844526,13.269102,12.75561,13.060956,13.159351,-8.021051,-10.603564,-10.231108,-9.810349,-6.9141607,-7.014378,-6.902029,-7.3395185,-9.781651,-9.855064,8.494611,2.1233013,2.3822215,14.102318,14.170345,-9.4268875,-1.252473,0.08550028,11.990079,12.567053,10.463565,3.459983,3.2599418,3.1474729,-4.906491,-4.6922536,1.9839321,2.4327526,-4.3934345,-0.09209949,-1.0122151,-1.4947093,-2.2695572,-2.040292,-2.1703134,1.8854513,-3.3502212,-3.4288769,-3.9663517,-2.7802217,-1.7697104,-1.4376137,-1.0768657,-2.1934905,-0.74301773,0.21798816,-0.3680444,0.62709635,2.1736214,2.6992128,1.7005048,1.9285084,2.8009772,-1.1064346,-1.4988428,2.7973092,0.33626094,-2.7001247,-2.6124043,-2.639284,-6.240863,-6.7036495,-6.147057,-2.3834362,-2.3635767,3.9494715,4.3842053,3.6856658,4.5949063,11.623498,6.6023374,-6.2240458,-5.855624,-2.7547472,-2.0406387,-5.9025927,12.967743,-5.8681545,-5.442758,-5.455305,-3.544525,-3.517754,-5.0342503,-4.4526014,-4.9341807,-4.288192,-5.332129,-5.7025967,-3.445478,-5.3218904,-3.7320638,0.029919984,-3.7761064,0.407928,1.3422806,0.7288191,0.3696105,-0.07501606,0.22756794,-0.26646796,0.08229109,0.10298357,-0.525399,-1.250359,7.5101595,13.340528,-14.976074,-21.73271,-17.766455,-18.616745,11.667952,11.878591,11.474022,-3.1453712,5.431351,4.026021,8.315197,-2.2436879,0.43860877,-1.2126635,-3.5940478,-3.1125612,-4.316752,4.9927235,4.3736725,5.1538014,4.913597,4.962598,5.2587457,5.257218,5.2602663,-1.8466048,-1.1568422,-1.84879,9.100128,9.14603,0.28684834,-3.0080013,-3.717565,12.715726,12.563961,11.623487,0.25136203,0.115757555,0.14055237,0.7489234,11.523724,3.2291465,2.4767735,6.8833947,6.882714,-5.6496434,-5.6633706,-5.6347227,-5.168259,-4.85557,-5.051884,-5.1151204,-4.9237924,-4.98727,-5.2537622,3.236183,3.3147848,2.6631703,-3.405327,-3.3611593,-3.3241317,-3.3242688,19.579317,19.864868,18.435135,17.876148,-4.6282077,-0.13291068,-4.8109403,13.639191,-0.1969675,0.098968565,0.1405303,0.083863296,-4.7538047,-7.094653,-6.8238,-6.9488463,-4.5675225,-4.619147,-4.5177865,12.443228,12.579068,-4.178665,-5.929209,-14.590951,-13.60586,-17.416859,3.5981312,3.1350887,-3.9437964,-9.370556,-9.884038,-9.317222,8.280503,8.302807,8.352492,-12.258929,-12.309032,-12.297741,0.15725219,0.09301443,0.08055701,0.12130124,-3.356403,-3.8263357,-3.254274,-6.519701,-1.3414483,-1.8765876,-4.3128567,-3.675436,-1.4005564,-1.2429464,-3.0949836,6.950115,-14.825906,-21.732689,-18.065142,-2.8876271,-2.4478238,13.256036,-4.551352,0.44576702,0.4176471,-1.7690717,-2.5968428,-1.6744581,0.18082066,-1.4972103,-1.4048336,-0.47768626,-0.83901805,-1.474732,-0.67198634,0.24063295,0.008713834,-0.16728804,-0.5218322,0.20444854,-0.67551553,-3.666064,-6.7026844,-5.503819,-3.0481877,4.13112,-5.049721,-5.0401454,-0.68266237,-1.8253739,-1.7741162,-1.7199538,-1.9748454,-12.758061,-12.272156,-12.874812,-12.737985,-12.513049,-12.82591,-12.453318,-12.082159,-12.561314,-12.671566,-12.707079,-12.837885,-12.511951,-12.495667,7.257108,8.209228,8.110655,-11.141858,6.023301,9.27058,8.930917,-17.414202,-3.7087703,-1.6332649,-3.71114,-1.5910397,-6.9274755,-6.1849856,3.1078537,-2.3665771,-10.6959715,-6.505931,-7.0263643,-6.568671,-2.4711747,-2.9276688,-6.8767786,19.116526,19.661633,19.710087,19.680202,19.351383,16.885077,15.932173,17.981298,18.482029,19.543724,-3.3548381,-0.8326512,-1.1738695,-1.9254634,-4.9363494,-3.7938707,-3.0064301,-2.2109692,-3.1102993,-3.6486204,12.469449,-14.462717,-13.606081,-17.415327,-18.030159,4.5371313,-6.3210692,2.6582584,-3.604723,1.684697,4.136089,-1.2186776,-1.1947262,-1.2255677,0.68160766,-1.8775918,-0.5604302,-7.0270147,3.4028342,2.6146982,8.144274,7.9538307,8.164699,8.274141,-2.933867,8.461161,8.126073,8.060417,8.06731,1.2934685,7.6809554,7.8461447,-12.666219,-12.681936,-12.931079,-12.731445,-12.7403965,-12.4723425,-12.54794,-12.574555,-12.64322,-12.469246,-12.562001,-12.30286,-12.563645,-12.339096,-12.294339,-6.3457003,-2.3906221,4.8103113,4.377579,6.0240765,5.5399513,12.804648,12.313932,11.921067,-6.966194,1.2625092,1.8139248,-6.138356,-5.8937206,12.821065,12.497422,11.190406,12.649477,-1.7597572,11.452775,-2.969397,-2.5898833,-1.7907783,-1.9173099,-1.8008912,-1.9079685,-1.960623,-2.1561348,-2.1040492,-1.589176,-1.6896033,-1.3864053,-1.4548701,-1.781864,-1.90609,-1.1644104,-1.877807,-1.8173776,-2.0923169,-2.2163985,-3.0772402,-2.235129,-2.7596996,-2.3218977,-1.7723992,-1.7703071,-2.677325,-2.3195014,-2.3360472,-1.6626256,-1.6985269,-1.3902333,-1.7279669,-2.2981687,-3.0543299,-2.8608737,-2.2322369,-1.8712866,-1.7962443,9.044312,8.16209,8.395266,-6.297523,-7.2451673,-2.5465927,-2.4405172,-2.7871404,-3.1853962,-0.18254793,-0.18198422,-0.026666204,-0.15163685,-2.1411455,0.44605738,-0.21329083,0.1588156,0.032774895,-0.024156075,-5.0427194,-3.2216663,4.928524,4.9716616,9.116976,5.112212,5.062936,-0.018442491,-4.117161,-4.823626,0.86748034,-0.81312954,-0.5707569,-1.2422705,-0.7599026,0.13598178,3.6584704,0.06440994,-10.216666,-10.01739,3.2879977,-6.851252,13.210782,10.453553,10.687496,12.777273,12.857679,-0.59215575,-0.79871595,1.6898748,4.5596876,4.529429,-9.409021,4.10407,-6.2330866,8.170638,8.1259985,-3.5987973,-3.84257,-3.7714534,-3.6196558,-3.53879,-3.5386858,-3.5557497,-3.5305183,-3.5374942,-3.4737327,5.4989066,5.5283546,5.3075743,5.416598,8.755515,8.872664,8.432471,8.444125,8.823305,8.719572,9.001913,-6.6762524,-3.1766198,-1.9785539,-2.5887752,-7.329701,-7.4035983,-7.109337,-7.3890057,-7.2933774,-7.458921,-7.6401467,8.550664,8.870438,-4.753139,-3.6225824,12.189415,-14.684911,-13.6050625,-17.416826,-17.861303,3.703785,7.5087147,13.660559,6.1800265,-4.772649,3.1910546,2.9658453,2.0024412,3.6198297,3.4117668,3.3527877,3.326322,3.6112883,2.5805469,3.237778,2.8173466,2.9890592,3.0604973,3.7160037,3.3378034,3.2181628,3.266032,-5.924808,-5.8294687,-5.8090935,-0.20908464,0.8976468,-5.7487063,-6.0045795,-5.4012055,-0.37805215,0.17352608,0.1428418,-0.41528466,0.44328398,-0.53798014,0.4670555,-0.42607498,0.24549143,-7.128052,-9.015307,-2.6010041,-2.4119341,2.1776457,2.371388,2.3810663,-9.595535,-9.57569,-0.31062648,-8.807291,-0.21300836,-0.17169508,0.12848532,0.0842142,-0.9116575,0.029222053,5.7489004,-7.8395905,-7.170309,-8.240503,-2.4157832,-2.7369995,3.0549667,13.532393,-4.3396745,-4.6082964,-0.51923513,-4.637608,7.7093062,-2.0452218,-0.21762039,5.8307886,-4.077481,5.219017,-4.032348,-3.4576406,-3.677936,1.5285397,-2.175921,-0.9704199,-1.93709,-2.1962004,-2.5522997,-1.7720923,-3.1566377,-7.398612,-5.0123215,-4.638816,0.7970512,-4.4404087,-5.2348413,-7.962324,-5.24806,-4.6946554,-4.783563,19.520988,19.185587,4.0938725,4.3723574,-4.679799,-4.469578,-0.4294687,-5.282131,-4.203059,-2.8552449,-2.5026293,-1.9988947,-1.6963829,-1.7417849,-2.3347301,-2.5039225,-1.4839259,-1.9405717,-0.8089447,-1.7063146,-3.486085,-0.17297138,0.59956604,-0.37980714,-0.25479028,0.5226138,-0.8342654,-0.42046663,-2.4314866,-1.213482,-1.4033101,-0.9331713,-14.998196,18.103125,16.695396,13.137625,-4.2376566,-3.1474035,-2.1993165,-2.5412545,-0.9547435,-1.9293063,-2.386018,-2.2901504,1.2372105,-6.751899,-5.965297,-18.769138,-18.880064,-18.684652,-18.844847,-18.795622,-18.788403,-18.456999,-6.512121,-4.005533,-3.7694316,-3.9758766,-4.130809,3.4341624,3.3212104,11.689939,11.69042,11.06834,-5.868704,15.745458,17.42397,16.366722,17.95653,17.941631,17.951332,17.975239,15.814849,14.983034,13.102181,14.341507,14.429762,13.752536,16.722519,16.775259,-6.1424894,-6.2635756,-6.3516273,7.9344196,-5.8823915,7.601047,7.638249,-6.332948,-6.093836,-6.0025167,-6.2879915,-5.9595566,-6.419092,-6.111516,2.506314,-1.9917103,-1.8491169,-1.4894212,-1.4281384,-1.091496,5.270254,5.89889,4.9574404,5.3025928,5.3828254,5.2074466,5.304982,-3.0982883,-3.4336448,-3.548938,-3.5592465,-3.5203245,-3.400634,-3.004978,-3.4090288,-4.7354302,-4.0606565,-4.441834,-5.8542924,-5.7900624,13.20323,12.948001,-5.7985477,-0.27260125,0.11766414,-3.4765456,0.32922366,0.008519092,-4.9499593,-4.7832403,-2.6060658,-5.1934423,-3.998137,-4.1780477,-3.8698978,-4.4605126,-5.4248967,-5.3405128,-3.642887,-4.781638,-5.069184,-4.1574397,-4.1798964,-4.221563,-4.5737286,-4.44191,1.1729122,0.7868874,6.3953238,6.501738,6.3735366,6.432972,6.44933,6.833757,9.126634,8.921109,15.794736,15.824815,15.790229,15.783726,15.780308,15.784791,15.778311,15.774697,15.77661,15.781261,15.7835245,15.776945,15.777912,15.760827,15.758001,15.778223,15.774113,15.774063,15.776769,15.785477,15.77714,-2.6494977,-2.3838792,-6.860988,3.489895,4.455927,2.24461,3.012891,-3.581047,-3.4821713,-5.615567,-1.9781238,-1.5563394,-1.3798829,-6.9045005,0.2604388,-0.60507864,-1.0967488,-2.3890016,-2.8965464,-2.0193381,-2.0891323,-7.087341,-6.161493,-7.147175,1.1751711,1.2019451,0.9288561,1.2387568,1.6113641,1.1291554,1.0661908,1.0163614,0.7249255,1.014179,1.2408674,1.1067439,1.1760637,3.6504443,2.105619,2.0836434,12.232715,-0.18432988,0.65352273,0.46519378,2.3798964,2.024947,-1.3114263,-1.0705911,-2.1656334,3.7561789,-4.1236315,-4.1213784,-4.960674,-1.5576367,-2.0796435,0.28200054,-6.053733,-0.18802229,-1.0557411,-0.19501959,-2.494981,-6.521577,-4.5898848,7.2580914,-7.5089755,-14.685882,-18.688084,-18.831388,-5.428671,-5.5113716,-5.430476,7.581993,-6.1056623,-4.500778,1.7200326,-4.528188,-1.1950725,0.21109678,-5.9622293,7.95463,7.838321,7.8595004,7.896672,8.163835,7.4579225,7.8809915,-6.5111575,-5.991578,-6.430974,-5.576203,-6.850083,-7.1857877,-7.0840783,-2.747328,-2.7622435,0.37177825,-7.086387,16.610638,18.502716,17.97593,17.333485,16.966528,17.91794,19.431458,16.803913,16.793863,9.060825,16.456339,17.644854,17.70323,19.056122,18.906132,17.278978,19.573408,18.232912,18.140556,17.31258,17.638918,16.910328,-5.8748136,-5.851201,-5.9364038,-5.9012775,-6.0316515,-6.209192,8.521419,12.9280615,2.0635564,-2.8715222,-4.631254,-7.3850617,-2.802307,2.2914202,2.0405004,-3.461466,-8.511893,-8.4758,-2.4167547,-3.4240854,-2.9501371,-3.0681453,12.78424,12.147388,12.593979,-14.37428,-21.730394,-0.23231032,-8.381408,-8.31273,-8.267715,-8.233189,-8.307799,-7.9672384,-8.386721,-8.304282,-8.312667,-8.324169,-8.326397,-8.279401,-8.389944,-8.207857,-7.7032366,-8.210189,14.521597,-4.0428944,2.8476737,5.043852,5.2650933,5.4313016,5.938887,5.725514,4.0522895,-7.1317053,-6.5801578,-6.369941,-2.4537196,-0.6776528,-0.6947758,-0.715413,-0.9286594,-0.7646455,0.2668113,0.54958373,0.547623,-9.92434,-10.013018,8.545576,-9.943999,-3.9650311,-3.6283653,6.1979446,6.2088776,-2.2372954,-2.2715535,-2.6449897,-2.1986535,-2.2690308,-1.8484303,-1.2533705,-2.1629758,-3.676599,13.181613,-4.436733,-0.88866365,8.089895,12.533849,-3.142832,-3.8109772,14.17252,13.319643,13.529341,13.4207115,13.323993,13.012816,13.013946,13.306084,-5.216163,-5.547022,-4.072693,-4.9442406,12.86694,2.1025093,12.250641,3.3193111,12.606321,12.709576,11.996142,11.720213,11.615922,12.692047,13.276552,12.703186,-5.006418,3.390243,3.6035807,-3.6859245,-2.2477074,-0.9025501,-2.529529,-1.2260212,-2.3961966,-1.3216234,-2.560968,-2.5726454,-2.023411,-1.7203097,-1.2535822,-3.1816256,-2.6640527,-3.6358726,-4.6302724,-3.8251739,-2.2959769,-1.6304239,-7.951162,-8.555342,-1.3918351,-1.05965,-7.199049,-7.3460646,-4.3922877,7.0389347,3.853612,3.7439792,3.8730526,-3.6283758,-2.722105,-2.6208308,-2.546483,-7.605986,-7.635681,-9.119818,-7.2847958,-2.4021714,-4.4548626,12.927584,12.926163,14.197327,12.11411,6.207952,-1.956235,-2.0023355,-2.2888641,-5.2523804,-6.4164147,-2.3711426,7.542828,0.6481595,0.4392325,-1.1818653,-4.3502445,-4.98967,-1.5643013,-1.9085791,-1.7253463,-3.5446446,-3.7822757,1.4232813,-4.446188,-4.111382,-3.490249,5.3501387,5.2572565,4.9358783,0.117622025,-0.82129663,-1.9040872,-6.2746596,-4.167973,-4.319562,-3.8692322,-3.8603902,-2.1458027,-3.9939647,-4.246613,-4.3808465,7.2987466,12.327094,8.951179,9.128752,-0.24472147,0.41239974,-0.16660967,-0.1545366,0.5073813,-0.8066759,3.6736891,-9.5562315,-2.4728148,8.613515,8.599881,-3.4700446,-2.272514,-1.9234143,-2.0588508,-2.4136143,-1.444654,-7.306581,-2.2760181,-1.9780695,-0.87963897,-4.365285,-0.16525774,-0.06956769,-0.22281201,-7.027093,-4.7423716,-7.0224156,-4.506647,-5.2961516,13.314864,-6.8803387,0.5414187,7.2222176,-4.798868,-2.4127,2.9788568,2.9212801,2.7193038,3.1312158,2.1988168,2.3893807,2.1044626,2.728033,11.7771845,7.041481,11.40511,12.120306,11.559408,-4.8148813,-2.431933,-6.457505,-2.5485258,-7.4844804,-8.841659,-8.000643,-8.892077,-3.3080401,-2.1483133,-2.9849048,-2.3239336,-6.508101,-6.1349454,-6.5235596,-4.371209,-1.2547051,-4.436456,1.5332124,-4.7809114,-1.4679568,-1.9247313,-2.0096352,-4.772555,-4.0091248,-18.698757,-3.4968445,-3.4796803,-3.2967947,-3.2356699,-3.3586867,-3.1049464,-3.0880017,-2.7204409,-2.9324336,-8.135604,-3.0660095,-3.7727616,-2.6648426,13.4450245,12.923668,10.434832,-4.564192,5.9571614,-0.7622435,0.51092345,4.40488,3.3970296,-4.3739996,-4.2264104,-4.8215737,7.6517534,-2.246954,-1.8903456,-10.473491,0.35277548,8.257645,-0.22513603,0.1403097,-2.0257401,-4.272979,-7.0049024,-7.049855,-7.1195536,-7.2963705,-6.88433,-6.9569135,-5.1782155,-5.654726,-5.789201,-10.43479,-10.172029,-10.194814,0.0990654,-10.413983,-14.86588,-13.605944,-17.41482,-13.247112,-3.9155755,2.455267,2.1872437,2.459434,2.3542936,1.8592799,2.0337536,1.907014,2.188566,2.021127,-1.9028952,8.599222,7.77889,-5.6376286,-1.744383,-1.004017,-1.710698,-1.3938882,-1.8806894,-1.857226,-1.2785002,-1.8770013,-1.8597486,-1.7235531,-1.7638294,-1.8707366,-1.9860299,-1.5005513,-2.3602576,-7.27803,7.981151,-7.38171,-7.042308,7.935242,-7.163291,-6.346569,-6.9876256,-7.1972623,-6.8318577,-7.126668,-7.179563,-7.0354414,-6.542881,3.7973406,3.64184,3.3771257,1.757053,-2.7069733,1.5317973,3.3919144,3.9792745,3.6254196,2.2185576,-5.9606624,-6.974414,3.5469732,-7.1515713,-3.1761372,-3.7520015,-2.6678607,-1.7793058,-2.5777116,-2.7542064,-0.50901216,-1.9602882,-0.2660926,-3.3728673,-2.1632044,-1.631232,12.807436,12.960903,12.439275,12.798721,12.2559395,12.642394,11.085889,12.984865,12.791253,-9.790409,-9.8309145,-9.535701,10.154848,10.1692915,10.512953,-9.681198,-1.2238463,-4.3037443,14.696293,-0.25979322,0.23281948,-0.24309053,-0.8551534,-1.5051074,-1.7722882,13.366455,13.052557,13.606209,13.30627,12.6702385,13.107996,-1.6519938,-6.7356963,-7.05019,-0.49564764,-6.648138,-2.8549514,-6.608618,-7.1175213,-6.891127,-2.0152125,13.703363,-5.1675715,-5.097559,-4.652503,-2.9844575,-3.3089273,-5.64624,3.8874059,2.857892,1.6730475,3.1728077,3.1871314,5.2471757,-9.475702,-3.304565,-5.4481015,6.92807,6.9525113,2.5768912,2.3877127,2.369346,3.9989526,4.442111,5.331193,4.2499266,-7.204688,14.464431,-0.10385741,0.7452276,-0.43351203,-1.1309682,-1.139161,8.271882,-0.99403113,8.487737,-3.4380493,-1.425396,-6.9109087,-6.83369,-7.4070773,-7.7386675,-6.9865694,-5.4168706,7.8214464,-7.1782484,-6.4922504,-7.799625,1.0996134,0.68178356,0.6695382,0.43983045,-2.3680007,-3.9755867,5.143805,5.7212877,-3.5331774,-6.25924,-0.13959005,0.3809947,0.3518966,-3.446724,-3.094418,-2.7147295,0.8937083,-3.3474488,-2.6325486,-2.2075083,-2.679375,-0.951263,4.775178,-4.073195,-3.8234224,-4.06884,-4.7570624,-5.0833583,-5.099544,-5.058626,-5.159494,-5.018708,-14.585238,-21.735434,-17.877838,-10.090975,-9.692606,-1.2432967,-0.6902723,-9.910477,-1.2179643,-3.9075506,2.5320714,8.888383,9.137703,-3.7940023,3.7506843,9.598858,9.812812,9.525905,13.118374,3.3926992,3.5854075,3.2194748,3.185831,-0.5442736,-21.71958,0.25205588,-1.052491,-14.931697,-0.93969566,-4.1892476,-4.0351,-5.7849226,-6.2154245,-7.6189528,7.80009,-4.0729156,-1.945975,-3.6018543,-4.0408835,-1.4135679,-1.7227248,-1.1224401,-4.622007,-4.30776,12.659979,-3.5597644,-3.5410879,-3.2485938,-3.1871011,12.674575,12.601412,12.467634,12.881987,12.051892,12.999473,12.899972,-10.983837,13.018137,12.584674,13.319655,-1.1543494,-4.962973,-1.0875288,9.889376,11.158867,10.39679,11.758543,11.932642,-18.362974,-18.761393,-18.678793,-18.764175,-7.016086,-7.178781,-5.9081287,-5.7846355,-5.1758823,-5.1831946,-1.0947483,7.9545918,-10.15822,-3.2742755,8.114294,8.076129,8.082513,13.249264,5.7655535,1.9805058,4.84283,-3.064827,-10.161919,-10.158815,-9.299833,-6.1947055,-7.884038,-7.824664,7.8179183,-2.9943252,-7.6073465,-7.7119117,-7.788392,-7.7496963,-7.3345246,-3.1858299,-7.782245,-6.906835,-6.633268,0.6337537,8.839391,0.81784415,1.5842988,13.338939,11.276703,6.4743476,-0.87934273,-0.07332519,0.61517644,7.9736896,-0.24343237,0.100512795,7.6976366,-0.52312016,-0.58726317,-7.0099626,-6.784492,-6.3497553,-6.871539,-3.0560513,-3.5079803,-2.8587751,-1.8921314,-0.68465376,-0.43524474,-0.036247883,0.46478084,-0.34130085,-0.47927603,-1.1472244,-2.609067,-1.5369822,-1.026619,0.3133318,-1.8556216,-1.7643207,-0.56644374,0.085164204,1.4280354,-0.9616428,-2.6900663,-1.0074636,-1.6609316,-0.8882573,-5.929885,-3.1437109,-0.5879259,-5.015564,-3.255785,-0.98478204,-2.8882124,-3.0635455,-0.7732965,-5.6948256,-3.24149,-0.4633139,-3.430538,-1.0843884,-0.6186803,-1.6312212,-1.0196674,-6.01368,-6.1136336,-2.4142082,-7.3174434,-6.7452407,-7.1573257,-3.2167048,-1.6626726,-0.19403973,-4.237358,-2.317928,-2.4960005,-3.995587,-3.6349077,-3.308387,1.8448561,-2.0936024,-0.7029464,-10.264293,-10.293875,-9.541265,-0.001709946,0.121143416,11.027693,-10.557698,-9.877998,13.084414,-5.6867356,-5.6270556,-5.835567,-6.54042,-5.7243395,-1.3174089,12.346874,4.7161403,8.859593,8.644429,8.386034,8.75512,8.510324,8.680188,8.54315,8.14117,8.575208,8.551335,8.435386,8.471355,8.773841,8.821126,8.485692,8.785311,3.856361,-2.5081263,-2.39228,-2.0707057,-1.48929,-3.9083605,-4.470978,-3.8178144,-3.8498354,-3.9052403,-10.612498,-10.464913,5.487125,3.8956802,2.0715458,-0.9994898,-0.8322754,-1.1456044,-4.1282763,-4.2144833,4.0417423,-3.0271358,-1.8639259,-3.4717343,-3.0151553,-3.0294297,-1.2119032,0.029948287,-1.3652604,-1.6085632,0.22740301,0.00051156565,-4.798869,0.13885488,-2.2815452,4.552476,3.247508,-4.166363,0.7899608,1.4195609,-3.0235507,-3.195556,-2.6598413,-2.6118364,-2.30754,-2.3423698,-1.5091219,-1.8649081,-1.9256731,-6.5492435,-5.069042,0.19479181,-3.1764593,-0.6112919,-4.7637367,-8.485891,-8.53397,-2.369091,4.43117,-2.9585042,-2.5542238,-3.440425,-2.4615684,-2.6628788,-2.1794515,-2.9490814,-2.2782416,13.698057,-4.607474,-6.1547284,-6.263036,-5.869416,-6.6011024,2.14389,0.678073,0.765039,6.8992786,-1.1292087,-1.5805688,-1.5286846,-1.5141668,-0.9158586,-2.3697422,-2.3588731,-2.1900826,-1.6354642,-1.4380021,-2.1803067,-2.0606575,-2.2361405,-4.03786,-3.5715883,-3.7973313,-3.7253664,-4.0739894,-2.2981324,-1.6035992,-3.6031027,13.273348,8.575657,-1.5739117,-1.4129627,-1.5937822,0.6296529,-1.2953572,-1.4855992,-1.5072966,-0.84042966,-1.5317528,-1.2337347,-0.98573554,-1.1852845,-1.4236425,-1.2464688,-1.2314124,-1.517913,-9.075468,-9.004593,-3.5990157,-1.2816693,-4.83989,-5.147808,11.592457,2.841614,3.8242116,-9.786893,-9.658417,8.2566595,1.3897321,8.443844,8.230036,2.627151,-9.620381,8.310249,2.7116563,2.436974,-0.0036294363,8.3229,8.435045,8.465147,-3.3601215,-4.590887,-0.4949044,-8.975809,-8.193546,-9.121752,-9.166588,-9.155561,-8.735774,-8.63809,3.3062723,3.5465386,3.41766,5.8110194,8.644408,8.028419,1.1095232,7.294886,7.9959326,-12.839839,-12.877916,-12.8940325,-12.651835,-12.834028,-12.663482,-12.832189,-12.740158,12.682019,-6.593481,-6.5851703,-2.8824887,-6.3371377,-2.8124306,-2.3617992,-3.401917,-3.454905,-2.5984917,-2.4291744,-1.3558027,-3.4414108,-4.0446997,-4.7854323,0.654481,-3.7963572,-0.059573084,-4.0217686,-2.8465142,-2.4933274,-1.5226288,-2.3614306,-2.7700067,-10.414235,-10.323613,-10.222171,0.14478081,-10.222756,5.4474807,-2.6701524,0.25069115,0.46216458,0.31603283,6.85702,13.771605,13.274727,9.686465,10.014885,-6.0990825,-4.5086174,-0.50441074,8.658414,8.78268,-2.106945,0.3502638,0.19125251,4.3802896,4.1667213,4.440047,4.661694,4.991876,-1.0839853,-1.3234154,-1.3969179,-1.5700594,9.968275,9.871441,10.87651,-8.770097,-8.503396,-8.623646,-2.0816202,-2.2609122,-3.563499,-8.224595,-8.663733,-8.337979,-9.057276,-9.5108,-9.45743,-9.5309725,-9.151281,-14.85587,-21.735662,-11.014416,-3.382913,3.377423,-0.38267812,-0.4132876,-0.098529816,0.5566532,-0.14904387,-0.1807739,-0.05712444,-0.10512475,3.6394372,8.586783,8.60886,8.633105,8.980044,8.464404,-3.972533,-1.734331,-2.1555696,-3.5277202,13.766791,13.357316,1.1182613,4.058165,1.2692063,0.84919006,1.2632592,1.4663463,-7.0954576,-6.6811028,-2.4254756,-21.730284,-17.890581,12.840344,-7.08659,-8.672747,-8.62307,-7.937905,-8.324997,-8.24338,-8.021709,-8.688064,-8.721868,-8.79022,-8.746077,-7.9453034,-8.575449,-6.539253,-4.040925,-4.5307903,-4.8127475,-4.9025116,-10.444051,-10.325429,-4.8979077,-4.550843,-10.380888,-10.436795,-9.9674015,-10.08481,-10.340653,-1.812338,-0.37584925,-10.4301815,1.4518443,0.13918027,-0.017119726,1.5053302,-10.405556,-10.012789,-5.9466925,-5.8694386,-5.9027557,13.023684,-2.7580779,-2.528916,-14.477465,-13.603335,13.561827,-0.62551343,-0.5591407,-0.85232306,-6.326629,-7.3153224,-5.6643887,-6.676452,-7.0116754,2.0609026,2.6206603,2.0019941,2.0456202,-1.1439973,-2.1869352,-3.6310706,-3.604689,2.3010726,2.33305,0.9950105,1.864767,1.7300264,2.0609946,1.2875516,8.480794,8.531132,-2.8965278,-0.8391173,-5.9806542,2.313772,-1.8216044,-3.9087772,1.2521734,-2.6197543,1.1678355,-6.2389174,-21.728981,-18.457174,-18.679098,16.979336,18.127808,19.441631,17.887302,17.888783,19.522955,-3.4086986,-3.7570407,5.17809,5.2793727,5.7395487,-1.0770509,-1.0149901,-0.28302845,-0.54935956,-0.43872246,-1.8291348,-1.4762192,-1.5421307,-1.0406398,-1.0602418,-3.6431322,-3.6711926,-3.544675,10.753126,-7.069662,-2.705067,1.8503878,6.0209427,5.813049,5.912704,6.2366424,3.8235207,-4.106991,-4.339873,-10.564901,-10.528309,0.528424,0.5018301,-0.30767077,-6.942389,-6.59035,11.656408,-2.0953755,8.238229,0.4061293,0.21575457,-0.5332625,-4.972969,-1.5706531,-2.4249804,-14.989673,-13.6056595,-17.41657,14.323143,10.692966,13.6395645,-6.371657,-5.9576974,-0.9765132,-5.4112635,-5.4183006,-5.4071894,-3.875321,-5.416643,-3.8758433,-5.4189563,-5.418007,-5.41543,-5.4153647,-5.4178133,-2.8572822,-5.4061,5.9907494,5.98623,5.982932,5.9821763,6.025575,5.9848804,5.9920197,8.683373,8.276694,-4.5344963,-4.8854,-2.780575,-7.1699615,-6.655639,-2.9172468,-3.9772615,-6.675916,2.283958,1.696934,2.2836392,2.2139785,2.8903265,2.7353466,2.4710767,-9.687395,-7.1591687,-7.245552,-6.7771473,-6.154428,-6.887185,13.477129,-3.424022,-3.6499858,-3.2824523,-5.0761623,7.8362656,9.861487,11.947477,11.536837,11.976059,11.637411,0.27397537,11.042304,-10.506721,6.67023,-15.016624,-21.732946,-18.291801,-18.641554,-18.780146,-18.827915,-18.683437,-18.52299,-18.6595,-18.841623,-4.749782,-3.5725806,-3.9159229,-3.9876661,-4.172829,-3.556579,-3.422776,-3.9903727,-4.211769,-3.4877853,-3.5906675,13.097051,-14.9360695,-21.730055,-4.5683646,-4.8851037,-3.9036841,-2.5153272,-4.1650553,-4.536295,-3.0731103,2.5350084,2.305506,2.501812,1.6007379,2.7604215,-5.410639,-2.5854905,-3.2756891,-3.2478004,-3.5402284,-2.7533844,-2.638028,-2.2928195,-1.8291588,19.765814,19.947237,19.739939,-2.3877335,8.764704,-5.3146887,-5.476205,-5.3477473,-5.1550527,-5.205361,-6.119479,-4.206864,-3.1099465,-0.97951984,-1.8321052,-3.2972426,8.207058,8.249123,-3.855487,-8.834872,-9.237204,1.9031434,-0.21634726,-9.135095,-0.52435005,-1.2544737,-2.0899568,7.024225,0.50117016,-1.489561,-0.23154284,-0.07483477,-0.7621567,-14.889233,-13.605786,-5.955732,-5.6573024,-5.6258554,-5.163858,-4.4633484,-5.4879403,-2.0263877,-2.4260218,-4.298407,-14.965534,-21.73206,-6.2490044,-5.627719,-4.4463854,-6.478832,-5.9009156,-4.8180084,-3.8796806,5.9632497,6.3802104,11.787286,11.813564,11.580617,11.523565,-0.79317826,0.6033067,-0.7862796,12.039915,-0.9481615,0.45196056,0.5159104,-1.5174112,0.2572811,-1.7081987],\"xaxis\":\"x\",\"y\":[3.9882128,2.6898236,2.6917415,2.2310467,2.3267617,3.2044134,2.2827682,2.4626143,0.52333766,-1.7552305,2.3185794,2.1205704,2.1253169,1.8687302,2.0531332,1.9383981,1.8197254,2.5054977,4.8055897,5.8305116,-6.0833797,-6.2558193,-6.097147,-6.095142,-6.1546745,-0.729244,-6.128364,-1.1699681,-11.204029,-11.014101,-11.459438,-0.16646175,5.4515147,-11.292832,-0.86502695,5.678906,4.0042553,4.475176,4.209929,-2.1017144,8.551946,-4.2715664,-4.3554153,-4.275744,-4.3456287,-3.7999032,4.4560237,4.687471,4.277807,-6.5022936,-1.1822667,-0.5159575,-0.1238905,-0.31805572,0.049661707,0.18718156,0.118111365,3.5248241,3.5251427,2.4097269,3.4570825,-5.3959255,-5.395504,3.090131,0.5292541,7.2652946,0.4572752,0.30587447,0.3721304,3.7805576,0.23991488,5.047474,0.07772672,3.479853,3.2336755,2.2840924,-0.16977257,0.44394132,2.346648,1.7945824,0.5653115,1.0662757,0.67564833,0.86374,-18.372375,-18.254763,-18.60253,-19.280346,-18.480814,-18.363932,-18.137623,-18.311712,-19.06644,4.5150013,0.81079537,1.1309643,8.048766,7.792333,7.547656,7.0854197,7.1885843,7.3216577,2.6745028,2.8259747,-12.344752,-5.9485054,-5.5517893,-6.504618,-6.0068088,-6.7193103,-6.238333,-6.006671,-6.5270095,-6.2356772,-6.54996,-6.104375,-7.0173626,-7.198711,-6.1859183,-6.679612,-7.2377477,-5.6272287,-2.8116584,-3.9172153,-5.0927944,-3.6922717,2.9135015,2.9647357,2.829814,-1.2255342,3.835031,-7.802078,-7.718164,4.339875,4.1985803,4.1390867,4.215819,3.6097722,3.016798,3.0302155,5.8717523,5.9577584,5.929514,5.9486537,5.8626523,6.009164,5.9139237,5.713564,0.12086214,1.081084,5.4859033,5.3850894,6.088701,6.4545426,6.354062,6.3736663,6.4570684,6.7279544,-9.635416,4.243338,3.0896087,2.3473854,2.7036595,2.8086007,2.401374,3.2134306,3.030104,1.3558834,1.0131484,4.298038,-2.0795858,4.76237,1.0151054,-4.3233933,-4.3629704,-3.5744174,-1.9324534,3.1339407,-4.2885566,-4.326956,-2.1119637,19.451347,14.811766,-4.2127995,-4.180842,2.3526583,-1.0471054,0.2767037,3.4260833,5.2777905,5.416373,-8.125946,-7.1777716,1.5471677,0.67400056,1.2212425,2.197939,0.72950155,2.439309,1.0508225,0.8082351,0.8444071,2.6380303,1.6787273,3.9946132,1.3836093,1.4929506,1.6161667,3.4153385,4.2647963,2.3245635,2.1192245,1.9030751,1.4932889,1.1864942,-2.1746051,-1.7044426,-1.7364156,1.8548858,-2.0243597,-2.7806203,5.9497113,-2.3854654,-2.7231586,-2.787083,-2.6645641,-2.2939174,-2.8690932,3.4101775,-2.014534,-1.4476622,0.86743486,0.6225051,0.006926315,0.5463942,0.40132454,0.25504902,-0.14619936,1.4248021,5.4193115,4.9080606,0.96498907,-1.1335157,0.29940623,-0.029714335,1.5285627,-0.87273705,-1.9190041,1.0259703,-1.3876898,0.18392006,1.92094,-1.4200686,1.0619202,-0.060621813,-1.1069465,1.0446278,0.6581245,0.20017834,3.9431472,3.411519,3.94717,2.5324566,3.9501758,-0.8833601,1.70873,1.6467944,1.6216464,1.5925419,1.1512952,1.6480788,2.1478856,2.2207863,2.5144565,2.4949677,2.3614266,2.2526727,2.2108817,2.3756497,2.3275146,-1.211301,-0.6097537,-1.1341095,-1.2882549,-5.2013626,-0.90984535,-2.5736365,-2.5050688,3.441677,3.4497864,3.2777534,2.0180724,3.5629854,4.0961733,3.2827663,3.9824662,3.6018176,3.694256,3.184785,2.1057544,2.852447,2.9925046,3.1414506,2.7030728,3.2916448,3.2507753,3.5723507,2.9465065,3.550073,3.1567106,3.7117608,3.5153487,3.5246954,3.2909734,2.3546803,3.255005,2.3569386,3.181037,3.6056662,3.9476655,4.344398,3.4270933,2.2113762,1.8357903,2.5922692,3.667926,2.9046726,2.9240184,1.9137847,2.3662236,2.3625393,2.6367073,3.1270857,2.3691537,2.6938434,3.3047965,3.5534496,3.3234057,2.6554954,4.6436815,4.1428375,3.6384406,3.1496658,2.2016706,2.2793956,3.9215202,3.8642893,3.4626567,3.623788,4.0441504,4.1249766,3.667852,3.7794745,2.3380542,3.4686546,3.1597521,3.1325884,3.0194468,2.962537,2.999589,3.0398116,3.0285494,2.6371732,2.5321393,3.7965717,-1.5266436,-2.3768778,-2.2857428,4.8098483,14.811176,-4.197725,-4.202731,1.5524539,7.133319,3.037298,2.814977,2.381887,2.563651,2.7797337,3.175509,4.1505923,-2.1161823,-2.5726848,-1.5479455,1.4773438,6.0400157,1.5373957,6.1145177,2.4480298,4.403963,0.6862894,0.6780721,0.39844197,0.2862041,1.3707789,0.9041393,1.0870417,2.4904103,3.4780238,3.328092,3.447517,4.4020295,4.737211,-9.678855,4.253566,3.3025107,2.3704064,-1.7271708,-1.8068475,-1.8140761,-1.7836736,-1.8131075,-1.8017235,4.5454316,4.2829137,-0.6735453,-0.5833012,-0.46432126,0.80509704,0.112434946,3.767492,3.84909,3.9722047,4.027846,3.688618,3.8449988,2.9735289,3.8962734,4.033489,4.4011207,3.8703027,1.5238657,0.3844425,9.914787,7.6455054,9.910084,7.9058123,8.171339,8.395049,0.75038755,3.7558649,3.8622456,7.3417153,3.1421175,3.692545,3.2922268,5.102547,3.0268922,2.8324225,2.5690625,1.7135152,1.3478487,-0.17434984,3.9398298,0.84531176,1.3731567,3.5227168,-7.893946,-10.355307,0.015141924,-0.27223277,-0.33777547,4.2655244,-0.037959777,6.5321455,6.5722566,3.915851,1.8622609,3.8785381,4.2337284,4.322292,4.4703546,4.477711,3.0375743,4.2335973,4.552892,2.9543676,4.239434,4.342948,4.2019677,-7.005406,9.83712,8.3197,8.274641,7.8604565,7.2975416,7.402321,7.875375,6.1916423,6.1295567,6.1970496,0.5219693,6.7747674,6.0780716,6.1582527,6.9020205,7.0130816,6.4182944,0.71807295,0.3521953,0.74476284,0.63375115,0.23356254,0.3731154,0.44238746,3.5303109,3.6086128,0.34303012,3.065433,3.809175,0.5134968,-1.774293,-2.2848141,0.18388683,0.27085015,5.5649004,2.5739758,2.9224527,3.1877077,2.9037411,3.2950692,2.789237,2.9048307,4.164681,3.0171475,2.0682316,3.35196,3.171861,2.8427339,4.477565,4.7775373,-1.0555937,0.12951349,0.54573077,0.23134406,-0.33892924,0.5074191,3.836398,0.0012511762,0.062850125,-0.30954832,4.559728,2.0655248,4.247165,8.265339,7.769777,8.056197,9.862343,7.9650793,7.3610697,3.2173026,2.8487835,0.84829783,2.994462,5.3085284,0.69539875,-0.8526843,-12.473088,1.2388089,0.37406006,1.4816808,1.5191349,1.3706731,-0.46800098,1.6121953,1.2194549,5.7505317,-0.6741405,-9.516104,-9.505879,-9.439922,-9.478224,-9.76212,-1.901938,8.553579,14.810976,-4.238927,-4.270663,-4.2896214,-4.3698063,-4.2328367,1.7147975,5.3408766,-1.1059488,0.04314206,4.0172415,3.2282836,2.239131,2.3479116,1.9287162,1.619337,1.4006597,5.779641,1.0297108,0.4816469,0.2880559,0.25274336,4.9262342,6.903081,3.6591573,-0.21158186,-6.3703976,5.4271746,2.8671906,-1.2736357,-1.7252856,-1.6856427,-1.933832,2.3604786,-0.10750974,-0.19828582,2.2017088,1.1115946,4.431991,5.052544,7.5858574,7.8762465,7.854377,2.048708,-0.0004423265,0.5566799,-1.7331539,-5.393543,-5.892945,1.9916314,4.008602,-1.1216645,5.3328786,3.8419268,1.4476761,5.349888,-1.9513326,-2.0232382,0.59286195,3.6484926,1.2561865,-1.713646,2.3233185,2.3485072,3.539137,4.200906,-4.224222,-4.3121824,-4.346684,-3.797722,7.356869,7.137011,1.6063395,2.2983165,4.436094,2.127968,1.419794,6.262029,6.4125466,7.387658,6.6515408,-1.9306206,-2.0877278,-2.158498,-4.1285563,-5.9558325,-3.1426804,-5.1705594,-4.4537697,-6.427719,-3.2711964,-2.351456,8.00886,8.100909,8.32464,7.6115017,-2.2452729,19.451656,14.811538,-4.0307236,-0.42204994,4.274832,6.434311,-0.5300102,-1.6688378,3.1576366,8.552944,-1.0963008,-4.2285433,-4.2675414,-4.165073,-4.154266,3.5625408,-0.071784034,2.0466726,0.6893801,0.35327497,0.77345014,1.0284858,-0.26524204,0.09012263,-18.631676,-18.09148,-17.9567,-18.286695,-18.328327,-18.06747,-17.972637,-18.276766,-18.166756,-18.643959,-18.804178,-19.215836,-18.469746,-18.41031,-18.166094,-18.237581,0.53137136,2.4058,2.4586427,-1.3112862,1.2166256,2.9223292,4.961809,8.255934,-0.13053466,1.3055145,-11.422094,-0.08719012,0.02974158,1.9244146,-0.7201599,0.43771073,0.109265305,-0.48914713,5.0586076,-5.5126085,4.121855,2.1840816,4.006238,0.9394677,0.9034698,0.08279557,1.4373388,4.5066824,-5.791365,-6.2381954,-6.2636204,-6.934749,-7.1520042,-6.6161733,-6.2631903,-0.45802355,-0.70456034,0.76416963,0.70620805,1.0171233,0.0480713,0.9920385,3.3527043,2.9482605,3.3307238,2.6979036,1.3708899,2.278962,3.8328056,2.833295,2.7033792,2.784497,2.765579,2.7503545,-0.05067149,-0.050578963,3.8267877,3.822942,2.8567584,3.5519524,-0.21166149,-0.033728,-0.08004406,-0.28983513,-1.5963839,-1.31578,-1.2872043,-1.9883046,4.7702384,2.324659,2.391863,-1.4096209,-0.2784238,-9.493969,-9.671438,-1.5827663,-1.8430737,-1.6415168,1.8614914,2.9738245,6.3582973,5.7395024,2.1106234,-1.2910815,-1.059311,-2.1416836,0.1632962,-9.308481,-0.8076601,3.4911346,-2.5592778,-5.472408,-5.318326,-3.1201196,-5.5116534,-6.559061,1.0465683,1.1334732,1.5650942,1.5294325,1.6357888,5.3269806,6.0496836,6.0569444,3.5569558,0.8168323,1.5326915,-0.5579712,1.4230559,6.0900784,6.667092,9.951694,6.8570433,-1.7617002,-1.7831055,-1.6260211,-0.41420868,-0.24946134,-2.0205991,-2.400605,-5.4452925,3.1329103,2.4268074,0.7730249,3.2315817,3.7521405,5.7215796,5.5817156,1.9389672,-20.006716,-20.002853,0.5849588,-8.176163,2.005749,4.7814937,1.5051782,1.939868,-2.5819395,-9.519571,-9.485426,0.7740145,1.0938115,1.2511023,1.0695033,1.3384397,2.6008523,1.0265174,0.7078338,0.5967514,1.0676552,0.9207511,2.6343403,3.0628378,-12.096941,-12.255162,-0.3948328,1.3761508,1.2243245,1.8454804,1.6606935,2.069624,4.290559,2.7140355,0.829491,-0.4543983,4.548229,3.1589918,6.039215,0.7123657,5.334229,5.1002116,5.280312,3.5124023,4.225715,5.322322,5.1919823,5.3593373,4.9321814,4.367501,-2.1315856,5.0093017,4.287781,3.2580278,1.8599274,1.8888894,1.9772563,2.473579,5.078379,0.9680035,1.111533,-1.0757517,3.0631216,-6.258384,-6.225486,6.54577,7.2665577,0.4139183,-1.441946,-1.5463728,-0.9322653,-1.7251803,-2.2743142,-6.283927,-6.2703185,-1.2392912,3.6255338,3.646041,3.5401597,-7.0991216,-6.9271297,1.1782377,1.4421335,2.0184996,1.4337162,1.819104,-0.7619864,1.3914464,1.2599367,1.446955,0.26893228,2.4113872,2.481248,4.0127172,3.4891043,-5.8259926,7.8652616,6.9039025,3.4116988,2.928978,1.5928533,3.1286461,3.6636198,3.4704344,3.4674916,2.0165815,0.09866458,-0.070583194,0.13098858,2.7660005,3.3227544,3.1328385,6.3771033,0.4693144,0.32942086,-11.807894,4.1478963,4.179903,3.7835844,4.001006,0.016987767,-6.44963,0.6811585,4.018362,3.8818727,3.3838696,0.41046116,-0.65331894,0.69957787,1.7722574,-1.7651094,0.69565994,0.53714424,0.30521867,0.2288868,-7.0620136,-6.8992996,-7.4240985,-7.021044,-7.159283,4.5393996,3.9722588,3.8773382,2.9288864,4.5718493,4.412954,0.7631495,4.1264825,3.3108451,8.197429,7.4634953,8.304614,8.046631,3.9557066,2.8609571,2.730427,4.311342,3.30583,0.37920964,-0.2724599,0.2679959,-0.21956727,3.8753648,3.7824955,3.6827219,4.26402,3.0560791,2.8291922,-12.390615,-12.542044,1.1033319,1.6429801,0.87882054,0.35415643,2.1773584,-0.85797936,1.5696423,1.9444116,3.3536005,-0.078643724,-0.7169512,5.2027946,0.56055474,-0.0025550702,0.57254124,-1.8306544,-1.8575988,-1.5304955,-1.6763847,-1.4320806,-0.73281604,1.4967804,1.1890496,-1.8440346,2.8725393,2.3398376,0.061375163,1.5676261,-0.19102053,4.1117597,3.8648,3.1935422,8.504259,8.287434,6.089846,8.440998,8.468262,7.4895673,7.667369,2.9732823,5.363162,-2.150219,8.553381,-3.9425569,-4.1831946,2.1324055,2.2753186,2.077323,-9.108517,0.3194968,1.5493895,2.7311492,3.4471223,7.8421183,3.25121,5.607577,5.732813,-8.115753,1.7700291,3.878345,1.6643159,1.9034111,1.6989934,1.7051908,1.7735802,0.8511517,0.5985469,0.70983726,-6.717339,-11.366957,-11.390173,5.730207,3.8307416,4.0476995,5.0547915,4.780009,4.4583645,5.1123233,5.014322,4.991333,4.8974566,4.25966,2.7354107,2.354134,1.6024863,1.5328202,-0.8567965,-0.532132,-0.59494054,-1.5589025,-1.8705636,-2.024575,-1.855128,-1.8998582,-1.7777559,-1.446015,3.514709,3.8869388,3.7164385,6.802465,6.929631,7.079724,6.9949646,2.3098826,2.3614721,3.7437499,4.330592,1.677593,8.142705,2.0988338,5.2521625,9.907283,8.487054,8.452395,8.638516,1.3693129,3.681658,3.3324811,3.5878706,2.5392172,2.380742,2.325426,2.4629507,2.5601017,-1.3037063,1.3873906,-1.9373921,19.450378,14.812356,2.4869742,3.4750772,-2.788094,0.30844572,0.4122153,0.14174671,5.3299994,5.2692027,5.3907995,-1.6203784,-1.5804791,-1.5768217,8.171924,8.436228,8.414545,8.538175,1.2759925,-6.9343753,0.4764009,0.19004244,-3.3941014,-3.8923337,-2.9502578,-4.5719495,-2.7998154,-3.5627377,-4.857309,-1.033934,-2.0186632,8.553732,-4.1163325,1.5507586,1.4807053,5.3682895,1.8055025,8.388578,8.384777,-1.4646975,4.4300513,1.6207696,4.0949636,4.2615156,3.206707,4.213697,-0.1948257,2.1354525,-0.12117154,8.156357,8.496797,6.154602,8.244222,8.057994,8.271651,-6.450868,-0.0731023,0.6991133,0.6771097,0.8054488,3.4066098,3.3260758,-1.0905908,-1.438849,-1.2966398,-1.6214253,-0.8752904,-18.581562,-18.438496,-18.285206,-18.311337,-18.77245,-19.25506,-18.54285,-18.448896,-18.095741,-18.284725,-18.66178,-19.224518,-18.54782,-18.582443,2.4834168,-0.21913847,-0.12446756,4.5162606,1.948883,2.8829322,2.9793034,14.812311,-6.390361,-6.7132287,-6.1526895,-6.2629905,6.2176843,7.042268,0.7773492,-12.532725,-1.1005578,7.4722695,7.3054533,7.2119265,-2.0746756,-2.3326952,7.207349,2.2948525,2.3473012,2.3532262,2.3442507,2.4644907,3.86912,4.13881,4.198712,3.7307367,2.4344223,-5.729265,-2.6757748,-3.2803357,-2.4815557,-2.869214,-3.5060973,-2.2882547,-4.1469316,-5.0405316,-4.6336923,4.939348,-1.8896852,19.44979,14.810832,-4.1028957,3.5187933,0.65796345,2.5106704,-0.61782074,5.1313376,3.8907773,6.860135,6.7495117,6.635324,4.143211,3.6757894,5.216833,6.951814,3.232248,4.2045417,0.056942165,3.477787,3.6337082,1.2713871,3.1048267,-0.026247779,1.0308543,1.0336896,0.34953436,0.7728008,0.0031364704,0.1464072,-18.210205,-17.798206,-17.85676,-17.846659,-17.956942,-17.893131,-17.951206,-17.837202,-18.331053,-18.299574,-17.587776,-17.398554,-17.792719,-17.358356,-17.288488,2.8123481,-12.297148,0.64521295,0.7599748,0.1937562,1.0992577,4.771284,4.636272,4.0099444,-0.89202267,4.305922,4.066758,-1.3113061,-1.304321,4.817308,4.24465,4.0944543,4.208476,-7.591423,4.1030183,-2.250494,-2.2008142,-1.297766,-1.6840423,-1.9839424,-2.1959856,-2.255241,-2.332918,-2.329512,-2.1966372,-2.0467525,-1.9268094,-2.1531,-2.2622185,-2.373751,-2.0255978,-2.363891,-2.1810875,-2.2422884,-2.6808803,-2.5076976,-2.6463504,-2.528421,-2.5598893,-2.618726,-2.4204056,-2.4687212,-2.5264354,-2.56446,-2.5360568,-2.3808799,-0.014386084,-2.4491198,-2.7437363,-2.311038,-2.1466603,-2.1888738,-1.6443484,-2.1951005,2.8124487,-9.73434,-9.97646,1.513562,0.40817136,5.7128186,-6.9478927,-6.7842526,-5.4837,8.923307,8.778536,8.508255,8.581593,0.5501844,6.5024395,9.89199,8.3334875,8.30612,8.151219,-6.587876,-8.57851,3.7574291,3.8463464,2.8322048,3.9287264,3.8576243,0.19502053,-6.4274607,-6.489066,1.139117,1.4124392,1.4095854,1.2204536,1.8312179,1.3960065,0.1750521,-6.040679,-0.23801032,-0.34453085,-1.9787999,-0.8755347,3.5373287,3.3330803,3.6043508,3.781586,3.6404772,1.1843033,-0.026649866,0.8569612,4.003532,3.9543366,-0.49210086,3.3058922,0.500472,2.693067,2.4599621,6.461943,6.3497043,6.091208,6.520704,6.483325,6.461833,6.5166497,6.41141,6.3246436,6.2366977,1.3529797,1.4905819,1.4480104,3.440687,2.7695768,3.000738,4.1751237,4.192363,2.8133678,2.8559947,2.9987257,2.7796223,-4.2461104,-5.7679405,-5.520351,-2.2996767,-2.3261178,-2.7418227,-2.5382323,-1.3594745,-2.3481271,-2.2337675,-11.173501,-11.251588,-6.8745017,-6.336858,5.121345,-1.9981124,19.450315,14.811587,-3.9953475,2.851781,-9.602782,5.4098325,1.725775,-6.4529333,3.7448707,4.0176554,4.464277,3.181568,3.7354336,4.2144876,4.410494,4.4452996,4.7332664,4.369333,4.7195334,4.447979,4.490087,4.4641643,4.4773474,4.3332357,4.467341,0.9251428,2.622241,1.670146,3.6811054,4.4196825,1.4448577,1.4074233,0.7267305,8.400838,8.11905,7.6314955,7.039017,8.045562,6.986826,8.00908,6.7744794,7.870775,2.7508917,0.52382207,-7.9662256,-12.075303,3.91285,3.7813518,3.3408341,2.7335815,2.964747,4.2000237,-0.44217873,9.844828,7.2361584,7.2663736,7.2557793,-2.3778098,-6.2818995,-0.17550945,2.2545066,2.8149993,2.4856007,-12.049086,-2.4017897,0.85278696,5.6745987,-1.1235714,-0.50154805,-0.07318919,-0.6181048,-0.59518206,1.2957673,9.938337,1.0212299,-8.462426,1.1814947,-8.644509,-8.251396,-8.380448,4.1826587,-0.9719173,-0.15182738,-6.559536,-6.5670943,-6.206255,-6.553599,-7.6925583,-1.1750082,-1.183108,-1.1213691,5.7077074,-0.98377556,-1.534899,1.6567578,-1.1845907,-1.5366116,-1.1765194,2.3320804,2.4712927,-0.76581144,-0.46238714,3.731365,3.430916,5.554424,2.8159516,0.9018678,3.2805622,4.033625,3.696335,4.048188,3.9091516,5.025348,5.007508,2.9586825,3.7931452,5.4452415,5.1164412,5.142932,8.196493,8.206977,7.862267,7.6875243,8.08885,5.106999,4.977679,6.0875287,7.535238,7.4422574,7.6662483,-2.1310737,2.0893126,2.970268,5.0925126,-6.8819294,-6.3142014,-6.6727986,-7.1662717,-7.126039,-7.0425243,-6.360236,-7.5721493,2.1335683,5.579802,7.1959877,-4.332426,-4.371269,-4.2611556,-4.3578143,-4.4142957,-4.332895,-3.738579,2.820436,-8.522065,-8.039479,-8.022232,-8.553318,1.0848615,1.134989,2.4222188,2.7524307,2.3825972,2.8384342,3.0233078,3.0989377,3.154251,3.4623754,3.5598066,3.4611883,3.3028889,3.298852,3.277714,3.7998507,3.2185225,3.4657145,3.4676852,2.3526223,2.6415725,0.11126453,0.33038545,0.37827855,3.757411,1.5912368,3.6478033,3.5829477,0.28998333,0.5511998,0.3581746,0.3854368,0.38903734,0.31873637,0.21834582,1.3516047,-7.6609983,-5.9445806,-5.873286,-4.738449,-6.5150404,2.0527046,1.6526705,1.6337749,1.5183582,1.6111786,1.7695305,1.5533688,-2.3227086,-2.4103138,-2.1178737,-2.1737103,-2.0505102,-1.4530711,-2.2452996,-2.1274989,-0.6921043,-1.618376,-1.0405793,0.35798502,-0.016063266,5.343593,5.0279455,3.438606,8.381112,8.403222,6.033745,7.70447,8.453443,-6.2686086,-6.485961,-6.4668183,-6.3625536,-6.4725256,-7.33697,-4.877839,-6.562383,-5.398138,-5.4518266,-6.7456098,-6.4799356,-6.4151926,-6.160842,-7.672821,-7.786467,-6.9634085,-7.02529,0.65373296,0.81434643,2.8349094,2.5503438,2.8732266,2.8951616,2.849997,2.82358,1.7517385,1.802434,-16.026394,-16.003069,-16.032251,-16.035671,-16.037098,-16.03344,-16.033592,-16.038202,-16.036837,-16.035309,-16.033855,-16.036154,-16.041082,-16.038288,-16.044252,-16.038374,-16.037132,-16.038994,-16.036604,-16.033962,-16.039158,-8.059944,-7.8917522,0.21573897,2.1000385,1.2566,3.2489808,2.242784,1.7367496,5.437677,6.7910466,1.4236908,0.88889104,1.3845094,2.905659,-1.044359,0.29075652,0.9462453,1.879953,3.1867478,3.333831,3.44586,2.7025163,1.8925817,5.624458,1.7605395,1.5269405,1.5977812,1.7016188,1.4810716,1.5132267,1.3461978,1.5893819,1.258316,1.2163484,2.009992,1.7867278,1.4395943,0.9081115,1.4072764,1.7434964,4.810643,9.990874,7.9908133,7.8353224,3.4917138,3.8559566,-1.2567161,-0.7256712,-1.320756,3.7985444,-3.8550172,-4.4913974,-6.661739,4.6945825,4.3692307,6.5342155,5.968791,-0.44497433,-0.055794433,-0.5413672,-0.71804416,6.0930204,-5.4262333,2.568238,-2.3296425,-2.0132394,-3.8451178,-4.331759,2.6368465,2.8372915,2.8533123,3.476066,1.3738754,-1.3786311,0.87442154,2.6836982,2.8247757,0.04842454,1.91332,-0.19430774,0.06583406,0.032977153,3.4491591,3.5746672,-0.2055857,-0.06509758,7.1504,6.8753986,6.6296883,5.9881544,6.428088,6.3943877,6.6322827,-8.096909,-8.211441,1.0111511,-1.9564927,3.3978322,2.8068664,3.1094673,3.2906482,3.5576851,3.106246,2.4014757,3.6312454,4.0036345,2.0248787,3.4344041,4.0661397,4.048053,2.762109,3.5373764,3.1375072,2.390207,2.9622092,2.0626738,3.9062643,3.1859818,3.1768823,0.9181845,0.49138978,0.7972281,0.6795812,0.738837,0.84835935,-11.230306,5.093783,3.53771,4.174332,2.4081829,6.097326,-4.4549246,1.4399348,0.94199747,-7.7748303,2.5559516,1.7193236,-12.098764,0.03180112,0.67579466,0.638844,3.7714157,3.9358714,3.3769991,-1.8837113,8.552904,9.924516,-2.7946396,-2.763239,-2.7783425,-2.7124164,-2.7716227,-2.5113835,-2.7943523,-2.7241383,-2.73513,-2.6196976,-2.8292742,-2.7713392,-2.841542,-2.6008925,-1.9261274,-2.5781746,5.179,-6.981774,3.2811732,1.4474013,1.4913583,1.5205793,1.6929854,1.6240805,1.0425959,6.4499583,6.8745265,7.445348,-11.796724,2.2390268,2.183936,2.174844,2.420765,2.2088726,8.124641,7.910188,7.9101806,0.1688486,0.3806017,1.4680752,-0.15275078,-6.0168667,-8.358,20.724102,20.756289,-4.829306,-5.601416,3.8776166,-5.4635067,-6.040859,-4.576906,-3.1466546,-5.728622,-5.746843,5.3555517,3.8332694,4.242326,-9.984096,5.2628613,-8.021713,-8.255101,3.5924063,3.5354755,5.083085,3.6353858,3.5631144,3.8231986,3.6645691,3.7950552,0.29586965,2.8965561,0.41910824,0.53064305,3.6470385,3.997065,3.578277,2.4110193,3.540559,3.2239158,4.3788247,4.546716,4.4400663,3.4503374,5.6458015,4.0729866,-6.426744,0.1987184,0.23758903,-6.4152317,-6.3454504,-6.6652064,-6.7561684,-6.484776,-6.8096337,-6.6326385,-7.4415607,-5.811211,-6.084993,-6.130807,-6.834075,-7.4578714,-9.752628,-1.911481,-1.0931672,-2.379078,-1.992309,-2.2585762,1.5877047,1.8854024,-2.2903585,-2.6248658,0.056292873,-0.77447313,-1.8207759,0.37392116,-0.9476792,-0.90049917,-0.80364484,-6.851479,-6.247558,-6.817925,-6.3585043,6.5512137,6.4189587,-0.47709912,6.5728164,-12.195465,-5.819179,3.5953047,4.140242,3.62435,3.2337875,20.752941,-2.7156062,-5.6019177,1.3670952,1.5167892,0.71585697,-12.49171,-9.675826,6.2281847,6.8980937,0.65534616,-2.2043986,-1.5335371,1.6696529,0.14165512,1.0449244,-0.5267997,-1.5361062,0.8074537,-1.8250997,-7.3614993,-7.6236234,0.16844442,2.3084364,1.8456954,2.4810445,3.493203,3.948616,7.178867,-5.980799,-6.025155,-5.4969783,-5.5877175,-4.752327,-5.3821545,-6.0340033,-6.399676,-9.567076,5.1355257,-11.37576,-11.405422,9.684674,8.1034155,8.479649,8.699189,8.155134,7.819715,1.0640566,0.35269803,-11.849368,-11.0072155,-11.140812,3.5119772,3.6289217,3.8088684,3.613412,2.1100633,2.9456427,2.257418,2.5667875,3.2295084,3.2361333,3.9026215,9.224713,8.754568,8.673401,2.355322,3.611051,4.419881,2.2298717,-0.12159245,4.1538315,7.0064673,5.7683444,2.8100698,-6.809256,-11.994412,4.0691333,4.3992624,4.3081594,4.3451242,3.5330834,4.398967,0.2621347,0.54548734,2.3702004,0.8183473,4.128941,2.7082875,2.0489352,-6.722307,-11.84808,2.4236634,3.9053526,6.2314115,0.7055202,2.568517,-0.35543332,4.1146026,2.1605282,4.405262,4.474902,7.112507,7.1439753,7.0480523,3.153295,1.2593238,3.0689533,1.9905157,3.4336197,-3.22064,-3.0530949,-5.4700093,-7.440092,-7.507172,-4.2340455,6.3892984,5.7503576,5.7890773,5.7925143,5.063833,-0.09681605,-0.6137768,2.2863173,-0.413029,1.3658255,-1.0463184,-0.21771751,-1.0785738,5.3647995,3.9406471,3.3805668,-6.5715265,-0.7442823,2.490996,4.524623,2.8868556,4.0630608,-6.948125,-6.6735196,-5.6474504,3.4940796,-6.9508076,-6.9237437,-0.47863978,-6.295344,4.086568,-6.2716045,-6.1178336,-5.7246714,-5.5673013,6.0081906,5.837422,6.273171,6.321332,5.6862087,2.06,0.14584568,3.0340102,2.9348102,-0.28538498,0.075787194,-0.46428806,-6.1098633,-0.45677394,-2.0579565,19.448748,14.81091,-1.0171809,-0.94770503,1.7546761,0.97470456,1.0318189,1.3959125,1.2343087,0.9145495,1.1190593,1.067401,1.4195492,0.54359627,0.6622898,0.45171332,2.5148056,0.52654976,1.1493542,2.2011924,1.1349338,1.10833,0.5720888,1.6067965,0.7816648,0.66917163,0.6899175,0.43280017,-0.19615555,0.9109659,0.014249446,-0.36011907,-0.65139496,2.6350155,-0.7096736,-0.5588081,2.5635874,-0.49351496,1.4651796,-0.2988059,-0.4266612,0.33734858,-0.2598797,-0.3471602,-0.37989584,0.68418413,1.0544175,0.772585,0.9662768,1.4064375,3.9108818,0.19939734,1.0964643,0.36107215,0.49837515,0.46965614,2.5291078,6.4148216,1.0806079,-2.704912,1.7765329,2.0041363,1.8410529,1.6005865,2.675259,2.419064,-0.4776452,1.7208973,4.386169,-6.5753083,-6.235374,-7.0776734,3.971395,3.7028937,4.0843754,3.9049737,3.90397,4.189257,3.7278755,3.8880143,3.8610592,0.05443772,0.17222504,-0.12462806,3.4870803,3.5161781,3.4744205,0.09745094,0.6289064,1.2393382,3.4297595,9.846324,7.9666853,8.147314,8.0354395,-2.599152,-3.6987312,3.4797025,3.6352768,3.4257329,3.526423,3.5440106,3.515813,-1.0874083,2.468311,6.405294,0.31488988,6.6773934,-1.8188622,7.491386,7.213624,7.4305844,-6.916916,5.211097,-5.6744266,-5.933685,-6.4989834,0.44575495,0.28722042,1.7768027,1.13794,1.359763,1.7292488,1.1714318,2.0495362,1.7252623,0.42132485,2.4677927,2.9481015,2.3845744,2.0449388,3.9076881,3.8169358,3.8328543,1.0113928,1.1049522,1.442451,1.1192678,4.417324,5.024871,8.159667,7.949039,4.0050864,2.320636,2.225427,-10.568042,2.1731849,-11.688555,-6.0147963,-2.745127,1.1252987,-0.4269019,-1.7199657,-2.4033277,-1.0314862,0.85645074,2.4543319,-1.4999398,-1.4083955,-2.4767973,4.623513,4.342432,4.5700393,4.319252,-12.520354,-7.901034,2.302208,3.090825,6.5554733,3.8619149,9.58505,5.719569,5.8855205,1.0770983,1.3469647,1.0879534,0.9423742,1.5132978,1.6693285,-5.8321257,-5.0605674,-2.690583,0.50201386,-0.71987545,-0.29342797,-2.2765722,-2.4149902,-2.6669798,-2.7567077,-2.6596346,-2.8354108,-2.5675974,-2.214357,8.552687,-4.0338664,-0.20887063,-0.5182485,-1.5581127,-1.9544977,-0.13586837,-2.6653066,-4.4630995,2.5563793,-10.910292,-11.352885,3.2390783,-1.0796038,3.6458058,3.8188648,3.3870366,3.6843214,2.1258888,2.0542898,2.7051105,2.4822419,0.6567402,8.549762,-6.167937,2.3548274,-2.1712623,3.3867161,-1.195044,-2.1707277,2.0787163,0.20182833,0.35071605,2.6923492,1.4438546,1.3992605,6.5619206,1.6869267,-2.0735915,-1.5057639,-1.6957579,-6.0049787,-5.557761,4.150511,-5.1655045,-5.698538,-4.6504135,-4.873099,3.1935601,3.0150237,3.043098,3.1288426,3.1571593,3.0961497,3.3461707,-0.21698277,3.878341,4.225157,3.7348359,3.512599,3.2000344,4.280068,3.2514858,2.3164032,2.9650836,2.2527726,2.3965406,-4.2084856,-4.308001,-3.701191,-4.24801,0.74676675,-0.79783136,0.31248766,0.2052979,0.81122166,-0.83788335,0.9306508,-9.790074,-0.44459024,0.33961397,-9.843659,-9.602791,-9.641965,5.1150546,1.7648567,1.6179104,3.9086492,-8.271127,-0.010917227,0.10433176,-0.65168697,1.0227098,6.916248,6.945196,2.4346297,-1.0656118,6.890494,6.8830147,7.010212,6.933876,6.110612,-0.6658618,6.814165,6.274677,6.551746,-0.4131015,1.7076368,0.3446833,1.6961154,5.58851,3.9285023,2.0673616,5.7091503,5.5736895,4.6875043,2.9391332,5.367592,5.158522,3.513145,5.5027475,5.39333,3.0707839,2.851783,2.7766373,2.7503996,4.616123,4.793779,5.1020737,4.857216,7.0135703,7.619387,7.621486,7.3173103,7.5170455,7.45101,6.0063233,2.4713066,0.5006824,1.5086465,7.100847,4.2114463,3.7133036,5.251209,0.45971665,0.23210926,0.18790938,1.6055355,4.451386,3.7803013,0.5320199,2.3411703,5.188527,-1.499125,1.8282976,5.6639466,0.27383715,5.2916265,5.5007606,0.4708701,2.1175847,5.054956,-1.4650571,-0.4870151,-0.05439012,-1.5246104,-1.0623698,-1.3511227,2.3871717,2.5519903,-12.0782795,6.525271,6.6306515,6.7446213,-1.1602435,-1.1849235,7.3385878,-6.7329392,-7.136452,-7.0493317,-7.021193,-6.388478,-6.997046,0.5707916,3.3176053,3.0991888,0.11346047,-0.06019084,-0.738956,-6.2506227,-6.23179,4.162875,-0.8061004,-0.72782165,4.204161,-6.3883376,-6.4698443,-6.45709,3.1127884,3.277767,2.3772914,4.8592725,0.6399545,-11.112543,-10.846378,-10.325221,-11.419354,-11.162643,-11.492661,-11.202916,-10.441886,-11.39368,-11.281165,-11.022444,-10.995539,-11.606053,-11.8474045,-10.4738035,-10.92918,0.9925029,-6.06687,-5.6442313,-4.8347154,-3.7059503,-2.2337384,-1.7808721,-2.456264,-2.5036616,-2.2248383,-0.73340124,-0.8115223,1.929322,1.5038775,4.1198387,-1.4196051,-1.5851127,-1.5434902,3.6517446,3.402352,1.5638802,-2.3678896,3.2879007,5.6839514,5.5035963,5.5887403,2.811106,3.450018,1.5350306,1.5827616,3.5831487,7.1598725,0.3931424,4.451087,1.3801166,2.5285053,1.255961,1.2178223,2.2009602,1.162407,-4.4364886,-4.7711663,-4.513812,-4.9297132,-4.663783,-4.4376345,0.60951144,-4.8628144,-4.840011,2.8161047,3.9596176,3.3720355,3.985391,4.1663,3.899646,2.6166933,2.754524,5.5696974,1.7991943,-8.59452,-7.6092234,-7.5648246,-7.3700466,-4.7439113,-4.7477427,-8.257403,4.4928803,5.291874,2.7385314,1.0465122,-0.113160886,2.0685866,-0.47117615,2.356032,7.33527,7.4719543,-1.1599158,-6.2942905,1.3812855,1.7847705,2.6525342,3.0786338,1.4600682,1.3953338,1.6384125,1.7569249,2.313926,0.8769225,0.2929233,-7.019666,-6.8132424,-7.6915393,-7.390845,-7.3915277,-6.8389826,-7.113956,-7.429943,-7.050718,4.021597,-11.387526,1.2638097,2.4315968,2.7869577,6.623227,3.0534759,2.6301324,2.920618,2.4508748,4.2534733,0.7122279,1.5491837,0.6125985,2.8663208,3.9356344,-0.037737165,-0.0963633,2.7465143,2.7983925,-5.5087214,-3.7720323,-1.2999188,-1.6318855,17.954733,4.248125,4.350482,-0.53674436,0.16148795,2.6740122,0.5752654,2.6693196,2.8395555,3.4532464,-0.5941088,3.225413,3.583415,2.691753,-5.8520274,3.0614924,2.5936365,2.7568023,-0.09513203,3.78779,-1.741491,2.3189116,2.141755,2.723612,2.7647583,2.411729,1.897564,2.3669786,2.0143597,3.0396993,2.460124,1.6101447,1.6082373,0.35686567,0.71962243,-0.28241676,0.019923143,-18.579536,-18.302206,-18.138834,-17.412596,-18.274916,-17.124537,-18.170452,-18.30173,4.3781466,2.9040537,3.0058198,3.6408372,0.8069746,3.318976,-12.5773735,1.0397736,1.34574,1.3053302,1.8016247,1.0386145,1.1294316,1.6492399,2.3293505,0.8866615,1.8476001,1.3746024,1.72961,1.941877,1.5608685,1.2547053,-4.6726737,-5.7030663,-0.25583708,-0.12902638,-0.32542658,-6.0793495,-0.6417198,0.6470407,-6.2825623,7.9452453,8.096728,8.383282,1.846999,3.9610457,4.08532,3.1980247,3.1128585,5.961834,-1.0436076,0.30271426,-11.445251,-11.729322,2.3835957,8.128319,6.715735,2.041905,2.3358946,1.9816135,1.7899613,1.5573459,4.3274193,4.1309705,4.288518,3.5586104,2.9218888,3.097913,3.4232192,0.18142056,-0.13717951,0.21244924,3.5281415,-7.4718103,-5.845012,-0.27709845,-0.023572782,-0.42459685,-0.15451899,0.043994613,-0.3980282,-0.64620894,-0.04828612,-2.1140525,8.553587,-0.99269134,-3.8225737,0.8290266,7.384979,8.405374,8.699745,8.042552,8.534198,8.714278,8.549648,8.590829,2.907103,-10.423648,-10.380051,-9.9761715,-10.217137,-10.32087,-1.1100606,-0.4466861,-0.8662115,0.09050131,3.9095173,5.169721,3.745418,0.585367,0.84661585,0.90853137,0.86943144,0.9327279,3.0266242,3.0433173,-12.088836,8.553255,-4.0948806,4.9577026,-0.37557328,2.4751549,2.5630627,2.8100064,2.7170274,2.5421293,2.8145466,2.5654914,2.554275,2.4103217,2.5264838,2.819744,2.4889822,-0.39646092,-6.864454,-6.7354746,-7.1095304,-7.0980387,0.102277644,0.15677536,-6.783593,-6.6417174,-0.4853838,-0.5424079,-0.18275866,-0.42458716,-0.4454816,-6.0904965,-6.338,-0.81923145,3.0523338,-6.2200384,-6.114203,2.8299453,-0.6857935,-0.99471587,3.5943751,3.8457215,3.6521597,5.397268,4.100944,4.345239,-2.2660244,19.452597,5.275224,8.197222,8.194227,8.210205,1.5167458,-1.684149,2.6269538,2.9821153,2.957739,-0.058013722,0.5173913,0.5575393,-0.013187825,-3.810567,-5.6374044,-4.6767783,-5.0807786,-1.828247,-1.9570688,-0.3273964,-1.253801,-1.1454147,4.0366044,4.281967,-10.984634,-11.022399,4.260112,2.3075478,2.7828262,2.8986397,-1.4343748,-1.0023991,0.7253069,-1.2054616,3.5693278,2.7126412,8.55344,-4.1916523,-4.2611947,3.1997335,3.1630096,2.357078,4.1146407,4.284985,3.2883961,-0.33370578,0.43969437,0.14924632,2.3309479,2.4781518,-0.06529152,0.011032416,-0.007215259,0.15217118,-0.052577272,-1.8854871,-2.1002486,-2.2624419,-2.1645293,-2.0202107,-1.9941692,-2.008934,-2.1219,4.3577857,6.5329266,-1.6065319,4.082249,1.7310804,1.6398716,1.5501186,1.6758718,1.0953354,-2.8422172,-2.794104,-0.49406427,-0.5130394,8.385292,8.465942,7.4964724,3.0370822,3.0148418,4.2411427,0.8491894,-10.072527,6.438681,-1.0777836,0.24044368,-6.522676,-7.281387,-11.978918,-2.122149,19.449787,14.811553,4.780831,3.357586,5.364085,0.89350665,1.3446962,-1.9482967,13.704477,13.729366,13.662058,1.3502519,13.724258,0.29358748,13.724537,13.7307005,13.7253275,13.714776,13.736454,5.378052,13.691984,-1.0758234,-1.0673629,-1.1320252,-1.0697806,-1.081482,-1.0805683,-1.1143632,-11.762873,-10.155153,-6.862098,-6.652376,-8.568458,2.612654,3.1225865,3.6574285,4.6598845,3.00546,0.57342917,0.55931896,0.8128004,0.5615088,3.667936,3.759935,3.7081642,2.8567903,6.412593,6.185967,6.08035,3.3217123,6.2070026,5.2522454,-7.8482795,-8.340583,-8.275563,3.3988428,2.703715,2.088466,2.4491599,2.0758233,2.624299,2.1822965,0.9269763,1.949142,-0.1736684,2.4351668,-2.1428292,8.554257,-4.247249,-4.307304,-4.333848,-4.3607736,-4.2300477,-4.232114,-4.2824945,-4.364616,3.8939126,4.222965,4.7405314,4.998049,4.8895965,5.9548755,6.103627,-7.87716,-7.571057,-8.55872,-7.960342,3.6511621,-1.9578316,8.553302,-7.4350886,-7.3826118,5.372884,4.5909815,4.1657624,3.082454,1.16274,3.9205706,3.8256483,3.9486241,4.173531,3.8232555,3.1634948,3.7131138,3.7584972,3.85096,4.0645814,3.772651,1.758067,0.9426863,0.35930467,2.3415396,2.311775,2.3330421,-12.27475,-10.509461,-1.0168581,-1.3593146,-1.2712666,-1.5347914,-1.2765375,-0.38035825,-3.8660724,-4.8945107,-3.2357554,-5.1790876,-4.6267157,-10.048577,-10.433253,-6.168178,1.2326932,1.1271894,1.9138542,4.4026623,1.121636,-6.090482,4.322281,4.572145,3.3145478,-0.18562414,2.7156396,9.816237,6.6831264,6.0559244,-2.0605764,19.449198,2.4124954,2.658523,2.7840574,2.8640218,3.4492316,2.983307,-2.169039,-3.0680702,-0.69939685,-2.1364262,8.554364,2.396773,1.7180896,0.9978069,2.631271,3.5838351,3.6575975,2.6199846,1.9660331,2.157847,2.1544793,2.1531632,2.1569257,2.416007,0.3156063,0.7266139,-6.4159226,2.3855448,4.505751,6.924644,6.3111157,0.90405214,-1.0740732,0.76000345],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Git over SSH\\n\\nYou can access and write data in repositories on huggingface.co using SSH (Secure Shel...\"],[\"If you chose a different location than the default to store your SSH key, you would have to replace ...\"],[\"Using Flair at Hugging Face\\n\\n[Flair](https:\\u002f\\u002fgithub.com\\u002fflairNLP\\u002fflair) is a very simple framework f...\"],[\"If you want to load a specific Flair model, you can click `Use in Flair` in the model card and you w...\"],[\"Widget Examples\\n\\nNote that each widget example can also optionally describe the corresponding model ...\"],[\"### Summarization\\n\\n```yaml\\nwidget:\\n- text: \\\"The tower is 324 metres (1,063 ft) tall, about the same ...\"],[\"### Text Classification\\n\\n```yaml\\nwidget:\\n- text: \\\"I love football so much\\\"\\n  example_title: \\\"Positiv...\"],[\"```yaml\\nwidget:\\n- text: \\\"Hey my name is Julien! How are you?\\\"\\n  example_title: \\\"Julien\\\"\\n- text: \\\"Hey...\"],[\"### Object Detection\\n\\n```yaml\\nwidget:\\n- src: https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmishig\\u002fsample_images\\u002fre...\"],[\"## Other\\n\\n### Structured Data Classification\\n\\n```yaml\\nwidget:\\n- structured_data:\\n    fixed_acidity:\\n...\"],[\"Configure the Dataset Viewer\\n\\nThe Dataset Viewer supports many [data files formats](.\\u002fdatasets-addin...\"],[\"Adding a Sign-In with HF button to your Space\\n\\nYou can enable a built-in sign-in flow in your Space ...\"],[\"This will add the following [environment variables](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fspaces-overview#...\"],[\"## Adding the button to your Space\\n\\nYou now have all the information to add a \\\"Sign-in with HF\\\" butt...\"],[\"User access tokens\\n\\n## What are User Access Tokens?\\n\\nUser Access Tokens are the preferred way to aut...\"],[\"## How to manage User Access Tokens?\\n\\nTo create an access token, go to your settings, then click on ...\"],[\"\\u003cTip warning={true}\\u003e\\nTry not to leak your token! Though you can always rotate it, anyone will be abl...\"],[\"ZenML on Spaces\\n\\n[ZenML](https:\\u002f\\u002fgithub.com\\u002fzenml-io\\u002fzenml) is an extensible, open-source MLOps fram...\"],[\"![ZenML on HuggingFace Spaces -- default deployment](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"![Choose the ZenML Docker template](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"Once you have your ZenML server up and running, you can connect to it from your\\nlocal machine. To do...\"],[\"\\u003cTip\\u003e\\nIf you are using the space just for testing and experimentation, you don't need\\nto make any ch...\"],[\"## Upgrading your ZenML Server on HF Spaces\\n\\nThe default space will use the latest version of ZenML ...\"],[\"Using Spaces for Organization Cards\\n\\nOrganization cards are a way to describe your organization to o...\"],[\"Repository Settings \\n\\n## Private repositories\\n\\nYou can choose a repository's visibility when you cre...\"],[\"If these are use cases you need help with, please send us an email at **website at huggingface.co**....\"],[\"--\\n# Example metadata to be added to a dataset card.  \\n# Full dataset card template at https:\\u002f\\u002fgithu...\"],[\"configs:  # Optional for datasets with multiple configurations like glue.\\n- {config_0}  # Example fo...\"],[\"# Optional. This part can be used to store the feature types and size of the dataset to be used in p...\"],[\"# It can also be a list of multiple configurations:\\n# ```yaml\\n# dataset_info:\\n#   - config_name: {co...\"],[\"# Optional. Add this if you want to encode a train and evaluation info in a structured way for AutoT...\"],[\"Repository limitations and recommendations\\n\\nThere are some limitations to be aware of when dealing w...\"],[\"Under the hood, the Hub uses Git to version the data, which has structural implications on what you ...\"],[\"- **Repository size**: The total size of the data you're planning to upload. There is no hard limit ...\"],[\"our experience, the user experience on the Hub starts to degrade after a few thousand commits. We ar...\"],[\"Dask\\n\\n[Dask](https:\\u002f\\u002fgithub.com\\u002fdask\\u002fdask) is a parallel and distributed computing library that scal...\"],[\"For more information on the Hugging Face paths and how they are implemented, please refer to the [th...\"],[\"Access control in organizations\\n\\n\\u003cTip\\u003e\\n\\nYou can set up [Single Sign-On (SSO)](.\\u002fsecurity-sso) to be ...\"],[\"Billing\\n\\nAt Hugging Face, we build a collaboration platform for the ML community (i.e., the Hub), an...\"],[\"## Invoicing\\n\\nHugging Face paid services are billed in arrears, meaning you get charged for monthly ...\"],[\"Streamlit Spaces\\n\\n**Streamlit** gives users freedom to build a full-featured web app with Python in ...\"],[\"## Your First Streamlit Space: Hot Dog Classifier\\n\\nIn the following sections, you'll learn the basic...\"],[\"st.title(\\\"Hot Dog? Or Not?\\\")\\n\\nfile_name = st.file_uploader(\\\"Upload a hot dog candidate image\\\")\\n\\nif f...\"],[\"```\\n\\u003ciframe\\n  src=\\\"https:\\u002f\\u002fNimaBoscarino-hotdog-streamlit.hf.space?embed=true\\\"\\n  title=\\\"My awesome S...\"],[\"We can pass options to the first argument of `iFrameResize()`. See [the document](https:\\u002f\\u002fgithub.com...\"],[\"Next Steps\\n\\nThese next sections highlight features and additional information that you may find usef...\"],[\"Beyond making it easy to identify important commits in your repo's history, using Git tags also allo...\"],[\"For example, say you have an upstream repository, **upstream**, and you just created your own reposi...\"],[\"Run with Docker\\n\\nYou can use Docker to run most Spaces locally.\\nTo view instructions to download and...\"],[\"Advanced Topics\\n\\n## Contents\\n\\n- [Using OpenCV in Spaces](.\\u002fspaces-using-opencv)\\n- [More ways to crea...\"],[\"Using spaCy at Hugging Face\\n\\n`spaCy` is a popular library for advanced Natural Language Processing u...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"To push with the CLI, you can use the `huggingface-hub push` command as seen below.\\n\\n```bash\\npython ...\"],[\"In just a minute, you can get your packaged model in the Hub, try it out directly in the browser, an...\"],[\"Audit Logs\\n\\n\\u003cTip warning={true}\\u003e\\nThis feature is part of the \\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fenterpr...\"],[\"Spaces Overview\\n\\nHugging Face Spaces make it easy for you to create and deploy ML-powered demos in m...\"],[\"For step-by-step tutorials to creating your first Space, see the guides below:\\n* [Creating a Gradio ...\"],[\"| **Hardware**        \\t| **GPU Memory** \\t| **CPU** \\t| **Memory** \\t| **Disk** \\t| **Hourly Price** \\t|\\n...\"],[\"Note: Find more detailed and comprehensive pricing information on [our pricing page](https:\\u002f\\u002fhugging...\"],[\"Secrets are private and their value cannot be retrieved once set. They won't be added to Spaces dupl...\"],[\"Some Spaces might have environment variables that you may need to set up. In these cases, the duplic...\"],[\"In case [OAuth](.\\u002fspaces-oauth) is enabled for your Space, the following variables will also be avai...\"],[\"## Linking Models and Datasets on the Hub\\n\\nYou can showcase all the models and datasets that your Sp...\"],[\"Search\\n\\nYou can now easily search anything on the Hub with **Full-text search**. We index model card...\"],[\"[paddlenlp-banner](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub...\"],[\"`paddlenlp` offers a quick one-line install through pip:\\n\\n```\\npip install -U paddlenlp\\n```\\n\\n## Using...\"],[\"tokenizer.save_to_hf_hub(repo_id=\\\"\\u003cmy_org_name\\u003e\\u002f\\u003cmy_repo_name\\u003e\\\")\\nmodel.save_to_hf_hub(repo_id=\\\"\\u003cmy_o...\"],[\"Reference\\n\\n## Deep Learning Container\\n\\nBelow you can find a version table of currently available Hug...\"],[\"| 🤗 Transformers version | 🤗 Datasets version | PyTorch\\u002fTensorFlow version | type     | device | Pyt...\"],[\"| 4.12.3                  | 1.15.1              | TensorFlow 2.5.1           | training | GPU    | 3...\"],[\"## Inference DLC Overview\\n\\nThe Inference DLC overview includes all released and available Hugging Fa...\"],[\"| 🤗 Transformers version | PyTorch\\u002fTensorFlow version | type      | device | Python Version |\\n| ----...\"],[\"| 4.12.3                  | TensorFlow 2.5.1           | inference | GPU    | 3.7            |\\n| 4.1...\"],[\"## Hugging Face Transformers Amazon SageMaker Examples\\n\\nExample Jupyter notebooks that demonstrate h...\"],[\"| Notebook                                                                                          ...\"],[\"| [05 How to use Spot Instances & Checkpointing](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002f...\"],[\"| [12 Batch Processing with Amazon SageMaker Batch Transform](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"## Inference Toolkit API\\n\\nThe Inference Toolkit accepts inputs in the `inputs` key, and supports add...\"],[\"**`table-question-answering`**\\n\\n```json\\n{\\n  \\\"inputs\\\": {\\n    \\\"query\\\": \\\"How many stars does the transf...\"],[\"**`HF_MODEL_REVISION`**\\n\\n`HF_MODEL_REVISION` is an extension to `HF_MODEL_ID` and allows you to defi...\"],[\"Pandas\\n\\n[Pandas](https:\\u002f\\u002fgithub.com\\u002fpandas-dev\\u002fpandas) is a widely used Python data analysis toolkit...\"],[\"To have more information on the Hugging Face paths and how they are implemented, please refer to the...\"],[\"Datasets without language challenge\\n\\nRelated to https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fissues\\u002f986.\\n...\"],[\"1. Find a dataset that doesn't have the `language` field filled in. You can find a list of datasets ...\"],[\"4. Once you've identified the language(s) of the dataset, you can add the language tag(s) to the dat...\"],[\"## F.A.Q.\\n\\n### Does it make sense to add language metadata to all datasets?\\n\\nNo! This is why we have...\"],[\"| status | pr_url                                                                                   ...\"],[\"|        |[here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggan\\u002fwikiart\\u002fdiscussions\\u002f3)                      ...\"],[\"|        |                                                                                          ...\"],[\"|        |                                                                                          ...\"],[\"|        |  [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvivym\\u002fmidjourney-prompts\\u002fdiscussions\\u002f1)          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fiamtarun\\u002fcode_instructions_120k_alpaca\\u002fdiscussions...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ffathyshalab\\u002fDialogsum-german-kurz\\u002fdiscussions\\u002f1)  ...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002frizerphe\\u002fsharegpt-hyperfiltered-3k-llama\\u002fdiscussio...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvhtran\\u002funiq-de-en\\u002fdiscussions\\u002f1#651abb5e2bc734f0fa...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fPhotolens\\u002fDISC-Med-SFT-en-translated-only-CMeKG\\u002fdi...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fkmkarakaya\\u002fturkishReviews-ds\\u002fdiscussions\\u002f1#651ae84...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvhtran\\u002funiq-id-en\\u002fdiscussions\\u002f1#651ab8329e0bf1e7f8...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fIskaj\\u002fdutch_corpora_parliament_processed\\u002fdiscussio...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002froskoN\\u002fstereoset_german\\u002fdiscussions\\u002f1)            ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fstas\\u002fwmt16-en-ro-pre-processed\\u002fdiscussions\\u002f1#651ab...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fSuchinthana\\u002fDatabricks-Dolly-15k-si-en-mix\\u002fdiscuss...\"],[\"|        |                                                                                          ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftbboukhari\\u002fAlpaca-in-french\\u002fdiscussions\\u002f1)        ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fFreedomIntelligence\\u002falpaca-gpt4-french\\u002fdiscussions...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fthomasavare\\u002fitalian-dataset-deepl2\\u002fdiscussions\\u002f2) ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fserbog\\u002fjob_listing_german_cleaned\\u002fdiscussions\\u002f1)  ...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fthesistranslation\\u002fdistilled-ccmatrix-en-es\\u002fdiscuss...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fFreedomIntelligence\\u002falpaca-gpt4-italian\\u002fdiscussion...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002ft5-qe-2023-enmr-da-sys-test\\u002fdiscuss...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002ft5-qe-2023-enhi-da-test\\u002fdiscussions...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-engu-da-sys-test\\u002fdi...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enta-sys-test\\u002fdiscu...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdipteshkanojia\\u002fllama-2-qe-2023-enta-test\\u002fdiscussio...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fphi0108\\u002fdemo-noun-phrase-en\\u002fdiscussions\\u002f1#651ac6f8...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fflozi00\\u002foasst1-en-to-de\\u002fdiscussions\\u002f1#651ac67f655e...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-en-ru\\u002fdiscussions\\u002f1#651ac558bab32...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-en-pl\\u002fdiscussions\\u002f1#651ac4e2c69ca...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyezhengli9\\u002fwmt20-en-de\\u002fdiscussions\\u002f1#651ac4326e33b...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fshreevigneshs\\u002fiwslt-2023-en-ko-train-val-split-0.2...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fshreevigneshs\\u002fiwslt-2022-en-de\\u002fdiscussions\\u002f1#651ac...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fyogiyulianto\\u002ftwitter-sentiment-dataset-en\\u002fdiscussi...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fMakxxx\\u002ffrench_CEFR\\u002fdiscussions\\u002f1)                 ...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgollumeo\\u002ffrench-litterature\\u002fdiscussions\\u002f1)        ...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ffathyshalab\\u002fgermanquad_qaeval_dataset\\u002fdiscussions\\u002f...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgermank\\u002fhh-rlhf_with_features_flan_t5_large_lll_re...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fvolkanaltintas\\u002fturkishTradeReviews-ds-mini-4000\\u002fdi...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002forhanxakarsu\\u002fturkishPoe-generation-1\\u002fdiscussions\\u002f1...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fozz\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#651aeb55a...\"],[\"|        | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fkaaniince\\u002fturkishReviews-project\\u002fdiscussions\\u002f1#651...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fMursel\\u002fturkishReviews-ds-mini\\u002fdiscussions\\u002f1#651aec...\"],[\"| Merged | [here](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fPulsarAI\\u002fturkish_movie_sentiment\\u002fdiscussions\\u002f1#651...\"],[\"Advanced Topics\\n\\n## Contents\\n\\n- [Integrate your library with the Hub](.\\u002fmodels-adding-libraries)\\n- [...\"],[\"Storage Regions on the Hub\\n\\nRegions let you decide where your org's models and datasets will be stor...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub\\u002fstorage-region...\"],[\"Managing Spaces with Github Actions\\n\\nYou can keep your app in sync with your GitHub repository with ...\"],[\"```yaml\\nname: Check file size\\non:               # or directly `on: [push]` to run the action on ever...\"],[\"Webhook guide: Setup an automatic metadata quality review for models and datasets \\n\\n\\u003cTip\\u003e\\n\\nWebhooks ...\"],[\"Since the metadata defined in this block is essential for potential users of our models and datasets...\"],[\"```python\\nfrom huggingface_hub import DatasetCard, ModelCard\\nfrom huggingface_hub.utils import Entry...\"],[\"This function will return a dictionary containing keys representing the metadata fields we require f...\"],[\"## How to post the review automatically?\\n\\nWe now have a markdown formatted metadata review report. W...\"],[\"Your Webhook will look like this:\\n\\n![webhook settings](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fd...\"],[\"The above function will receive Webhook events and creates or updates the metadata review report for...\"],[\"🟧 Label Studio on Spaces\\n\\n[Label Studio](https:\\u002f\\u002flabelstud.io) is an [open-source data labeling\\nplat...\"],[\"\\u003cTip warning={true}\\u003e\\nStorage in Hugging Face Spaces is ephemeral, and the data you store in the defa...\"],[\"* `LABEL_STUDIO_USERNAME`: This is the username of the account that you will\\n  use as the first user...\"],[\"### Enable Cloud Storage\\n\\nBy default, the only data storage enabled for this Space is local. In the ...\"],[\"Webhooks\\n\\n\\u003cTip\\u003e\\n\\nWebhooks are now publicly available!\\n\\n\\u003c\\u002fTip\\u003e\\n\\nWebhooks are a foundation for MLOps-r...\"],[\"![image.png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub\\u002fwebho...\"],[\"It has two sub-properties: `event.action` and `event.scope`.\\n\\n`event.scope` will be one of the follo...\"],[\"`repo.headSha` is the sha of the latest commit on the repo's `main` branch. It is only sent when `ev...\"],[\"This can be helpful if accessing the HTTP headers of the request is complicated for your Webhook han...\"],[\"No, this is not currently supported.\\n\\n##### How can I subscribe to events on all repos (or across a ...\"],[\"Dataset viewer\\n\\nThe dataset page includes a table with the contents of the dataset, arranged by page...\"],[\"## Access the parquet files\\n\\nTo power the dataset viewer, every dataset is auto-converted to the Par...\"],[\"The Model Hub\\n\\n## What is the Model Hub?\\n\\nThe Model Hub is where the members of the Hugging Face com...\"],[\"Signing commits with GPG\\n\\n`git` has an authentication layer to control who can push commits to a rep...\"],[\"Commits can have the following signing statuses:\\n\\n| Status            | Explanation                 ...\"],[\"Copy & paste the output of the `gpg --export` command in the text area and click on **Add Key**.\\n\\n4....\"],[\"Spaces Changelog\\n\\n## [2023-07-28] - Upstream Streamlit frontend for `\\u003e=1.23.0`\\n\\n- Streamlit SDK uses...\"],[\"- All Spaces using Gradio 3+ and Streamlit 1.x.x have a significant speedup in loading.\\n- System the...\"],[\"## [2021-09-07] - Streamlit version pinning\\n\\n- You can now choose which version of Streamlit will be...\"],[\"Licenses\\n\\nYou are able to add a license to any repo that you create on the Hugging Face Hub to let o...\"],[\"\\u003c!-- region licenses --\\u003e\\nFullname | License identifier (to use in model card)\\n--- | ---\\nApache licen...\"],[\"Community Data License Agreement – Sharing, Version 1.0 | `cdla-sharing-1.0`\\nCommunity Data License ...\"],[\"In case of `license: other` please add the license's text to a `LICENSE` file inside your repo (or c...\"],[\"Hugging Face Hub documentation\\n\\nThe Hugging Face Hub is a platform with over 350k models, 75k datase...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-orange-100 bg-gradient-to-br from...\"],[\"\\u003ca class=\\\"transform !no-underline transition-colors hover:translate-x-px hover:text-gray-700\\\" href=\\\"...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-indigo-100 bg-gradient-to-br from...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fm...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-red-100 bg-gradient-to-br from-re...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fd...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-blue-100 bg-gradient-to-br from-b...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fs...\"],[\"\\u003cdiv class=\\\"group flex flex-col space-y-2 rounded-xl border border-green-100 bg-gradient-to-br from-...\"],[\"\\u003ca class=\\\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\\\" href=\\\".\\u002fs...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\n## What's the Hugging Face Hub?\\n\\nWe are helping the community work together towards the goal...\"],[\"## Models\\n\\nYou can discover and use dozens of thousands of open-source ML models shared by the commu...\"],[\"The [🤗 `datasets`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002findex) library allows you to programmaticall...\"],[\"## Organizations\\n\\nCompanies, universities and non-profits are an essential part of the Hugging Face ...\"],[\"Using ESPnet at Hugging Face\\n\\n`espnet` is an end-to-end toolkit for speech processing, including aut...\"],[\"text2speech = Text2Speech.from_pretrained(\\\"model_name\\\")\\nspeech = text2speech(\\\"foobar\\\")[\\\"wav\\\"]\\nsoundf...\"],[\"Model Card components\\n\\n**Model Card Components** are special elements that you can inject directly i...\"],[\"Annotated Model Card Template\\n\\n\\n## Template\\n\\n[modelcard_template.md file](https:\\u002f\\u002fgithub.com\\u002fhugging...\"],[\"_Instructions are provided below, in italics._\\n\\nTemplate variable names appear in `monospace`.\\n\\n# Mo...\"],[\"* **Finetuned From Model [optional]:** `base_model`\\n\\n_If this model has another model as its base, l...\"],[\"`bias_recommendations`\\n\\n_What are recommendations with respect to the foreseeable issues? This can i...\"],[\"## Testing Data, Factors & Metrics\\n\\n### Testing Data\\n\\n`testing_data`\\n\\n_Ideally this links to a Datas...\"],[\"## Model Architecture and Objective\\n\\n`model_specs`\\n\\n## Compute Infrastructure\\n\\n`compute_infrastructu...\"],[\"Organizations\\n\\nThe Hugging Face Hub offers **Organizations**, which can be used to group accounts an...\"],[\"Using 🤗 Datasets\\n\\nOnce you've found an interesting dataset on the Hugging Face Hub, you can load the...\"],[\"Appendix\\n\\n## Appendix A: User Study\\n_Full text responses to key questions_\\n\\n### How would you define...\"],[\"* Model cards are model descriptions, both of how they were trained, their use cases, and potential ...\"],[\"### What do you like about model cards?\\n\\n* They are interesting to teach people about new models\\n* A...\"],[\"### What do you dislike about model cards?\\n\\n* Might get to technical and\\u002for dense\\n* \\u003cmark \\u003eThey cont...\"],[\"## Appendix B: Landscape Analysis\\n_Overview of the state of model documentation in Machine Learning_...\"],[\"### MODEL CARDS FOR LARGE LANGUAGE MODELS\\nLarge language models are often released with associated d...\"],[\"Notifications\\n\\nNotifications allow you to know when new activities (Pull Requests or discussions) ha...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"![Notifications settings page](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"How to configure SAML SSO with Azure\\n\\nIn this guide, we will use Azure as the SSO provider and with ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"- Login Url -\\u003e Sign-on URL\\n- Certificate -\\u003e Public certificate\\n\\nThe public certificate must have the...\"],[\"Gradio Spaces\\n\\n**Gradio** provides an easy and intuitive interface for running a model from a list o...\"],[\"## Add the dependencies\\n\\nFor the **Hot Dog Classifier** we'll be using a [🤗 Transformers pipeline](h...\"],[\"## Embed Gradio Spaces on other webpages\\n\\nYou can embed a Gradio Space on other webpages by using ei...\"],[\"Cookie limitations in Spaces\\n\\nIn Hugging Face Spaces, applications have certain limitations when usi...\"],[\"Argilla on Spaces\\n\\n**Argilla** is an open-source, data labelling tool, for highly efficient human-in...\"],[\"You need to define the **Owner** (your personal account or an organization), a **Space name**, and t...\"],[\"The usernames, passwords, and API keys to upload, read, update, and delete datasets can be configure...\"],[\"The combination of these secret variables gives you the following setup options:\\n\\n1. *I want to avoi...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentatio...\"],[\"Fourth, you need to init the `argilla` client with your Space URL and API key and upload the records...\"],[\"# Train and evaluate\\ntrainer.train()\\nmetrics = trainer.evaluate()\\n```\\n\\nOptionally, you can push the ...\"],[\"Using Stable-Baselines3 at Hugging Face\\n\\n`stable-baselines3` is a set of reliable implementations of...\"],[\"You need to define seven parameters:\\n- `--model`: your trained model.\\n- `--model_architecture`: name...\"],[\"File names and splits\\n\\nTo host and share your dataset, create a dataset repository on the Hugging Fa...\"],[\"For example, the following file names are all acceptable:\\n\\n- train split: `train.csv`, `my_train_fil...\"],[\"### Custom split name\\n\\nIf your dataset splits have custom names that aren't `train`, `test`, or `val...\"],[\"Integrate your library with the Hub\\n\\nThe Hugging Face Hub aims to facilitate sharing machine learnin...\"],[\"2. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging Face a...\"],[\"For example, download the `config.json` file from the [lysandre\\u002farxiv-nlp](https:\\u002f\\u002fhuggingface.co\\u002fly...\"],[\"```typescript\\nconst asteroid = (model: ModelData) =\\u003e\\n`from asteroid.models import BaseModel\\n  \\nmodel...\"],[\"If you need to upload more than one file, look at the [utilities offered by the `Repository` class](...\"],[\"```python\\n       ALLOWED_TASKS: Dict[str, Type[Pipeline]] = {\\n           \\\"token-classification\\\": Tok...\"],[\"With these simple but powerful methods, you brought the full functionality of the Hub into your libr...\"],[\"Aim on Spaces\\n\\n**Aim** is an easy-to-use & supercharged open-source experiment tracker. Aim logs you...\"],[\"# Track weights and gradients distributions\\ntrack_params_dists(model, aim_run)\\ntrack_gradients_dists...\"],[\"# Model `license:other` challenge\\n\\nRelated to https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fissues\\u002f985.\\n\\n#...\"],[\"This challenge aims to improve the completeness of this metadata on the Hub, which will ultimately b...\"],[\"For each model, the workflow looks like this:\\n1. Choose a model in the list below. We suggest focusi...\"],[\"### How do I choose `license_name`?\\n\\nThere is no clear answer for that. You can either check other r...\"],[\"|status|pr_url                                                                      |model_id|nb_dow...\"],[\"|Opened|[HUB_PR](https:\\u002f\\u002fhuggingface.co\\u002fhuggyllama\\u002fllama-7b\\u002fdiscussions\\u002f7)|[huggyllama\\u002fllama-7b](htt...\"],[\"|Opened|[hub_pr](https:\\u002f\\u002fhuggingface.co\\u002felinas\\u002fchronos-13b-v2\\u002fdiscussions\\u002f3) |[elinas\\u002fchronos-13b-v2...\"],[\"|Opened|[HUB_PR](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fOpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ\\u002fdiscu...\"],[\"|      |                                                                            |[decapoda-resea...\"],[\"|      |                                                                            |[huggyllama\\u002flla...\"],[\"|      |                                                                            |[TheBloke\\u002fLlama...\"],[\"|Opened|[HUB_PR](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama-2-7B-Chat-GGML\\u002fdiscussions\\u002f33)   |[TheBloke\\u002fL...\"],[\"|Opened|[HUB_PR](https:\\u002f\\u002fhuggingface.co\\u002fhuggyllama\\u002fllama-30b\\u002fdiscussions\\u002f1)                         ...\"],[\"|Opened|[HUB_PR](https:\\u002f\\u002fhuggingface.co\\u002fsambanovasystems\\u002fBLOOMChat-176B-v1\\u002fdiscussions\\u002f9)           ...\"],[\"|      |                                                                            |[decapoda-resea...\"],[\"|      |                                                                            |[georgesung\\u002flla...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[shibing624\\u002fchi...\"],[\"|      |                                                                            |[h2oai\\u002fh2ogpt-r...\"],[\"|      |                                                                            |[TheBloke\\u002fKimik...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[TheBloke\\u002fLlama...\"],[\"|      |                                                                            |[valurank\\u002fMiniL...\"],[\"|      |                                                                            |[Enoch\\u002fllama-65...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[4bit\\u002fLlama-2-7...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[aipicasso\\u002fcool...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenA...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[aipicasso\\u002fcool...\"],[\"|      |                                                                            |[TheBloke\\u002fZarab...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[shalomma\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[valurank\\u002fdisti...\"],[\"|      |                                                                            |[valurank\\u002fdisti...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenC...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[valurank\\u002fdisti...\"],[\"|      |                                                                            |[valurank\\u002fdisti...\"],[\"|      |                                                                            |[TheBloke\\u002fZaraf...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[Neko-Institute...\"],[\"|      |                                                                            |[aleksickx\\u002fllam...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[Enoch\\u002fllama-7b...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fHerme...\"],[\"|      |                                                                            |[Neko-Institute...\"],[\"|      |                                                                            |[TheBloke\\u002fZarab...\"],[\"|      |                                                                            |[valurank\\u002ffinet...\"],[\"|      |                                                                            |[coqui\\u002fXTTS-v1]...\"],[\"|      |                                                                            |[elinas\\u002fchronos...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenA...\"],[\"|      |                                                                            |[alfredplpl\\u002funl...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[dfurman\\u002fllama-...\"],[\"|      |                                                                            |[massh3dpotato\\u002f...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[ehartford\\u002fsama...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002fqCamm...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[TheBloke\\u002fqCamm...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[valurank\\u002fdisti...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[TheBloke\\u002fLlama...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[TheBloke\\u002fCodeF...\"],[\"|      |                                                                            |[alfredplpl\\u002funl...\"],[\"|      |                                                                            |[TheBloke\\u002fsqlco...\"],[\"|      |                                                                            |[TheBloke\\u002fKuchi...\"],[\"|      |                                                                            |[yswill\\u002fllama-1...\"],[\"|      |                                                                            |[luodian\\u002fllama-...\"],[\"|      |                                                                            |[nonlinearshima...\"],[\"|      |                                                                            |[turboderp\\u002fLlam...\"],[\"|      |                                                                            |[TheBloke\\u002fChron...\"],[\"|      |                                                                            |[TheBloke\\u002fllama...\"],[\"|      |                                                                            |[4bit\\u002fllama-13b...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[zohaib99k\\u002fNous...\"],[\"|      |                                                                            |[pankaj-munde\\u002fl...\"],[\"|      |                                                                            |[TheBloke\\u002fLlama...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002fTulu-...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenA...\"],[\"|      |                                                                            |[valurank\\u002fdisti...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fft-ro...\"],[\"|      |                                                                            |[Jsevisal\\u002frober...\"],[\"|      |                                                                            |[nenkoru\\u002falpaca...\"],[\"|      |                                                                            |[research-rabbi...\"],[\"|      |                                                                            |[TheBloke\\u002fKimik...\"],[\"|      |                                                                            |[TheBloke\\u002fOpenC...\"],[\"|      |                                                                            |[TheBloke\\u002fqCamm...\"],[\"|      |                                                                            |[valurank\\u002fdisti...\"],[\"|      |                                                                            |[Agtian\\u002fllama-6...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Neko-Institute...\"],[\"|      |                                                                            |[shekharchatter...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fChron...\"],[\"|      |                                                                            |[TheBloke\\u002fLlama...\"],[\"|      |                                                                            |[TheBloke\\u002fTulu-...\"],[\"|      |                                                                            |[4bit\\u002fRedmond-P...\"],[\"|      |                                                                            |[agonh\\u002fLlama-2-...\"],[\"|      |                                                                            |[Agtian\\u002fllama-3...\"],[\"|      |                                                                            |[AI-Engine\\u002fMyth...\"],[\"|      |                                                                            |[alfredplpl\\u002funt...\"],[\"|      |                                                                            |[amdnsr\\u002fllama-7...\"],[\"|      |                                                                            |[aoyoo\\u002fllama-7b...\"],[\"|      |                                                                            |[ashi-ta\\u002fjapane...\"],[\"|      |                                                                            |[baffo32\\u002fllama-...\"],[\"|      |                                                                            |[bluefoxcreatio...\"],[\"|      |                                                                            |[camelids\\u002falpac...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[camelids\\u002fllama...\"],[\"|      |                                                                            |[cekal\\u002fLLaMA-7B...\"],[\"|      |                                                                            |[decapoda-resea...\"],[\"|      |                                                                            |[decapoda-resea...\"],[\"|      |                                                                            |[decapoda-resea...\"],[\"|      |                                                                            |[deepsbn\\u002fllama-...\"],[\"|      |                                                                            |[dontito\\u002fllama-...\"],[\"|      |                                                                            |[fragro\\u002fllama-7...\"],[\"|      |                                                                            |[heegyu\\u002fLIMA-13...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fbalan...\"],[\"|      |                                                                            |[Jsevisal\\u002fbert-...\"],[\"|      |                                                                            |[keyfan\\u002fvicuna-...\"],[\"|      |                                                                            |[khachdallak\\u002fll...\"],[\"|      |                                                                            |[lyogavin\\u002fAnima...\"],[\"|      |                                                                            |[muneerhanif7\\u002fL...\"],[\"|      |                                                                            |[nonlinearshima...\"],[\"|      |                                                                            |[openerotica\\u002fLl...\"],[\"|      |                                                                            |[perfectlyunrea...\"],[\"|      |                                                                            |[prodm93\\u002fllama_...\"],[\"|      |                                                                            |[prodm93\\u002fllama_...\"],[\"|      |                                                                            |[ruibin-wang\\u002fll...\"],[\"|      |                                                                            |[SerrasKowalsky...\"],[\"|      |                                                                            |[shekharchatter...\"],[\"|      |                                                                            |[shirayu\\u002fsd-toh...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fairob...\"],[\"|      |                                                                            |[TheBloke\\u002fHerme...\"],[\"|      |                                                                            |[TheBloke\\u002fKuchi...\"],[\"|      |                                                                            |[TheBloke\\u002fLLaMA...\"],[\"|      |                                                                            |[TheBloke\\u002fMytho...\"],[\"|      |                                                                            |[TheBloke\\u002forca_...\"],[\"|      |                                                                            |[TheBloke\\u002fqCamm...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[TheBloke\\u002ftulu-...\"],[\"|      |                                                                            |[TheBloke\\u002fZarab...\"],[\"|      |                                                                            |[Thireus\\u002fVicuna...\"],[\"|      |                                                                            |[universonic\\u002fll...\"],[\"|      |                                                                            |[valurank\\u002fparap...\"],[\"|      |                                                                            |[Zhejian\\u002fllama-...\"],[\"|      |                                                                            |[zib16\\u002fllama_ad...\"],[\"|      |                                                                            |[zohaib99k\\u002fQnA_...\"],[\"ChatUI on Spaces\\n\\n**Hugging Chat** is an open-source interface enabling everyone to try open-source ...\"],[\"\\u003ca href=\\\"Parameters\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"Model Cards\\n\\n\\u003cTip\\u003e\\n\\n[New! Try our experimental Model Card Creator App](https:\\u002f\\u002fhuggingface.co\\u002fspaces...\"],[\"### Adding metadata to your model card\\n\\nThere are a few different ways to add metadata to your model...\"],[\"```yaml\\n---\\nlanguage: \\n  - \\\"List of ISO 639-1 code for your language\\\"\\n  - lang1\\n  - lang2\\nthumbnail:...\"],[\"```yaml\\nbase_model: HuggingFaceH4\\u002fzephyr-7b-beta\\n```\\n\\nThis metadata will be used to display the base...\"],[\"### Specifying a license\\n\\nYou can specify the license in the model card metadata section. The licens...\"],[\"```yaml\\n---\\nmodel-index:\\n  - name: Yi-34B\\n    results:\\n      - task:\\n          type: text-generation...\"],[\"### Can I add custom tags to my model?\\n\\nYes, you can add custom tags to your model by adding them to...\"],[\"Uploading datasets\\n\\nThe [Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets) is home to an extensive collection of...\"],[\"3. After uploading your dataset files, they are stored in your dataset repository.\\n\\n\\u003cdiv class=\\\"flex...\"],[\"You can click on the **Import dataset card template** link at the top of the editor to automatically...\"],[\"It also supports files compressed using ZIP (.zip), GZIP (.gz), ZSTD (.zst), BZ2 (.bz2), LZ4 (.lz4) ...\"],[\"WebDataset\\n\\n[WebDataset](https:\\u002f\\u002fgithub.com\\u002fwebdataset\\u002fwebdataset) is a library to write I\\u002fO pipelin...\"],[\"Pull requests and Discussions\\n\\nHub Pull requests and Discussions allow users to do community contrib...\"],[\"If you opened a PR or discussion, are the author of the repository, or have write access to it, you ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```bash\\ngit fetch origin refs\\u002fpr\\u002f42:pr\\u002f42\\ngit checkout pr\\u002f42\\n# Do your changes\\ngit add .\\ngit commit ...\"],[\"Using fastai at Hugging Face\\n\\n`fastai` is an open-source Deep Learning library that leverages PyTorc...\"],[\"# Probability it's a cat: 100.00%\\n```\\n\\n\\nIf you want to see how to load a specific model, you can cli...\"],[\"Using SpeechBrain at Hugging Face\\n\\n`speechbrain` is an open-source and all-in-one conversational too...\"],[\"If you want to see how to load a specific model, you can click `Use in speechbrain` and you will be ...\"],[\"Model Card Guidebook \\n\\nModel cards are an important documentation and transparency framework for mac...\"],[\"To work towards that goal, it is important to recognize the thoughtful, dedicated efforts that have ...\"],[\"Our work presents a view of where we think model cards stand right now and where they could go in th...\"],[\"Models Frequently Asked Questions\\n\\n## How can I see what dataset was used to train the model?\\n\\nIt's ...\"],[\"Spaces are a great way to show off a model you've made or explore new ways to use existing models! V...\"],[\"* Visit the paper page\\n* Filter for other models on the Hub that cite the same paper.\\n\\n\\u003cdiv class=\\\"f...\"],[\"Using OpenCLIP at Hugging Face\\n\\n[OpenCLIP](https:\\u002f\\u002fgithub.com\\u002fmlfoundations\\u002fopen_clip) is an open-so...\"],[\"text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\\n\\nprint(\\\"Label probs:\\\", text_...\"],[\"Libraries\\n\\nThe Datasets Hub has support for several libraries in the Open Source ecosystem.\\nThanks t...\"],[\"Using Stanza at Hugging Face\\n\\n`stanza` is a collection of accurate and efficient tools for the lingu...\"],[\"Panel on Spaces\\n\\n[Panel](https:\\u002f\\u002fpanel.holoviz.org\\u002f) is an open-source Python library that lets you ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub\\u002fspaces-p...\"],[\"## 🌐 Join Our Community\\nThe Panel community is vibrant and supportive, with experienced developers a...\"],[\"User Studies\\n## Model Card Audiences and Use Cases\\n\\nDuring our investigation into the landscape of m...\"],[\"As we began to structure our user studies, two variations of model cards - that made use of the [ini...\"],[\"Our user studies provided further clarity on the sections that different user profiles\\u002fstakeholders ...\"],[\"## User Study Details\\n\\nWe selected people from a variety of different backgrounds relevant to machin...\"],[\"#### Format 3: \\n**Ezi Ozoani's [Semi-Interactive Model Card Space](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fEzi...\"],[\"Docker Spaces Examples\\n\\nWe gathered some example demos in the [Spaces Examples](https:\\u002f\\u002fhuggingface....\"],[\"Datasets\\n\\nThe Hugging Face Hub is home to a growing collection of datasets that span a variety of do...\"],[\"Using GPU Spaces\\n\\nYou can upgrade your Space to use a GPU accelerator using the _Settings_ button in...\"],[\"## Hardware Specs\\n\\nIn the following table, you can see the Specs for the different upgrade options.\\n...\"],[\"```\\n--extra-index-url https:\\u002f\\u002fdownload.pytorch.org\\u002fwhl\\u002fcu113\\ntorch\\n```\\n\\nYou can verify whether the i...\"],[\"```Python\\nimport tensorflow as tf\\nprint(tf.config.list_physical_devices('GPU'))\\n# [PhysicalDevice(na...\"],[\"If you want your Space never to deactivate or if you want to set a custom sleep time, you need to up...\"],[\"How to Add a Space to ArXiv\\n\\nDemos on Hugging Face Spaces allow a wide audience to try out state-of-...\"],[\"1. First, upload the model associated with the ArXiv paper onto the Hugging Face Hub if it is not al...\"],[\"4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A ...\"],[\"Datasets Overview\\n\\n## Datasets on the Hub\\n\\nThe Hugging Face Hub hosts a [large number of community-c...\"],[\"Using AllenNLP at Hugging Face\\n\\n`allennlp` is a NLP library for developing state-of-the-art models o...\"],[\"To get a snippet such as this, you can click `Use in AllenNLP` at the top right,\\n\\n\\u003cdiv class=\\\"flex j...\"],[\"### Using the AllenNLP CLI\\n\\nTo push with the CLI, you can use the `allennlp push_to_hf` command as s...\"],[\"serialization_dir = \\\"path\\u002fto\\u002fserialization\\u002fdirectory\\\"\\npush_to_hf(\\n    repo_name=\\\"my_repo_name\\\",\\n    ...\"],[\"Webhook guide: Setup an automatic system to re-train a model when a dataset changes\\n\\n\\u003cTip\\u003e\\n\\nWebhooks...\"],[\"## Create a Space to react to your Webhook\\n\\nWe now need a way to react to your Webhook events. An ea...\"],[\"```python\\n# defined in src\\u002fmodels.py\\nclass WebhookPayloadEvent(BaseModel):\\n\\taction: Literal[\\\"create\\\"...\"],[\"In this example, we used Hugging Face AutoTrain to fine-tune our model quickly, but you can of cours...\"],[\"Downloading models\\n\\n## Integrated libraries\\n\\nIf a model on the Hub is tied to a [supported library](...\"],[\"If you have write-access to the particular model repo, you'll also have the ability to commit and pu...\"],[\"Getting Started with Repositories\\n\\nThis beginner-friendly guide will help you get the basic skills y...\"],[\"1. To create a new repository, visit [huggingface.co\\u002fnew](http:\\u002f\\u002fhuggingface.co\\u002fnew):\\n\\n\\u003cdiv class=\\\"f...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"## Adding files to a repository (terminal)[[terminal]]\\n\\n### Cloning repositories\\n\\nDownloading reposi...\"],[\"```bash\\n# Create any files you like! Then...\\ngit add .\\ngit commit -m \\\"First model version\\\"  # You ca...\"],[\"Embed your Space in another website\\n\\nOnce your Space is up and running you might wish to embed it in...\"],[\"## Embedding with WebComponents\\n\\nIf the Space you wish to embed is Gradio-based, you can use Web Com...\"],[\"Tabby on Spaces\\n\\n[Tabby](https:\\u002f\\u002ftabby.tabbyml.com) is an open-source, self-hosted AI coding assista...\"],[\"### Connect VSCode Extension to Space backend\\n\\n1. Install the [VSCode Extension](https:\\u002f\\u002fmarketplace...\"],[\"Data files Configuration\\n\\nThere are no constraints on how to structure dataset repositories.\\n\\nHoweve...\"],[\"Single Sign-On (SSO)\\n\\n\\u003cTip warning={true}\\u003e\\nThis feature is part of the \\u003ca href=\\\"https:\\u002f\\u002fhuggingface....\"],[\"Models\\n\\nThe Hugging Face Hub hosts many models for a [variety of machine learning tasks](https:\\u002f\\u002fhug...\"],[\"Using Asteroid at Hugging Face\\n\\n`asteroid` is a Pytorch toolkit for audio source separation. It enab...\"],[\"If you want to see how to load a specific model, you can click `Use in Adapter Transformers` and you...\"],[\"Models Download Stats\\n\\n## How are download stats generated for models?\\n\\nCounting the number of downl...\"],[\"```json\\n{\\n    \\\"adapter-transformers\\\": {\\n        filter: [\\n            {\\n                term: { path...\"],[\"},\\n    \\\"stable-baselines3\\\": {\\n        filter: [\\n            {\\n                wildcard: { path: \\\"*.z...\"],[\"Using RL-Baselines3-Zoo at Hugging Face\\n\\n`rl-baselines3-zoo` is a training framework for Reinforceme...\"],[\"You can define three parameters:\\n- `--repo-name`: The name of the repo.\\n- `-orga`: Your Hugging Face...\"],[\"Using OpenCV in Spaces\\n\\nIn order to use OpenCV in your Gradio or Streamlit Spaces, you'll need to ma...\"],[\"Uploading models\\n\\nTo upload models to the Hub, you'll need to create an account at [Hugging Face](ht...\"],[\"2. From there, select a file from your computer to upload and leave a helpful commit message to know...\"],[\"Read more about model tags [here](.\\u002fmodel-cards#model-card-metadata).\\n\\n6. Add TensorBoard traces\\n\\nAn...\"],[\"Digital Object Identifier (DOI)\\n\\nThe Hugging Face Hub offers the possibility to generate DOI for you...\"],[\"After you agree to those terms, your model or dataset will get a DOI assigned, and a new tag should ...\"],[\"Secrets Scanning\\n\\nIt is important to manage [your secrets (env variables) properly](.\\u002fspaces-overvie...\"],[\"Downloading datasets\\n\\n## Integrated libraries\\n\\nIf a dataset on the Hub is tied to a [supported libra...\"],[\"```bash\\ngit lfs install\\ngit clone git@hf.co:datasets\\u002f\\u003cdataset ID\\u003e # example: git clone git@hf.co:dat...\"],[\"Handling Spaces Dependencies\\n\\n## Default dependencies\\n\\nThe default Spaces environment comes with sev...\"],[\"Inference API\\n\\nPlease refer to [Inference API Documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fapi-inferen...\"],[\"## Can I send large volumes of requests? Can I get accelerated APIs?\\n\\nIf you are interested in accel...\"],[\"Enterprise Hub\\n\\nEnterprise Hub adds advanced capabilities to organizations, enabling safe, compliant...\"],[\"How to configure SAML SSO with Okta\\n\\nIn this guide, we will use Okta as the SSO provider and with th...\"],[\"Copy the \\\"Assertion Consumer Service URL\\\" from the organization's settings on Hugging Face, and past...\"],[\"You should be redirected to your SSO provider (IdP) login prompt. Once logged in, you'll be redirect...\"],[\"Using 🧨 `diffusers` at Hugging Face\\n\\nDiffusers is the go-to library for state-of-the-art pretrained ...\"],[\"## Using existing pipelines\\n\\nAll `diffusers` pipelines are a line away from being used! To run gener...\"],[\"Security\\n\\nThe Hugging Face Hub offers several security features to ensure that your code and data ar...\"],[\"Using SpanMarker at Hugging Face\\n\\n[SpanMarker](https:\\u002f\\u002fgithub.com\\u002ftomaarsen\\u002fSpanMarkerNER) is a fram...\"],[\"Once loaded, you can use [`SpanMarkerModel.predict`](https:\\u002f\\u002ftomaarsen.github.io\\u002fSpanMarkerNER\\u002fapi\\u002fs...\"],[\"Disk usage on Spaces\\n\\nEvery Space comes with a small amount of disk storage. This disk space is ephe...\"],[\"\\u003cTip warning={true}\\u003e\\n\\tWARNING: all data stored in the storage is lost when you delete it.\\n\\u003c\\u002fTip\\u003e\\n\\n##...\"],[\"Hugging Face on Amazon SageMaker\\n\\n![cover](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation...\"],[\"- Cost-effective: Training instances are only live for the duration of your job. Once your job is co...\"],[\"**Accelerate machine learning from science to production**\\n\\nIn addition to Hugging Face DLCs, we cre...\"],[\"Take a look at our published blog posts, videos, documentation, sample notebooks and scripts for add...\"],[\"### Documentation\\n\\n- [Run training on Amazon SageMaker](\\u002fdocs\\u002fsagemaker\\u002ftrain)\\n- [Deploy models to A...\"],[\"### Sample notebooks\\n\\n- [All notebooks](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002ftree\\u002fmaster\\u002fsagemak...\"],[\"Hub API Endpoints\\n\\nWe have open endpoints that you can use to retrieve information from the Hub as w...\"],[\"### GET \\u002fapi\\u002fmodels\\n\\nGet information from all models in the Hub. The response is paginated, use the ...\"],[\"### GET \\u002fapi\\u002fmodels-tags-by-type\\n\\nGets all the available model tags hosted in the Hub.\\n\\nThis is equi...\"],[\"Get the nth shard of the auto-converted parquet files.\\n\\n### GET \\u002fapi\\u002fdatasets-tags-by-type\\n\\nGets all...\"],[\"## Repo API\\n\\nThe following endpoints manage repository settings like creating and deleting a reposit...\"],[\"Get username and organizations the user belongs to.\\n\\nPayload:\\n\\n```js\\nheaders = { \\\"authorization\\\" :  ...\"],[\"If no parameter is set, all collections are returned.\\n\\nThe response is paginated. To get all collect...\"],[\"This is equivalent to `huggingface_hub.add_collection_item()`.\\n\\n### PATCH \\u002fapi\\u002fcollections\\u002f{namespac...\"],[\"Displaying carbon emissions for your model\\n\\n## Why is it beneficial to calculate the carbon emission...\"],[\"Considering the computing hardware, location, usage, and training time, you can estimate how much CO...\"],[\"Train and deploy Hugging Face on Amazon SageMaker\\n\\nThe get started guide will show you how to quickl...\"],[\"If you are planning on using SageMaker in a local environment, you need to provide the `role` yourse...\"],[\"## Upload dataset to S3 bucket\\n\\nNext, upload the preprocessed dataset to your S3 session bucket with...\"],[\"huggingface_estimator = HuggingFace(\\n    entry_point=\\\"train.py\\\",                 # fine-tuning scrip...\"],[\"Dataset Cards\\n\\n## What are Dataset Cards?\\n\\nEach dataset may be documented by the `README.md` file in...\"],[\"When creating a README.md file in a dataset repository on the Hub, use Metadata UI to fill the main ...\"],[\"Using sample-factory at Hugging Face\\n\\n[`sample-factory`](https:\\u002f\\u002fgithub.com\\u002falex-petrenko\\u002fsample-fac...\"],[\"```\\ngit clone git@hf.co:\\u003cName of HuggingFace Repo\\u003e # example: git clone git@hf.co:bigscience\\u002fbloom\\n`...\"],[\"Other relevant command line arguments are:\\n\\n- `--hf_repository`: The repository to push to. Must be ...\"],[\"hub-docs\\n\\nThis repository regroups documentation and information that is hosted on the Hugging Face ...\"],[\"Datasets Download Stats\\n\\n## How are download stats generated for datasets?\\n\\nThe Hub provides downloa...\"],[\"Using TensorBoard\\n\\nTensorBoard provides tooling for tracking and visualizing metrics as well as visu...\"],[\"Organizations, Security, and the Hub API\\n\\n## Contents\\n\\n- [Organizations](.\\u002forganizations)\\n  - [Manag...\"],[\"Using timm at Hugging Face\\n\\n`timm`, also known as [pytorch-image-models](https:\\u002f\\u002fgithub.com\\u002frwightma...\"],[\"If you want to see how to load a specific model, you can click **Use in timm** and you will be given...\"],[\"# Grab the values and indices of top 5 predicted classes\\nvalues, indices = torch.topk(probabilities,...\"],[\"# Load your model from the Hub\\nmodel_reloaded = timm.create_model(\\n    'hf-hub:\\u003cyour-username\\u003e\\u002fresne...\"],[\"Image Dataset\\n\\nThis guide will show you how to configure your dataset repository with image files. Y...\"],[\"```\\nmy_dataset_repository\\u002f\\n└── train\\n    ├── 1.jpg\\n    ├── 2.jpg\\n    ├── 3.jpg\\n    ├── 4.jpg\\n    └──...\"],[\"```\\nmy_dataset_repository\\u002f\\n├── green\\n│   ├── 1.jpg\\n│   └── 2.jpg\\n└── red\\n    ├── 3.jpg\\n    └── 4.jpg...\"],[\"Deploy models to Amazon SageMaker\\n\\nDeploying a 🤗 Transformers models in SageMaker for inference is a...\"],[\"Upgrade to the latest `sagemaker` version.\\n\\n```bash\\npip install sagemaker --upgrade\\n```\\n\\n**SageMaker...\"],[\"```python\\nfrom sagemaker.huggingface import HuggingFace\\n\\n############ pseudo code start ############...\"],[\"# request\\npredictor.predict(data)\\n```\\n\\nAfter you run our request, you can delete the endpoint again ...\"],[\"```python\\nfrom sagemaker.huggingface.model import HuggingFaceModel\\n\\n# Hub model configuration \\u003chttps...\"],[\"After training a model, you can use [SageMaker batch transform](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fsagemake...\"],[\"# Hub model configuration \\u003chttps:\\u002f\\u002fhuggingface.co\\u002fmodels\\u003e\\nhub = {\\n\\t'HF_MODEL_ID':'distilbert-base-un...\"],[\"```bash\\nmodel.tar.gz\\u002f\\n|- pytorch_model.bin\\n|- ....\\n|- code\\u002f\\n  |- inference.py\\n  |- requirements.txt ...\"],[\"def model_fn(model_dir):\\n    # implement custom code to load the model\\n    loaded_model = ...\\n    \\n ...\"],[\"Single Sign-On (SSO)\\n\\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-...\"],[\"## How to configure OIDC\\u002fSAML provider in the Hub\\n\\nWe have some guides available to help with config...\"],[\"Spaces Configuration Reference\\n\\nSpaces are configured through the `YAML` block at the top of the **R...\"],[\"**`app_file`** : _string_  \\nPath to your main application file (which contains either `gradio` or `s...\"],[\"**`disable_embedding`** : _boolean_  \\nWhether the Space iframe can be embedded in other websites.\\nDe...\"],[\"The format for each item is `\\\"repository_name\\\"` to download all files from a repository, or `\\\"reposi...\"],[\"Spaces Settings\\n\\nYou can configure your Space's appearance and other settings inside the `YAML` bloc...\"],[\"Your First Docker Space: Text Generation with T5\\n\\nIn the following sections, you'll learn the basics...\"],[\"```yaml\\napp_port: 7860\\n```\\n\\n## Add the dependencies\\n\\nFor the **Text Generation** Space, we'll be bui...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentatio...\"],[\"```python\\nfrom transformers import pipeline\\n\\npipe_flan = pipeline(\\\"text2text-generation\\\", model=\\\"goo...\"],[\"textGenParagraph.textContent = await translateText(textGenInput.value);\\n});\\n```\\n\\n5. Grant permission...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentatio...\"],[\"Using Sentence Transformers at Hugging Face\\n\\n`sentence-transformers` is a library that provides easy...\"],[\"## Using existing models\\n\\nThe pre-trained models on the Hub can be loaded with a single line of code...\"],[\"```py\\nfrom sentence_transformers import SentenceTransformer\\n\\n# Load or train a model\\nmodel.save_to_h...\"],[\"How to configure OIDC SSO with Okta\\n\\nIn this guide, we will use Okta as the SSO provider and with th...\"],[\"Copy the \\\"Redirection URI\\\" from the organization's settings on Hugging Face, and paste it in the \\\"Si...\"],[\"A green check mark near the OIDC selector will attest that the test was successful.\\n\\n\\n\\u003cdiv class=\\\"fl...\"],[\"Repositories\\n\\nModels, Spaces, and Datasets are hosted on the Hugging Face Hub as [Git repositories](...\"],[\"Malware Scanning\\n\\nWe run every file of your repositories through a [malware scanner](https:\\u002f\\u002fwww.cla...\"],[\"Managing Spaces with CircleCI Workflows\\n\\nYou can keep your app in sync with your GitHub repository w...\"],[\"```yaml\\nversion: 2.1\\n\\nworkflows:\\n  main:\\n    jobs:\\n      - sync-to-huggingface:\\n          context:\\n ...\"],[\"Docker Spaces\\n\\nSpaces accommodate custom [Docker containers](https:\\u002f\\u002fdocs.docker.com\\u002fget-started\\u002f) f...\"],[\"```Dockerfile\\n\\t# Declare your environment variables with the ARG directive\\n\\tARG MODEL_REPO_NAME\\n\\n\\tFR...\"],[\"# Switch to the \\\"user\\\" user\\nUSER user\\n\\n# Set home to the user's home directory\\nENV HOME=\\u002fhome\\u002fuser \\\\...\"],[\"\\u003cTip warning=\\\"{true}\\\"\\u003e\\n\\nAt the moment, `\\u002fdata` volume is only available at runtime, i.e. you cannot ...\"],[\"Webhook guide: build a Discussion bot based on BLOOM\\n\\n\\u003cTip\\u003e\\n\\nWebhooks are now publicly available!\\n\\n\\u003c...\"],[\"The Space's code is [here](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fdiscussion-bot\\u002fwebhook\\u002ftree\\u002fmain). \\n\\nWe use...\"],[\"const response = await fetch(INFERENCE_URL, {\\n\\t\\tmethod: \\\"POST\\\",\\n\\t\\tbody: JSON.stringify({ inputs: PRO...\"],[\"Sign in with Hugging Face\\n\\nYou can use the HF OAuth \\u002f OpenID connect flow to create a **\\\"Sign in wit...\"],[\"All other information is available in the [OpenID metadata](https:\\u002f\\u002fhuggingface.co\\u002f.well-known\\u002fopeni...\"],[\"[![Sign in with Hugging Face](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fbadges\\u002fresolve\\u002fmain\\u002fsign-i...\"],[\"Using `Transformers.js` at Hugging Face\\n\\nTransformers.js is a JavaScript library for running 🤗 Trans...\"],[\"Refer to the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers.js) for the full list of suppo...\"],[\"Using SetFit with Hugging Face\\n\\nSetFit is an efficient and prompt-free framework for few-shot fine-t...\"],[\"```\\npip install -U setfit\\n```\\n\\n## Using existing models\\n\\nAll `setfit` models can easily be loaded fr...\"],[\"Spaces\\n\\n[Hugging Face Spaces](https:\\u002f\\u002fhuggingface.co\\u002fspaces) offer a simple way to host ML demo apps...\"],[\"## Contact\\n\\nFeel free to ask questions on the [forum](https:\\u002f\\u002fdiscuss.huggingface.co\\u002fc\\u002fspaces\\u002f24) if...\"],[\"Using Adapter Transformers at Hugging Face\\n\\n`adapter-transformers` is a library that extends 🤗 `tran...\"],[\"If you want to see how to load a specific model, you can click `Use in Adapter Transformers` and you...\"],[\"Using PEFT at Hugging Face\\n\\n🤗 [Parameter-Efficient Fine-Tuning (PEFT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fp...\"],[\"It outputs the following:\\n\\n```text\\nTell me the recipe for chocolate chip cookie dough.\\n\\n1. Preheat o...\"],[\"Organization cards\\n\\nYou can create an organization card to help users learn more about what your org...\"],[\"Libraries\\n\\nThe Hub has support for dozens of libraries in the Open Source ecosystem. Thanks to the `...\"],[\"| Library                                                                     | Description         ...\"],[\"| [fastai](https:\\u002f\\u002fgithub.com\\u002ffastai\\u002ffastai)                                  | Library to train fas...\"],[\"| [PEFT](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fpeft)                      | Cutting-edge Parameter Efficien...\"],[\"| [SpanMarker](https:\\u002f\\u002fgithub.com\\u002ftomaarsen\\u002fSpanMarkerNER)                    | Familiar, simple and...\"],[\"### How can I add a new library to the Inference API?\\n\\nIf you're interested in adding your library, ...\"],[\"Gated models\\n\\nTo give more control over how models are used, the Hub allows model authors to enable ...\"],[\"If you want to manually approve which users can access your model, you must set it to **manual appro...\"],[\"#### From the UI\\n\\nYou can review who has access to your gated model from its settings page by clicki...\"],[\"| Method | URI | Description | Headers | Payload\\n| ------ | --- | ----------- | -------  | -------  ...\"],[\"\\u003ca id=\\\"modifying-the-prompt\\\"\\u003e\\u003c\\u002fa\\u003e \\u003c!-- backward compatible anchor --\\u003e\\n\\n### Customize requested infor...\"],[\"```yaml\\n---\\nextra_gated_heading: \\\"Acknowledge license to accept the repository\\\"\\nextra_gated_button_c...\"],[\"Requesting access can only be done from your browser. Go to the model on the Hub and you will be pro...\"],[\"```bash\\nhuggingface-cli login\\n```\\n\\nAlternatively, you can programmatically login using `login()` in ...\"],[\"Gated datasets\\n\\nTo give more control over how datasets are used, the Hub allows datasets authors to ...\"],[\"If you want to manually approve which users can access your dataset, you must set it to **manual app...\"],[\"### From the UI\\n\\nYou can review who has access to your gated dataset from its settings page by click...\"],[\"| Method | URI | Description | Headers | Payload\\n| ------ | --- | ----------- | -------  | -------  ...\"],[\"\\u003ca id=\\\"modifying-the-prompt\\\"\\u003e\\u003c\\u002fa\\u003e \\u003c!-- backward compatible anchor --\\u003e\\n\\n### Customize requested infor...\"],[\"```yaml\\n---\\nextra_gated_heading: \\\"Acknowledge license to accept the repository\\\"\\nextra_gated_button_c...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Download files\\n\\nTo download files from a gated dataset you'll need to be authenticated. ...\"],[\"Custom Python Spaces\\n\\n\\u003cTip\\u003e\\n\\nSpaces now support arbitrary Dockerfiles so you can host any Python app...\"],[\"Paper Pages\\n\\nPaper pages allow people to find artifacts related to a paper such as models, datasets ...\"],[\"If your paper is not linked to your account, you can click in your name in the corresponding Paper p...\"],[\"DuckDB\\n\\n[DuckDB](https:\\u002f\\u002fgithub.com\\u002fduckdb\\u002fduckdb) is an in-process SQL [OLAP](https:\\u002f\\u002fen.wikipedia....\"],[\"More ways to create Spaces\\n\\n## Duplicating a Space\\n\\nYou can duplicate a Space by clicking the three ...\"],[\"Shiny on Spaces\\n\\n[Shiny](https:\\u002f\\u002fshiny.posit.co\\u002f) is an open-source framework for building simple, b...\"],[\"_app.py_\\n\\nThis file defines your app's logic. To learn more about how to modify this file, see [the ...\"],[\"_app.R_\\nThis file contains all of your application logic. If you prefer, you can break this file up ...\"],[\"Using ML-Agents at Hugging Face\\n\\n`ml-agents` is an open-source toolkit that enables games and simula...\"],[\"## Sharing your models\\n\\nYou can easily upload your models using `mlagents-push-to-hf`:\\n\\n```\\nmlagents...\"],[\"--\\n# Example metadata to be added to a model card.  \\n# Full model card template at https:\\u002f\\u002fgithub.co...\"],[\"# Optional. Add this if you want to encode your eval results in a structured way.\\nmodel-index:\\n- nam...\"],[\"args:\\n          {arg_0}: {value_0}        # Optional. The arguments passed during `Metric.compute()`...\"],[\"This markdown file contains the spec for the modelcard metadata regarding evaluation parameters. Whe...\"],[\"Using 🤗 `transformers` at Hugging Face\\n\\n🤗 `transformers` is a library maintained by Hugging Face and...\"],[\"You can find models for many different tasks:\\n\\n* Extracting the answer from a context ([question-ans...\"],[\"You can try out the models directly in the browser if you want to test them out without downloading ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"THE LANDSCAPE OF ML DOCUMENTATION TOOLS\\nThe development of the model cards framework in 2018 was ins...\"],[\"* Focused on documentation of some (or multiple) aspects of the ML system lifecycle\\n* Included the r...\"],[\"| **Stage of ML System Lifecycle** \\t|  **Tool**                                                     ...\"],[\"| DATA                             \\t| ***Datasheets*** [(Gebru et al., 2018)](https:\\u002f\\u002fwww.fatml.org\\u002f...\"],[\"| DATA                             \\t| ***Dataset Nutrition Labels*** [(Holland et al., 2018)](https:...\"],[\"| DATA                             \\t| ***Dataset Development Lifecycle Documentation Framework*** [(...\"],[\"| DATA                             \\t| ***CrowdWorkSheets***  [(Díaz et al., 2022)](https:\\u002f\\u002fhuggingfa...\"],[\"| MODELS AND METHODS               \\t| ***Value Cards*** [Shen et al. (2021)](https:\\u002f\\u002fdl.acm.org\\u002fdoi\\u002f...\"],[\"| MODELS AND METHODS               \\t| ***Consumer Labels for ML Models*** [Seifert et al. (2019)](ht...\"],[\"| SYSTEMS                          \\t| ***System Cards***  [Procope et al. (2022)](https:\\u002f\\u002fai.faceboo...\"],[\"| SYSTEMS                          \\t| ***Robustness Gym***  [Goel et al. (2021)](https:\\u002f\\u002fhuggingface...\"],[\"### DATA-FOCUSED DOCUMENTATION TOOLS\\n\\nSeveral proposed documentation tools focus on datasets used in...\"],[\"* Extending the concept of datasheets in the electronics industry, [Gebru et al. (2018)](https:\\u002f\\u002fwww...\"],[\"* [Pushkarna et al. (2021)](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2204.01075) propose the data cards as part...\"],[\"### MODEL-AND-METHOD-FOCUSED DOCUMENTATION TOOLS\\n\\nAnother set of documentation tools can be thought ...\"],[\"* They envision the relationship between model cards and method cards, in part, by stating: “The sec...\"],[\"* [Procope et al. (2022)](https:\\u002f\\u002fai.facebook.com\\u002fresearch\\u002fpublications\\u002fsystem-level-transparency-of...\"],[\"## THE EVOLUTION OF MODEL CARDS\\n\\nSince the proposal for model cards by Mitchell et al. in 2018, mode...\"],[\"The high number of models uploaded to the Hugging Face Hub (101,041 models at the point of writing),...\"],[\"[^6]: See Appendix A.\\n\\n[^7]: See GSA \\u002f US Census Bureau Collaboration on Model Card Generator.\\n\\n[^8]...\"],[\"Using Keras at Hugging Face\\n\\n`keras` is an open-source machine learning library that uses a consiste...\"],[\"# The image is a sunflower!\\n```\\n\\nIf you want to see how to load a specific model, you can click **Us...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"Widgets\\n\\n## What's a widget?\\n\\nMany model repos have a widget that allows anyone to run inferences di...\"],[\"**You can always manually override your pipeline type with `pipeline_tag: xxx` in your [model card m...\"],[\"For example, allow users to choose from two sample audio files for automatic speech recognition task...\"],[\"```yaml\\nwidget:\\n  - src: sample1.flac\\n    output:\\n      text: \\\"Hello my name is Julien\\\"\\n```\\n\\n\\u003cdiv cl...\"],[\"```yaml\\nwidget:\\n  - text: \\\"picture of a futuristic tiger, artstation\\\"\\n    output:\\n      url: images\\u002f...\"],[\"You can find all the supported tasks in [pipelines.ts file](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingfa...\"],[\"## How can I control my model's widget Inference API parameters?\\n\\nGenerally, the Inference API for a...\"],[\"No-license models challenge\\n\\n## Context\\n\\nThe Hugging Face Hub hosts hundreds of thousands of public ...\"],[\"```yaml\\n# Example from https:\\u002f\\u002fhuggingface.co\\u002fcoqui\\u002fXTTS-v1\\n---\\nlicense: other\\nlicense_name: coqui-p...\"],[\"For each model, the workflow looks like this:\\n1. Choose a model in the list below. We suggest focusi...\"],[\"## F.A.Q.\\n\\n### What if the model has 2 licenses?\\n\\nThis use case can happen when a model is finetuned...\"],[\"|status|pr_url|model_id                                                                             ...\"],[\"|opened|[here](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fStableBeluga-7B\\u002fdiscussions\\u002f6)|[stabilityai\\u002fStable...\"],[\"|opened|[here](https:\\u002f\\u002fhuggingface.co\\u002fNousResearch\\u002fLlama-2-7b-hf\\u002fdiscussions\\u002f4)|[NousResearch\\u002fLlama-...\"],[\"|      |      |[huggyllama\\u002fllama-7b](https:\\u002f\\u002fhuggingface.co\\u002fhuggyllama\\u002fllama-7b)                    ...\"],[\"|      |      |[SAPOSS\\u002fpassword-model](https:\\u002f\\u002fhuggingface.co\\u002fSAPOSS\\u002fpassword-model)                ...\"],[\"|      |      |[NousResearch\\u002fLlama-2-7b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fNousResearch\\u002fLlama-2-7b-chat...\"],[\"|      |      |[vinai\\u002fphobert-base-v2](https:\\u002f\\u002fhuggingface.co\\u002fvinai\\u002fphobert-base-v2)                ...\"],[\"|      |      |[symanto\\u002fsn-xlm-roberta-base-snli-mnli-anli-xnli](https:\\u002f\\u002fhuggingface.co\\u002fsymanto\\u002fsn-x...\"],[\"|      |      |[stabilityai\\u002fStableBeluga-13B](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fStableBeluga-13B)  ...\"],[\"|      |      |[Qwen\\u002fQwen-7B](https:\\u002f\\u002fhuggingface.co\\u002fQwen\\u002fQwen-7B)                                  ...\"],[\"|      |      |[huggyllama\\u002fllama-13b](https:\\u002f\\u002fhuggingface.co\\u002fhuggyllama\\u002fllama-13b)                  ...\"],[\"|      |      |[sambanovasystems\\u002fBLOOMChat-176B-v1](https:\\u002f\\u002fhuggingface.co\\u002fsambanovasystems\\u002fBLOOMCha...\"],[\"|      |      |[TheBloke\\u002fLlama-2-13B-Chat-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama-2-13B-Chat-fp1...\"],[\"|      |      |[THUDM\\u002fchatglm-6b-int4](https:\\u002f\\u002fhuggingface.co\\u002fTHUDM\\u002fchatglm-6b-int4)                ...\"],[\"|      |      |[TheBloke\\u002fLlama-2-7b-chat-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama-2-7b-chat-fp16)...\"],[\"|      |      |[TheBloke\\u002forca_mini_v3_7B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002forca_mini_v3_7B-GPTQ)...\"],[\"|      |      |[camembert\\u002fcamembert-large](https:\\u002f\\u002fhuggingface.co\\u002fcamembert\\u002fcamembert-large)        ...\"],[\"|      |      |[Voicelab\\u002ftrurl-2-13b](https:\\u002f\\u002fhuggingface.co\\u002fVoicelab\\u002ftrurl-2-13b)                  ...\"],[\"|      |      |[samrawal\\u002fbert-base-uncased_clinical-ner](https:\\u002f\\u002fhuggingface.co\\u002fsamrawal\\u002fbert-base-u...\"],[\"|      |      |[TheBloke\\u002fVicUnlocked-alpaca-65B-QLoRA-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fVicUnloc...\"],[\"|      |      |[TheBloke\\u002fKimiko-13B-fp16](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fKimiko-13B-fp16)          ...\"],[\"|      |      |[TheTravellingEngineer\\u002fbloom-560m-RLHF](https:\\u002f\\u002fhuggingface.co\\u002fTheTravellingEngineer\\u002f...\"],[\"|      |      |[Qwen\\u002fQwen-14B-Chat-Int4](https:\\u002f\\u002fhuggingface.co\\u002fQwen\\u002fQwen-14B-Chat-Int4)            ...\"],[\"|      |      |[kuelumbus\\u002fpolyBERT](https:\\u002f\\u002fhuggingface.co\\u002fkuelumbus\\u002fpolyBERT)                      ...\"],[\"|      |      |[camembert\\u002fcamembert-base](https:\\u002f\\u002fhuggingface.co\\u002fcamembert\\u002fcamembert-base)          ...\"],[\"|      |      |[PeanutJar\\u002fLLaMa-2-PeanutButter_v19_R8-7B](https:\\u002f\\u002fhuggingface.co\\u002fPeanutJar\\u002fLLaMa-2-P...\"],[\"|      |      |[TheBloke\\u002fOpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke...\"],[\"|      |      |[ClueAI\\u002fChatYuan-large-v2](https:\\u002f\\u002fhuggingface.co\\u002fClueAI\\u002fChatYuan-large-v2)          ...\"],[\"|      |      |[TheBloke\\u002fLLaMA-7b-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLLaMA-7b-GPTQ)              ...\"],[\"|      |      |[Mitsua\\u002fmitsua-diffusion-one](https:\\u002f\\u002fhuggingface.co\\u002fMitsua\\u002fmitsua-diffusion-one)    ...\"],[\"|      |      |[liuhaotian\\u002fllava-llama-2-7b-chat-lightning-lora-preview](https:\\u002f\\u002fhuggingface.co\\u002fliuh...\"],[\"|      |      |[aipicasso\\u002fcool-japan-diffusion-2-1-1-1](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fcool-japan-...\"],[\"|      |      |[aipicasso\\u002fcool-japan-diffusion-2-1-2-beta](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fcool-jap...\"],[\"|      |      |[aipicasso\\u002fcool-japan-diffusion-2-1-0-beta](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fcool-jap...\"],[\"|      |      |[aipicasso\\u002fcool-japan-diffusion-2-1-1-beta](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fcool-jap...\"],[\"|      |      |[allenai\\u002fopen-instruct-stanford-alpaca-7b](https:\\u002f\\u002fhuggingface.co\\u002fallenai\\u002fopen-instru...\"],[\"|      |      |[THUDM\\u002fWebGLM-2B](https:\\u002f\\u002fhuggingface.co\\u002fTHUDM\\u002fWebGLM-2B)                            ...\"],[\"|      |      |[THUDM\\u002fcodegeex2-6b-int4](https:\\u002f\\u002fhuggingface.co\\u002fTHUDM\\u002fcodegeex2-6b-int4)            ...\"],[\"|      |      |[aipicasso\\u002fcool-japan-diffusion-2-1-0](https:\\u002f\\u002fhuggingface.co\\u002faipicasso\\u002fcool-japan-di...\"],[\"|      |      |[TheBloke\\u002fAiroboros-L2-70B-GPT4-m2.0-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fAiroboros-...\"],[\"|      |      |[samrawal\\u002fbert-large-uncased_med-ner](https:\\u002f\\u002fhuggingface.co\\u002fsamrawal\\u002fbert-large-unca...\"],[\"|      |      |[valurank\\u002fdistilroberta-propaganda-2class](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fdistilrobe...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-7B-gpt4-2.0-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros-l2...\"],[\"|      |      |[allenai\\u002fopen-instruct-opt-6.7b-tulu](https:\\u002f\\u002fhuggingface.co\\u002fallenai\\u002fopen-instruct-op...\"],[\"|      |      |[TheBloke\\u002fLLaMA-65B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLLaMA-65B-GPTQ)            ...\"],[\"|      |      |[OAOA\\u002fDifFace](https:\\u002f\\u002fhuggingface.co\\u002fOAOA\\u002fDifFace)                                  ...\"],[\"|      |      |[allenai\\u002ftulu-7b](https:\\u002f\\u002fhuggingface.co\\u002fallenai\\u002ftulu-7b)                            ...\"],[\"|      |      |[bibimbap\\u002fQwen-7B-Chat](https:\\u002f\\u002fhuggingface.co\\u002fbibimbap\\u002fQwen-7B-Chat)                ...\"],[\"|      |      |[localmodels\\u002fLlama-2-7B-Chat-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002flocalmodels\\u002fLlama-2-7B-Chat...\"],[\"|      |      |[TheBloke\\u002fZarablend-L2-7B-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fZarablend-L2-7B-GGUF)...\"],[\"|      |      |[TheBloke\\u002fLlama-2-70B-GGML](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama-2-70B-GGML)        ...\"],[\"|      |      |[Faradaylab\\u002fAria-70B](https:\\u002f\\u002fhuggingface.co\\u002fFaradaylab\\u002fAria-70B)                    ...\"],[\"|      |      |[michaelfeil\\u002fct2fast-Llama-2-13b-chat-hf](https:\\u002f\\u002fhuggingface.co\\u002fmichaelfeil\\u002fct2fast-...\"],[\"|      |      |[localmodels\\u002fLlama-2-13B-Chat-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002flocalmodels\\u002fLlama-2-13B-Ch...\"],[\"|      |      |[TheBloke\\u002fOpenAssistant-Llama2-13B-Orca-8K-3319-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke...\"],[\"|      |      |[elinas\\u002fchronos-13b-v2-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002felinas\\u002fchronos-13b-v2-GPTQ)      ...\"],[\"|      |      |[flax-community\\u002fmedclip-roco](https:\\u002f\\u002fhuggingface.co\\u002fflax-community\\u002fmedclip-roco)    ...\"],[\"|      |      |[alfredplpl\\u002funlimited-replicant](https:\\u002f\\u002fhuggingface.co\\u002falfredplpl\\u002funlimited-replican...\"],[\"|      |      |[TheBloke\\u002fMythoLogic-Mini-7B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fMythoLogic-Mini-7B...\"],[\"|      |      |[ehartford\\u002fsamantha-falcon-7b](https:\\u002f\\u002fhuggingface.co\\u002fehartford\\u002fsamantha-falcon-7b)  ...\"],[\"|      |      |[TheBloke\\u002fCodeFuse-CodeLlama-34B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fCodeFuse-CodeL...\"],[\"|      |      |[TheBloke\\u002fZarafusionex-1.1-L2-7B-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fZarafusionex-1...\"],[\"|      |      |[michaelfeil\\u002fct2fast-Llama-2-7b-hf](https:\\u002f\\u002fhuggingface.co\\u002fmichaelfeil\\u002fct2fast-Llama-...\"],[\"|      |      |[khuranagarvit019\\u002fMentalHealthChatbot](https:\\u002f\\u002fhuggingface.co\\u002fkhuranagarvit019\\u002fMental...\"],[\"|      |      |[TheBloke\\u002fllama-2-70b-Guanaco-QLoRA-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fllama-2-70b...\"],[\"|      |      |[TheBloke\\u002forca_mini_v3_70B-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002forca_mini_v3_70B-GGU...\"],[\"|      |      |[TheBloke\\u002fCodeFuse-CodeLlama-34B-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fCodeFuse-CodeL...\"],[\"|      |      |[RicardoLee\\u002fLlama2-chat-Chinese-50W](https:\\u002f\\u002fhuggingface.co\\u002fRicardoLee\\u002fLlama2-chat-Ch...\"],[\"|      |      |[valurank\\u002fen_pos_counter](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fen_pos_counter)            ...\"],[\"|      |      |[TheBloke\\u002fOpenOrcaxOpenChat-Preview2-13B-GGML](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fOpenOr...\"],[\"|      |      |[camelids\\u002fllama-65b-fp16-safetensors](https:\\u002f\\u002fhuggingface.co\\u002fcamelids\\u002fllama-65b-fp16-...\"],[\"|      |      |[Neko-Institute-of-Science\\u002fLLaMA-13B-HF](https:\\u002f\\u002fhuggingface.co\\u002fNeko-Institute-of-Sci...\"],[\"|      |      |[localmodels\\u002fLlama-2-13B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002flocalmodels\\u002fLlama-2-13B-GPTQ)  ...\"],[\"|      |      |[4bit\\u002fllama-13b-4bit-hf](https:\\u002f\\u002fhuggingface.co\\u002f4bit\\u002fllama-13b-4bit-hf)              ...\"],[\"|      |      |[TheBloke\\u002fLlama2-13B-MegaCode2-OASST-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fLlama2-13B...\"],[\"|      |      |[gsaivinay\\u002fLlama-2-7b-Chat-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fgsaivinay\\u002fLlama-2-7b-Chat-GPT...\"],[\"|      |      |[valurank\\u002fxsum_headline_generator](https:\\u002f\\u002fhuggingface.co\\u002fvalurank\\u002fxsum_headline_gene...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-7b-gpt4-1.4.1-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros-...\"],[\"|      |      |[TheBloke\\u002fOpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML](https:\\u002f\\u002fhuggingface.co\\u002fTheBl...\"],[\"|      |      |[localmodels\\u002fLlama-2-70B-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002flocalmodels\\u002fLlama-2-70B-GPTQ)  ...\"],[\"|      |      |[silver\\u002fchatglm-6b-int4-qe-slim](https:\\u002f\\u002fhuggingface.co\\u002fsilver\\u002fchatglm-6b-int4-qe-sli...\"],[\"|      |      |[TheBloke\\u002fllama2-22B-daydreamer-v2-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fllama2-22B-d...\"],[\"|      |      |[TheBloke\\u002fTulu-30B-SuperHOT-8K-GPTQ](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fTulu-30B-SuperHO...\"],[\"|      |      |[Agtian\\u002fllama-65b-int4](https:\\u002f\\u002fhuggingface.co\\u002fAgtian\\u002fllama-65b-int4)                ...\"],[\"|      |      |[TheBloke\\u002fairoboros-l2-13b-gpt4-2.0-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fairoboros-l...\"],[\"|      |      |[TheBloke\\u002fPuddleJumper-13B-V2-GGUF](https:\\u002f\\u002fhuggingface.co\\u002fTheBloke\\u002fPuddleJumper-13B-...\"],[\"Manual Configuration\\n\\nThis guide will show you how to configure a custom structure for your dataset ...\"],[\"Your dataset might have several subsets of data that you want to be able to use separately.\\nFor exam...\"],[\"Static HTML Spaces\\n\\nSpaces also accommodate custom HTML for your app instead of using Streamlit or G...\"],[\"Tasks\\n\\n## What's a task?\\n\\nTasks, or pipeline types, describe the \\\"shape\\\" of each model's API (inputs...\"],[\"Finally, you can add a couple of UI elements, such as the task icon and the widget, that complete th...\"],[\"The Hub also supports over 10 open-source libraries in the [Community Inference API](https:\\u002f\\u002fgithub....\"],[\"The Hub allows users to filter models by a given task. To do this, you need to add the task to sever...\"],[\"Collections\\n\\nUse Collections to group repositories from the Hub (Models, Datasets, Spaces and Papers...\"],[\"![Add items to collections](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"![Collections sort](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fco...\"],[\"![Collection image drop zone with images](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"Run training on Amazon SageMaker\\n\\n\\u003ciframe width=\\\"700\\\" height=\\\"394\\\" src=\\\"https:\\u002f\\u002fwww.youtube.com\\u002fembe...\"],[\"**SageMaker environment**\\n\\nSetup your SageMaker environment as shown below:\\n\\n```python\\nimport sagema...\"],[\"```python\\nimport transformers\\nimport datasets\\nimport argparse\\nimport os\\n\\nif __name__ == \\\"__main__\\\":\\n...\"],[\"## Training Output Management\\n\\nIf `output_dir` in the `TrainingArguments` is set to '\\u002fopt\\u002fml\\u002fmodel' ...\"],[\"## Create a Hugging Face Estimator\\n\\nRun 🤗 Transformers training scripts on SageMaker by creating a [...\"],[\"## Execute training\\n\\nStart your `TrainingJob` by calling `fit` on a Hugging Face Estimator. Specify ...\"],[\"### Data parallelism\\n\\nThe Hugging Face [Trainer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain_class...\"],[\"distribution={\\n    \\\"smdistributed\\\": {\\\"modelparallel\\\": smp_options},\\n    \\\"mpi\\\": mpi_options\\n}\\n\\n # cre...\"],[\"# create the Estimator\\nhuggingface_estimator = HuggingFace(\\n        entry_point='train.py',\\n        ...\"],[\"```python\\n# configure git settings\\ngit_config = {'repo': 'https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformer...\"],[\"Managing organizations\\n\\n## Creating an organization\\n\\nVisit the [New Organization](https:\\u002f\\u002fhf.co\\u002forga...\"],[\"Pickle Scanning\\n\\nPickle is a widely used serialization format in ML. Most notably, it is the default...\"],[\"When you run this, it will create a pickle file and print the following instructions in your termina...\"],[\"We can see that there isn’t much in there, a few opcodes and the associated data. You might be think...\"],[\"Basically, this is what’s happening when you unpickle:\\n\\n```python\\n# ...\\nopcodes_stack = [exec_func, ...\"],[\"E.g.:\\n\\n```python\\nfrom transformers import AutoModel\\n\\nmodel = AutoModel.from_pretrained(\\\"bert-base-ca...\"],[\"We get this data thanks to [`pickletools.genops`](https:\\u002f\\u002fdocs.python.org\\u002f3\\u002flibrary\\u002fpickletools.html...\"],[\"[Dangerous Pickles - Malicious Python Serialization](https:\\u002f\\u002fintoli.com\\u002fblog\\u002fdangerous-pickles\\u002f)\\n\\n[G...\"],[\"Moderation\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [Code of Conduct](https:\\u002f\\u002fhuggingface.co\\u002fcode-of-conduct) and the [...\"],[\"Livebook on Spaces\\n\\n**Livebook** is an open-source tool for writing interactive code notebooks in [E...\"],[\"This will start building your Space using Livebook's Docker image.\\n\\nThe visibility of the Space must...\"],[\"Go to the Settings page of your Space and create a secret called `XLA_TARGET` with the value `cuda11...\"]],\"hovertemplate\":\"source=hub-docs\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"hub-docs, circle\",\"marker\":{\"color\":\"#19d3f3\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"hub-docs, circle\",\"showlegend\":true,\"x\":[11.92402,-3.7754378,-12.783026,-12.737478,-0.97881883,-2.377487,-1.3296928,-1.1154401,-2.0166502,-0.16903548,0.3120042,-6.240009,-6.4377284,-6.366215,-0.026085224,0.033067662,0.071619354,5.7002907,5.3074756,4.7593412,-3.9865668,-3.6782014,-4.0777164,-3.8516126,16.19897,16.650803,16.790861,16.179115,16.368557,16.452766,16.307158,16.941622,16.48167,16.544426,15.745973,13.560536,7.829794,13.955009,14.50813,16.21859,14.553385,13.527,16.42004,13.782915,13.869743,16.596334,14.886503,8.987474,-9.470747,0.52602935,-3.895437,-4.010423,-4.1151204,-0.09023132,3.4423306,14.367473,14.088893,14.271868,13.82858,14.00777,13.723699,12.051767,12.846316,-0.67651963,-1.6626168,-4.410191,-4.5758977,-6.2870164,-5.370468,-2.4224632,-4.5243254,-4.205354,5.2973,13.501067,12.899079,11.239956,13.056694,13.43142,14.690543,-7.281374,-18.485504,-3.5606425,-0.5058697,-1.14028,-1.04635,-1.6869136,-0.5711072,-0.99898833,-1.4829837,-9.524964,-1.8438325,-3.3848214,6.1968713,6.2089934,-1.3867387,-2.249469,-1.8697404,-2.347662,-1.5000136,-2.1291044,-2.1134858,-2.1556835,-2.2218518,-1.5863391,-2.001474,-3.55552,-2.8967164,-3.958637,-4.807036,-1.385559,-2.3046398,-3.4902344,-3.2555976,-3.5016158,-3.6277833,-9.6563,-9.570699,-2.435872,13.098248,2.856508,2.6714864,15.835483,6.9172015,8.045807,8.321763,8.172649,7.763448,8.144819,8.091831,7.908003,17.561422,17.561434,7.283757,8.423046,-1.2851857,0.18123247,-12.65878,-12.837687,-3.4263167,0.014453065,0.3455556,0.018868772,-3.5007923,-2.8901606,2.3604615,-1.4288598,-1.6276867,-1.9906837,12.686611,-6.2211156,13.083303,-3.781682,-3.9162645,6.199782,-10.393741,20.019812,19.283949,-21.73548,-3.0509176,2.6775298,-6.2361712,-0.14922869,-0.057990585,-0.22309457,9.295418,9.298509,9.29977,9.298602,5.2705355,-3.4294288,1.8094224,1.7264028,1.5852042,1.7694583,-7.5554566,-4.717394,-5.973842,-3.0948627,-2.0132558,-4.3368263,-4.158522,-3.9935038,-2.9050214,-5.8619537,-3.9229095,-2.1563687,5.1067715,4.780978,0.33526105,6.9340835,6.929983,2.9482608,-0.5139231,1.108099,-1.4334939,-1.1687319,-5.5400233,-0.29190966,0.11509254,0.019249005,-4.9858313,-4.3520684,-3.9329352,11.981759,-11.9115095,-12.46735,-5.192906,-12.248212,-12.339551,4.060117,6.0919685,6.7115893,6.594295,6.6486773,6.622016,8.476893,-0.24192314,-0.22003323,0.498503,0.5812476,0.5927299,0.23443472,10.6953,0.078060456,11.603506,-6.506386,-1.6403673,-3.7617402,-3.9895797,11.067116,11.752274,8.205865,4.1524725,4.129186,-9.70579,-9.870337,4.8558393,-7.020206,-4.4772587,-3.874191,-6.0580287,-2.4496608,-6.5800548,-5.8633065,-3.4193833,-3.1216636,-3.4773679,-5.368738,-1.5449148,-2.005672,-2.389957,-0.5010571,-1.4637403,-5.9005938,-2.511055,-4.729826,-0.21012303,0.45255125,0.38980576,0.33428228,-6.4598904,-0.38889727,13.182326,12.276992,13.25766,13.328718,-3.376268,-1.9497392,-1.5786239,-1.7170506,-1.060695,-4.096037,3.9748628,12.354045,4.322677,4.0898175,4.268395,-3.91119,-2.6055312,-2.812245,-0.96637434,-1.9294119,-2.178865,-2.164714,-1.6933727,-1.2955418,-1.3717886,-3.8214161,-7.1575775,-6.7012863,-8.817924,-10.533967,-10.491452,-4.9406567,-3.0088925,-2.8503146,-2.3909743,-2.0251005,-2.3425078,-2.2225175,-2.556793,-2.405595,-2.1695735,-1.6966416,-2.7128549,13.32429,-3.882762,6.195392,6.207947,-3.1389484,-2.2569542,-3.1013076,-1.2593122,-3.313048,-1.146324,-1.4279889,-2.9870164,-2.6370385,-2.0303342,-1.1440301,-3.3143785,-2.191409,-4.4361033,-12.718963,-12.973246,-14.923417,-13.605646,-17.41513,-18.401043,-18.724846,-18.655163,-18.737602,-18.874195,-18.732555,11.932643,11.832133,12.063223,-0.10861341,-0.19277322,-0.26306075,0.5121168,-6.2660613,-0.04637238,1.0159322,1.7746947,1.4145776,-0.24353942,0.42084274,-1.4206808,-0.88745767,-0.81278574,-1.8434241,-7.3350387,13.091821,-4.6835394,-3.2086318,11.591739,3.231936,-6.786726,-6.098963,-5.6557603,12.646362,12.337187,12.83026,12.093095,12.7952175,12.3779125,11.592101,11.328603,6.0846553,5.859506,4.0645456,3.9640944,4.213288,4.1770887,3.6026773,13.265732,12.914402,13.417952,13.486582,13.936647,12.603154,11.73132,6.037723,-3.4690955,-3.1442363,-14.961878,-21.731482,-3.5294218,-1.2933614,-5.357257,6.724347,15.3630085,12.920783,12.998602,12.8273535,-7.005173,-0.7409132,-2.5251107,-0.59724176,-5.3807273,-4.2850037,-7.8044963,-3.3043525,-2.1463675,-1.7713023,-1.4410468,-2.3704426,-2.315131,-2.5292563,-2.611778,-2.8926065,-2.2619202,-2.0923321,8.61187,-12.459494,-12.401782,-12.432896,-12.347396,-12.477144,-12.632578,-12.692084,19.175682,19.996382,19.37314,17.243546,2.122079,1.6309646,2.2018504,1.1112031,0.6857186,0.6928535,7.57066,11.948996,1.5646427,8.771357,-0.016677655,-0.76651585,2.3849823,-1.4173318,-1.8067291,-2.9048905,-2.7742603,-5.360271,-1.3155676,-4.2624974,16.465012,17.9236,17.932049,-0.87011343,-1.4332864,-3.4634407,-7.232303,-6.885778,0.53228223,-0.51221687,15.611942,15.57348,15.574928,15.51209,15.782971,15.778326,15.761829,15.629571,15.787809,9.051207,9.04338,0.8105717,2.393653,7.6720796,-3.123636,-0.53860366,7.6629777,-1.9798129,-1.5598758,0.33654985,0.44036204,0.482413,12.739267,-0.2468782,8.538433,-4.570819,1.8786355,-2.7923558,-7.376175,-4.1340766,-6.0346737,-2.8107638,-4.484248,0.7227578,-0.51536363,-2.9224393,-3.3418214,-0.3959762,-6.6097536,-4.4151587,-2.932,-3.502115,-2.8962092,8.003396,0.99850607,3.5935147,3.719174,2.018312,7.5911884,13.361046,12.375692,-7.198617,-2.544654,7.1224365,13.277262,-6.104869,7.283331,-0.15914333,-0.2635383,-1.7379401,-1.6859357,11.114455,2.1914837,1.8034467,1.6356078,1.4919833,1.3048011,-3.9492488,5.8513885,-3.1859407,-2.8311923,-2.408652,-2.975502,-0.7298504,-1.7603701,-1.5864077,-1.7240794,-2.2064,-1.5365025,-1.9663466,-2.401341,-1.1195023,-1.5390596,-2.7212284,-2.5634055,-2.1210392,-3.4290478,3.0054283,-2.9738922,12.296644,-0.22753681,0.36966842,-0.9084696,13.1728325,-0.065521196,-0.018153466,15.162772,-0.70852524,-0.9758174,-0.8368364,-1.0151848,-9.706725,-9.643327,-0.23553845,-1.3848071,-8.142767,0.6375175,3.686085,11.312733,11.237464,11.670573,12.019056,-5.463701,13.250396,-2.0207224,-5.5814085,-8.494366,-2.8572145,-6.981235,-4.3472466,-4.0531116,-7.499456,-4.0384088,-4.123397,-3.844537,-1.815581,-1.419029,3.0287423,3.7725303,3.3525987,3.1077802,-0.17397,-0.13866009,-4.9226913,-5.8463225,7.4197507,-12.9263115,-13.1150465,-12.63863,-3.8589358,-1.9433197,-5.679279,-5.125239,-2.7647002,-1.1211792,-1.5116735,-1.7472506,-6.3075824,-7.8647614,-5.4169464,-6.5079465,-0.18879007,-0.21014562,13.305241,3.8547966,4.9369183,13.744034,10.572276,14.127529,14.064391,14.093209,14.089163,3.0388906,3.0887456,-0.070560805,2.3380232,1.9149958,2.0188375,-2.934199,-1.4765009,2.029271,-9.605716,-11.43692,-11.37523,2.0591154,19.504017,19.837538,18.364046,17.51036,8.452654,8.4714575,5.7674274,-0.17428909,-0.5944171,-9.782802,-7.3800907,-6.454796,-6.1259985,-4.4449325,-6.3566775,-5.876156,-6.007016,-5.758309,-6.102042,-6.314485,-6.2247744,-5.4544334,-6.28087,-6.836156,-5.6731863,-5.24611,-4.9582,-5.668504,-6.598681,16.297749,16.91277,18.12639,16.90114,17.627422,-1.6780955,-2.8320518,-8.026949,13.032036,13.233822,12.91064,13.1476555,13.058882,13.293182,-3.9757228,3.2601414,2.526776,2.3289607,2.999371,2.423612,2.6893876,2.0831537,-4.69539,13.218572,13.23951,3.2493017,-10.51987,-10.587024,-10.159685,-9.970876,-10.656239,-10.171724,-10.646841,-1.2897712,-1.4367629,0.2056289,-10.556989,-1.3182775,-10.418426,-10.440224,14.342092,1.5008774,14.27062,14.669605,-3.1180584,-1.3880532,-1.054345,-0.48646352,-1.1292366,-0.22814105,-0.7564196,-0.6767393,1.8074057,1.3589289,-0.17901598,-0.29665953,-0.006721521,0.30137053,1.7483914,1.382197,5.9746947,1.7772168,5.7308927,9.912672,10.424398,4.3344154,5.936944,5.876766,-5.9723845,-5.869394,-2.417018,-4.293394,-5.9654536,-0.22591813,-1.5006365,-0.30287254,0.32358184,0.26573983,3.2475407,3.3560195,11.59222,12.932044,1.8572577,-3.5030746,6.1983747,6.207144,-1.3809187,-2.0998368,-1.8763827,-2.1498098,-1.1338623,-2.2018068,-2.2785184,-7.3315415,-6.0804796,0.7054842,2.6975555,7.1870894,7.3518777,7.4771934,7.076403,7.2438097,7.039576,-2.2122073,-1.909239,-0.64668435,-2.703229,-9.547609,-4.838242,-3.591209,-3.9163387,-3.9178653,2.1785927,-4.5869007,-0.4780996,-9.2037115,-0.5937244,-2.98929,13.094003,12.335324,11.932917,2.1524837,-0.36297178,-1.4437541,-0.9085126,-0.34553564,13.048337,-5.394317,-3.7255063,-5.787636,-7.0371227,-5.9978313,-4.8395734,-2.3662727,-1.8439516,-0.99815834,-4.7033086,0.22727618],\"xaxis\":\"x\",\"y\":[2.3247867,0.40816537,-18.070715,-18.508747,1.6372502,-3.821188,-3.3454807,-3.1092937,-5.543653,10.049476,7.5044446,3.7475162,4.0171328,3.8386488,3.4719608,3.7216582,4.0855145,1.4439536,0.9609338,0.39870006,-7.6743937,-7.539789,-7.2212048,-0.55175275,3.5582204,3.6339698,2.5560343,2.9814088,3.4329352,3.0560071,2.93903,3.108165,3.6392326,3.3756657,3.4290974,3.8009822,2.7189796,3.3745887,3.4245255,2.772164,3.4244912,3.6210544,2.8773339,3.7354412,3.5584774,2.9194968,3.4922202,2.1553507,0.8028625,7.7101836,0.79431623,0.88524634,0.936397,5.3967724,1.4282831,3.8800907,3.82557,4.9487257,3.8211162,3.4328935,3.9584887,3.7352252,3.7971117,7.062093,-6.549029,-5.7448254,-5.59048,2.504029,-0.16803913,-12.060155,-7.837263,-7.4621043,0.8763386,3.8123286,3.9081001,3.8007123,3.8896585,3.8124816,3.7502937,-0.26597992,-3.4996212,-5.5263076,4.424584,4.7345695,4.2692366,3.6459904,4.1985393,-0.09974432,2.5246575,0.48707095,2.8931139,-5.5399323,20.71929,20.75605,-2.9287584,-4.58863,-6.027273,-6.263016,1.4180822,-5.528306,-4.8839025,-2.9288568,-2.783618,-4.254693,-5.2466116,-4.7722445,0.7410588,-7.450056,1.8797615,0.21406084,-1.1392184,-0.42596024,-0.30356887,-1.6379567,-1.9670575,2.847178,2.9427376,-11.914095,5.4578867,3.652458,4.003141,3.176178,-1.2198917,-0.33356962,0.2903787,0.14902046,3.5955746,3.6457732,3.0151038,3.489853,-5.39509,-5.397186,3.118074,-0.22565927,-0.83006746,-0.24031174,-17.277216,-18.286493,2.6845279,0.27121642,5.3608966,7.5500255,-1.639136,5.19259,1.8405242,-1.7477976,-0.61873025,-0.18622118,5.1934204,4.001359,3.7002954,2.8540812,2.4353218,1.7438385,-0.3165187,2.2744632,2.4233928,8.554379,-0.5681131,-2.2794726,2.9523354,6.6425366,6.049641,5.4195786,-20.007427,-20.006876,-20.00338,-20.006176,0.15518408,0.91179305,4.3056993,3.9881735,3.702489,2.281261,3.193076,-0.6395596,1.306158,-1.9648541,-0.36779177,-6.8188763,-6.991517,-7.1084175,-7.5453687,1.3436544,2.373579,2.5856993,1.5788656,1.6876726,-5.9396334,0.20046781,2.2345533,3.64879,3.164477,4.346902,3.7380626,3.2118735,1.2655594,8.002167,7.798978,6.906919,-1.0939817,-0.6049401,0.2126456,2.2024372,-1.4028733,-1.5946383,-1.3321565,-1.5451746,-1.6219052,1.5578698,2.35438,2.7970371,2.9583137,2.9426887,2.8361485,-10.589953,9.961833,7.563046,8.437406,7.838193,7.822612,6.414958,2.5757318,4.5583396,2.2966483,1.7491419,0.46709868,4.114905,-2.9161446,2.4403076,2.6199067,-9.884406,-0.37481293,-0.26109576,0.32886297,0.6710507,1.3010981,3.3070214,2.680051,3.660294,1.7841234,-11.812456,1.7617155,2.338963,5.84668,5.768994,5.862704,2.667053,3.5342054,3.9710267,-0.13726464,5.6228695,5.635168,2.478147,5.291044,0.6088094,9.932632,6.684283,8.130692,6.6201706,0.8925756,7.0130005,3.4704611,2.8914723,3.1661878,3.6052806,-6.278343,-5.965872,-5.705327,-1.8198191,-2.821354,0.46430632,-0.9395663,4.5648327,0.45365193,1.4438046,1.7990187,-4.921537,-4.768442,-4.6797814,-2.6948683,-5.6351223,-6.2215166,-5.9220614,-5.7791853,-4.4501987,-6.1224265,0.088343985,2.205678,2.4369783,0.19225408,-0.28073004,-0.62301075,-7.1325707,-9.58442,4.116791,4.0172367,3.547707,3.378023,3.7242188,4.421176,4.150836,3.7317295,3.2657626,4.547006,5.5407934,-6.411479,20.715187,20.752428,-5.2880464,-4.505908,-5.8021355,1.9281955,-5.7966785,1.930265,1.6447841,-5.5939703,-5.837303,-4.655699,-3.8361843,-5.832782,-5.4479995,-6.2361746,-0.87340117,-0.9915937,-2.115203,19.450815,14.811288,-4.217388,-4.2338676,-4.2042522,-3.7138174,-4.399114,-3.6835656,2.2184243,2.1663048,2.2855797,2.0435512,2.9632325,3.335887,2.849875,7.7285767,2.6722038,3.884314,4.282593,4.4357505,-0.4460429,4.430075,4.304208,-0.21311697,2.4552188,3.4585433,-0.8199021,3.6993573,-6.2721686,-8.445973,17.957516,3.3784838,-0.89903075,-0.91532,-0.78394127,3.6362002,3.4106915,3.9208436,4.45262,3.9783227,5.0488734,4.5831776,4.4654202,0.14233622,0.1918084,1.2441502,1.0519217,0.27258652,0.7249866,0.7381646,3.0641465,3.3578901,3.196686,3.3699222,3.4546711,2.862589,2.467737,1.616297,5.5583825,5.669425,-2.1050558,8.552712,-5.7597322,-3.002292,2.4457664,1.7987268,3.4886618,4.078217,4.5304995,4.0362034,6.4602222,-1.2242862,-1.3444926,-0.9164937,5.1113596,-0.5418871,6.9976435,-6.589493,-6.8084793,-7.1460996,-6.7971377,-6.77331,-7.0325446,-6.728908,-6.785946,-7.1690087,-7.0889177,0.03988315,2.4836006,-18.140121,-17.89884,-17.90112,-17.385748,-17.919218,-18.20955,-18.062273,2.423607,2.3022423,2.6597052,3.1080513,0.16617319,0.43714374,0.6705076,1.8827145,-0.34808215,1.492098,2.9830112,2.2761931,3.7164192,-11.569568,3.548414,-0.4619981,0.59159285,1.2587276,1.526156,2.7746336,-5.064208,-6.4020243,-7.0286326,4.1005564,3.4309416,4.134459,4.3396177,5.1054745,-0.48203024,-1.5610274,2.8386405,3.1395447,8.565885,7.5341744,-16.174953,-16.210056,-16.205341,-16.259075,-16.037693,-16.036608,-16.048286,-16.161428,-16.029602,0.37249008,0.38438568,0.76378894,1.3896978,3.5935345,4.332552,4.685003,3.5834372,-2.552226,-2.3333294,8.232785,8.4833355,8.4518385,4.8425455,1.061251,-11.738212,-6.5341125,-0.07295447,-1.3856744,-2.3311307,-6.235615,0.6999465,1.2761686,-0.76110744,7.092196,7.698721,1.5288756,3.103768,1.960324,6.609469,1.2645503,0.2631216,0.91427624,-7.859982,-9.476341,0.81034696,-1.9123392,-1.8007463,3.1248548,2.281259,5.547153,3.6320963,5.020027,1.2182934,3.0646942,5.620179,2.2716184,3.1217353,4.664168,4.499746,-1.9619097,-1.6984012,1.8548727,4.106206,4.288083,4.391959,4.567232,4.7301207,-0.25717592,-0.08885264,-5.3873763,-5.1218243,-5.9004803,-4.559063,-7.1502504,-7.6293607,-7.8006897,-7.8105865,-7.343054,-0.11068903,-6.441699,-7.193676,1.4250035,1.2922359,1.4314885,1.4212894,1.4464897,-2.0004978,-0.55127984,3.5555036,4.666685,9.910619,8.060254,7.7973747,3.270232,-6.3977175,-6.07731,3.134818,2.7184381,1.9518884,2.739587,3.24358,0.9520309,0.98362976,4.417142,4.0980563,0.4620366,2.9010408,-1.7479302,3.8540804,3.8583739,3.9481916,3.8418756,1.9249355,3.540359,-6.6442585,-6.445096,-0.777848,0.0069586304,2.671376,-1.6750782,-2.4158397,0.27567533,-3.21325,-3.2117522,-2.518675,2.7961922,1.304478,3.9467857,4.3573895,0.90344286,-1.6733601,8.247768,5.1715302,-0.5591183,-0.36403266,2.0403512,-18.109161,-18.25158,-18.499868,-3.0658097,0.28453815,-0.26257232,-0.702151,1.6214273,1.2719615,-7.107283,-7.4765463,2.0088284,1.9999127,1.6466949,4.1278787,9.851398,6.8123517,5.3928514,-1.7859527,-0.16369839,3.7401803,3.3609605,4.1970315,4.4293523,3.9898071,3.7190032,-2.4362407,-2.4067907,3.9057498,2.7411752,-2.0291984,-1.8641918,-5.5323315,-2.7012703,-0.4380064,-0.7995,-1.4294664,-1.37601,0.004630676,2.3108282,2.4100373,2.173153,3.7693036,-10.265457,-10.327911,-0.12467524,9.739345,7.4511933,0.17119044,6.2580132,6.597709,7.3587713,5.138016,0.40424368,0.92475057,0.7995351,0.058117274,0.5998479,-0.0007004567,0.2631054,0.4869965,0.074137695,-0.63625294,-0.090988085,-0.17030644,-1.6885452,-1.184584,-0.3540572,3.50393,3.3435822,3.479008,3.3072329,3.278283,0.36959612,2.3083265,0.26846555,3.708756,3.5844657,3.9061832,3.4583924,3.5830934,5.724969,-2.9626513,-1.8902336,-0.8539155,-1.5193583,-2.2219927,-1.6678207,-1.9355191,-1.4178293,2.392526,3.6022494,3.7323015,-2.1854835,-0.4559198,-0.3331245,-0.11101039,0.5524035,-0.6838692,-0.16197985,-0.6229367,0.62105995,0.593428,5.029843,-0.6736803,1.8575319,-0.5802142,-0.5544063,3.6732986,1.819856,3.682866,3.5882616,1.0962481,1.2629383,1.1468531,0.78301454,1.9240035,0.589257,0.8369979,1.0437225,0.12655948,0.31140926,9.950232,3.4425974,3.1345391,6.3920465,4.2312255,4.34578,1.6845484,0.4276989,1.6267284,3.2677357,3.4368541,1.309945,1.7858623,1.6297162,2.3405442,2.6045387,-12.066143,3.4089322,3.1536481,9.778466,6.4746413,7.999138,7.7075734,7.7086625,3.7812212,3.5286064,17.956404,5.241989,0.55105054,-5.533155,20.723406,20.757315,-3.1191823,-5.1590843,-5.86974,-4.942999,-3.7000942,-3.5695207,-4.5880957,3.2268934,3.264358,0.7065607,1.2192044,3.123824,2.9227428,2.8975415,2.8528397,2.8629253,2.8635228,4.4634757,4.2266855,0.8707114,2.2192223,0.15847515,2.2046044,2.0942473,2.728187,2.4717333,4.41195,1.243057,0.092687756,-0.77341086,-0.71715856,-1.9467918,5.431923,4.072123,3.831317,2.347842,-0.4330321,4.3064113,-0.8174771,-1.660938,5.01939,1.4415238,3.7872932,3.1937654,4.392146,3.4448416,3.474383,-12.553816,5.0931134,4.1758456,2.5849938,-0.9367142],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Net...\"],[\"- Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - ...\"],[\"Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2Net\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale ...\"],[\"Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: re...\"],[\"File Size: 101421406\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Aver...\"],[\"# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural archit...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Ensemble Adversarial\\n  Paper:\\n    Title: Adversarial Att...\"],[\"TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) that aim...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TResNet\\n  Paper:\\n    Title: 'TResNet: High Performance G...\"],[\"- Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    T...\"],[\"Top 1 Accuracy: 80.8%\\n      Top 5 Accuracy: 94.86%\\n- Name: tresnet_m_448\\n  In Collection: TResNet\\n  ...\"],[\"ID: tresnet_xl\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: ...\"],[\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"Replace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the IDs i...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Net...\"],[\"- Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - ...\"],[\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNeXt\\n  Paper:\\n    Title: 'CSPNet: A New Backbone ...\"],[\"RexNet\\n\\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bo...\"],[\"Replace the model name with the variant you want to use, e.g. `rexnet_100`. You can find the IDs in ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RexNet\\n  Paper:\\n    Title: 'ReXNet: Diminishing Represen...\"],[\"Training Resources: 4x NVIDIA V100 GPUs\\n    ID: rexnet_130\\n    LR: 0.5\\n    Epochs: 400\\n    Dropout: ...\"],[\"Metadata:\\n    FLOPs: 1960224938\\n    Parameters: 16370000\\n    File Size: 65862221\\n    Architecture:\\n ...\"],[\"RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parame...\"],[\"To get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\\\"...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RegNetY\\n  Paper:\\n    Title: Designing Network Design Spa...\"],[\"ID: regnety_004\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Ima...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - G...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77....\"],[\"ID: regnety_040\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Imag...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - G...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"ID: regnety_320\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Imag...\"],[\"Feature Extraction\\n\\nAll of the models in `timm` have consistent mechanisms for obtaining various typ...\"],[\"#### Remove it later\\n```python hl_lines=\\\"3 6\\\"\\nimport torch\\nimport timm\\nm = timm.create_model('densen...\"],[\"`timm` allows a consistent interface for creating any of the included models as feature backbones th...\"],[\"* `out_indices` selects which indices to output\\n* `output_stride` limits the feature output stride o...\"],[\"Wide ResNet\\n\\n**Wide Residual Networks** are a variant on [ResNets](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod...\"],[\"Replace the model name with the variant you want to use, e.g. `wide_resnet101_2`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Wide ResNet\\n  Paper:\\n    Title: Wide Residual Networks\\n ...\"],[\"Res2NeXt\\n\\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https:\\u002f\\u002fpaperswithcod...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2NeXt\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale...\"],[\"RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parame...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RegNetY\\n  Paper:\\n    Title: Designing Network Design Spa...\"],[\"ID: regnety_004\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Ima...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - G...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77....\"],[\"ID: regnety_040\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Imag...\"],[\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - G...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"ID: regnety_320\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Imag...\"],[\"Archived Changes\\n\\n### Nov 22, 2021\\n* A number of updated weights anew new model defs\\n  * `eca_halone...\"],[\"### Oct 19, 2021\\n* ResNet strikes back (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2110.00476) weights added, plus any ex...\"],[\"### Aug 18, 2021\\n* Optimizer bonanza!\\n  * Add LAMB and LARS optimizers, incl trust ratio clipping op...\"],[\"### June 23, 2021\\n* Reproduce gMLP model training, `gmlp_s16_224` trained to 79.6 top-1, matching [p...\"],[\"### June 20, 2021\\n* Release Vision Transformer 'AugReg' weights from [How to train your ViT? Data, A...\"],[\"### June 8, 2021\\n* Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w\\u002f my XLA branch. 24 b...\"],[\"### May 5, 2021\\n* Add MLP-Mixer models and port pretrained weights from [Google JAX impl](https:\\u002f\\u002fgi...\"],[\"### April 1, 2021\\n* Add snazzy `benchmark.py` script for bulk `timm` model benchmarking of train and...\"],[\"### Feb 18, 2021\\n* Add pretrained weights and model variants for NFNet-F* models from [DeepMind Haik...\"],[\"### Feb 12, 2021\\n* Update Normalization-Free nets to include new NFNet-F (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102...\"],[\"### Jan 30, 2021\\n* Add initial \\\"Normalization Free\\\" NF-RegNet-B* and NF-ResNet model definitions bas...\"],[\"### Dec 7, 2020\\n* Simplify EMA module (ModelEmaV2), compatible with fully torchscripted models\\n* Mis...\"],[\"### Sept 18, 2020\\n* New ResNet 'D' weights. 72.7 (top-1) ResNet-18-D, 77.1 ResNet-34-D, 80.5 ResNet-...\"],[\"### Aug 5, 2020\\nUniversal feature extraction, new models, new weights, new test sets.\\n* All models s...\"],[\"### June 11, 2020\\nBunch of changes:\\n* DenseNet models updated with memory efficient addition from to...\"],[\"### April 5, 2020\\n* Add some newly trained MobileNet-V2 models trained with latest h-params, rand au...\"],[\"### Feb 18, 2020\\n* Big refactor of model layers and addition of several attention mechanisms. Severa...\"],[\"### Jan 31, 2020\\n* Update ResNet50 weights with a new 79.038 result from further JSD \\u002f AugMix experi...\"],[\"### Dec 23, 2019\\n* Add RandAugment trained MixNet-XL weights with 80.48 top-1.\\n* `--dist-bn` argumen...\"],[\"(Tensorflow) EfficientNet CondConv\\n\\n**EfficientNet** is a convolutional neural network architecture ...\"],[\"config = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename ...\"],[\"## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet CondConv\\n  Paper:\\n    Title: 'CondConv: ...\"],[\"- Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n ...\"],[\"Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image...\"],[\"SWSL ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-blo...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SWSL ResNext\\n  Paper:\\n    Title: Billion-scale semi-supe...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - IG-1B-Targe...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 84.27%\\n      Top 5 Accuracy: 97.17%\\n- Name: sws...\"],[\"Model Summaries\\n\\nThe model architectures included come from a wide variety of sources. Sources, incl...\"],[\"## DLA [[dla.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fdla.py)]\\n...\"],[\"## Inception-ResNet-V2 [[inception_resnet_v2.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fb...\"],[\"## EfficientNet [[efficientnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftim...\"],[\"## ResNet, ResNeXt [[resnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fm...\"],[\"## Res2Net [[res2net.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fr...\"],[\"## VGG [[vgg.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fvgg.py)]\\n...\"],[\"## Xception (Modified Aligned, TF) [[aligned_xception.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image...\"],[\"(Tensorflow) MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mo...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF MobileNet V3\\n  Paper:\\n    Title: Searching for Mobile...\"],[\"- ReLU\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - I...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_mobilen...\"],[\"- Dropout\\n    - Global Average Pooling\\n    - Hard Swish\\n    - Inverted Residual Block\\n    - ReLU\\n   ...\"],[\"Interpolation: bilinear\\n    RMSProp Decay: 0.9\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mo...\"],[\"Sharing and Loading Models From the Hugging Face Hub\\n\\nThe `timm` library has a built-in integration ...\"],[\"Big Transfer (BiT)\\n\\n**Big Transfer (BiT)** is a type of pretraining recipe that pre-trains  on a lar...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Big Transfer\\n  Paper:\\n    Title: 'Big Transfer (BiT): Ge...\"],[\"- Weight Standardization\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Mixup\\n...\"],[\"- Name: resnetv2_152x4_bitm\\n  In Collection: Big Transfer\\n  Metadata:\\n    FLOPs: 21317584\\n    Parame...\"],[\"LR: 0.03\\n    Epochs: 90\\n    Layers: 50\\n    Crop Pct: '1.0'\\n    Momentum: 0.9\\n    Batch Size: 4096\\n  ...\"],[\"MobileNet v2\\n\\n**MobileNetV2** is a convolutional neural network architecture that seeks to perform w...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MobileNet V2\\n  Paper:\\n    Title: 'MobileNetV2: Inverted ...\"],[\"- RMSProp\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 16x GPUs\\n    ...\"],[\"- Name: mobilenetv2_140\\n  In Collection: MobileNet V2\\n  Metadata:\\n    FLOPs: 770196784\\n    Parameter...\"],[\"EfficientNet (Knapsack Pruned)\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"\\u003e\\u003e\\u003e config = resolve_data_config({}, model=model)\\n\\u003e\\u003e\\u003e transform = create_transform(**config)\\n\\n\\u003e\\u003e\\u003e ur...\"],[\"## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing th...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: EfficientNet Pruned\\n  Paper:\\n    Title: Knapsack Pruning...\"],[\"Weights: https:\\u002f\\u002fimvl-automl-sh.oss-cn-shanghai.aliyuncs.com\\u002fdarts\\u002fhyperml\\u002fhyperml\\u002fjob_45403\\u002foutputs...\"],[\"(Tensorflow) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the ...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF Inception v3\\n  Paper:\\n    Title: Rethinking the Incep...\"],[\"DenseNet\\n\\n**DenseNet** is a type of convolutional neural network that utilises dense connections bet...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"```\\n@misc{rw2019timm,\\n  author = {Ross Wightman},\\n  title = {PyTorch Image Models},\\n  year = {2019},...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DenseNet\\n  Paper:\\n    Title: Densely Connected Convoluti...\"],[\"Training Data:\\n    - ImageNet\\n    ID: densenet161\\n    LR: 0.1\\n    Epochs: 90\\n    Layers: 161\\n    Dro...\"],[\"- 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Bloc...\"],[\"Top 5 Accuracy: 93.2%\\n- Name: tv_densenet121\\n  In Collection: DenseNet\\n  Metadata:\\n    FLOPs: 364184...\"],[\"Training Examples\\n\\n## EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5\\nThese params are for...\"],[\"`.\\u002fdistributed_train.sh 2 \\u002fimagenet\\u002f --model seresnext26t_32x4d --lr 0.1 --warmup-epochs 5 --epochs ...\"],[\"`.\\u002fdistributed_train.sh 2 \\u002fimagenet -b 64 --model resnet50 --sched cosine --epochs 200 --lr 0.05 --a...\"],[\"MnasNet\\n\\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MNASNet\\n  Paper:\\n    Title: 'MnasNet: Platform-Aware Neu...\"],[\"- Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageN...\"],[\"SK-ResNet\\n\\n**SK ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SKResNet\\n  Paper:\\n    Title: Selective Kernel Networks\\n ...\"],[\"ID: skresnet34\\n    LR: 0.1\\n    Epochs: 100\\n    Layers: 34\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n  ...\"],[\"Inception v4\\n\\n**Inception-v4** is a convolutional neural network architecture that builds on previou...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception v4\\n  Paper:\\n    Title: Inception-v4, Inception...\"],[\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNeXt\\n  Paper:\\n    Title: 'CSPNet: A New Backbone ...\"],[\"MnasNet\\n\\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MNASNet\\n  Paper:\\n    Title: 'MnasNet: Platform-Aware Neu...\"],[\"- Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageN...\"],[\"NASNet\\n\\n**NASNet** is a type of convolutional neural network discovered through neural architecture ...\"],[\"Replace the model name with the variant you want to use, e.g. `nasnetalarge`. You can find the IDs i...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: NASNet\\n  Paper:\\n    Title: Learning Transferable Archite...\"],[\"SelecSLS\\n\\n**SelecSLS** uses novel selective long and short range skip connections to improve the inf...\"],[\"Replace the model name with the variant you want to use, e.g. `selecsls42b`. You can find the IDs in...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SelecSLS\\n  Paper:\\n    Title: 'XNect: Real-time Multi-Per...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-selecsls\\u002fselecsls6...\"],[\"MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that utilises [Mi...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MixNet\\n  Paper:\\n    Title: 'MixConv: Mixed Depthwise Con...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77....\"],[\"DenseNet\\n\\n**DenseNet** is a type of convolutional neural network that utilises dense connections bet...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"```\\n@misc{rw2019timm,\\n  author = {Ross Wightman},\\n  title = {PyTorch Image Models},\\n  year = {2019},...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DenseNet\\n  Paper:\\n    Title: Densely Connected Convoluti...\"],[\"Training Data:\\n    - ImageNet\\n    ID: densenet161\\n    LR: 0.1\\n    Epochs: 90\\n    Layers: 161\\n    Dro...\"],[\"- 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Bloc...\"],[\"Top 5 Accuracy: 93.2%\\n- Name: tv_densenet121\\n  In Collection: DenseNet\\n  Metadata:\\n    FLOPs: 364184...\"],[\"SK-ResNeXt\\n\\n**SK ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnext...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SKResNeXt\\n  Paper:\\n    Title: Selective Kernel Networks\\n...\"],[\"Recent Changes\\n\\n### Aug 29, 2022\\n* MaxVit window size scales with img_size by default. Add new RelPo...\"],[\"### Aug 15, 2022\\n* ConvNeXt atto weights added\\n  * `convnext_atto` - 75.7 @ 224, 77.0 @ 288\\n  * `con...\"],[\"### July 8, 2022\\nMore models, more fixes\\n* Official research models (w\\u002f weights) added:\\n  * EdgeNeXt...\"],[\"* Add support to change image extensions scanned by `timm` datasets\\u002freaders. See (https:\\u002f\\u002fgithub.com...\"],[\"### May 13, 2022\\n* Official Swin-V2 models and weights added from (https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fSwin...\"],[\"### May 2, 2022\\n* Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (`visi...\"],[\"### March 21, 2022\\n* Merge `norm_norm_norm`. **IMPORTANT** this update for a coming 0.6.x release wi...\"],[\"* Swin-S3 (AutoFormerV2) models \\u002f weights added from https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fCream\\u002ftree\\u002fmain\\u002fAu...\"],[\"### Feb 2, 2022\\n* [Chris Hughes](https:\\u002f\\u002fgithub.com\\u002fChris-hughes10) posted an exhaustive run through...\"],[\"### Dec 23, 2022 🎄☃\\n* Add FlexiViT models and weights from https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002fbig_vi...\"],[\"### Dec 6, 2022\\n* Add 'EVA g', BEiT style ViT-g\\u002f14 model weights w\\u002f both MIM pretrain and CLIP pretr...\"],[\"| model                                            |   top1 |   param_count |   gmac |   macts | hub...\"],[\"| vit_large_patch14_clip_336.laion2b_ft_in1k       |   87.9 |         304.5 |  191.1 |   270.2 | [li...\"],[\"| vit_base_patch32_clip_448.laion2b_ft_in12k_in1k  |   85.8 |          88.3 |   17.9 |    23.9 | [li...\"],[\"* Port of MaxViT Tensorflow Weights from official impl at https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002fmaxvit\\n...\"],[\"| model                              |   top1 |   param_count |   gmac |   macts | hub              ...\"],[\"| maxvit_large_tf_384.in1k           |   86.2 |         212   |  132.6 |   445.8 | [link](https:\\u002f\\u002fhu...\"],[\"### Oct 15, 2022\\n* Train and validation script enhancements\\n* Non-GPU (ie CPU) device support\\n* SLUR...\"],[\"### Sept 7, 2022\\n* Hugging Face [`timm` docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002ftimm) home now exists,...\"],[\"### Aug 29, 2022\\n* MaxVit window size scales with img_size by default. Add new RelPosMlp MaxViT weig...\"],[\"### Aug 15, 2022\\n* ConvNeXt atto weights added\\n  * `convnext_atto` - 75.7 @ 224, 77.0 @ 288\\n  * `con...\"],[\"### July 8, 2022\\nMore models, more fixes\\n* Official research models (w\\u002f weights) added:\\n  * EdgeNeXt...\"],[\"* Add support to change image extensions scanned by `timm` datasets\\u002fparsers. See (https:\\u002f\\u002fgithub.com...\"],[\"### May 13, 2022\\n* Official Swin-V2 models and weights added from (https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fSwin...\"],[\"### May 2, 2022\\n* Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (`visi...\"],[\"### March 21, 2022\\n* Merge `norm_norm_norm`. **IMPORTANT** this update for a coming 0.6.x release wi...\"],[\"* Swin-S3 (AutoFormerV2) models \\u002f weights added from https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fCream\\u002ftree\\u002fmain\\u002fAu...\"],[\"### Feb 2, 2022\\n* [Chris Hughes](https:\\u002f\\u002fgithub.com\\u002fChris-hughes10) posted an exhaustive run through...\"],[\"Hugging Face Timm Docs\\n\\n## Getting Started\\n\\n```\\npip install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdoc-b...\"],[\"ResNeSt\\n\\nA **ResNeSt** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet), which i...\"],[\"To get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\\\"...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNeSt\\n  Paper:\\n    Title: 'ResNeSt: Split-Attention Ne...\"],[\"- Label Smoothing\\n    - Mixup\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - Im...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83....\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 64x NVIDIA V100 GPUs\\n    ID...\"],[\"Metrics:\\n      Top 1 Accuracy: 80.96%\\n      Top 5 Accuracy: 95.38%\\n- Name: resnest50d_1s4x24d\\n  In C...\"],[\"- ImageNet\\n    Training Resources: 64x NVIDIA V100 GPUs\\n    ID: resnest50d_4s2x40d\\n    LR: 0.1\\n    E...\"],[\"FBNet\\n\\n**FBNet** is a type of convolutional neural architectures discovered through [DNAS](https:\\u002f\\u002fp...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: FBNet\\n  Paper:\\n    Title: 'FBNet: Hardware-Aware Efficie...\"],[\"HRNet\\n\\n**HRNet**, or **High-Resolution Net**, is a general purpose convolutional neural network for ...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: HRNet\\n  Paper:\\n    Title: Deep High-Resolution Represent...\"],[\"Weight Decay: 0.001\\n    Interpolation: bilinear\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-m...\"],[\"- ImageNet\\n    Training Resources: 4x NVIDIA V100 GPUs\\n    ID: hrnet_w30\\n    Epochs: 100\\n    Layers:...\"],[\"- Convolution\\n    - ReLU\\n    - Residual Connection\\n    Tasks:\\n    - Image Classification\\n    Trainin...\"],[\"- Name: hrnet_w48\\n  In Collection: HRNet\\n  Metadata:\\n    FLOPs: 22285865760\\n    Parameters: 77470000...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Learning Rate Schedulers\\n\\nThis page contains the API reference documentation for learning rate sched...\"],[\"MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mobile phone CP...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MobileNet V3\\n  Paper:\\n    Title: Searching for MobileNet...\"],[\"- Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Tech...\"],[\"ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer ...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNet\\n  Paper:\\n    Title: Deep Residual Learning for Im...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fresnet101-5d3b4d8f.pth\\n  Results:\\n  - Task: Image Class...\"],[\"LR: 0.1\\n    Epochs: 90\\n    Crop Pct: '0.875'\\n    LR Gamma: 0.1\\n    Momentum: 0.9\\n    Batch Size: 32\\n...\"],[\"SK-ResNet\\n\\n**SK ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SKResNet\\n  Paper:\\n    Title: Selective Kernel Networks\\n ...\"],[\"ID: skresnet34\\n    LR: 0.1\\n    Epochs: 100\\n    Layers: 34\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n  ...\"],[\"SelecSLS\\n\\n**SelecSLS** uses novel selective long and short range skip connections to improve the inf...\"],[\"Replace the model name with the variant you want to use, e.g. `selecsls42b`. You can find the IDs in...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SelecSLS\\n  Paper:\\n    Title: 'XNect: Real-time Multi-Per...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-selecsls\\u002fselecsls6...\"],[\"PNASNet\\n\\n**Progressive Neural Architecture Search**, or **PNAS**, is a method for learning the struc...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: PNASNet\\n  Paper:\\n    Title: Progressive Neural Architect...\"],[\"(Tensorflow) EfficientNet CondConv\\n\\n**EfficientNet** is a convolutional neural network architecture ...\"],[\"To load and preprocess the image:\\n\\n```py\\n\\u003e\\u003e\\u003e import urllib\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e from timm.d...\"],[\"To extract image features with this model, follow the [timm feature extraction examples](..\\u002ffeature_...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet CondConv\\n  Paper:\\n    Title: 'CondConv: ...\"],[\"- Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n ...\"],[\"Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image...\"],[\"ECA-ResNet\\n\\nAn **ECA ResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) t...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ECAResNet\\n  Paper:\\n    Title: 'ECA-Net: Efficient Channe...\"],[\"- Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classi...\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.61%\\n      T...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n  ...\"],[\"Validation and Benchmark Results\\n\\nThis folder contains validation and benchmark results for the mode...\"],[\"An ImageNet test set of 10,000 images sampled from new images roughly 10 years after the original. C...\"],[\"## Benchmark\\n\\nCSV files with a `model_benchmark` prefix include benchmark numbers for models on vari...\"],[\"Results\\n\\nCSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation resul...\"],[\"|Model | Acc@1 (Err) | Acc@5 (Err) | Param # (M) | Interpolation | Image Size |\\n|---|---|---|---|---...\"],[\"| efficientnet_es | 78.066 (21.934) | 93.926 (6.074) | 5.44 | bicubic | 224 |\\n| seresnext26t_32x4d |...\"],[\"| spnasnet_100 | 74.084 (25.916)  | 91.818 (8.182) | 4.42 | bilinear | 224 |\\n| skresnet18 | 73.038 (...\"],[\"## Ported and Other Weights\\n\\nFor weights ported from other deep learning frameworks (Tensorflow, MXN...\"],[\"Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Inception fam...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception v3\\n  Paper:\\n    Title: Rethinking the Inceptio...\"],[\"CSP-DarkNet\\n\\n**CSPDarknet53** is a convolutional neural network and backbone for object detection th...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP DarkNet\\n  Paper:\\n    Title: 'YOLOv4: Optimal Speed a...\"],[\"SPNASNet\\n\\n**Single-Path NAS** is a novel differentiable NAS method for designing hardware-efficient ...\"],[\"Replace the model name with the variant you want to use, e.g. `spnasnet_100`. You can find the IDs i...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SPNASNet\\n  Paper:\\n    Title: 'Single-Path NAS: Designing...\"],[\"Wide ResNet\\n\\n**Wide Residual Networks** are a variant on [ResNets](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Wide ResNet\\n  Paper:\\n    Title: Wide Residual Networks\\n ...\"],[\"SSL ResNeXT\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-bloc...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SSL ResNext\\n  Paper:\\n    Title: Billion-scale semi-super...\"],[\"- SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    - YFCC-100M\\n    Trainin...\"],[\"Top 1 Accuracy: 81.61%\\n      Top 5 Accuracy: 96.04%\\n- Name: ssl_resnext50_32x4d\\n  In Collection: SSL...\"],[\"RegNetX\\n\\n**RegNetX** is a convolutional network design space with simple, regular models with parame...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RegNetX\\n  Paper:\\n    Title: Designing Network Design Spa...\"],[\"ID: regnetx_004\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Ima...\"],[\"- Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    Tasks:\\n   ...\"],[\"Metrics:\\n      Top 1 Accuracy: 76.95%\\n      Top 5 Accuracy: 93.43%\\n- Name: regnetx_032\\n  In Collecti...\"],[\"Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n  ...\"],[\"Metadata:\\n    FLOPs: 20491740672\\n    Parameters: 54280000\\n    File Size: 217623862\\n    Architecture:...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-regnet\\u002fregnetx_320...\"],[\"Installation\\n\\nBefore you start, you'll need to setup your environment and install the appropriate pa...\"],[\"```bash\\ngit clone https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models.git\\ncd timm\\npip install -e .\\n```...\"],[\"Results\\n\\nCSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation resul...\"],[\"|Model | Acc@1 (Err) | Acc@5 (Err) | Param # (M) | Interpolation | Image Size |\\n|---|---|---|---|---...\"],[\"| efficientnet_es | 78.066 (21.934) | 93.926 (6.074) | 5.44 | bicubic | 224 |\\n| seresnext26t_32x4d |...\"],[\"| spnasnet_100 | 74.084 (25.916)  | 91.818 (8.182) | 4.42 | bilinear | 224 |\\n| skresnet18 | 73.038 (...\"],[\"## Ported and Other Weights\\n\\nFor weights ported from other deep learning frameworks (Tensorflow, MXN...\"],[\"(Tensorflow) MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mo...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF MobileNet V3\\n  Paper:\\n    Title: Searching for Mobile...\"],[\"- ReLU\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - I...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_mobilen...\"],[\"- Dropout\\n    - Global Average Pooling\\n    - Hard Swish\\n    - Inverted Residual Block\\n    - ReLU\\n   ...\"],[\"Interpolation: bilinear\\n    RMSProp Decay: 0.9\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mo...\"],[\"Big Transfer (BiT)\\n\\n**Big Transfer (BiT)** is a type of pretraining recipe that pre-trains  on a lar...\"],[\"Replace the model name with the variant you want to use, e.g. `resnetv2_101x1_bitm`. You can find th...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Big Transfer\\n  Paper:\\n    Title: 'Big Transfer (BiT): Ge...\"],[\"- Weight Standardization\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Mixup\\n...\"],[\"- Name: resnetv2_152x4_bitm\\n  In Collection: Big Transfer\\n  Metadata:\\n    FLOPs: 21317584\\n    Parame...\"],[\"LR: 0.03\\n    Epochs: 90\\n    Layers: 50\\n    Crop Pct: '1.0'\\n    Momentum: 0.9\\n    Batch Size: 4096\\n  ...\"],[\"Instagram ResNeXt WSL\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fre...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: IG ResNeXt\\n  Paper:\\n    Title: Exploring the Limits of W...\"],[\"- Weight Decay\\n    Training Data:\\n    - IG-3.5B-17k\\n    - ImageNet\\n    Training Resources: 336x GPUs...\"],[\"In Collection: IG ResNeXt\\n  Metadata:\\n    FLOPs: 21180417024\\n    Parameters: 88790000\\n    File Size:...\"],[\"(Gluon) Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on ...\"],[\"Replace the model name with the variant you want to use, e.g. `gluon_xception65`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun Xception\\n  Paper:\\n    Title: 'Xception: Deep Learn...\"],[\"This guideline is very much a work-in-progress.*\\n\\nContributions to `timm` for code, documentation, t...\"],[\"```\\nblack --skip-string-normalization --line-length 120 \\u003cpath-to-file\\u003e\\n```\\n\\nAvoid formatting code th...\"],[\"```\\npytest -k \\\"substring-to-match\\\" -n 4 tests\\u002f\\n```\\n\\n## Building documentation\\n\\nPlease refer to [this...\"],[\"Deep Layer Aggregation\\n\\nExtending  “shallow” skip connections, **Dense Layer Aggregation (DLA)** inc...\"],[\"To get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\\\"...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DLA\\n  Paper:\\n    Title: Deep Layer Aggregation\\n    URL: ...\"],[\"ID: dla102x\\n    LR: 0.1\\n    Epochs: 120\\n    Layers: 102\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    ...\"],[\"- Convolution\\n    - DLA Bottleneck Residual Block\\n    - DLA Residual Block\\n    - Global Average Pool...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74....\"],[\"ID: dla46x_c\\n    LR: 0.1\\n    Epochs: 120\\n    Layers: 46\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    ...\"],[\"- DLA Bottleneck Residual Block\\n    - DLA Residual Block\\n    - Global Average Pooling\\n    - Max Pool...\"],[\"Top 5 Accuracy: 94.16%\\n- Name: dla60x\\n  In Collection: DLA\\n  Metadata:\\n    FLOPs: 3544204264\\n    Par...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla60x_c-b870c45c.pth\\n  Results:\\n  - Task: Image Classi...\"],[\"Data\\n\\n[[autodoc]] timm.data.create_dataset\\n\\n[[autodoc]] timm.data.create_loader\\n\\n[[autodoc]] timm.da...\"],[\"CSP-ResNet\\n\\n**CSPResNet** is a convolutional neural network where we apply the Cross Stage Partial N...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNet\\n  Paper:\\n    Title: 'CSPNet: A New Backbone t...\"],[\"Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Inception fam...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception v3\\n  Paper:\\n    Title: Rethinking the Inceptio...\"],[\"Getting Started\\n\\n## Welcome\\n\\nWelcome to the `timm` documentation, a lean set of docs that covers the...\"],[\"## List Model Architectures by Wildcard\\n```python\\nimport timm\\nfrom pprint import pprint\\nmodel_names ...\"],[\"Model Summaries\\n\\nThe model architectures included come from a wide variety of sources. Sources, incl...\"],[\"## Dual-Path Networks\\n\\n* Implementation: [dpn.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002f...\"],[\"## Inception-ResNet-V2\\n\\n* Implementation: [inception_resnet_v2.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpyto...\"],[\"## EfficientNet\\n\\n* Implementation: [efficientnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mode...\"],[\"## ResNet, ResNeXt\\n\\n* Implementation: [resnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002f...\"],[\"## Res2Net\\n\\n* Implementation: [res2net.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fma...\"],[\"* Paper: `Squeeze-and-Excitation Networks` - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1709.01507\\n* Code: https:\\u002f\\u002fgithub...\"],[\"## Xception (Modified Aligned, Gluon)\\n\\n* Implementation: [gluon_xception.py](https:\\u002f\\u002fgithub.com\\u002frwig...\"],[\"Deep Layer Aggregation\\n\\nExtending  “shallow” skip connections, **Dense Layer Aggregation (DLA)** inc...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DLA\\n  Paper:\\n    Title: Deep Layer Aggregation\\n    URL: ...\"],[\"ID: dla102x\\n    LR: 0.1\\n    Epochs: 120\\n    Layers: 102\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    ...\"],[\"- Convolution\\n    - DLA Bottleneck Residual Block\\n    - DLA Residual Block\\n    - Global Average Pool...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74....\"],[\"ID: dla46x_c\\n    LR: 0.1\\n    Epochs: 120\\n    Layers: 46\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    ...\"],[\"- DLA Bottleneck Residual Block\\n    - DLA Residual Block\\n    - Global Average Pooling\\n    - Max Pool...\"],[\"Top 5 Accuracy: 94.16%\\n- Name: dla60x\\n  In Collection: DLA\\n  Metadata:\\n    FLOPs: 3544204264\\n    Par...\"],[\"Weights: http:\\u002f\\u002fdl.yf.io\\u002fdla\\u002fmodels\\u002fimagenet\\u002fdla60x_c-b870c45c.pth\\n  Results:\\n  - Task: Image Classi...\"],[\"Feature Extraction\\n\\nAll of the models in `timm` have consistent mechanisms for obtaining various typ...\"],[\"Output:\\n\\n```text\\nOriginal shape: torch.Size([2, 1000])\\nUnpooled shape: torch.Size([2, 1024, 7, 7])\\n`...\"],[\"### Create a feature map extraction model\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e import timm\\n\\u003e\\u003e\\u003e m = timm.creat...\"],[\"`output_stride` is achieved by converting layers to use dilated convolutions. Doing so is not always...\"],[\"(Gluon) ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to th...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun ResNet\\n  Paper:\\n    Title: Deep Residual Learning ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-pretrained-gluonresnet\\u002freleases\\u002fdownload\\u002fv0.1\\u002fgluon_re...\"],[\"Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mode...\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resn...\"],[\"- Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection...\"],[\"- Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling...\"],[\"Parameters: 25580000\\n    File Size: 102573166\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch No...\"],[\"- Name: gluon_resnet50_v1s\\n  In Collection: Gloun ResNet\\n  Metadata:\\n    FLOPs: 7019495424\\n    Param...\"],[\"Adversarial Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the I...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Adversarial Inception v3\\n  Paper:\\n    Title: Adversarial...\"],[\"ResNet-D\\n\\n**ResNet-D** is a modification on the [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) a...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNet-D\\n  Paper:\\n    Title: Bag of Tricks for Image Cla...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fresnet152d...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fresnet200d...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fresnet34d_...\"],[\"(Tensorflow) EfficientNet Lite\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"To load a pretrained model:\\n\\n```py\\n\\u003e\\u003e\\u003e import timm\\n\\u003e\\u003e\\u003e model = timm.create_model('tf_efficientnet_li...\"],[\"Replace the model name with the variant you want to use, e.g. `tf_efficientnet_lite0`. You can find ...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet Lite\\n  Paper:\\n    Title: 'EfficientNet: ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"CSP-DarkNet\\n\\n**CSPDarknet53** is a convolutional neural network and backbone for object detection th...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP DarkNet\\n  Paper:\\n    Title: 'YOLOv4: Optimal Speed a...\"],[\"EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scaling method tha...\"],[\"\\u003e\\u003e\\u003e config = resolve_data_config({}, model=model)\\n\\u003e\\u003e\\u003e transform = create_transform(**config)\\n\\n\\u003e\\u003e\\u003e ur...\"],[\"## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing th...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: EfficientNet\\n  Paper:\\n    Title: 'EfficientNet: Rethinki...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fefficientn...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fefficientn...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fefficientn...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fefficientn...\"],[\"timm\\n\\n\\u003cimg class=\\\"float-left !m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[150px]\\\" src=\\\"ht...\"],[\"Scripts\\nA train, validation, inference, and checkpoint cleaning script included in the github root f...\"],[\"(Legacy) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmetho...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SE ResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excita...\"],[\"- Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - ...\"],[\"Weights: http:\\u002f\\u002fdata.lip6.fr\\u002fcadene\\u002fpretrainedmodels\\u002fse_resnext50_32x4d-a260b3a4.pth\\n  Results:\\n  - ...\"],[\"RexNet\\n\\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bo...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RexNet\\n  Paper:\\n    Title: 'ReXNet: Diminishing Represen...\"],[\"Training Resources: 4x NVIDIA V100 GPUs\\n    ID: rexnet_130\\n    LR: 0.5\\n    Epochs: 400\\n    Dropout: ...\"],[\"Metadata:\\n    FLOPs: 1960224938\\n    Parameters: 16370000\\n    File Size: 65862221\\n    Architecture:\\n ...\"],[\"MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that utilises [Mi...\"],[\"Replace the model name with the variant you want to use, e.g. `mixnet_l`. You can find the IDs in th...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MixNet\\n  Paper:\\n    Title: 'MixConv: Mixed Depthwise Con...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77....\"],[\"(Gluon) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun SEResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excitati...\"],[\"Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mode...\"],[\"RegNetX\\n\\n**RegNetX** is a convolutional network design space with simple, regular models with parame...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RegNetX\\n  Paper:\\n    Title: Designing Network Design Spa...\"],[\"ID: regnetx_004\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Ima...\"],[\"- Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    Tasks:\\n   ...\"],[\"Metrics:\\n      Top 1 Accuracy: 76.95%\\n      Top 5 Accuracy: 93.43%\\n- Name: regnetx_032\\n  In Collecti...\"],[\"Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n  ...\"],[\"Metadata:\\n    FLOPs: 20491740672\\n    Parameters: 54280000\\n    File Size: 217623862\\n    Architecture:...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-regnet\\u002fregnetx_320...\"],[\"(Tensorflow) EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scali...\"],[\"\\u003e\\u003e\\u003e config = resolve_data_config({}, model=model)\\n\\u003e\\u003e\\u003e transform = create_transform(**config)\\n\\n\\u003e\\u003e\\u003e ur...\"],[\"## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing th...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet\\n  Paper:\\n    Title: 'EfficientNet: Rethi...\"],[\"- Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Clas...\"],[\"Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image...\"],[\"File Size: 77989689\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normal...\"],[\"Momentum: 0.9\\n    Batch Size: 2048\\n    Image Size: '456'\\n    Weight Decay: 1.0e-05\\n    Interpolation...\"],[\"Top 5 Accuracy: 96.89%\\n- Name: tf_efficientnet_b7\\n  In Collection: TF EfficientNet\\n  Metadata:\\n    F...\"],[\"Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_b8\\n    LR: 0.256\\n    Epochs: 350\\n    Crop Pct:...\"],[\"- Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n...\"],[\"Parameters: 480310000\\n    File Size: 1925950424\\n    Architecture:\\n    - 1x1 Convolution\\n    - Averag...\"],[\"(Legacy) SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fr...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitat...\"],[\"- Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - ...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 71....\"],[\"- Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    T...\"],[\"Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on t...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception ResNet v2\\n  Paper:\\n    Title: Inception-v4, In...\"],[\"(Gluon) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Incep...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun Inception v3\\n  Paper:\\n    Title: Rethinking the In...\"],[\"(Legacy) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SENet\\n  Paper:\\n    Title: Squeeze-and-Excitation ...\"],[\"Quickstart\\n\\nThis quickstart is intended for developers who are ready to dive into the code and see a...\"],[\"To fine-tune on your own dataset, you have to write a PyTorch training loop or adapt `timm`'s [train...\"],[\"To figure out which transformations were used for a given pretrained model, we can start by taking a...\"],[\"Here, we will put together the above sections and use a pretrained model for inference.\\n\\nFirst we'll...\"],[\"```py\\n\\u003e\\u003e\\u003e values, indices = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e indices\\ntensor([162, 166, 161, 164, 167...\"],[\"Optimization\\n\\nThis page contains the API reference documentation for learning rate optimizers includ...\"],[\"SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresneXt...\"],[\"Replace the model name with the variant you want to use, e.g. `seresnext26d_32x4d`. You can find the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SEResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excitation Net...\"],[\"- Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - ...\"],[\"SWSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the l...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SWSL ResNet\\n  Paper:\\n    Title: Billion-scale semi-super...\"],[\"- SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - IG-1B-Targeted\\n    - ImageNet\\n    Tr...\"],[\"FBNet\\n\\n**FBNet** is a type of convolutional neural architectures discovered through [DNAS](https:\\u002f\\u002fp...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: FBNet\\n  Paper:\\n    Title: 'FBNet: Hardware-Aware Efficie...\"],[\"Dual Path Network (DPN)\\n\\nA **Dual Path Network (DPN)** is a convolutional neural network which prese...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DPN\\n  Paper:\\n    Title: Dual Path Networks\\n    URL: http...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-dpn-pretrained\\u002freleases\\u002fdownload\\u002fv0.1\\u002fdpn131-71dfe43e0...\"],[\"Batch Size: 1280\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightm...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 40x K80 GPUs\\n    ID: dpn98\\n...\"],[\"(Tensorflow) MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF MixNet\\n  Paper:\\n    Title: 'MixConv: Mixed Depthwise ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_mixnet_...\"],[\"Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on t...\"],[\"Replace the model name with the variant you want to use, e.g. `inception_resnet_v2`. You can find th...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception ResNet v2\\n  Paper:\\n    Title: Inception-v4, In...\"],[\"(Gluon) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun SEResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excitati...\"],[\"Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mode...\"],[\"(Tensorflow) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the ...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF Inception v3\\n  Paper:\\n    Title: Rethinking the Incep...\"],[\"SK-ResNeXt\\n\\n**SK ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnext...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SKResNeXt\\n  Paper:\\n    Title: Selective Kernel Networks\\n...\"],[\"HRNet\\n\\n**HRNet**, or **High-Resolution Net**, is a general purpose convolutional neural network for ...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: HRNet\\n  Paper:\\n    Title: Deep High-Resolution Represent...\"],[\"Weight Decay: 0.001\\n    Interpolation: bilinear\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-m...\"],[\"- ImageNet\\n    Training Resources: 4x NVIDIA V100 GPUs\\n    ID: hrnet_w30\\n    Epochs: 100\\n    Layers:...\"],[\"- Convolution\\n    - ReLU\\n    - Residual Connection\\n    Tasks:\\n    - Image Classification\\n    Trainin...\"],[\"- Name: hrnet_w48\\n  In Collection: HRNet\\n  Metadata:\\n    FLOPs: 22285865760\\n    Parameters: 77470000...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"(Legacy) SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNeXt](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmetho...\"],[\"Replace the model name with the variant you want to use, e.g. `legacy_seresnext101_32x4d`. You can f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SE ResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excita...\"],[\"- Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - ...\"],[\"Weights: http:\\u002f\\u002fdata.lip6.fr\\u002fcadene\\u002fpretrainedmodels\\u002fse_resnext50_32x4d-a260b3a4.pth\\n  Results:\\n  - ...\"],[\"SE-ResNeXt\\n\\n**SE ResNeXt** is a variant of a [ResNext](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresneXt...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SEResNeXt\\n  Paper:\\n    Title: Squeeze-and-Excitation Net...\"],[\"- Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - ...\"],[\"(Legacy) SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fr...\"],[\"Replace the model name with the variant you want to use, e.g. `legacy_seresnet101`. You can find the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitat...\"],[\"- Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - ...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 71....\"],[\"- Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    T...\"],[\"EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scaling method tha...\"],[\"config = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename ...\"],[\"## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: EfficientNet\\n  Paper:\\n    Title: 'EfficientNet: Rethinki...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fefficientn...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fefficientn...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fefficientn...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fefficientn...\"],[\"Res2NeXt\\n\\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https:\\u002f\\u002fpaperswithcod...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2NeXt\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale...\"],[\"ECA-ResNet\\n\\nAn **ECA ResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) t...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ECAResNet\\n  Paper:\\n    Title: 'ECA-Net: Efficient Channe...\"],[\"- Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classi...\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.61%\\n      T...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n  ...\"],[\"Instagram ResNeXt WSL\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fre...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: IG ResNeXt\\n  Paper:\\n    Title: Exploring the Limits of W...\"],[\"- Weight Decay\\n    Training Data:\\n    - IG-3.5B-17k\\n    - ImageNet\\n    Training Resources: 336x GPUs...\"],[\"In Collection: IG ResNeXt\\n  Metadata:\\n    FLOPs: 21180417024\\n    Parameters: 88790000\\n    File Size:...\"],[\"Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2Net\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale ...\"],[\"Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: re...\"],[\"File Size: 101421406\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Aver...\"],[\"Scripts\\n\\nA train, validation, inference, and checkpoint cleaning script included in the github root ...\"],[\"## Training Examples\\n\\n### EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5\\n\\nThese params ar...\"],[\"The training of this model started with the same command line as EfficientNet-B2 w\\u002f RA above. After ...\"],[\"Trained by [Andrew Lavin](https:\\u002f\\u002fgithub.com\\u002fandravin) with 8 V100 cards. Model EMA was not used, fi...\"],[\"ResNeSt\\n\\nA **ResNeSt** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet), which i...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNeSt\\n  Paper:\\n    Title: 'ResNeSt: Split-Attention Ne...\"],[\"- Label Smoothing\\n    - Mixup\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - Im...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83....\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 64x NVIDIA V100 GPUs\\n    ID...\"],[\"Metrics:\\n      Top 1 Accuracy: 80.96%\\n      Top 5 Accuracy: 95.38%\\n- Name: resnest50d_1s4x24d\\n  In C...\"],[\"- ImageNet\\n    Training Resources: 64x NVIDIA V100 GPUs\\n    ID: resnest50d_4s2x40d\\n    LR: 0.1\\n    E...\"],[\"ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer ...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNet\\n  Paper:\\n    Title: Deep Residual Learning for Im...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fresnet101-5d3b4d8f.pth\\n  Results:\\n  - Task: Image Class...\"],[\"LR: 0.1\\n    Epochs: 90\\n    Crop Pct: '0.875'\\n    LR Gamma: 0.1\\n    Momentum: 0.9\\n    Batch Size: 32\\n...\"],[\"NASNet\\n\\n**NASNet** is a type of convolutional neural network discovered through neural architecture ...\"],[\"Replace the model name with the variant you want to use, e.g. `nasnetalarge`. You can find the IDs i...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: NASNet\\n  Paper:\\n    Title: Learning Transferable Archite...\"],[\"(Gluon) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the Incep...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun Inception v3\\n  Paper:\\n    Title: Rethinking the In...\"],[\"(Gluon) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and-...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun SENet\\n  Paper:\\n    Title: Squeeze-and-Excitation N...\"],[\"Vision Transformer (ViT)\\n\\nThe **Vision Transformer** is a model for image classification that employ...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Vision Transformer\\n  Paper:\\n    Title: 'An Image is Wort...\"],[\"- Tanh Activation\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Cosine Anneal...\"],[\"- Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81.66%\\n      T...\"],[\"- ImageNet\\n    - JFT-300M\\n    Training Resources: TPUv3\\n    ID: vit_large_patch16_224\\n    Crop Pct: ...\"],[\"In Collection: Vision Transformer\\n  Metadata:\\n    FLOPs: 28236450816\\n    Parameters: 48750000\\n    Fi...\"],[\"ResNet-D\\n\\n**ResNet-D** is a modification on the [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) a...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNet-D\\n  Paper:\\n    Title: Bag of Tricks for Image Cla...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fresnet152d...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fresnet200d...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fresnet34d_...\"],[\"PNASNet\\n\\n**Progressive Neural Architecture Search**, or **PNAS**, is a method for learning the struc...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: PNASNet\\n  Paper:\\n    Title: Progressive Neural Architect...\"],[\"AdvProp (EfficientNet)\\n\\n**AdvProp** is an adversarial training scheme which treats adversarial examp...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: AdvProp\\n  Paper:\\n    Title: Adversarial Examples Improve...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AdvProp\\n    - AutoAugment\\n    - Lab...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"- Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Classification\\n    Training Techni...\"],[\"Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image...\"],[\"Parameters: 66349999\\n    File Size: 266850607\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average ...\"],[\"Momentum: 0.9\\n    Batch Size: 2048\\n    Image Size: '672'\\n    Weight Decay: 1.0e-05\\n    Interpolation...\"],[\"MobileNet v2\\n\\n**MobileNetV2** is a convolutional neural network architecture that seeks to perform w...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MobileNet V2\\n  Paper:\\n    Title: 'MobileNetV2: Inverted ...\"],[\"- RMSProp\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 16x GPUs\\n    ...\"],[\"- Name: mobilenetv2_140\\n  In Collection: MobileNet V2\\n  Metadata:\\n    FLOPs: 770196784\\n    Parameter...\"],[\"TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) that aim...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TResNet\\n  Paper:\\n    Title: 'TResNet: High Performance G...\"],[\"- Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    T...\"],[\"Top 1 Accuracy: 80.8%\\n      Top 5 Accuracy: 94.86%\\n- Name: tresnet_m_448\\n  In Collection: TResNet\\n  ...\"],[\"ID: tresnet_xl\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: ...\"],[\"ESE-VoVNet\\n\\n**VoVNet** is a convolutional neural network that seeks to make [DenseNet](https:\\u002f\\u002fpaper...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ESE VovNet\\n  Paper:\\n    Title: 'CenterMask : Real-Time A...\"],[\"CSP-ResNet\\n\\n**CSPResNet** is a convolutional neural network where we apply the Cross Stage Partial N...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNet\\n  Paper:\\n    Title: 'CSPNet: A New Backbone t...\"],[\"AdvProp (EfficientNet)\\n\\n**AdvProp** is an adversarial training scheme which treats adversarial examp...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: AdvProp\\n  Paper:\\n    Title: Adversarial Examples Improve...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AdvProp\\n    - AutoAugment\\n    - Lab...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"- Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Classification\\n    Training Techni...\"],[\"Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image...\"],[\"Parameters: 66349999\\n    File Size: 266850607\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average ...\"],[\"Momentum: 0.9\\n    Batch Size: 2048\\n    Image Size: '672'\\n    Weight Decay: 1.0e-05\\n    Interpolation...\"],[\"(Legacy) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and...\"],[\"Replace the model name with the variant you want to use, e.g. `legacy_senet154`. You can find the ID...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Legacy SENet\\n  Paper:\\n    Title: Squeeze-and-Excitation ...\"],[\"(Gluon) SENet\\n\\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and-...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun SENet\\n  Paper:\\n    Title: Squeeze-and-Excitation N...\"],[\"SWSL ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-blo...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SWSL ResNext\\n  Paper:\\n    Title: Billion-scale semi-supe...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - IG-1B-Targe...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 84.27%\\n      Top 5 Accuracy: 97.17%\\n- Name: sws...\"],[\"ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-block) t...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNeXt\\n  Paper:\\n    Title: Aggregated Residual Transfor...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fresnext50_32x4d-7cdf4587.pth\\n  Results:\\n  - Task: Image...\"],[\"PyTorch Image Models\\n- [What's New](#whats-new)\\n- [Introduction](#introduction)\\n- [Models](#models)\\n...\"],[\"## What's New\\n\\n❗Updates after Oct 10, 2022 are available in version \\u003e= 0.9❗\\n* Many changes since the...\"],[\"### Nov 23, 2023\\n* Added EfficientViT-Large models, thanks [SeeFun](https:\\u002f\\u002fgithub.com\\u002fseefun)\\n* Fix...\"],[\"### Sep 1, 2023\\n* TinyViT added by [SeeFun](https:\\u002f\\u002fgithub.com\\u002fseefun)\\n* Fix EfficientViT (MIT) to u...\"],[\"### Aug 25, 2023\\n* Many new models since last release\\n  * FastViT - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2303.14189...\"],[\"### July 27, 2023\\n* Added timm trained `seresnextaa201d_32x8d.sw_in12k_ft_in1k_384` weights (and `.s...\"],[\"### April 27, 2023\\n* 97% of `timm` models uploaded to HF Hub and almost all updated to support multi...\"],[\"### April 5, 2023\\n* ALL ResNet models pushed to Hugging Face Hub with multi-weight support\\n  * All p...\"],[\"### March 31, 2023\\n* Add first ConvNext-XXLarge CLIP -\\u003e IN-1k fine-tune and IN-12k intermediate fine...\"],[\"| model                                              |top1  |top5  |param_count|img_size|\\n|---------...\"],[\"* Multi-weight and HF hub for DeiT and MLP-Mixer based models\\n\\n### March 22, 2023\\n* More weights pus...\"],[\"### Feb 16, 2023\\n* `safetensor` checkpoint support added\\n* Add ideas from 'Scaling Vision Transforme...\"],[\"### Jan 20, 2023\\n* Add two convnext 12k -\\u003e 1k fine-tunes at 384x384\\n  * `convnext_tiny.in12k_ft_in1k...\"],[\"|model                                                                                              ...\"],[\"|[maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxxvitv2_rmlp_base_rw_38...\"],[\"|[maxvit_large_tf_384.in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxvit_large_tf_384.in1k)                   ...\"],[\"|[coatnet_rmlp_2_rw_224.sw_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fcoatnet_rmlp_2_rw_224.sw_in1k)         ...\"],[\"|[maxxvit_rmlp_nano_rw_256.sw_in1k](https:\\u002f\\u002fhuggingface.co\\u002ftimm\\u002fmaxxvit_rmlp_nano_rw_256.sw_in1k)   ...\"],[\"### Jan 11, 2023\\n* Update ConvNeXt ImageNet-12k pretrain series w\\u002f two new fine-tuned weights (and p...\"],[\"### Dec 8, 2022\\n* Add 'EVA l' to `vision_transformer.py`, MAE style ViT-L\\u002f14 MIM pretrain w\\u002f EVA-CLI...\"],[\"### Dec 5, 2022\\n\\n* Pre-release (`0.8.0dev0`) of multi-weight support (`model_arch.pretrained_tag`). ...\"],[\"| model                                            |   top1 |   param_count |   gmac |   macts | hub...\"],[\"| vit_large_patch14_clip_336.laion2b_ft_in1k       |   87.9 |         304.5 |  191.1 |   270.2 | [li...\"],[\"| vit_base_patch32_clip_448.laion2b_ft_in12k_in1k  |   85.8 |          88.3 |   17.9 |    23.9 | [li...\"],[\"* Port of MaxViT Tensorflow Weights from official impl at https:\\u002f\\u002fgithub.com\\u002fgoogle-research\\u002fmaxvit\\n...\"],[\"| model                              |   top1 |   param_count |   gmac |   macts | hub              ...\"],[\"| maxvit_large_tf_384.in1k           |   86.2 |         212   |  132.6 |   445.8 | [link](https:\\u002f\\u002fhu...\"],[\"### Oct 15, 2022\\n* Train and validation script enhancements\\n* Non-GPU (ie CPU) device support\\n* SLUR...\"],[\"### Sept 7, 2022\\n* Hugging Face [`timm` docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002ftimm) home now exists,...\"],[\"* Aggregating Nested Transformers - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2105.12723\\n* BEiT - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f...\"],[\"* EfficientViT (MIT) - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2205.14756\\n* EfficientViT (MSRA) - https:\\u002f\\u002farxiv.org\\u002fab...\"],[\"* NesT - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2105.12723\\n* NFNet-F - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.06171\\n* NF-RegNet \\u002f...\"],[\"* TResNet - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2003.13630\\n* Twins (Spatial Attention in Vision Transformers) - ht...\"],[\"## Features\\n\\nSeveral (less common) features that I often utilize in my projects are included. Many o...\"],[\"* All models have a common default configuration interface and API for\\n    * accessing\\u002fchanging the ...\"],[\"* A 'Test Time Pool' wrapper that can wrap any of the included models and usually provides improved ...\"],[\"* Mixup (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1710.09412)\\n* CutMix (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1905.04899)\\n* AutoAugment...\"],[\"## Results\\n\\nModel validation results can be found in the [results tables](results\\u002fREADME.md)\\n\\n## Get...\"],[\"### Computer Vision \\u002f Image Augmentation\\n* Albumentations - https:\\u002f\\u002fgithub.com\\u002falbumentations-team\\u002fa...\"],[\"#### Pretrained on more than ImageNet\\nSeveral weights included or references here were pretrained wi...\"],[\"ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-block) t...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ResNeXt\\n  Paper:\\n    Title: Aggregated Residual Transfor...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fresnext50_32x4d-7cdf4587.pth\\n  Results:\\n  - Task: Image...\"],[\"Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on [depthwi...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Xception\\n  Paper:\\n    Title: 'Xception: Deep Learning wi...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Dual Path Network (DPN)\\n\\nA **Dual Path Network (DPN)** is a convolutional neural network which prese...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DPN\\n  Paper:\\n    Title: Dual Path Networks\\n    URL: http...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-dpn-pretrained\\u002freleases\\u002fdownload\\u002fv0.1\\u002fdpn131-71dfe43e0...\"],[\"Batch Size: 1280\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightm...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 40x K80 GPUs\\n    ID: dpn98\\n...\"],[\"(Gluon) ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun ResNeXt\\n  Paper:\\n    Title: Aggregated Residual Tr...\"],[\"Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mode...\"],[\"# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural archit...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Ensemble Adversarial\\n  Paper:\\n    Title: Adversarial Att...\"],[\"(Gluon) ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun ResNeXt\\n  Paper:\\n    Title: Aggregated Residual Tr...\"],[\"Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mode...\"],[\"SSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the la...\"],[\"To get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\\\"...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SSL ResNet\\n  Paper:\\n    Title: Billion-scale semi-superv...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    - YFCC-100M\\n    Training Resources: 64x GPUs\\n  ...\"],[\"Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on [depthwi...\"],[\"Replace the model name with the variant you want to use, e.g. `xception`. You can find the IDs in th...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Xception\\n  Paper:\\n    Title: 'Xception: Deep Learning wi...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"(Gluon) ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to th...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun ResNet\\n  Paper:\\n    Title: Deep Residual Learning ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-pretrained-gluonresnet\\u002freleases\\u002fdownload\\u002fv0.1\\u002fgluon_re...\"],[\"Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-mode...\"],[\"- Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: gluon_resn...\"],[\"- Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection...\"],[\"- Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling...\"],[\"Parameters: 25580000\\n    File Size: 102573166\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch No...\"],[\"- Name: gluon_resnet50_v1s\\n  In Collection: Gloun ResNet\\n  Metadata:\\n    FLOPs: 7019495424\\n    Param...\"],[\"(Tensorflow) EfficientNet Lite\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"To load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data impo...\"],[\"## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet Lite\\n  Paper:\\n    Title: 'EfficientNet: ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"Models\\n\\n[[autodoc]] timm.create_model\\n\\n[[autodoc]] timm.list_models...\"],[\"SPNASNet\\n\\n**Single-Path NAS** is a novel differentiable NAS method for designing hardware-efficient ...\"],[\"Replace the model name with the variant you want to use, e.g. `spnasnet_100`. You can find the IDs i...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SPNASNet\\n  Paper:\\n    Title: 'Single-Path NAS: Designing...\"],[\"MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mobile phone CP...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MobileNet V3\\n  Paper:\\n    Title: Searching for MobileNet...\"],[\"- Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Tech...\"],[\"EfficientNet (Knapsack Pruned)\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"config = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename ...\"],[\"## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: EfficientNet Pruned\\n  Paper:\\n    Title: Knapsack Pruning...\"],[\"Weights: https:\\u002f\\u002fimvl-automl-sh.oss-cn-shanghai.aliyuncs.com\\u002fdarts\\u002fhyperml\\u002fhyperml\\u002fjob_45403\\u002foutputs...\"],[\"Inception v4\\n\\n**Inception-v4** is a convolutional neural network architecture that builds on previou...\"],[\"Replace the model name with the variant you want to use, e.g. `inception_v4`. You can find the IDs i...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception v4\\n  Paper:\\n    Title: Inception-v4, Inception...\"],[\"Noisy Student (EfficientNet)\\n\\n**Noisy Student Training** is a semi-supervised learning approach. It ...\"],[\"To get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\\\"...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Noisy Student\\n  Paper:\\n    Title: Self-training with Noi...\"],[\"- Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n...\"],[\"Training Resources: Cloud TPU v3 Pod\\n    ID: tf_efficientnet_b2_ns\\n    LR: 0.128\\n    Epochs: 700\\n   ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"- Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n...\"],[\"ID: tf_efficientnet_b6_ns\\n    LR: 0.128\\n    Epochs: 350\\n    Dropout: 0.5\\n    Crop Pct: '0.942'\\n    M...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"Noisy Student (EfficientNet)\\n\\n**Noisy Student Training** is a semi-supervised learning approach. It ...\"],[\"To get the model predictions:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e with torch.no_grad():\\n...     out = model(...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Noisy Student\\n  Paper:\\n    Title: Self-training with Noi...\"],[\"- Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n...\"],[\"Training Resources: Cloud TPU v3 Pod\\n    ID: tf_efficientnet_b2_ns\\n    LR: 0.128\\n    Epochs: 700\\n   ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"- Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n...\"],[\"ID: tf_efficientnet_b6_ns\\n    LR: 0.128\\n    Epochs: 350\\n    Dropout: 0.5\\n    Crop Pct: '0.942'\\n    M...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_efficie...\"],[\"(Tensorflow) MixNet\\n\\n**MixNet** is a type of convolutional neural network discovered via AutoML that...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF MixNet\\n  Paper:\\n    Title: 'MixConv: Mixed Depthwise ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_mixnet_...\"],[\"SSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the la...\"],[\"To get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, filename...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](..\\u002fscripts) for training a n...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SSL ResNet\\n  Paper:\\n    Title: Billion-scale semi-superv...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    - YFCC-100M\\n    Training Resources: 64x GPUs\\n  ...\"],[\"Adversarial Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the I...\"],[\"To get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\\\"...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Adversarial Inception v3\\n  Paper:\\n    Title: Adversarial...\"],[\"SWSL ResNet\\n\\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the l...\"],[\"To get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\\\"...\"],[\"## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https:\\u002f\\u002frwightman.github.io\\u002f...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SWSL ResNet\\n  Paper:\\n    Title: Billion-scale semi-super...\"],[\"- SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - IG-1B-Targeted\\n    - ImageNet\\n    Tr...\"],[\"(Tensorflow) EfficientNet\\n\\n**EfficientNet** is a convolutional neural network architecture and scali...\"],[\"config = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename ...\"],[\"## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet\\n  Paper:\\n    Title: 'EfficientNet: Rethi...\"],[\"- Inverted Residual Block\\n    - Squeeze-and-Excitation Block\\n    - Swish\\n    Tasks:\\n    - Image Clas...\"],[\"Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n  Code: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image...\"],[\"File Size: 77989689\\n    Architecture:\\n    - 1x1 Convolution\\n    - Average Pooling\\n    - Batch Normal...\"],[\"Momentum: 0.9\\n    Batch Size: 2048\\n    Image Size: '456'\\n    Weight Decay: 1.0e-05\\n    Interpolation...\"],[\"Top 5 Accuracy: 96.89%\\n- Name: tf_efficientnet_b7\\n  In Collection: TF EfficientNet\\n  Metadata:\\n    F...\"],[\"Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_b8\\n    LR: 0.256\\n    Epochs: 350\\n    Crop Pct:...\"],[\"- Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n...\"],[\"Parameters: 480310000\\n    File Size: 1925950424\\n    Architecture:\\n    - 1x1 Convolution\\n    - Averag...\"],[\"ESE-VoVNet\\n\\n**VoVNet** is a convolutional neural network that seeks to make [DenseNet](https:\\u002f\\u002fpaper...\"],[\"# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: ESE VovNet\\n  Paper:\\n    Title: 'CenterMask : Real-Time A...\"],[\"(Gluon) Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on ...\"],[\"\\u003e\\u003e\\u003e # Print top categories per image\\n\\u003e\\u003e\\u003e top5_prob, top5_catid = torch.topk(probabilities, 5)\\n\\u003e\\u003e\\u003e fo...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Gloun Xception\\n  Paper:\\n    Title: 'Xception: Deep Learn...\"]],\"hovertemplate\":\"source=pytorch-image-models\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"pytorch-image-models, circle\",\"marker\":{\"color\":\"#FF6692\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"pytorch-image-models, circle\",\"showlegend\":true,\"x\":[-1.8169893,-2.5181384,-3.9762278,-3.8249586,0.9876417,1.2625266,1.1968944,1.3549926,1.300545,1.2664647,-12.8897915,-12.9121895,-12.593306,-12.667295,8.127072,8.33283,3.1947656,8.159237,8.337447,12.254799,10.157885,10.070727,10.022912,9.654129,-2.411321,-3.5381029,-3.6137378,-6.289637,-3.8306355,-5.73714,-5.761642,-6.5871706,7.243787,-9.65808,-9.645054,-9.197503,-9.232994,-9.153907,-9.029409,-8.998254,-8.920169,-2.0417309,-1.0344042,-1.0395294,-9.72034,0.17059809,0.29914826,-0.17028658,-0.0032361771,-0.04190395,-4.340938,-4.9459386,6.735196,0.90988445,-1.0931207,-0.88234496,-1.1163434,-0.8209503,-2.4669542,-4.0225596,-4.0645103,-2.651242,-1.2571794,-3.4050083,-4.093121,-3.0669694,1.1163166,-2.7695014,0.25295323,-1.2678604,-1.8421053,-1.2452531,-3.0371506,-1.7221857,-1.459812,-2.701987,3.8173938,4.1441956,-3.019336,-2.2454946,-1.1407939,-1.1958909,16.229073,15.948498,16.324638,16.026772,8.980406,16.247149,15.795377,16.327112,17.033913,16.623232,15.973498,16.31137,16.87793,16.406412,16.547396,16.167183,16.684374,16.706118,12.60837,13.314485,-9.606418,-9.526847,-9.673224,-9.35806,-9.457867,6.182169,-9.600002,-8.159374,-9.481527,-2.893513,-3.7473261,-4.058112,8.640484,-5.0827503,-3.2203195,-7.02809,-8.620974,-8.416735,-8.450557,-9.105069,-8.87993,3.0960915,0.8869378,1.1020004,2.767637,3.0794544,8.981209,-5.989461,-6.311933,-10.258088,-2.5959592,-7.2303596,-2.7933729,-6.736827,-6.325717,-6.1985226,-2.1482487,17.775023,16.900957,18.3508,11.839291,6.8102436,-9.866063,-9.873612,-9.912136,-9.59442,-9.82613,-9.940587,18.61102,17.113659,17.809523,6.7455792,-8.00636,-7.7988744,-2.1416698,-1.859999,-1.9944792,-2.2980406,-14.865521,-13.605162,-17.417038,-6.7609396,-7.3908596,-5.23435,-5.8490887,4.6123867,4.6274066,4.6867228,4.559739,-3.3646607,2.9702392,2.8861542,1.9781232,-2.9143267,-3.109353,2.529611,7.993087,-3.8779535,-3.7124674,12.91311,-5.8173203,-1.3308097,-2.1348522,0.36428893,-0.047259793,-1.6436808,-1.5420728,-1.6412785,0.67581624,8.779606,8.655425,-9.62899,-9.626581,-2.4597738,8.543644,8.643564,8.271621,8.144876,8.447259,8.308981,13.39484,-3.3753114,-3.1631994,-3.3318086,-3.0926507,-3.0600865,-3.269516,-2.7197134,-2.6833944,-3.9270346,-4.173458,-4.2345285,3.2143147,2.924811,5.48822,5.412344,5.415914,-4.4061484,-3.6948516,-3.7088048,-0.50065905,-2.7372804,-2.2132082,-3.8406084,-5.338618,10.812259,3.5066266,3.6251833,3.4700625,9.297396,-1.4426769,-0.86269766,-0.95489186,-1.049233,-0.94819295,3.463432,13.382748,6.9143267,-1.1601574,13.261434,13.366056,12.601208,5.206621,4.816185,3.2500181,2.998019,-7.493259,-6.8985863,-5.2477307,-2.7525828,4.8357186,5.977231,5.38898,14.543043,10.603826,0.9940419,11.158475,-0.8530681,-3.5638683,-4.110148,-4.204028,4.370696,-4.245625,12.776422,13.121915,-7.9472704,-7.4345303,-7.362432,-1.3623226,-0.66317636,-1.061667,-1.0353007,-1.4628994,-11.441298,-12.458069,-12.235236,-12.236343,2.0998943,8.284313,8.252724,4.020496,5.9333773,3.7011714,-3.7675838,-3.5629098,-3.7995293,-7.981689,-7.5131903,13.341722,13.269997,3.8081295,6.8474092,6.965824,-5.015924,7.2323084,-9.471642,-3.433451,1.6443608,1.4306153,1.8266171,-2.763955,11.592601,-2.3544624,-0.06571603,8.056071,-6.5501294,-6.109192,-6.4303412,13.225341,-7.2835827,-7.119343,-6.1828647,-6.1645436,-6.131052,-6.261201,-7.0895963,-2.4266186,8.761435,-7.2685943,-7.509048,-7.2940683,-7.4531403,-7.151586,-3.5664353,-6.5392447,-6.6324415,-6.9355226,-3.1348724,-2.3051188,-1.8362638,-1.457961,17.56257,7.454752,7.873466,-12.446623,-12.65745,-12.617013,-12.558753,-12.221547,-12.614372,-12.579679,-12.631206,-12.385981,-12.650319,-2.7168505,-2.146792,-3.3756852,-1.6621561,2.985817,-6.781299,-5.7965045,8.884428,4.4939113,11.895428,5.963531,10.394709,11.574616,-9.919539,-10.196439,-2.8047884,-6.0770826,-2.624169,-3.721886,-0.8845227,-2.6576192,-1.85985,-3.0089135,-2.0755513,-2.458669,-1.7301843,-5.716529,2.6617842,1.8929155,1.1339464,-0.9348493,8.064534,4.997164,4.724265,6.6113324,4.6796784,4.8425717,4.621032,4.951619,5.3902392,5.138464,-3.1207564,0.17744519,0.33956844,-6.5155163,-6.7084556,-2.3016949,-3.869684,-3.7648225,-3.8410814,-4.056791,-4.026844,-2.0407605,-1.8277854,-0.16728914,-1.7198793,0.0013632606,5.536513,3.1708837,-6.6806054,-5.377292,-4.295179,-3.889657,-3.4890065,-3.440489,-3.5026753,-3.577476,-3.2320602,15.76955,15.775324,2.448142,2.2633812,2.864031,12.474131,-0.45881402,-1.5028446,-2.1896386,8.769717,8.832661,8.744327,1.2092593,1.3416793,1.3005495,1.1565926,-1.5687317,-6.321335,2.4952872,-0.35685852,12.881793,-2.597686,-1.912186,4.31859,4.2793202,14.070812,-15.143644,-13.604808,-17.414389,-16.465727,-7.454017,2.3937006,2.2670102,2.4167092,2.262386,-7.3662086,-7.3261952,2.3550503,-0.14837505,1.1149411,-7.4720864,2.422764,2.7562678,1.4806411,2.6692128,2.2135587,-0.331082,-0.50912267,-6.978136,-7.173351,-7.1878676,-5.7794633,-5.9381437,-5.6985736,-5.9833026,8.546378,-6.093802,-6.0368366,-10.481838,-10.600543,-10.638765,7.278774,7.13125,1.1043793,19.953058,19.6139,19.584698,19.114662,-0.22653179,-0.13990001,1.6581625,-8.357517,-8.159291,-7.3667984,-7.2798824,-7.8301697,-3.9475951,-4.7281165,-3.9135926,-5.070963,-3.88488,-4.70957,-3.6728613,-3.2348404,-0.1817591,0.6373699,-5.296997,-5.6388607,-4.669979,-3.6584935,-3.0213473,-0.54717296,-3.9584072,-0.880938,-1.1094971,0.31367388,-4.709794,3.6930227,3.6493335,3.347398,-0.38896862,-0.14119275,-2.584543,-1.7083894,0.20957042,-0.8425916,-0.9323423,-6.696626,12.5028105,12.712219,-3.7357,-3.3050997,-2.3481317,-5.2799263,-5.7049756,-6.1201644,-2.4707167,-0.28168607,0.09407915,-0.7641673,0.2626572,-1.1695641,-0.98324263,-8.202743,-8.202454,-8.180489,-8.211107,-8.235413,-8.198147,-8.339922,-8.288354,-3.677922,-3.6826897,-3.4268432,-0.2659529,-0.9719973,-7.439182,-7.1843214,-3.861713,-2.8083444,-2.6757872,-2.7800422,-2.9598281,-2.7386322,-0.50104165,-1.9238529,-3.6999795,-3.1546323,-6.7163234,-6.603236,-2.1697204,7.9154773,7.9270906,0.77954245,2.645452,-0.98875564,-0.87490875,-0.7859827,-10.5426,-10.827318,-10.660138,-10.786006,-10.564599,-10.790203,-2.2953935,-2.3450472,-2.298667,-1.997835,-2.598772,-2.1776457,-15.088039,-6.016583,-6.0353184,-6.3106213,-2.531791,11.75915,11.622653,10.365286,7.3666177,8.18943,-13.483864,-6.60978,-7.149774,-3.7468364,19.469847,19.609205,17.925043,19.292385,-4.9493175,-2.4519358,3.9968061,13.676397,-6.5279956,13.534598,1.5933576,4.4003515,-1.4931034,-3.863753,-3.8088794,-3.7066011,-3.3841283,-2.0505245,-1.9822681,-3.683277,-1.3346337,2.1808462,3.7666388,3.974724,3.9656875,3.6023483,3.418722,8.489397,18.85347,18.003075,19.217072,13.438506,-3.5295765,-2.2296803,-4.6675205,-1.9096345,2.4223824,-2.3812673,0.2546976,-2.300273,9.18362,8.996166,9.104623,9.0678835,8.639728,-4.0710382,-1.5586832,-2.5863233,-8.9268675,-5.0913825,-4.7434726,-4.6553473,2.7893155,1.1746557,1.3773979,-12.823699,-13.0178795,-12.646511,-12.719683,-13.229812,-12.770144,-12.453761,-12.873882,-12.733251,-12.753485,-12.390058,-12.901637,-1.1512415,-3.2336526,13.004804,8.578827,0.3616215,0.4632981,0.7652834,-15.077504,-13.605203,8.819371,8.47014,8.743714,8.739793,8.811196,-7.008077,-6.675939,-2.4080782,13.083047,19.444605,18.547901,17.967493,19.538876,18.497576,17.216402,16.409227,3.9249766,3.5615387,3.9873705,3.8097286,4.1266074,3.5595245,2.532771,3.0708742,3.1206906,-1.7780644,-9.717886,-9.909897,-2.4449112,8.729041,13.452608,8.372353,6.8900757,-3.3276415,3.7877996,8.29515,-10.385709,-8.088215,-7.3865027,-7.7694883,-10.147451,-10.18021,-9.997145,-7.9163837,-9.725946,-2.3739243,1.7208726,2.76219,-7.5701165,-7.345476,-6.625842,0.87289715,0.6881671,6.9746513,-5.949583,-5.4589577,-5.678506,-5.7267675,-5.489329,3.594798,3.540694,3.5693288,3.6052651,3.925125,-0.87098414,0.3030645,-1.2479414,-1.5705625,6.782164,-6.6495833,-6.466808,-6.304993,8.061515,-14.983865,-13.6036005,-17.869839,-7.1449366,-7.1788363,8.685582,-6.825981,-5.688333,8.770588,8.730667,-5.5037384,2.1941018,0.35079184,-6.8175983,-3.425105,-2.607829,-3.286144,-3.149299,-3.1099691,-3.378485,3.4398537,-2.148357,-2.0616643,-1.0832292,-0.7395787,-2.391437,12.313208,12.130021,-4.5084996,-5.885955,-5.133513,18.406935,17.615221,18.465378,0.38997224,2.5723178,0.507957,1.576043,4.094591,13.381276,-5.4998975,-4.8243723,-5.0716085,-2.4516053,-3.6767251,-2.5576646,-4.8295746,-5.6698313,10.35483,-2.7394762,-1.5313365,1.6500667,3.5919094,3.6138093,-3.7821927,-2.5594363,-2.655608,9.033721,8.978706,9.011977,8.861353,-5.2295175,-4.8935986,-4.748239,-4.9365025,-5.1060786,-14.816224,-21.72966,-17.999653,-18.637938,-4.8617687,-2.5982215,6.1675935,4.44611,3.8154736,3.8339107,5.1851554,3.633678,0.3360169,-1.801029,-4.249772,12.529944,11.32996,11.340402],\"xaxis\":\"x\",\"y\":[2.9555535,2.6647372,1.179815,0.007856891,4.6574435,4.636938,4.4976673,4.4224997,4.6961517,4.562404,-18.318531,-18.497326,-18.460712,-17.596127,-9.956359,-9.97588,0.7236513,-9.918,-10.064407,4.8428245,3.4969988,3.4455857,3.6359828,3.3461304,-12.122729,6.2165313,6.431461,3.4142432,-0.06913484,1.9554312,1.9806249,3.8653736,2.269947,2.8800092,2.8871512,2.7916226,2.6165013,2.6958408,2.6542163,2.6211536,2.6862433,2.7236907,1.5111821,3.406739,2.938661,4.539984,4.7119617,3.0913155,3.6936953,3.4946008,4.2788506,4.0891137,-0.9957403,2.6109245,2.1014915,2.7357419,2.2437418,3.446339,3.8447773,1.7368548,1.3361349,1.4253964,1.108604,1.2646391,1.9019548,1.2962406,0.7975657,1.2512497,1.6522822,0.6077049,1.5150293,2.5081363,1.4532932,1.5630891,2.2423873,1.2959518,0.99102134,1.049378,1.9983859,1.3757626,1.2949821,1.3242178,3.3948612,3.515157,3.3148122,3.166795,2.0657117,3.3522806,3.3736875,3.532851,2.9779732,3.2499075,3.3418903,2.8402197,1.9086109,2.9218717,2.876257,3.21757,2.5368297,2.8358512,4.0454254,4.44675,1.0286945,1.0787348,0.86971456,0.873961,0.79114217,2.8144968,0.9376508,0.5728634,-0.572812,-6.097988,-6.0411024,-6.858837,-10.339102,-6.320107,-8.3037405,1.183972,2.4160979,2.3632298,2.509028,2.526775,2.4667761,-0.07930717,-0.35713005,-0.09309158,0.338999,0.5903373,1.7962033,2.5634968,3.7108123,0.035554457,-6.457292,5.932612,2.11606,5.041508,6.4341655,5.600909,3.7212894,3.1838145,2.9344628,3.146869,4.16294,-1.124083,0.49230516,0.57815844,0.47200185,0.42076528,0.39569727,0.3787892,3.4280732,3.0358615,3.0735352,2.0953138,-1.7718955,0.18639676,3.8373113,3.379178,4.3996034,3.613248,-1.9663937,19.449684,14.811802,2.8357704,5.9439006,-1.2738101,-0.8066945,3.8374064,3.7949433,3.8567514,3.589813,1.9099128,2.1814785,-2.2575588,-2.2632027,-1.0044513,-0.6117637,-2.374505,-9.353947,-5.814512,-6.0045643,5.267645,2.8006687,0.22170058,1.3957745,4.6155715,4.600263,-0.48037252,-1.6873859,2.3228958,6.886696,-11.579611,-11.483058,2.9214146,2.9772787,-11.810086,-11.107731,-11.390018,-10.709177,-10.387441,-11.128079,-11.543463,5.404334,6.8652763,6.7599187,6.9599385,6.7371716,6.8373938,6.984249,-1.604712,-1.2002524,-1.4125738,-1.9367563,-1.7392118,3.2732594,3.3375087,1.4581511,1.4644948,1.3660839,-6.5682173,-3.6423178,-4.5420613,3.4481392,2.7559466,3.2206366,2.191122,1.6267946,4.511982,5.8643274,5.7898,5.782654,-20.007206,-1.9508276,-2.0190897,-2.9309528,-2.1855354,-1.9693571,-0.67560387,5.5228662,-1.1833857,6.927796,5.3463616,5.433619,3.9263282,0.22353186,1.8043785,3.550137,3.9648418,3.1615915,2.9347866,2.6694458,-0.09246092,2.0737243,1.5396961,1.4676142,5.149719,2.1806307,1.4993371,2.2858956,7.80351,1.371862,-7.587568,-7.5225396,0.78309774,-8.096421,3.941788,3.8743372,-1.8235929,-1.8393414,-1.5894675,4.9108996,4.338533,4.4485884,3.8133423,4.24464,-1.4774301,-1.585218,-1.5323696,-1.5680922,1.9291388,-10.092791,-10.04542,1.2726413,1.7009368,2.0496385,-0.4498946,-0.10155119,-0.27357358,-1.810106,-2.1041117,5.4575152,5.357571,-1.4406732,-0.9701302,-9.3647175,-6.5026584,-9.342435,0.59979755,0.9040193,3.216378,2.479572,2.9380052,-5.6014476,17.957794,1.7360854,-0.52975583,-9.502039,0.6237036,0.7299397,0.5539701,5.3202653,5.66358,6.0664206,6.6569176,6.834667,6.4050083,6.0758386,6.0153394,-11.860934,-10.948606,7.054474,7.0846634,2.511643,6.9922166,7.0787077,-7.557054,7.20574,7.4958215,7.262655,-5.597203,-6.930301,-6.671265,-6.6786213,-5.395891,3.162903,0.17846711,-17.473646,-17.930676,-17.492414,-18.303572,-18.335165,-17.92646,-18.04606,-18.131817,-18.239994,-17.83197,-7.196317,-6.1714334,-6.769577,-7.6542826,2.735261,1.3133335,2.0569456,2.9804225,-0.34728724,2.4830606,0.39911637,2.0066605,2.1092,0.11065691,-0.040092386,-10.794716,2.5699935,-6.622923,-6.541504,-6.522277,-6.8306856,-6.861799,-6.6688046,-7.263933,-6.757066,-6.9042945,0.56267166,-0.14408551,0.3686608,1.1028655,1.3885083,2.8003075,3.8036835,3.84628,2.5170739,3.5130067,3.8764093,3.944384,3.8511636,3.415949,3.808189,4.973229,-5.882247,-6.098123,7.2855597,7.016561,3.0772371,-7.1737123,-6.557947,-7.0720134,-8.308796,-0.92462146,0.33101574,-1.2802442,5.495248,-1.6555923,3.8049042,1.6186981,3.8342438,-1.2405362,-1.6412444,-2.0343597,-1.9785672,-2.1561477,-2.0680835,-1.7601472,-1.8071089,-2.092573,-16.041634,-16.03627,3.7355847,4.0144877,3.1538405,4.3978677,8.287362,4.0197697,4.6705155,-11.813534,-11.869165,-11.869478,1.4279317,1.4474573,1.6698661,1.36624,4.5454893,7.7849355,3.9029558,7.9111476,4.6078625,4.7380104,5.2116704,0.691336,0.93182874,4.452601,-2.2886558,19.44957,14.812831,-3.11821,-2.3546014,5.8726315,5.839696,5.7767706,5.8457727,-2.3912854,-2.397187,5.7922506,6.9678206,6.247147,-2.4761908,5.7204285,-2.4085848,-1.9890335,-2.3826807,-1.4445939,0.8144567,-1.4350523,6.062983,6.6150765,6.4815826,0.9184576,0.84633774,0.47798425,0.720606,-11.381509,0.6779019,0.6213024,0.12676184,-0.010792731,-0.6829475,-9.707237,-9.618814,-1.2681695,2.2968378,2.363801,2.2863812,2.7170205,8.460176,8.549563,2.2192664,-2.7337544,-2.4833214,6.475634,6.4136105,-1.9063945,-6.165405,-4.787786,-5.315994,-2.0995224,-4.421961,-2.9949465,3.9643888,5.5853477,10.0660515,7.9020934,-1.5975347,-1.0056722,-2.1008837,-7.881383,-8.500498,0.85238457,-7.5915394,3.6857514,4.2075334,4.2705584,3.558384,1.1076609,2.8698936,4.8017592,3.125007,4.036972,1.6051935,4.776208,4.6790876,3.5571282,2.4978015,2.8037348,4.381673,4.6565127,1.3690464,0.34406838,-1.6553957,1.9737934,2.0888667,1.6833156,-11.719377,9.673888,5.256794,7.7425537,8.34699,7.6157103,7.6833944,-2.981777,-3.0775025,-2.9209552,-3.0007012,-3.1037812,-2.9302251,-3.136867,-3.0507023,-3.8948517,-3.932042,-4.326748,-0.52230656,1.8472931,6.3809366,6.437281,-7.209103,-8.875947,-8.116624,-8.197115,-8.140013,-8.322044,1.8708943,-6.980221,-5.5657673,-7.6281705,2.3249354,1.3659391,0.79911804,-10.459151,-10.592911,-0.32061538,0.3468662,-0.16406123,-0.5713359,-0.31751356,-0.60620314,-0.707623,-0.70180917,-0.781103,-0.7547758,-0.75704145,-4.5336986,-4.647283,-4.2934074,-6.603642,-4.5160713,-4.691693,-2.1572602,-0.36626846,2.5123508,2.6861942,5.2742767,2.1335192,2.2701852,2.5294797,-0.23263855,0.10700184,-18.289032,2.664638,4.451998,-7.6074924,2.3287764,2.5799274,4.4065175,3.2478817,-6.844369,-11.781713,-1.3262687,4.5093017,6.3252697,3.6212966,0.82403094,2.404974,1.464658,-3.4866023,-3.4150329,-2.9876928,-3.206281,-5.5874786,-5.7736154,-3.0024736,-2.9473243,-0.079682626,1.1479777,1.24745,0.8629873,1.8407669,1.2110207,-11.843261,3.044055,4.227397,2.4823263,5.301733,-7.480426,-6.560989,-6.188301,-1.7492417,0.34864217,1.4484729,5.798497,1.4469464,-11.498304,-11.452924,-11.438451,-11.349927,-10.491639,2.0753706,2.928599,3.9640946,2.6502957,3.0704935,3.766899,3.5628314,1.4333754,0.64138186,0.68871236,-18.224371,-18.31815,-18.361519,-18.540863,-18.267178,-18.398113,-18.495808,-18.329235,-17.820969,-18.480846,-18.40108,-18.343374,-3.7964466,-4.57114,4.950719,-10.362342,5.728979,6.608087,6.4327226,-2.1487858,19.450487,-11.534892,-10.971963,-11.614044,-11.498863,-11.497015,2.8199883,2.8560457,-12.161203,5.353566,2.358138,3.8706245,4.3109226,3.2291567,2.3193867,2.3743947,2.8849785,1.0747051,1.6309577,0.8043417,1.6873499,0.9868325,1.3515857,1.8475405,1.4330924,-0.6766388,4.4643373,0.45462707,0.5449367,-11.935612,-10.643963,5.492867,-10.269636,0.15098958,-1.2875243,1.2846978,-10.441272,-0.65137845,3.3395286,3.569795,3.3980575,0.1484232,0.04594151,-0.32646203,0.46998096,-0.7391425,-12.484111,-2.1450372,-2.3458993,5.9698553,6.3353415,3.1081655,7.531727,7.7678595,3.5063434,1.7971286,3.9022362,3.797131,3.4925687,3.9999955,5.915603,5.8419905,6.00442,5.9046454,2.0019326,0.57871294,-6.232963,1.7351685,-1.4112426,2.7539546,0.95923775,0.76551294,0.67811537,-9.595154,-2.0847323,19.453312,-4.013475,6.4220986,6.501438,-9.879683,3.5332596,3.1834943,-11.199827,-11.538726,3.1370528,-1.6518722,8.033724,0.19492008,5.8514595,0.939388,-0.0063016615,0.85204446,0.47280142,-0.14886035,2.7089665,0.4024341,-0.4801803,-0.17912827,-0.2027725,-12.287793,4.6701365,4.76045,-2.128502,1.6173798,1.5047358,3.8168964,3.220569,2.2304122,0.022083638,0.6126517,0.47972053,1.0898231,1.628923,5.5283475,2.7094202,3.6145008,1.4941791,2.8907945,-1.8789307,3.0950532,3.3915355,3.0589783,4.049098,-1.7270696,-1.3044667,0.212987,-1.8574563,-1.8811429,3.5114315,3.5448635,3.418685,-11.361594,-11.443319,-11.406445,-11.506434,-1.277463,-2.6733854,-2.2810717,-1.5313497,-1.4929446,-2.0174768,8.553675,-4.073916,-4.064286,-7.269353,-9.711122,1.8031082,1.523262,1.0694978,1.4595402,1.4085708,0.921903,4.7620597,4.297521,3.2353437,4.1005893,4.097268,4.1659703],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"--\\ntitle: \\\"Large Language Models: A New Moore's Law?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f33_large_language_mode...\"],[\"### Deep Learning, Deep Pockets?\\n\\nAs you would expect, training a 530-billion parameter model on hum...\"],[\"I'm left wondering what's the point of it all. Science for the sake of science? Good old marketing? ...\"],[\"Downsizing efforts are also under way in the Natural Language Processing community, using transfer l...\"],[\"If you need a tutorial, the Hugging Face [course](https:\\u002f\\u002fhuggingface.co\\u002fcourse) will get you starte...\"],[\"* Specialized hardware that speeds up training ([Graphcore](https:\\u002f\\u002fwww.graphcore.ai\\u002f), [Habana](htt...\"],[\"--\\ntitle: \\\"Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too\\\"\\nthumbn...\"],[\"## What do we do now?\\n\\nWith Inference Endpoints, our flow looks like this:\\n\\n- Train model on a GPU i...\"],[\"The following table shows latency (ms ± standard deviation and time to complete test in seconds) for...\"],[\"Inference Endpoints are more expensive that what we were doing before, there’s an increased cost of ...\"],[\"```bash\\nhugie endpoint create example\\u002fdevelopment.json\\n```\\n\\nFor me, what’s lacking is a [custom terr...\"],[\"--\\ntitle: \\\"DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub\\\" \\nthumbnail: \\u002fblog\\u002fassets...\"],[\"## TLDR\\n\\n[Datasets Server](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets-server\\u002findex) **automatically conver...\"],[\"To learn more, check out the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets-server\\u002fparquet_pro...\"],[\"--\\ntitle: \\\"Building an AI WebTV\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f156_ai_webtv\\u002fthumbnail.gif\\nauthors:\\n- user:...\"],[\"Here's a diagram of the current architecture of the AI WebTV:\\n\\n![diagram.jpg](https:\\u002f\\u002fhuggingface.co...\"],[\"Here is an example:\\n\\n```typescript\\nimport { client } from \\\"@gradio\\u002fclient\\\"\\n\\nexport const generateVid...\"],[\"const allFiles = await fs.readdir(\\\"** PATH TO VIDEO FOLDER **\\\")\\nconst allVideos = allFiles\\n  .map(fi...\"],[\"## Characters and scene composition\\n\\n\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo7....\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo8....\"],[\"💡 It will be interesting to see these capabilities explored more in the future, for instance by trai...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo1....\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"fail2....\"],[\"# Recommendations\\n\\nHere are some early recommendations that can be made from the previous observatio...\"],[\"# Future work\\n\\nWe hope you enjoyed watching the AI WebTV stream and that it will inspire you to buil...\"],[\"--\\ntitle: Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"Our team, like probably many others, is a distributed one, spanning 12 time zones. Our common thread...\"],[\"The ability to search through large collections of images using text queries is an immensely powerfu...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f30_clip_rsicd\\u002frsicd-images-sampling.png\\\"\\u002f\\u003e\\n\\u003ccenter\\u003e\\u003ci\\u003eSome examples of images...\"],[\"#### Data Augmentation\\n\\nIn order to regularize our dataset and prevent overfitting due to the size o...\"],[\"The `baseline` model represents the pre-trained `openai\\u002fclip-vit-base-path32` CLIP model. This model...\"],[\"* Text to Image search\\n* Image to Image search\\n* Find text feature in image\\n\\nThe first two functiona...\"],[\"--\\ntitle: \\\"Multivariate Probabilistic Time Series Forecasting with Informer\\\" \\nthumbnail: \\u002fblog\\u002fasset...\"],[\"Modeling the full joint conditional distribution of high dimensional data can get computationally ex...\"],[\"As you can see, the motivation for the Informer model is similar to Longformer ([Beltagy et el., 202...\"],[\"$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nWhere \\\\\\\\(Q\\\\in \\\\math...\"],[\"| ![informer_probsparse](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fma...\"],[\"# calculate the query sparsity measurement with Q_K_sample\\n    M = Q_K_sample.max(dim=-1)[0] - torch...\"],[\"$$\\nX_{n+1} = \\\\textrm{MaxPool} ( \\\\textrm{ELU}(\\\\textrm{Conv1d}(X_n))\\n$$\\n\\n\\nLet's see this in code:\\n    ...\"],[\"```python\\n!pip install -q transformers datasets evaluate accelerate gluonts ujson\\n```\\n\\n## Load Datas...\"],[\"The `start` simply indicates the start of the time series (as a datetime), and the `target` contains...\"],[\"plt.show()\\n```\\n    \\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002f...\"],[\"Note that the target is now 2-dimensional, where the first dimension is the number of variates (numb...\"],[\"\\u003e\\u003e\\u003e [1, 2, 3, 4, 5, 6, 7, 23, 24, 25, 47, 48, 49, 71, 72, 73, 95, 96, 97, 119, 120, \\n     121, 143, ...\"],[\"\\u003e\\u003e\\u003e {'hour_of_day': array([-0.45652174]), 'day_of_week': array([0.]), 'day_of_month': array([-0.5]),...\"],[\"```python\\nfrom gluonts.time_feature import TimeFeature\\nfrom gluonts.dataset.field_names import Field...\"],[\"return Chain(\\n        # step 1: remove static\\u002fdynamic fields if not specified\\n        [RemoveFields(...\"],[\"output_field=FieldName.FEAT_TIME,\\n                time_features=time_features_from_frequency_str(fre...\"],[\"## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitter` w...\"],[\"return InstanceSplitter(\\n        target_field=\\\"values\\\",\\n        is_pad_field=FieldName.IS_PAD,\\n     ...\"],[\"# we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \\\"train\\\"...\"],[\"def create_test_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n ...\"],[\"As can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the case ...\"],[\"Also note that the decoder uses a causal mask to not look into the future as the values it needs to ...\"],[\"loss_history.append(loss.item())\\n        if idx % 100 == 0:\\n            print(loss.item())\\n\\n\\u003e\\u003e\\u003e -108...\"],[\"The model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `input_...\"],[\"```python\\nprint(f\\\"MASE: {np.mean(mase_metrics)}\\\")\\n\\n\\u003e\\u003e\\u003e MASE: 1.1913437728068093\\n\\nprint(f\\\"sMAPE: {np....\"],[\"For example:\\n\\n\\n```python\\nplot(0, 344)\\n```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocume...\"],[\"--\\ntitle: \\\"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\\"...\"],[\"It is perhaps unsurprising that English is by far the most common language for datasets on the Hub, ...\"],[\"#### Why is Language Metadata Important?\\n\\nLanguage metadata can be a vital tool for finding relevant...\"],[\"```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"biglam\\u002fon_the_books\\\")\\n```\\n\\nHowe...\"],[\"#### Predicting the Language of a Dataset \\n\\nOnce we have some examples of text from a dataset, we ne...\"],[\"Once we’ve done this filtering, we have a further step of deciding how to use these predictions. The...\"],[\"### Using Librarian-Bot to Update Metadata\\n\\nTo ensure this valuable language metadata is incorporate...\"],[\"--\\ntitle: \\\"Generating Human-level Text with Contrastive Search in Transformers 🤗\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"****\\n\\n\\u003cspan id='demo'\\u002f\\u003e\\n\\n### 2. Hugging Face 🤗 Demo of Contrastive Search:\\n\\nContrastive Search is no...\"],[\"Below, let's see an example of generated text from greedy search using GPT-2 model.\\n\\n```python\\nfrom ...\"],[\"Below, we illustrate an example of generated text by nucleus sampling (p=0.95) using the GPT-2 model...\"],[\"We note that this semantic inconsistency problem can partially be remedied by lowering the temperatu...\"],[\"\\u003ccenter class=\\\"half\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f115_introducing_contrastive_search\\u002fformulation.png\\\" width...\"],[\"\\u003cspan id='contrastive_generation'\\u002f\\u003e\\n\\n#### 5.2. Generating Text with Contrastive Search:\\n\\nBelow, we u...\"],[\"The victory is a testament to the power of deep learning, and to the incredible work of our\\nresearch...\"],[\"In addition to the win in Go, DeepMind has also developed an AI system that can learn to play a\\nnumb...\"],[\"**[Remark]** From the result of greedy search, we see high similarity scores in the off-diagonal ent...\"],[\"\\u003cspan id='gpt2_greedy_example_one'\\u002f\\u003e\\n\\n##### 6.1.1. Generating Text with Greedy Search:\\n\\n\\u003cdetails\\u003e\\n\\u003cs...\"],[\"The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\nare...\"],[\"Polygynous mammals such as unicorns have remained largely unknown to science. Professor Gustavo\\nGiac...\"],[\"\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n--------------------------------...\"],[\"While the discovery is exciting, it's not the first time scientists have discovered an animal that s...\"],[\"\\u003cdetails\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output: [click to expand]\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n-------------------...\"],[\"\\u003cspan id='opt_greedy_example_two'\\u002f\\u003e\\n\\n##### 6.2.2. Generating Text with Nucleus Sampling:\\n\\n\\u003cdetails\\u003e\\n...\"],[\"In this paper, we propose a model-based residual learning (MBRL) framework that is based on neural\\nn...\"],[\"@article{su2022contrastiveiswhatyouneed,\\n  title={Contrastive Search Is What You Need For Neural Tex...\"],[\"--\\ntitle: \\\"AMD + 🤗: Large Language Models Out-of-the-Box Acceleration with AMD GPU\\\"\\nthumbnail: \\u002fblog...\"],[\"inp = tokenizer([\\\"Today I am in Paris and\\\"], padding=True, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\nres = mod...\"],[\"* Flash Attention v2 from AMD Open Source efforts in [ROCmSoftwarePlatform\\u002fflash-attention](https:\\u002f\\u002f...\"],[\"One AMD Instinct MI250 GPU with 128 GB of High Bandwidth Memory has two distinct ROCm devices (GPU 0...\"],[\"Running [training benchmarks](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-benchmark\\u002ftree\\u002fmain\\u002fexamples\\u002ftr...\"],[\"With all of the above being said, we are thrilled to show the very first performance numbers demonst...\"],[\"In the coming months, we will be working on bringing more support and validation for AMD Radeon GPUs...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Lewis Tunstall\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f60_lewis_tunstall_inte...\"],[\"### Welcome, Lewis! Thank you so much for taking time out of your busy schedule to chat with me toda...\"],[\"### That is incredible. How does it feel to have a copy of your book in your hands? \\n\\n**Lewis:** I h...\"],[\"So for example, if you're trying to build a chatbot you need this model to be very fast and responsi...\"],[\"And the model wrote a compelling essay on why recycling was bad. Leandro and I were working at a sta...\"],[\"And, we've released two parts of this course and planning to release the third part this year. I'm r...\"],[\"### Where do you want to see more ML applications?\\n\\n**Lewis:** So I think personally, the area that ...\"],[\"And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the ...\"],[\"And you have all these subtle failure modes where the models will maybe provide completely wrong ans...\"],[\"And then I think the second big lesson I’ve learned from building a lot of projects is that you can ...\"],[\"And this is a really great skill to have. But what I later discovered is that my true driving passio...\"],[\"### Should people be afraid of AI taking over the world?\\n\\n**Lewis:** Maybe. It’s a tough one because...\"],[\"**Lewis:** It's a great question. So for me, I like podcasts in general. It’s my new way of reading ...\"],[\"And I was like, okay, I'll read this paper from the 2000s and see if I understand it. And it's a mod...\"],[\"It's very much like the universe is quite random and I suppose the only thing you can take from that...\"],[\"You know, if you talk to like a cockatoo it will swear at you or make jokes. That may not be a true ...\"],[\"**Lewis:** I'm fairly active on Twitter. You can just find me my handle [@_lewtun](https:\\u002f\\u002ftwitter.c...\"],[\"--\\ntitle: 'Welcome fastai to the Hugging Face Hub'\\nthumbnail: \\u002fblog\\u002fassets\\u002f64_fastai\\u002ffastai_hf_blog....\"],[\"👉 In this post, we will introduce the integration between fastai and the Hub. Additionally, you can ...\"],[\"## Joining Hugging Face and installation\\n\\nTo share models in the Hub, you will need to have a user. ...\"],[\"2. If in a python notebook, you can use `notebook_login`.\\n\\n```py\\nfrom huggingface_hub import noteboo...\"],[\"## Loading a `Learner` from the Hugging Face Hub\\n\\nLoading a model from the Hub is even simpler. We w...\"],[\"Collaboration and open-source are fantastic!\\n\\nFirst, install `blurr` and train the Learner.\\n\\n```bash...\"],[\"## What's next?\\n\\nTake the [fast.ai course](https:\\u002f\\u002fcourse.fast.ai\\u002f) (a new version is coming soon), ...\"],[\"--\\ntitle: \\\"StackLLaMA: A hands-on guide to train LLaMA with RLHF\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f138_stack...\"],[\"\\u003cscript\\n\\ttype=\\\"module\\\"\\n\\tsrc=\\\"https:\\u002f\\u002fgradio.s3-us-west-2.amazonaws.com\\u002f3.23.0\\u002fgradio.js\\\"\\u003e\\u003c\\u002fscript\\u003e\\n\\n...\"],[\"`score = log2 (1 + upvotes) rounded to the nearest integer, plus 1 if the questioner accepted the an...\"],[\"Loading the model in 8bit reduces the memory footprint drastically since you only need one byte per ...\"],[\"![chapter10_ddp.png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftrl-internal-testing\\u002fexample-images\\u002fresolve\\u002fmai...\"],[\"![chapter10_preprocessing-clm.png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftrl-internal-testing\\u002fexample-imag...\"],[\"**Disclaimer:** due to LLaMA's license, we release only the adapter weights for this and the model c...\"],[\"\\\\\\\\( \\\\operatorname{loss}(\\\\theta)=- E_{\\\\left(x, y_j, y_k\\\\right) \\\\sim D}\\\\left[\\\\log \\\\left(\\\\sigma\\\\left(r_...\"],[\"As detailed in the next section, the resulting adapter can be merged into the frozen model and saved...\"],[\"```python\\nfor epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\\n    question_tensors = batch[...\"],[\"## Challenges, instabilities and workarounds\\n\\nTraining LLMs with RL is not always plain sailing. The...\"],[\"\\\\\\\\( KL_{pen}(x,y) = \\\\log \\\\left(\\\\pi_\\\\phi^{\\\\mathrm{RL}}(y \\\\mid x) \\u002f \\\\pi^{\\\\mathrm{SFT}}(y \\\\mid x)\\\\right...\"],[\"By using `peft`, anyone can run our example on a single GPU! If training is too slow, you can use da...\"],[\"## Acknowledgements\\n\\nWe thank Philipp Schmid for sharing his wonderful [demo](https:\\u002f\\u002fhuggingface.co...\"],[\"--\\ntitle:  Deploy LLMs with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f155_inference_e...\"],[\"Here are some of the most important features for LLM deployment:\\n\\n1. [Easy Deployment](https:\\u002f\\u002fhuggi...\"],[\"Then, click on “New endpoint”. Select the repository, the cloud, and the region, adjust the instance...\"],[\"You can use different parameters to control the generation, defining them in the `parameters` attrib...\"],[\"## 3. Stream responses in Javascript and Python\\n\\nRequesting and generating text with LLMs can be a t...\"],[\"Replace the `print` command with the `yield` or with a function you want to stream the tokens to. \\n\\n...\"],[\"![Javascript Streaming](assets\\u002f155_inference_endpoints_llm\\u002fjs-stream.gif \\\"Javascript Streaming\\\")\\n\\n##...\"],[\"--\\ntitle: \\\"2D Asset Generation: AI for Game Development #4\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games...\"],[\"### Image2Image\\n\\n[Diffusion models](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fDiffusion_model) such as Stable Di...\"],[\"I used a denoising strength of 0.8, to encourage the model to be more creative. After generating sev...\"],[\"### Example: Scythe\\n\\nIn many cases, you may need to fight Stable Diffusion a bit to get the result y...\"],[\"--\\ntitle: \\\"Supercharged Customer Service with Machine Learning\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_superchar...\"],[\"To filter out unsatisfied messages in an automated way, we plan on applying natural language process...\"],[\"Let's take a look at all available Datasets on the [Hugging Face Hub](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"Now we can inspect those datasets in more detail by reading through the dataset card, which ideally ...\"],[\"Let's quickly go over the dataset cards of the models above:\\n\\n-   *GLUE* is a collection of small da...\"],[\"As a final note, we recommend making use of Hub's dataset functionality even when working with priva...\"],[\"Let's take a look at all models that have been fine-tuned on Amazon Reviews Multi. You can find the ...\"],[\"However, both of the above resources are currently suboptimal. The model summary is not always kept ...\"],[\"## Training \\u002f Fine-tuning a model with 🤗 Transformers and 🤗 Datasets\\n\\nIn this section, we will jump ...\"],[\"### Preprocess the dataset\\n\\nBefore we can start training the model, we should bring the dataset in a...\"],[\"Great, that was fast 🔥. Let's take a look at the structure of the dataset.\\n\\n\\n```python\\nprint(amazon_...\"],[\"```python\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_re...\"],[\"To apply this function to all data samples in our dataset, we use the [`map`](https:\\u002f\\u002fhuggingface.co...\"],[\"Alright, the input text is transformed into a sequence of integers which can be transformed to word ...\"],[\"```python\\nfrom transformers import AutoModelForSequenceClassification\\n\\nmodel = AutoModelForSequenceC...\"],[\"```python\\nfrom transformers import DataCollatorWithPadding\\n\\ndata_collator = DataCollatorWithPadding(...\"],[\"```python\\nimport numpy as np\\n\\ndef compute_metrics(pred):\\n    pred_logits = pred.predictions\\n    pred...\"],[\"The trainer is ready to go 🚀 You can start training by calling `trainer.train()`.\\n\\n\\n```python\\ntrain_...\"],[\"**Output:**\\n```\\n    ***** Running Evaluation *****\\n      Num examples = 5000\\n      Batch size = 8\\n  ...\"],[\"### Evaluate \\u002f Analyse the model\\n\\nNow that we have fine-tuned the model we need to be very careful a...\"],[\"**Output:**\\n```\\n    {'test_accuracy': 0.608,\\n     'test_loss': 0.9637690186500549,\\n     'test_runtim...\"],[\"# Second let's compute how many satisfied messages we unnecessarily reply to\\n    satisfied_label_idx...\"],[\"Obviously, the numbers don't represent the gained value of an actual use case, but we could come clo...\"],[\"## Optimization\\n\\nAs soon as you think the model's performance is good enough for production it's all...\"],[\"--\\ntitle: \\\"Accelerating Document AI\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f112_document-ai\\u002fthumbnail.png\\nauthors:...\"],[\"Turning typed, handwritten, or printed text into machine-encoded text is known as Optical Character ...\"],[\"A basic approach is applying OCR on a document image, after which a [BERT](https:\\u002f\\u002fhuggingface.co\\u002fdo...\"],[\"Document layout analysis is the task of determining the physical structure of a document, i.e., iden...\"],[\"\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemty...\"],[\"LayoutLMv1 now has many successors. [Donut](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdonut...\"],[\"Documents often contain tables, and most OCR tools don't work incredibly well out-of-the-box on tabu...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv item...\"],[\"DocVQA is typically evaluated using the Average Normalized Levenshtein Similarity (ANLS) metric. For...\"],[\"Industry and academia make enormous contributions to advancing Document AI. There are a wide assortm...\"],[\"Second, be flexible in your approaches. You may need to test several different methodologies to find...\"],[\"Do you want the model to handle the OCR? For example, [Donut](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransforme...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\n### Next Steps\\n\\nAre you seeing the possibilities of Document AI? ...\"],[\"| model | paper | license | checkpoints |\\n| --- | --- | --- | --- |\\n| [Donut](https:\\u002f\\u002fhuggingface.co...\"],[\"| [Table Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002ftable-transformer) ...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv item...\"],[\"--\\ntitle: \\\"How we sped up transformer inference 100x for 🤗 API customers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f09...\"],[\"-| Naive version                                                                                    ...\"],[\"Once the compute platform has been selected for the use case, we can go to work. Here are some CPU-s...\"],[\"If you want to feel the speed on our infrastructure, start a [free trial](https:\\u002f\\u002fhuggingface.co\\u002fpri...\"],[\"--\\ntitle: \\\"Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chine...\"],[\"We have been collaborating with organizations such as [PaddlePaddle](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fpad...\"],[\"## Beyond Boundaries: Embracing a Diverse AI Community\\n\\nAs we embark on this new chapter, our collab...\"],[\"--\\ntitle: \\\"How to generate text: using different decoding methods for language generation with Trans...\"],[\"$$ P(w_{1:T} | W_0 ) = \\\\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\\\text{ ,with }  w_{1: 0} = \\\\emptyset...\"],[\"``` python\\n# encode context the generation is conditioned on\\nmodel_inputs = tokenizer('I enjoy walki...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f02_how-to-generate\\u002fbeam_search.png\\\" alt=\\\"beam search\\\" style=\\\"margin: auto; di...\"],[\"I'm not sure if I'll ever be able to walk with him again. I'm not sure\\n```\\n\\n\\nWhile the result is arg...\"],[\"In `transformers`, we simply set the parameter `num_return_sequences` to\\nthe number of highest scori...\"],[\"In open-ended generation, a couple of reasons have been brought\\nforward why beam search might not be...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f02_how-to-generate\\u002fsampling_search.png\\\" alt=\\\"sampling search\\\" style=\\\"margin: ...\"],[\"A trick is to make the distribution \\\\\\\\(P(w|w_{1:t-1})\\\\\\\\) sharper\\n(increasing the likelihood of high ...\"],[\"### Top-K Sampling\\n\\n[Fan et. al (2018)](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f1805.04833.pdf) introduced a\\nsimple, b...\"],[\"Not bad at all\\\\! The text is arguably the most *human-sounding* text so\\nfar. One concern though with...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f02_how-to-generate\\u002ftop_p_sampling.png\\\" alt=\\\"Top p sampling\\\" style=\\\"margin: au...\"],[\"Finally, to get multiple independently sampled outputs, we can *again*\\nset the parameter `num_return...\"],[\"Cool, now you should have all the tools to let your model write your\\nstories with `transformers`!\\n\\n\\n...\"],[\"- [How to parameterize `generate`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fgeneration_strategies#de...\"],[\"--\\ntitle: \\\"Accelerate your models with 🤗 Optimum Intel and OpenVINO\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f113_ope...\"],[\"​Let us show you how to get started in minutes!​\\n\\n## Quantizing a Vision Transformer with Optimum In...\"],[\"As usual with image datasets, we need to apply the same image transformations that were used at trai...\"],[\"```python\\nsave_dir = \\\"quantized_model\\\"\\n\\n# Apply static quantization and export the resulting quantiz...\"],[\"# We run the evaluation step on 20% of the evaluation dataset\\neval_dataset = load_dataset(\\\"food101\\\",...\"],[\"## Now it's your turn\\n​\\nAs you can see, it's pretty easy to accelerate your models with 🤗 Optimum In...\"],[\"--\\ntitle: Guiding Text Generation with Constrained Beam Search in 🤗 Transformers\\nthumbnail: \\u002fblog\\u002fas...\"],[\"$$ S_{expected} = \\\\{ s_1, s_2, ..., s_k, t_1, t_2, s_{k+1}, ..., s_n \\\\} $$\\n\\nThe problem is that beam...\"],[\"And depending on the context, we might want one form of formality over the other, but how do we tell...\"],[\"outputs = model.generate(\\n    input_ids,\\n    force_words_ids=force_words_ids,\\n    num_beams=5,\\n    n...\"],[\"model = GPT2LMHeadModel.from_pretrained(\\\"gpt2\\\")\\ntokenizer = GPT2Tokenizer.from_pretrained(\\\"gpt2\\\")\\n\\nf...\"],[\"![Beam search](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fbeam_sear...\"],[\"Let's say that we're trying to force the phrase `\\\"is fast\\\"` in the generated output. \\n\\nIn the tradit...\"],[\"Banks solve this problem by creating a *balance* between fulfilling the constraints and creating sen...\"],[\"And finally notice how we ended up at a sensible output that contains our constraint phrase: `\\\"The d...\"],[\"print(\\\"Output:\\\\n\\\" + 100 * '-')\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\\n\\n  ...\"],[\"or if the user does not care about the number of tokens that can go in between two words, then one c...\"],[\"--\\ntitle: \\\"Making a web app generator with open ML models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f153_text_to_webap...\"],[\"However, running large language models in such an environment can be pretty resource-intensive, espe...\"],[\"If you are interested in other solutions, here are some pointers to alternative implementations:\\n\\n- ...\"],[\"You will have to select `WizardCoder` in the **Model Repository** dropdown and make sure that a GPU ...\"],[\"\\u002f\\u002f also print to the console for debugging\\n    process.stdout.write(output.token.text)\\n  }\\n\\n  req.en...\"],[\"You can try to use an imperative tone and repeat the instructions. An efficient way can also be to s...\"],[\"You can also try to be more specific, for example:\\n\\n```\\nOnly generate a few images and use descripti...\"],[\"To make the output more dense we can use [Daisy UI](https:\\u002f\\u002fdaisyui.com\\u002fdocs\\u002fuse\\u002f), a Tailwind plugi...\"],[\"--\\ntitle: 'Liftoff! How to get started with your first ML project 🚀'\\nthumbnail: \\u002fblog\\u002fassets\\u002f84_firs...\"],[\"\\u003e Compute dense vector representations for sentences, paragraphs, and images\\n\\nIn a nutshell, Sentenc...\"],[\"Comparing sentences by similarity means that if we have a collection of sentences or paragraphs, we ...\"],[\"Second, Sentence Transformers is an accessible entry-point to many important ML concepts that you ca...\"],[\"On top of it all, it’s also supported with a ton of [Hugging Face integrations](https:\\u002f\\u002fhuggingface....\"],[\"1. **Do a brain dump of everything you know the tool’s capable of**: For Sentence Transformers this ...\"],[\"4. **Ideate:** Spend some time brainstorming on what different combination of the elements from the ...\"],[\"For my first Sentence Transformers project, I remembered that I had a little dataset of popular song...\"],[\"## What can you expect to learn from your first project?\\n\\nSince every project is unique, your learni...\"],[\"--\\ntitle: \\\"Fit More and Train Faster With ZeRO via DeepSpeed and FairScale\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"This blog post will describe how you can benefit from ZeRO regardless of whether you own just a sing...\"],[\"1. `--fp16`\\n2. `--sharded_ddp` (fairscale)\\n3. `--sharded_ddp --fp16` (fairscale)\\n4. `--deepspeed` wi...\"],[\"It's easy to see that both FairScale and DeepSpeed provide great improvements over the baseline, in ...\"],[\"Let's try the impossible - let's train [t5-3b](https:\\u002f\\u002fhuggingface.co\\u002ft5-3b) on a 24GB RTX-3090 card...\"],[\"Simply amazing!\\n\\nI used only a tiny sample since I was primarily interested in being able to train a...\"],[\"This idea could be difficult to grasp, and you will find my attempt at an explanation [here](https:\\u002f...\"],[\"As ZeRO stands for Zero Redundancy Optimizer, it's easy to see that it lives up to its name.\\n\\n# The ...\"],[\"# Resources\\n\\nWhile you don't really need to understand how any of these projects work and you can ju...\"],[\"In particular I'd like to thank:\\n\\n* Benjamin Lefaudeux [@blefaudeux](https:\\u002f\\u002fgithub.com\\u002fblefaudeux)\\n...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #1\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f103_ethics-soc-1\\u002fthumbnail.png...\"],[\"How to operationalize ethics in AI is an open research area. Although theory and scholarship on appl...\"],[\"We are continuously researching practices and studies on the meaning of a “good” ML, trying to provi...\"],[\"Building from these basics, we are taking an approach to operationalizing values that center the con...\"],[\"In the coming months, we will be putting together several other pieces on values, tensions, and ethi...\"],[\"--\\ntitle: \\\"Open LLM Leaderboard: DROP deep dive\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fevaluating-mmlu-leaderboard...\"],[\"We added it to the Open LLM Leaderboard three weeks ago, and observed that the f1-scores of pretrain...\"],[\"Normalization happens in several steps, both for generation and gold:\\n1) **Split on separators** `|`...\"],[\"## Diving into the results\\nExtending our investigations, our friends at [Zeno](https:\\u002f\\u002fzenoml.com) j...\"],[\"At this point, we believed that both failure cases were actually caused by the same root factor: usi...\"],[\"## So what's next?\\nA quick calculation shows that re-running the full evaluation of all models would...\"],[\"We hope that interested members of the community will join forces with academics working on DROP eva...\"],[\"--\\ntitle: \\\"Evaluating Language Model Bias with 🤗 Evaluate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f112_evaluating-ll...\"],[\"Let's work through bias evaluation in 3 prompt-based tasks focused on harmful language: Toxicity, Po...\"],[\"Although we define these prompts directly for the sake of example here, more can be extracted direct...\"],[\"The toxicity measurement can be used to evaluate any kind of text, be it machine-generated or human-...\"],[\"And as before, we use GPT-2 to generate completions:\\n```python\\n\\u003e\\u003e\\u003e profession1_completions = [\\\"to ge...\"],[\"## Hurtful sentence completions\\n\\nThe latest bias evaluation metric that we've added to 🤗 Evaluate is...\"],[\"## Discussion\\n\\nBeyond the datasets presented above, you can also prompt models using other datasets ...\"],[\"--\\ntitle: \\\"Can foundation models label data like humans?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fllm-leaderboard\\u002fle...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fllm-leaderboa...\"],[\"## Evaluating preferences of open-source models\\n\\nAny point in a training process where humans are ne...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fllm-leaderboa...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fllm-leaderboa...\"],[\"****************Elo rankings w\\u002f ties (bootstrapped from 1000 rounds of sampling games)**************...\"],[\"### The Start of Assistant 1's Answer\\n{answer_1}\\n### The End of Assistant 1's Answer\\n\\n### The Start ...\"],[\"**Elo rankings without ties (bootstrapped from 1000 rounds of sampling games)**\\n\\n| Model | Elo ranki...\"],[\"## GPT-4 evaluation examples\\n\\nBelow we’ve included a couple examples of what the evaluations look li...\"],[\"---\\n\\n**Question:**\\n\\nWrite a LinkedIn post to announce that you have accepted a new job offer.\\\\n Inpu...\"],[\"---\\n\\n**Question:**\\n\\nYou are given a search query and a document. Classify whether the document is re...\"],[\"---\\n\\n## Further experiments\\n\\n### Correlating human and GPT-4 labels\\n\\nHere we break down the categori...\"],[\"```latex\\nBe aware that LLMs like yourself are extremely prone to positional bias and tend to return ...\"],[\"## Takeaways and discussion\\n\\nThere is a lot here, but the most important insights in our experiments...\"],[\"Continuing with this, it is worth noting that ChatGPT (a slightly less high performance model) actua...\"],[\"- **Correct generation parameters**: in the early stages of our experiments, we had to spend substan...\"],[\"### Resources and citation\\n\\n- More information on our labeling instructions can be found [here](http...\"],[\"--\\ntitle: \\\"Student Ambassador Program’s call for applications is open!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f67_a...\"],[\"🤗 Insight into the latest projects, features, and more!\\n\\n🎁 Merchandise and assets. \\n\\n✨ Being officia...\"],[\"--\\ntitle: \\\"Training a language model with 🤗 Transformers using TensorFlow and TPUs\\\"\\nthumbnail: \\u002fblog...\"],[\"Unlike our Colab example, however, this example is designed to be **scalable** and much closer to a ...\"],[\"## What to expect\\n\\nWe’re going to train a [RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_d...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"We then take these tokenized samples in batches and serialize those batches as multiple TFRecord sha...\"],[\"```python\\nimport tensorflow as tf\\n\\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver(...)\\nstra...\"],[\"Thankfully, TensorFlow provides seamless support for reading files from a GCS bucket:\\n\\n```python\\ntra...\"],[\"model_id = \\\"tf-tpu\\u002froberta-base-epochs-500-no-wd\\\"\\nunmasker = pipeline(\\\"fill-mask\\\", model=model_id, f...\"],[\"*If you can learn from errors, and proceed,*\\u003cbr\\u003e\\n*And optimize your aim to reach the sky,*\\u003cbr\\u003e\\n*Your...\"],[\"--\\ntitle: \\\"A Dive into Vision-Language Models\\\"\\nthumbnail: \\u002fblog\\u002f\\u002fassets\\u002f128_vision_language_pretrain...\"],[\"## Introduction\\n\\nWhat does it mean to call a model a “vision-language” model? A model that combines ...\"],[\"A vision-language model typically consists of 3 key elements: an image encoder, a text encoder, and ...\"],[\"Note that this section is a non-exhaustive list, and there are various other approaches, as well as ...\"],[\"For CLIP, the distance is simply the cosine distance between the text and image embeddings, whereas ...\"],[\"Visual transformers (ViT) apply the same concept of the prefix to images by dividing each image into...\"],[\"While fusing visual information into a language model is highly effective, being able to use a pre-t...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"### 4) Masked-Language Modeling \\u002f Image-Text Matching\\n\\nAnother line of vision-language models uses a...\"],[\"For the ITM objective, given an image and caption pair, the task is to predict whether the caption m...\"],[\"[ASIF](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2210.01738) proposes a simple method to turn pre-trained uni-modal imag...\"],[\"### Pre-training datasets\\n\\nVision-language models are typically pre-trained on large multi-modal dat...\"],[\"### Downstream datasets \\n\\nPre-trained vision-language models are often trained on various downstream...\"],[\"## Supporting Vision-Language Models in 🤗 Transformers\\n\\nUsing Hugging Face Transformers, you can eas...\"],[\"While models such as CLIP, FLAVA, BridgeTower, BLIP, LiT and `VisionEncoderDecoder` models provide j...\"],[\"### ViLT for VQA\\n\\nLet’s start with ViLT and download a model pre-trained on the VQA dataset. We can ...\"],[\"Straight-forward, right? Let’s do another demonstration with CLIPSeg and see how we can perform zero...\"],[\"```py\\nimport torch\\n\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n\\nlogits = outputs.logits\\npri...\"],[\"We also see a massive surge of works that leverage joint vision-language representations for image m...\"],[\"While robotics research hasn’t leveraged vision-language models on a wide scale yet, we see works su...\"],[\"We are continuing to integrate the most impactful computer vision and multi-modal models and would l...\"],[\"--\\ntitle: \\\"Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Dis...\"],[\"- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tw...\"],[\"\\u003c!-- \\u002fTOC --\\u003e\\n\\n\\n\\n## Introduction \\n\\nIn the fast-moving world of Natural Language Processing (NLP), we...\"],[\"## Dependencies\\n\\n```bash\\ndatasets\\nevaluate\\npeft\\nscikit-learn\\ntorch\\ntransformers\\nwandb \\n```\\nNote: For...\"],[\"### [Mistral 7B](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2310.06825)\\n\\nMistral 7B v0.1, with 7.3 billion parameters, is...\"],[\"```python\\nMAX_LEN = 512 \\nroberta_checkpoint = \\\"roberta-large\\\"\\nmistral_checkpoint = \\\"mistralai\\u002fMistra...\"],[\"- Test dataset\\n```\\n\\u003cclass 'pandas.core.frame.DataFrame'\\u003e\\nRangeIndex: 3263 entries, 0 to 3262\\nData co...\"],[\"The data comprises a keyword, a location and the text of the tweet. For the sake of simplicity, we s...\"],[\"- Now, let's  apply the preprocessing function to the entire dataset: \\n\\n```python\\ncol_to_delete = ['...\"],[\"**Note** that Llama 2 and Mistral 7B don't have a default `pad_token_id`. So, we use the `eos_token_...\"],[\"# Data collator for padding a batch of examples to the maximum length seen in the batch\\nllama_data_c...\"],[\"### Mistral\\n\\n#### Load checkpoints for the classfication model\\n\\nLet's load the pre-trained Mistral-7...\"],[\"```python\\nllama_model.config.pad_token_id = llama_model.config.eos_token_id\\n```\\n\\n#### LoRa setup for...\"],[\"logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\\n...\"],[\"### Trainer Setup\\n\\nLet's set the training arguments and the trainer for the three models.\\n\\n#### RoBE...\"],[\"```python\\nfrom transformers import TrainingArguments, Trainer\\n\\nmistral_model = mistral_model.cuda()\\n...\"],[\"llama_trainer = WeightedCELossTrainer(\\n    model=llama_model,\\n    args=training_args,\\n    train_data...\"],[\"For more information, you can check the Wandb experiment report in the [resources sections](#resourc...\"],[\"--\\ntitle: \\\"Introducing DOI: the Digital Object Identifier to Datasets and Models\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg alt=\\\"Cite DOI\\\" src=\\\"assets\\u002f107_launching_doi\\u002fcite-modal.jpeg\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\nIf ever there’s ...\"],[\"--\\ntitle: \\\"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 1\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"Xeon CPUs support advanced features such as Advanced Vector Extensions ([AVX-512](https:\\u002f\\u002fen.wikiped...\"],[\"The AMX instructions accelerate matrix multiplication, an operation central to training DL models on...\"],[\"* Allow all network traffic inside the cluster, so that distributed training runs unencumbered. AWS ...\"],[\"# Clone the transformers repository for its example scripts\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"```\\nsource ~\\u002fcluster_env\\u002fbin\\u002factivate\\ncd ~\\u002ftransformers\\u002fexamples\\u002fpytorch\\u002fquestion-answering\\npip3 ins...\"],[\"export MASTER_ADDR=172.31.3.190\\nexport NUM_PROCESSES=8\\nexport NUM_PROCESSES_PER_NODE=2\\nexport CCL_WO...\"],[\"--\\ntitle: Introducing our new pricing\\nthumbnail: \\u002fblog\\u002fassets\\u002f114_pricing-update\\u002fthumbnail.png\\nautho...\"],[\"--\\ntitle: Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac\\nthumbnail: \\u002fblog\\u002fassets\\u002f149_...\"],[\"## New Core ML Optimizations\\n\\nCore ML is a mature framework that allows machine learning models to r...\"],[\"`coremltools` now includes a new submodule called `coremltools.optimize` with all the compression an...\"],[\"The compressed 6-bit _weights_ cannot be used for computation, because they are just indices into a ...\"],[\"* Quantization is supported using `--quantize-nbits` during conversion. You can quantize to 8, 6, 4,...\"],[\"\\u003cbr\\u003e\\n\\u003cdiv style=\\\"background-color: #f0fcf0; padding: 8px 32px 1px; outline: 1px solid; border-radius...\"],[\"```Python\\nfrom huggingface_hub import snapshot_download\\nfrom pathlib import Path\\n\\nrepo_id = \\\"apple\\u002fc...\"],[\"```bash\\npython -m python_coreml_stable_diffusion.torch2coreml \\\\\\n    --model-version prompthero\\u002fopenj...\"],[\"5. To integrate the desired model in your own app:\\n    * If you are going to distribute the model in...\"],[\"## Conclusion\\n\\nQuantization methods can be used to reduce the size of Stable Diffusion models, make ...\"],[\"his notebook shows how to deploy a vision model from 🤗 Transformers (written in TensorFlow) to [Vert...\"],[\"```python\\nimport transformers\\n\\nprint(tf.__version__)\\nprint(transformers.__version__)\\n```\\n\\n## Save th...\"],[\"def preprocess(string_input):\\n    decoded = tf.io.decode_jpeg(string_input, channels=3)\\n    resized ...\"],[\"## Deployment on Vertex AI\\n\\n[This resource](https:\\u002f\\u002fcloud.google.com\\u002fvertex-ai\\u002fdocs\\u002fgeneral\\u002fgeneral-...\"],[\"tf28_gpu_deployed_model = endpoint_service_client.deploy_model(\\n    endpoint=tf28_gpu_endpoint,\\n    ...\"],[\"response = endpoint_service_client.delete_endpoint(name=endpoint)\\n    print(\\\"running delete_endpoint...\"],[\"--\\ntitle: \\\"Retrieval Augmented Generation with Huggingface Transformers and Ray\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"[RAG](https:\\u002f\\u002fai.facebook.com\\u002fblog\\u002fretrieval-augmented-generation-streamlining-the-creation-of-intel...\"],[\"The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https:\\u002f\\u002fpytorch.org...\"],[\"![alt_text](assets\\u002f12_ray_rag\\u002fray_arch_updated.png \\\"image_tooltip\\\")\\n_Document retrieval with the Ray...\"],[\"_A performance comparison of different retrieval implementations. For each document retrieval implem...\"],[\"# A sample finetuning run, you need to specify data_dir, output_dir and model_name_or_path\\n# run .\\u002fe...\"],[\"Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts ...\"],[\"--\\ntitle: Introducing Pull Requests and Discussions 🥳\\nthumbnail: \\u002fblog\\u002fassets\\u002f76_community_update\\u002fth...\"],[\"## Discussions\\n\\n![Discussions on the Hugging Face Hub](assets\\u002f76_community_update\\u002fnew-discussion.png...\"],[\"--\\ntitle: \\\"Introducing Agents.js: Give tools to your LLMs using JavaScript\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"The messages returned by the agent are objects with the following shape:\\n\\n```ts\\nexport interface Upd...\"],[\"const agent = new HfAgent(HF_ACCESS_TOKEN, llmOpenAI);\\n```\\n\\n## Custom Tools 🛠️\\n\\nAgents.js was design...\"],[\"If you have the following html:\\n\\n```html\\n\\u003cinput id=\\\"fileItem\\\" type=\\\"file\\\" \\u002f\\u003e\\n```\\n\\nThen you can do:\\n\\n...\"],[\"--\\ntitle: \\\"Using Machine Learning to Aid Survivors and Race through Time\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fu...\"],[\"![organization](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdisast...\"],[\"![NER](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdisaster-assets...\"],[\"![backend_pipeline](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdi...\"],[\"![intent_model](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdisast...\"],[\"These models are currently being used in production to create the points in the heat map below so th...\"],[\"![input_satellite](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdis...\"],[\"![output_satellite](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdi...\"],[\"--\\ntitle: \\\"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"Another factor to consider is the level of parallelism in the model and the inference task. GPUs are...\"],[\"sudo apt-get install python3-pip -y\\npip install pip --upgrade\\nexport PATH=\\u002fhome\\u002fubuntu\\u002f.local\\u002fbin:$P...\"],[\"```\\nsentence_short = \\\"This is a really nice pair of shoes, I am completely satisfied with my purchas...\"],[\"On the c6i (Ice Lake) instance, we only use a vanilla Transformers pipeline. \\n\\n```\\nfrom transformers...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"assets\\u002f129_intel_sapphire_rapids_inference\\u002f01.png\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\nAs you can see in the ...\"],[\"--\\ntitle: Getting Started with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f109_inferenc...\"],[\"Starting from my [model page](https:\\u002f\\u002fhuggingface.co\\u002fjuliensimon\\u002fautotrain-food101-1471154053), I cl...\"],[\"After a few minutes, the endpoint is up and running, and its URL is visible.\\n\\n\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"asse...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"assets\\u002f109_inference_endpoints\\u002fendpoints06.png\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\nFor additional details, I...\"],[\"After a few minutes, the Inference Endpoints user interface displays the name of the VPC service nam...\"],[\"This is all there is to it. Once I'm done testing, I delete the endpoints that I've created to avoid...\"],[\"--\\ntitle: \\\"Non-engineers guide: Train a LLaMA 2 chatbot\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_ml_director_insi...\"],[\"## Introduction to Spaces\\n\\nSpaces from Hugging Face is a service that provides easy to use GUI for b...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"2.2 Choose the LLM you want to train from the “Model Choice” field, you can select a model from the ...\"],[\"2.5 Optional: You can upload “Validation Data” to test your newly trained model against, but this is...\"],[\"### Step 3: Create a new ChatUI Space using your model\\n\\n3.1 Follow the same process of setting up a ...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"--\\ntitle: \\\"Ethical Guidelines for developing the Diffusers library\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fethics-...\"],[\"# Ethical guidelines\\n\\n* **Transparency**: we are committed to being transparent in managing PRs, exp...\"],[\"* **Encouraging safety in deployment**\\n    * **[Safe Stable Diffusion](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fd...\"],[\"--\\ntitle: \\\"Introducing BERTopic Integration with the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f145_b...\"],[\"## What is BERTopic?\\n\\nBERTopic is a state-of-the-art Python library that simplifies the topic modell...\"],[\"## BERTopic Model Management with the Hugging Face Hub\\n\\nWith the latest integration, BERTopic users ...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eClick here for an overview of all topics.\\u003c\\u002fsummary\\u003e\\n  \\n  | Topic ID | Topic Key...\"],[\"| 13 | parsing - parser - dependency - treebank - parsers | 370 | 13_parsing_parser_dependency_treeb...\"],[\"| 28 | sarcasm - humor - sarcastic - detection - humorous | 157 | 28_sarcasm_humor_sarcastic_detecti...\"],[\"| 43 | discourse - discourse relation - discourse relations - rst - discourse parsing | 117 | 43_dis...\"],[\"| 58 | agreement - syntactic - verb - grammatical - subject verb | 85 | 58_agreement_syntactic_verb_...\"],[\"| 74 | biased - biases - spurious - nlp - debiasing | 57 | 74_biased_biases_spurious_nlp | \\n| 75 | v...\"],[\"| 90 | emoji - emojis - sentiment - message - anonymous | 35 | 90_emoji_emojis_sentiment_message | \\n...\"],[\"Due to the improved saving procedure, training on large datasets generates small model sizes. In the...\"],[\"To illustrate some of the power of BERTopic let's look at an example of how it can be used to monito...\"],[\"![Words associated with top 8 topics](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"dataset = load_dataset(\\\"databricks\\u002fdatabricks-dolly-15k\\\")\\ndolly_docs = dataset['train']['response']\\n...\"],[\"Some examples of BERTopic models already on the hub:\\n- [MaartenGr\\u002fBERTopic_ArXiv](https:\\u002f\\u002fhuggingfac...\"],[\"--\\ntitle: \\\"OpenRAIL: Towards open and responsible AI licensing frameworks\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"Most current model developers seem to think so, as the majority of openly released models have an op...\"],[\"Same concerns are rising in commercial and government ML licensing practices. In the words of [Bowe ...\"],[\"## **A change of licensing paradigm: OpenRAIL**\\n\\nThe OpenRAIL [approach](https:\\u002f\\u002fwww.licenses.ai\\u002fblo...\"],[\"And even before thinking about enforcement, use-based restriction clauses might act as a deterrent f...\"],[\"## **OpenRAIL could be for good machine learning what open software licensing is to code**\\n\\nThree ex...\"],[\"The licenses are BigScience's reaction to 2 partially addressed challenges in the licensing space: (...\"],[\"Open licensing is one of the cornerstones of AI innovation. Licenses as social and legal institution...\"],[\"--\\ntitle: Using LoRA for Efficient Stable Diffusion Fine-Tuning\\nthumbnail: \\u002fblog\\u002fassets\\u002flora\\u002fthumbna...\"],[\"![Latent Diffusion Architecture](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"- Training is much faster, as already discussed.\\n- Compute requirements are lower. We could create a...\"],[\"Diffusers now provides a [LoRA fine-tuning script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmai...\"],[\"![Sample outputs from Sayak's LoRA model](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"After we determine the base model we used to fine-tune with LoRA, we load a normal Stable Diffusion ...\"],[\"For a quick, cheap and easy way to train your Dreambooth models with LoRA, please [check this Space]...\"],[\"--\\ntitle: \\\"Graph Classification with Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f125_intro-to-graphml\\u002fthu...\"],[\"## Requirements\\nTo follow this tutorial, you need to have installed `datasets` and `transformers` (v...\"],[\"# Plot\\nnx.draw(G)\\n```\\n\\n### Format\\nOn the Hub, graph datasets are mostly stored as lists of graphs (u...\"],[\"### Preprocessing\\nGraph transformer frameworks usually apply specific preprocessing to their dataset...\"],[\"It is also possible to create a new randomly initialized model to train from scratch, either followi...\"],[\"```python\\ntrain_results = trainer.train()\\ntrainer.push_to_hub()\\n```\\nWhen the model is trained, it ca...\"],[\"--\\ntitle: Fine-Tune a Semantic Segmentation Model with a Custom Dataset\\nthumbnail: \\u002fblog\\u002fassets\\u002f56_f...\"],[\"Because semantic segmentation is a type of classification, the network architectures used for image ...\"],[\"```bash\\npip install -q transformers datasets evaluate segments-ai\\napt-get install git-lfs\\ngit lfs in...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt...\"],[\"\\u003cfigure class=\\\"image table text-center m-0\\\"\\u003e\\n    \\u003cvideo \\n        alt=\\\"Labeling a sidewalk image on S...\"],[\"```python\\nfrom segments.huggingface import release2dataset\\n\\nrelease = segments_client.get_release(da...\"],[\"The SegFormer model we're going to fine-tune later expects specific names for the features. For conv...\"],[\"We'll extract the number of labels and the human-readable ids, so we can configure the segmentation ...\"],[\"def val_transforms(example_batch):\\n    images = [x for x in example_batch['pixel_values']]\\n    label...\"],[\"First, we'll set up the [`TrainingArguments`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain_classes\\u002f...\"],[\"```python\\nimport torch\\nfrom torch import nn\\nimport evaluate\\n\\nmetric = evaluate.load(\\\"mean_iou\\\")\\n\\ndef...\"],[\"```python\\ntrainer.train()\\n```\\n\\nWhen we're done with training, we can push our fine-tuned model and t...\"],[\"We'll first load the model from the Hub using `SegformerForSemanticSegmentation.from_pretrained()`.\\n...\"],[\"The result might not be perfect yet, but we can always expand our dataset to make the model more rob...\"],[\"--\\ntitle: \\\"Efficient Controllable Generation for SDXL with T2I-Adapters\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002ft2i...\"],[\"| **Model Type** | **Model Parameters** | **Storage (fp16)** |\\n| --- | --- | --- |\\n| [ControlNet-SDX...\"],[\"Most of the T2I-Adapter models we mention in this blog post were trained on 3M high-resolution image...\"],[\"# load adapter\\nadapter = T2IAdapter.from_pretrained(\\n    \\\"TencentARC\\u002ft2i-adapter-lineart-sdxl-1.0\\\", ...\"],[\"There are two important arguments to understand that help you control the amount of conditioning.\\n\\n1...\"],[\"### Lineart Guided\\n\\n![Lineart guided more results](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocum...\"],[\"--\\ntitle: \\\"Introduction to Graph Machine Learning\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f125_intro-to-graphml\\u002fthu...\"],[\"If you want to use your data, you must first consider its best characterisation (homogeneous\\u002fheterog...\"],[\"Working on these tasks can be done in two ways. \\n\\nWhen you want to predict the evolution of a specif...\"],[\"But what does this mean? If you have a sentence and shuffle its words, you create a new sentence. If...\"],[\"## Graph representations through ML\\n\\nThe usual process to work on graphs with machine learning is fi...\"],[\"The node **centrality** measures the node importance in the graph. It can be computed recursively by...\"],[\"### Walk-based approaches\\n\\n[**Walk-based approaches**](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fRandom_walk) us...\"],[\"A GNN is made of successive layers. A GNN layer represents a node as the combination (**aggregation*...\"],[\"### GNN shape and the over-smoothing problem\\n\\nAt each new layer, the node representation includes mo...\"],[\"Here are some interesting methods which got state-of-the-art results or close on one of the hardest ...\"],[\"The most recent approach is [*Pure Transformers are Powerful Graph Learners*](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f...\"],[\"Nice libraries to work on graphs are [PyGeometric](https:\\u002f\\u002fpytorch-geometric.readthedocs.io\\u002fen\\u002flates...\"],[\"### External images attribution\\nEmojis in the thumbnail come from Openmoji (CC-BY-SA 4.0), the Graph...\"],[\"--\\ntitle: \\\"Transformer-based Encoder-Decoder Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f05_encoder_decoder\\u002fthum...\"],[\"Transformer-based encoder-decoder models are the result of years of\\nresearch on _representation lear...\"],[\"$$\\\\mathbf{X}_{1:n} = \\\\{\\\\mathbf{x}_1, \\\\ldots, \\\\mathbf{x}_n\\\\}.$$\\n\\nConsequently, sequence-to-sequence p...\"],[\"$$ f_{\\\\theta_{enc}}: \\\\mathbf{X}_{1:n} \\\\to \\\\mathbf{c}. $$\\n\\nThen, the decoder\\\\'s hidden state is initi...\"],[\"$$ p(\\\\mathbf{y}_i | \\\\mathbf{l}_i) = \\\\textbf{Softmax}(\\\\mathbf{l}_i), \\\\text{ with } \\\\mathbf{l}_i = f_{...\"],[\"An important feature of RNN-based encoder-decoder models is the\\ndefinition of *special* vectors, suc...\"],[\"The unfolded RNN encoder is colored in green and the unfolded RNN\\ndecoder is colored in red.\\n\\nThe En...\"],[\"To generate the first target vector, the decoder is fed the \\\\\\\\(\\\\text{BOS}\\\\\\\\)\\nvector, illustrated as ...\"],[\"During inference, efficient decoding methods can auto-regressively\\ngenerate the target sequence \\\\\\\\(\\\\...\"],[\"\\\\\\\\({}^3\\\\\\\\) At the first step, the hidden state is initialized as a zero\\nvector and fed to the RNN to...\"],[\"\\\\\\\\({}^6\\\\\\\\) [Sutskever et al. (2014)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1409.3215)\\nreverses the order of the inpu...\"],[\"Similar to RNN-based encoder-decoder models, the transformer-based\\nencoder-decoder models define a c...\"],[\"$$\\np_{\\\\theta_{dec}}(\\\\mathbf{Y}_{1:n} | \\\\mathbf{\\\\overline{X}}_{1:n}) = \\\\prod_{i=1}^{n} p_{\\\\theta_{\\\\te...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder...\"],[\"Next, the first target vector \\\\\\\\(\\\\mathbf{y}_1\\\\\\\\) = \\\\\\\\(\\\\text{Ich}\\\\\\\\) is sampled\\nfrom the distribution...\"],[\"tokenizer = MarianTokenizer.from_pretrained(\\\"Helsinki-NLP\\u002fopus-mt-en-de\\\")\\nmodel = MarianMTModel.from...\"],[\"Great, now that we have gotten a general overview of how\\n*transformer-based* encoder-decoder models ...\"],[\"$$ f_{\\\\theta_{\\\\text{enc}}}: \\\\mathbf{X}_{1:n} \\\\to \\\\mathbf{\\\\overline{X}}_{1:n}. $$\\n\\nTaking a closer lo...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder...\"],[\"$$ \\\\mathbf{q}_i = \\\\mathbf{W}_q \\\\mathbf{x'}_i,$$\\n$$ \\\\mathbf{v}_i = \\\\mathbf{W}_v \\\\mathbf{x'}_i,$$\\n$$ \\\\...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder...\"],[\"To further understand the implications of the bi-directional\\nself-attention layer, let\\\\'s assume the...\"],[\"$$\\\\mathbf{X''}_{1:n} = \\\\mathbf{V}_{1:n} \\\\text{Softmax}(\\\\mathbf{Q}_{1:n}^\\\\intercal \\\\mathbf{K}_{1:n}) ...\"],[\"\\\\\\\\({}^2\\\\\\\\) However, the EOS input vector does not have to be appended to the\\ninput sequence, but has...\"],[\"_Outputs:_\\n```\\n    Length of input embeddings 7. Length of encoder_hidden_states 7\\n    Is encoding f...\"],[\"## **Decoder**\\n\\nAs mentioned in the *Encoder-Decoder* section, the *transformer-based*\\ndecoder defin...\"],[\"$$p_{\\\\theta_{dec}}(\\\\mathbf{y}_i | \\\\mathbf{Y}_{0: i-1}, \\\\mathbf{\\\\overline{X}}_{1:n}), \\\\forall i \\\\in \\\\...\"],[\"$$ p_{\\\\theta_{dec}}(\\\\mathbf{Y}_{1:m} | \\\\mathbf{\\\\overline{X}}_{1:n}) = \\\\prod_{i=1}^{m} p_{\\\\theta_{dec...\"],[\"Applying a softmax operation on each\\n\\\\\\\\(\\\\mathbf{l}_1, \\\\mathbf{l}_2, \\\\ldots, \\\\mathbf{l}_5\\\\\\\\) can thus...\"],[\"As in bi-directional self-attention, in uni-directional self-attention,\\nthe query vectors \\\\\\\\(\\\\mathbf...\"],[\"![](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder_decoder\\u002fcaus...\"],[\"This is obviously disadvantageous as the transformer-based decoder would\\nnever learn to predict the ...\"],[\"$$\\n\\\\mathbf{y'''}_i = \\\\mathbf{V}_{1:n} \\\\textbf{Softmax}(\\\\mathbf{K}_{1: n}^\\\\intercal \\\\mathbf{q}_i) + \\\\...\"],[\"So intuitively, what happens here exactly? Each output vector\\n\\\\\\\\(\\\\mathbf{y'''}_i\\\\\\\\) is a weighted su...\"],[\"Cool! Now we can see how this architecture nicely conditions each output\\nvector \\\\\\\\(\\\\mathbf{y'''}_i\\\\\\\\...\"],[\"To verify our theoretical understanding, let\\\\'s continue our code\\nexample from the encoder section a...\"],[\"# pass decoder input ids and encoded input vectors to decoder\\ndecoder_output_vectors = model.base_mo...\"],[\"_Output:_\\n\\n```\\n    Shape of decoder input vectors torch.Size([1, 5, 512]). Shape of decoder logits t...\"],[\"On a final side-note, _auto-regressive_ models, such as GPT2, have the\\nsame architecture as _transfo...\"],[\"# create BOS token\\ndecoder_input_ids = tokenizer(\\\"\\u003cpad\\u003e\\\", add_special_tokens=False, return_tensors=\\\"...\"],[\"# This can be written in a loop as well.\\n```\\n\\n_Outputs:_\\n\\n```\\n    Generated so far: Ich will ein\\n```...\"],[\"--\\ntitle: Block Sparse Matrices for Smaller and Faster Language Models\\nthumbnail: \\u002fblog\\u002fassets\\u002f04_py...\"],[\"```python\\n# from torch.nn import Linear\\nfrom pytorch_block_sparse import BlockSparseLinear\\n\\n...\\n\\n# s...\"],[\"But the more important point is that the performance gain of using sparse matrices grows with the sp...\"],[\"--\\ntitle: \\\"Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)\\\"\\nthumbnail: \\u002fb...\"],[\"Firstly, we will provide empirical evidence that **Transformers are indeed Effective for Time Series...\"],[\"|      Dataset      | Autoformer (uni.) MASE | DLinear  MASE |\\n|:-----------------:|:---------------...\"],[\"### Decomposition Layer\\nDecomposition has long been a popular method in time series analysis, but it...\"],[\"Autoformer incorporates a decomposition block as an inner operation of the model, as presented in th...\"],[\"# calculate the trend and seasonal part of the series\\n        x_trend = self.avg(x_padded.permute(0,...\"],[\"In theory, given a time lag \\\\\\\\(\\\\tau\\\\\\\\), _autocorrelation_ for a single discrete variable \\\\\\\\(y\\\\\\\\) is ...\"],[\"Quite simple! 😎 Please be aware that this is only a partial implementation of `autocorrelation(Q,K)`...\"],[\"It can be summarized with the following equations:\\n\\n$$\\n\\\\tau_1, \\\\tau_2, ... \\\\tau_k = \\\\textrm{arg Top-...\"],[\"# apply softmax on the channel dim\\n    top_k_autocorrelations = torch.softmax(top_k_autocorrelations...\"],[\"In the probabilistic setting one can project the context length arrays to  `prediction-length * hidd...\"],[\"## Load Dataset\\n\\nLet's first install the necessary libraries:\\n\\n```python\\n!pip install -q transformer...\"],[\"The transformations below are annotated with comments to explain what they do. At a high level, we w...\"],[\"return Chain(\\n        # step 1: remove static\\u002fdynamic fields if not specified\\n        [RemoveFields(...\"],[\"output_field=FieldName.FEAT_TIME,\\n                time_features=time_features_from_frequency_str(fre...\"],[\"## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitter` w...\"],[\"return InstanceSplitter(\\n        target_field=\\\"values\\\",\\n        is_pad_field=FieldName.IS_PAD,\\n     ...\"],[\"# we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \\\"train\\\"...\"],[\"return as_stacked_batches(\\n        testing_instances,\\n        batch_size=batch_size,\\n        output_...\"],[\"test_dataloader = create_backtest_dataloader(\\n    config=config,\\n    freq=freq,\\n    data=test_datase...\"],[\"```python\\nimport numpy as np\\n\\nforecasts = np.vstack(forecasts_)\\nprint(forecasts.shape)\\n\\n\\u003e\\u003e\\u003e (6034, 1...\"],[\"ax.plot(\\n        index[-5*prediction_length:], \\n        test_ds[ts_index][\\\"target\\\"][-5*prediction_le...\"],[\"And evaluate it on the test set:\\n\\n```python\\nfrom gluonts.evaluation import make_evaluation_predictio...\"],[\"## Conclusion\\n\\nHow do Transformer-based models compare against the above linear baseline? The test s...\"],[\"To summarize, Transformers are definitely far from being outdated when it comes to time-series forca...\"],[\"--\\ntitle: \\\"Image search with 🤗 datasets\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f54_image_search_datasets\\u002fspaces_ima...\"],[\"To start, let's take a look at the image feature. We can use the wonderful [rich](https:\\u002f\\u002frich.readt...\"],[\"\\u003cpre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e                               ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e│\\u003c\\u002fspan\\u003e   \\u003cspan style=\\\"color: #808000;...\"],[\"We can see there a few different ways in which we can pass in our images. We'll come back to this in...\"],[\"There have also been projects to tag the dataset [using machine learning](https:\\u002f\\u002fblogs.bl.uk\\u002fdigita...\"],[\"```python\\ndataset = dataset[\\\"train\\\"]\\ndataset[0]\\n```\\n```python\\n{'image': \\u003cPIL.JpegImagePlugin.JpegIma...\"],[\"``` python\\ndataset[10]['image']\\n```\\n\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_search_datasets\\u002fdataset_image.jpg\\\" a...\"],[\"``` python\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()\\n```\\n\\n\\n``` python\\ndataset.pus...\"],[\"We can download the model using the `SentenceTransformer` class.\\n\\n``` python\\nfrom sentence_transform...\"],[\"## Image search\\n\\n\\u003e **Note** that these examples were generated from the full version of the dataset ...\"],[\"Some of these results look fairly close to our input prompt. We can wrap\\nthis in a function so we ca...\"],[\"## Creating a Hugging Face Space? 🤷🏼 \\n\\nOne obvious next step for this kind of project is to create a...\"],[\"However, looking at the out-of-scope use cases in the model card:\\n\\n\\u003e ### Out-of-Scope Use Cases\\n\\u003e\\n\\u003e ...\"],[\"--\\ntitle: \\\"Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model\\\"\\nthumb...\"],[\"The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is t...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"## Ethical evaluation\\n\\nAt the outset of this project, through a set of discussions, we developed an ...\"],[\"## Getting Started with IDEFICS\\n\\nIDEFICS models are available on the Hugging Face Hub and supported ...\"],[\"--\\ntitle: \\\"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"### NLP\\n\\n[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fgpt2-medium-wikitext-103) (Generative Pre-trained ...\"],[\"[BART](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fbart-base-ipu) is a transformer encoder-encoder (seq2seq) mo...\"],[\"### Speech\\n\\n[HuBERT](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fhubert-base-ipu) (Hidden-Unit BERT) is a self-...\"],[\"Optimizing their performance in the real world requires considerable time, effort and skills that ar...\"],[\"Software also plays a vital role in unlocking the IPU’s capabilities, so naturally Optimum offers a ...\"],[\"--\\ntitle: \\\"Showcase Your Projects in Spaces using Gradio\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f28_gradio-spaces\\u002ft...\"],[\"interface.launch()\\n```\\n\\nYou can play with the Story Generation model [here](https:\\u002f\\u002fhuggingface.co\\u002fs...\"],[\"```python\\nimport gradio as gr\\nfrom gradio.mix import Series\\n\\ndescription = \\\"Generate your own D&D st...\"],[\"Some Notes on Pros of Open Science and Open Source\\n- **Pooling Resources**: Building off of one anot...\"],[\"# Cons of Closed Source\\n- **Centralization** of power.\\n- **Opacity** of subtle bias\\u002fharm issues.\\n- H...\"],[\"--\\ntitle: \\\"We are hiring interns!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002finterns-2023\\u002fthumbnail.png\\nauthors:\\n- use...\"],[\"The following Science team positions are available:\\n\\n* [Embodied AI Internship](https:\\u002f\\u002fapply.workab...\"],[\"--\\ntitle: \\\"Announcing the Open Source AI Game Jam 🎮\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f145_gamejam\\u002fthumbnail.p...\"],[\"Claim Your Free Spot in the Game Jam 👉 https:\\u002f\\u002fitch.io\\u002fjam\\u002fopen-source-ai-game-jam\\n\\n\\u003ch2\\u003eWho Can Part...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #3: Ethical Openness at Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fass...\"],[\"As hosts, we recognize the responsibility that comes with potentially amplifying harm to our users a...\"],[\"- Rigorous work pays special attention to developing with best practices in mind. In ML, this can me...\"],[\"We engage directly with contributors and have addressed pressing issues. To bring this to the next l...\"],[\"**How to use the flagging function:**\\nClick on the flag icon on any Model, Dataset, Space, or Discus...\"],[\"Should a specific model be flagged as high risk by our community, we consider:\\n- Downgrading the ML ...\"],[\"## Are you working on safeguards? Share them on Hugging Face Hub!\\n\\nThe most important part of Huggin...\"],[\"--\\ntitle: \\\"Deep Learning over the Internet: Training Language Models Collaboratively\\\"\\nthumbnail: \\u002fbl...\"],[\"Unfortunately, we use these pretrained models not only because it's convenient. The hardware resourc...\"],[\"As a solution to this problem, we propose a new training algorithm, called Distributed Deep Learning...\"],[\"Let's consider a couple of potential failure cases that we might encounter throughout a collaborativ...\"],[\"Because All-Reduce is decentralized, it seems like a good choice; however, we still need to take the...\"],[\"## sahajBERT\\n\\nAs always, having a well-designed algorithmic framework doesn't mean that it will work...\"],[\"### Tokenizer\\n\\nThe first brick of our model is called a _tokenizer_ and takes care of transforming r...\"],[\"1. **Normalization:** includes all preprocessing operations on raw text data. This was the step at w...\"],[\"4. **Post-processing:** After tokenization, we might want to add several special tokens required by ...\"],[\"\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp ali...\"],[\"The dataset streaming mode is available from version v1.9 of the 🤗 datasets library, so you can use ...\"],[\"In the following figure, you can see the activity of each volunteer. Over the experiment, the volunt...\"],[\"### Evaluation\\n\\nTo evaluate the performance of sahajBERT, we finetuned it on two downstream tasks in...\"],[\"| Model       | NER F1 (mean ± std) | NCC Accuracy (mean ± std)           |\\n|:-------------:|:------...\"],[\"# Initialize model\\nmodel = AlbertForSequenceClassification.from_pretrained(\\\"neuropark\\u002fsahajBERT-NCC\\\"...\"],[\"In addition, we would like to thank Stas Bekman, Dmitry Abulkhanov, Roman Zhytar, Alexander Ploshkin...\"],[\"--\\ntitle: \\\"Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fbl...\"],[\"## Benchmarks\\n\\nWithout any further delay let's show some numbers.\\n\\nFor the sake of consistency, unle...\"],[\"Here is the throughput in msecs on 8x80GB GPUs:\\n\\n| project      \\\\ bs |      1 |     8 |    16 |    3...\"],[\"Now let's look at the power of quantized int8-based models provided by Deepspeed-Inference and BitsA...\"],[\"It is also very flexible since the same code can run on any given setup. Accelerate will use all ava...\"],[\"### Setup\\n\\n```\\npip install deepspeed\\u003e=0.7.3\\n```\\n\\n### Run\\n\\n1. the fastest approach is to use a TP-pre...\"],[\"2. DeepSpeed-Inference also uses custom CUDA kernels to avoid allocating too much memory and doing t...\"],[\"CPU-Offload (1x GPUs):\\n```\\ndeepspeed --num_gpus 1 bloom-inference-scripts\\u002fbloom-ds-zero-inference.py...\"],[\"--\\ntitle: \\\"Summer at Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f27_summer_at_huggingface\\u002fsummer_intro.gif...\"],[\"- Create recipes with the help of [Chef Transformer](\\u002fspaces\\u002fflax-community\\u002fchef-transformer)\\n- Tran...\"],[\"Check out [this repo](https:\\u002f\\u002fhuggingface.co\\u002fnateraw\\u002fvit-base-beans-demo) as an example, paying clos...\"],[\"![Object Detection Widget](assets\\u002f27_summer_at_huggingface\\u002fobject-detection.png)\\n\\n\\n### More Features...\"],[\"We're really excited to share the work of the 3 winning teams!\\n\\n1. [Dall-e mini](https:\\u002f\\u002fhuggingface...\"],[\"## Bonus\\n\\nOn top of everything we just shared, our team has been doing lots of other things. Here ar...\"],[\"You can now easily publish your model to the Hub, including automatically authored model cards, eval...\"],[\"![DETR image](assets\\u002f27_summer_at_huggingface\\u002fdetr.png)\\n\\n- [ByT5](https:\\u002f\\u002fhuggingface.co\\u002ftransformer...\"],[\"![LayoutLM object detection](assets\\u002f27_summer_at_huggingface\\u002flayout.png)\\n\\n- [BEiT](https:\\u002f\\u002fhuggingfa...\"],[\"![Untitled](assets\\u002f27_summer_at_huggingface\\u002fstreaming.png)\\n\\nWhat are the new datasets highlights? Mi...\"],[\"But that's not all! You can now find over 100 Adapter Transformers in the Hub and try out Speechbrai...\"],[\"### **NEW: Inference on SageMaker**\\n\\nWe launched a [new integration with AWS](https:\\u002f\\u002fhuggingface.co...\"],[\"**Hugging Face** + **Zapier Demo**\\n\\n20,000+ Machine Learning models connected to 3,000+ apps? 🤯  By ...\"],[\"## Research\\n\\nAt BigScience we held our first live event (since the kick off) in July BigScience Epis...\"],[\"In June our [paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2103.08493), How Many Data Points is a Prompt Worth?, got ...\"],[\"--\\ntitle: \\\"Model Cards\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f121_model-cards\\u002fthumbnail.png\\nauthors:\\n- user: Ezi\\n...\"],[\"4) A [User Study](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fmodel-cards-user-studies) on model card usage at H...\"],[\"## Our Work\\n\\nOur work presents a view of where model cards stand right now and where they could go i...\"],[\"As ML continues to be more intertwined with different domains, collaborative and open-source ML proc...\"],[\"* The Hugging Face ecosystem will continue to advance methods that streamline Model Card creation [t...\"],[\"--\\ntitle: \\\"Introducing RWKV - An RNN with the advantages of a transformer\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"### Transformer Architecture vs RNNs\\n\\nThe RNN architecture is one of the first widely used Neural Ne...\"],[\"In the transformer architecture, the input tokens are processed simultaneously in the self-attention...\"],[\"During inference, RNNs have some advantages in speed and memory efficiency. These advantages include...\"],[\"The major drawbacks of traditional RNN models and how RWKV is different:\\n\\n1. Traditional RNN models ...\"],[\"| ![rwkv_loss](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f14...\"],[\"pipe = pipeline(\\\"text-generation\\\", model=model_id)\\nprint(pipe(prompt, max_new_tokens=20))\\n\\u003e\\u003e\\u003e [{'gen...\"],[\"inputs = tokenizer(prompt, return_tensors=\\\"pt\\\").to(0)\\noutput = model.generate(inputs[\\\"input_ids\\\"], m...\"],[\"## Future work\\n\\n### Multi-lingual RWKV\\n\\nBo is currently working on a multilingual corpus to train RW...\"],[\"## Acknowledgements\\n\\nThe Hugging Face team would like to thank Bo and RWKV community for their time ...\"],[\"--\\ntitle: Stable Diffusion with 🧨 Diffusers\\nthumbnail: \\u002fblog\\u002fassets\\u002f98_stable_diffusion\\u002fthumbnail.pn...\"],[\"The license is designed to mitigate the potential harmful effects of such a powerful machine learnin...\"],[\"You can do so by loading the weights from the `fp16` branch and by telling `diffusers` to expect the...\"],[\"The result would look as follows\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_14_1.png)\\n\\n\\nYou...\"],[\"If you use a very large value the images might look good, but will be less diverse. \\nYou can learn a...\"],[\"Let's run an example:\\n\\n```python\\nprompt = \\\"a photograph of an astronaut riding a horse\\\"\\nimage = pipe...\"],[\"There are three main components in latent diffusion.\\n\\n1. An autoencoder (VAE).\\n2. A [U-Net](https:\\u002f\\u002f...\"],[\"**3. The Text-encoder**\\n\\nThe text-encoder is responsible for transforming the input prompt, *e.g.* \\\"...\"],[\"Next the U-Net iteratively *denoises* the random latent image representations while being conditione...\"],[\"For example, we'll show how to use Stable Diffusion with a different scheduler, namely [Katherine Cr...\"],[\"```python\\nfrom diffusers import LMSDiscreteScheduler\\n\\nscheduler = LMSDiscreteScheduler(beta_start=0....\"],[\"```python\\nmax_length = text_input.input_ids.shape[-1]\\nuncond_input = tokenizer(\\n    [\\\"\\\"] * batch_siz...\"],[\"latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\\n\\n    # predict the ...\"],[\"### Citation:\\n```\\n@article{patil2022stable,\\n  author = {Patil, Suraj and Cuenca, Pedro and Lambert, ...\"],[\"--\\ntitle: 'Deploy Hugging Face models easily with Amazon SageMaker'\\nthumbnail: \\u002fblog\\u002fassets\\u002f17_the_p...\"],[\"# create Hugging Face Model Class and deploy it as SageMaker Endpoint\\nhuggingface_model = HuggingFac...\"],[\"---\\n\\n\\n# **SageMaker Hugging Face Inference Toolkit ⚙️**\\n\\nIn addition to the Hugging Face Transformer...\"],[\"```python\\n# text-classification request body\\n{\\n\\t\\\"inputs\\\": \\\"Camera - You are awarded a SiPix Digital ...\"],[\"After that we can install the required dependencies.\\n\\n\\n```bash\\npip install \\\"sagemaker\\u003e=2.48.0\\\" --upg...\"],[\"# starting the train job with our uploaded datasets as input\\nhuggingface_estimator.fit(...)\\n\\n#######...\"],[\"```python\\n# delete endpoint\\npredictor.delete_endpoint()\\n```\\n\\n\\n\\n## **Deploy one of the 10,000+ Huggin...\"],[\"# request\\npredictor.predict(data)\\n```\\n\\nAfter we run our request we can delete the endpoint again wit...\"],[\"_Q: Why should I use the Hugging Face Deep Learning Containers?_\\n\\nA: The DLCs are fully tested, main...\"],[\"If you have questions which the Hugging Face community can help answer and\\u002for benefit from, please *...\"],[\"--\\ntitle: \\\"Introducing Prodigy-HF: a direct integration with Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"After installing the plugin you can call the `hf.train.ner` recipe from the command line to train a ...\"],[\"## More to come\\n\\nWe hope that this direct integration with the Hugging Face ecosystem enables many u...\"],[\"--\\ntitle: \\\"How to Install and Use the Hugging Face Unity API\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-gam...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\\n\\n#...\"],[\"--\\ntitle: \\\"Proximal Policy Optimization (PPO)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f93_deep_rl_ppo\\u002fthumbnail.png\\n...\"],[\"Doing this will ensure **that our policy update will not be too large and that the training is more ...\"],[\"Sounds exciting? Let's get started!\\n\\n- [The intuition behind PPO](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fdeep-r...\"],[\"For two reasons:\\n- We know empirically that smaller policy updates during training are **more likely...\"],[\"This new function **is designed to avoid destructive large weights updates** :\\n\\n\\u003cimg src=\\\"assets\\u002f93_...\"],[\"### The clipped Part of the Clipped Surrogate Objective function\\n\\n\\u003cimg src=\\\"assets\\u002f93_deep_rl_ppo\\u002fcl...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f93_deep_rl_ppo\\u002frecap.jpg\\\" alt...\"],[\"If, like in situation 3, the advantage estimate is positive (A\\u003e0), then **you want to increase the p...\"],[\"**You might wonder why, when the minimum is the clipped ratio, the gradient is 0.** When the ratio i...\"],[\"So, to be able to code it, we're going to use two resources:\\n- A tutorial made by [Costa Huang](http...\"],[\"---\\n\\nCongrats on finishing this chapter! There was a lot of information. And congrats on finishing t...\"],[\"--\\ntitle: \\\"Very Large Language Models and How to Evaluate Them\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f106_zero_sh...\"],[\"We’ve upgraded the AutoTrain infrastructure for this project so that large models can be evaluated f...\"],[\"## Case study: Zero-shot evaluation on the WinoBias task\\n\\nThe [WinoBias](https:\\u002f\\u002fgithub.com\\u002fuclanlp\\u002f...\"],[\"![Winobias](.\\u002fassets\\u002f106_zero_shot_eval_on_the_hub\\u002fwinobias.png)\\n\\n## Enabling better research tools ...\"],[\"## Send us feedback!\\n\\nAt Hugging Face, we’re excited to continue democratizing access to state-of-th...\"],[\"--\\ntitle: Training Stable Diffusion with Dreambooth using Diffusers\\nthumbnail: \\u002fblog\\u002fassets\\u002fsd_dream...\"],[\"_Note: a previous version of this post was published [as a W&B report](https:\\u002f\\u002fwandb.ai\\u002fpsuraj\\u002fdream...\"],[\"## Learning Rate Impact\\n\\nDreambooth overfits very quickly. To get good results, tune the learning ra...\"],[\"Low Learning Rate (`2e-6`)\\n![Cat Toy, Low Learning Rate](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface...\"],[\"### Summary of Initial Results\\n\\nTo get good results training Stable Diffusion with Dreambooth, it's ...\"],[\"No prior preservation, 1200 steps, lr=`2e-6`.\\n![Faces, prior preservation](https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"`DDIM`, Potato Head\\n![DDIM Potato](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002f...\"],[\"![Textual Inversion + Dreambooth](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fr...\"],[\"--\\ntitle: 'Faster Text Generation with TensorFlow and XLA'\\nthumbnail: \\u002fblog\\u002fassets\\u002f91_tf_xla_generat...\"],[\"Let's start with the basics. Text generation can be deterministic or stochastic, depending on the\\n`d...\"],[\"```python\\ngenerated = model.generate(\\n    **inputs, do_sample=True, seed=(42, 0), max_new_tokens=5\\n)...\"],[\"```python\\ngenerated = model.generate(**inputs, num_beams=2)\\nprint(\\\"Beam Search output:\\\", tokenizer.d...\"],[\"## TensorFlow and XLA\\n\\n[XLA](https:\\u002f\\u002fwww.tensorflow.org\\u002fxla), or Accelerated Linear Algebra, is a co...\"],[\"Fortunately, the TensorFlow team has users like us covered 🥳! Wrapping a function containing TensorF...\"],[\"In one line, you can create an XLA-accelerated function from the function above.\\n\\n```python\\nxla_most...\"],[\"# Slow: XLA compilation will kick in, as it is the first call\\nmax_plus_constant(tf.constant([0, 0, 0...\"],[\"# One line to create a XLA generation function\\nxla_generate = tf.function(model.generate, jit_compil...\"],[\"print(\\\"Calling XLA generation with tokenized_input_1_with_padding...\\\")\\nprint(\\\"(slow, first time runn...\"],[\"From a developer perspective, relying on XLA implies being aware of a few additional nuances. XLA sh...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"--\\ntitle: \\\"Perceiver IO: a scalable, fully-attentional model that works on any modality\\\"\\nthumbnail: ...\"],[\"In all of these domains, state-of-the-art results were improved dramatically, thanks to the combinat...\"],[\"## The Perceiver\\n\\nThe [Perceiver](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2103.03206) aims to solve this limitation by...\"],[\"Note that each of these are optional. A `preprocessor` is only required in case one hasn't already e...\"],[\"Let's start off by showing how the Perceiver is implemented to work on text.\\n\\n## Perceiver for text\\n...\"],[\"tokenizer = PerceiverTokenizer.from_pretrained(\\\"deepmind\\u002flanguage-perceiver\\\")\\n\\ntext = \\\"hello world\\\"\\n...\"],[\"``` python\\nfrom torch import nn\\n\\nself.latents = nn.Parameter(torch.randn(config.num_latents, config....\"],[\"Ok, so now one has final hidden states of shape (batch_size, 256, 1280). Great, but one actually wan...\"],[\"Great, isn't it? The Perceiver authors also show that it is straightforward to pre-train the Perceiv...\"],[\"``` python\\nfrom torch import nn\\nfrom transformers import PerceiverModel\\nfrom transformers.models.per...\"],[\"url = 'http:\\u002f\\u002fimages.cocodataset.org\\u002fval2017\\u002f000000039769.jpg'\\nimage = Image.open(requests.get(url, ...\"],[\"The authors use 512 latents for all image models, and set the dimensionality of the latents to 1024....\"],[\"## Perceiver for optical flow\\n\\nThe authors show that it's straightforward to make the Perceiver also...\"],[\"fourier_position_encoding_kwargs_preprocessor = dict(\\n            num_bands=64,\\n            max_reso...\"],[\"position_encoding_type=\\\"fourier\\\",\\n                fourier_position_encoding_kwargs=fourier_position_...\"],[\"The preprocessor (with the settings defined above) will first concatenate the frames along the chann...\"],[\"The video below shows the predicted flow on 2 examples. \\n\\n\\u003cp float=\\\"left\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002flh3.g...\"],[\"- The images - actually a sequence of frames - of shape (batch_size, 16, 3, 224, 224) are turned int...\"],[\"Next, there is `PerceiverMultimodalDecoder`, which will first create output queries for each modalit...\"],[\"Similarly to the preprocessor, `PerceiverMultimodalDecoder` pads the different modalities to the sam...\"],[\"So now one ends up with tensors containing the reconstruction of the image, audio and class label mo...\"],[\"## Other applications of the Perceiver\\n\\nNote that there are no limits on the applications of the Per...\"],[\"It is important to note that the models currently implemented (such as `PerceiverForImageClassificat...\"],[\"### Appendix\\n\\nThe implementation in HuggingFace Transformers is based on the original JAX\\u002fHaiku impl...\"],[\"--\\ntitle: \\\"Train a Sentence Embedding Model with 1B Training Pairs\\\"\\nauthors:\\n- user: asi\\n  guest: tr...\"],[\"![snippet](assets\\u002f32_1b_sentence_embeddings\\u002fmodel.png)\\n\\n### Multiple Negative Ranking Loss\\n\\nThe para...\"],[\"![snippet](assets\\u002f32_1b_sentence_embeddings\\u002fcontrastive_2.png)\\n\\nIn the loss equation, `sim` indicate...\"],[\"![snippet](assets\\u002f32_1b_sentence_embeddings\\u002fbatch-size.png)\\n\\n#### 2. Hard Negatives\\n\\nIn the same fig...\"],[\"## Conclusion\\n\\nYou can find all models and datasets we created during the challenge in our [HuggingF...\"],[\"The [Community week using JAX\\u002fFlax for NLP & CV](https:\\u002f\\u002fdiscuss.huggingface.co\\u002ft\\u002fopen-to-the-commun...\"],[\"--\\ntitle: \\\"Large-scale Near-deduplication Behind BigCode\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fdedup\\u002fthumbnail.pn...\"],[\"## From BigScience to BigCode\\n\\nAllow me to share a story first on how I jumped on this near-deduplic...\"],[\"| Dataset                              | Input Size                       | Output Size or Deduction...\"],[\"| CC100-XL[[8]](#8)                    |                                  | 0.01 GiB ~ 3324.45 GiB  ...\"],[\"This is the one for code datasets we created for BigCode as well. Model names are used when the data...\"],[\"The typical workflow of MinHash is as follows:\\n\\n1. Shingling (tokenization) and fingerprinting (MinH...\"],[\"| shingle             | permuted hashes                                             |\\n| ------------...\"],[\"```python\\ndef embed_func(\\n    content: str,\\n    idx: int,\\n    *,\\n    num_perm: int,\\n    ngram_size: ...\"],[\"After the fingerprint calculation, one particular document is mapped to one array of integer values....\"],[\"For each row in the `doc_ids` column, we can generate candidate pairs by pairing every two of them. ...\"],[\"**Option 1: Iterate the datasets the old-fashioned way and collect edges. Then use a graph library t...\"],[\"```python\\nedges = (\\n\\trecords.flatMap(\\n\\t\\tlambda x: generate_hash_values(\\n\\t\\t\\tcontent=x[1],\\n\\t\\t\\tidx=x[0]...\"],[\"![A violin chart showing unigram impact in different settings](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fcheng...\"],[\"\\u003ccenter\\u003eImage: CPU usage screenshot for the cluster during processing JSON dataset.\\u003c\\u002fcenter\\u003e\\n\\nThis i...\"],[\"In terms of data leakage and benchmark contamination, there is still much to explore. We had to retr...\"],[\"## Credits\\n\\nThe banner image contains emojis (hugging face, Santa, document, wizard, and wand) from ...\"],[\"- \\u003ca id=\\\"1\\\"\\u003e[1]\\u003c\\u002fa\\u003e : Nikhil Kandpal, Eric Wallace, Colin Raffel, [Deduplicating Training Data Mitig...\"],[\"- \\u003ca id=\\\"12\\\"\\u003e[12]\\u003c\\u002fa\\u003e : Yujia Li, David Choi, et al., [Competition-Level Code Generation with AlphaC...\"],[\"--\\ntitle: 'Getting Started With Embeddings'\\nthumbnail: \\u002fblog\\u002fassets\\u002f80_getting_started_with_embeddin...\"],[\"## What are embeddings for?\\n\\n\\u003e \\\"[...] once you understand this ML multitool (embedding), you'll be a...\"],[\"But first, we need to embed our dataset (other texts use the terms encode and embed interchangeably)...\"],[\"```py\\nimport requests\\n\\napi_url = f\\\"https:\\u002f\\u002fapi-inference.huggingface.co\\u002fpipeline\\u002ffeature-extraction\\u002f...\"],[\"```py\\ntexts = [\\\"How do I get a replacement Medicare card?\\\",\\n        \\\"What is the monthly premium for...\"],[\"## 2. Host embeddings for free on the Hugging Face Hub\\n\\n🤗 Datasets is a library for quickly accessin...\"],[\"![](assets\\u002f80_getting_started_with_embeddings\\u002fcreateDataset.png)\\n\\n* Go to the \\\"Files\\\" tab (screensho...\"],[\"```py\\nimport torch\\nfrom datasets import load_dataset\\n\\nfaqs_embeddings = load_dataset('namespace\\u002frepo...\"],[\"The values ​​in `corpus_id` allow us to index the list of `texts` we defined in the first section an...\"],[\"--\\ntitle: Getting Started with Transformers on Habana Gaudi\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_getting_start...\"],[\"Then, I search the Amazon Marketplace for Habana AMIs.\\n\\n\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"assets\\u002f61_getting_started_...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"assets\\u002f61_getting_started_habana\\u002fhabana06.png\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\nFinally, I launch the inst...\"],[\"Then, I move to the subdirectory containing the text classification example and install the required...\"],[\"Last but not least, I terminate the EC2 instance to avoid unnecessary charges. Looking at the [Savin...\"],[\"--\\ntitle: \\\"Personal Copilot: Train Your Own Coding Assistant\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f170_personal_...\"],[\"Our desired dataset is conceptually simple, we structured it like so:\\n\\n| | | |\\n|---|---|---|\\n| Repos...\"],[\"The final dataset is [available on the Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fsayakpaul\\u002fhf-codegen-v2)...\"],[\"## Finetuning your own Personal Co-Pilot \\n\\nIn this section, we show how to fine-tune the following m...\"],[\"\\u003e trainable params: 110,428,160 || all params: 15,627,884,544 || trainable%: 0.7066097761926236\\n\\n1. ...\"],[\"## Full Finetuning\\n\\nWe will look at how to do full fine-tuning of `bigcode\\u002fstarcoder` (15B params) o...\"],[\"The total training time was **9 Hours**. Taking the cost of $12.00 \\u002f hr based on [lambdalabs](https:...\"],[\"## Comparison\\n\\nThe plot below shows the eval loss, train loss and learning rate scheduler for QLoRA ...\"],[\"![qualitative_comparison_1](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"Therefore, we can observe that the generations from both the variants are as per expectations. Aweso...\"],[\"![code_completion](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblo...\"],[\"## Mix-and-Match LoRAs\\n\\nPEFT currently supports 3 ways of combining LoRA models, `linear`, `svd` and...\"],[\"##### Let us now consider the `code-completion` task.\\n\\nOn disabling adapters, we observe that the co...\"],[\"![mix_chat_hf](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fpe...\"],[\"![octocoder_chat_hf](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fb...\"],[\"![loss_plots](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fper...\"],[\"6. Change the endpoint of HF Code Completion extension in VS Code to point to the local server:\\n\\n![l...\"],[\"--\\ntitle: \\\"Hugging Face on PyTorch \\u002f XLA TPUs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f13_pytorch_xla\\u002fpytorch_xla_th...\"],[\"```python\\nimport torch\\nimport torch_xla\\nimport torch_xla.core.xla_model as xm\\n\\nt = torch.randn(2, 2,...\"],[\"### PyTorch \\u002f XLA Input Pipeline\\n\\nThere are two main parts to running a PyTorch \\u002f XLA model: (1) tra...\"],[\"if xm.is_master_ordinal():\\n                # Save configuration file\\n                model_to_save.c...\"],[\"This means that when you call `model(input)` forward pass, calculate your loss `loss.backward()`, an...\"],[\"This HLO graph then gets compiled into a TPU binary and subsequently executed on the TPU devices. Ho...\"],[\"```python\\n\\u003e\\u003e\\u003e import torch_xla.debug.metrics as met\\n\\u003e\\u003e\\u003e print(met.metrics_report())\\nMetric: CompileT...\"],[\"```bash\\nconda activate torch-xla-1.7\\nexport TPU_IP_ADDRESS=\\\"ENTER_YOUR_TPU_IP_ADDRESS\\\"  # ex. 10.0.0...\"],[\"| Name               | Dataset     | Hardware                  | Global Batch Size | Precision | Tra...\"],[\"--\\ntitle: Deploying TensorFlow Vision Models in Hugging Face with TF Serving\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"To get the complete working code shown throughout this post, refer to\\nthe Colab Notebook shown at th...\"],[\"# Model Surgery\\n\\nUsually, every ML model has certain preprocessing and postprocessing\\nsteps. The ViT...\"],[\"```py\\nCONCRETE_INPUT = \\\"pixel_values\\\" # Which is what we investigated via the SavedModel CLI.\\nSIZE =...\"],[\"@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\\n    def serving_fn(string_input):\\n ...\"],[\"After exporting, let's inspect the model signatures again:\\n\\n```bash\\nsaved_model_cli show --dir {MODE...\"],[\"And voila! Within minutes, you should be up and running with a deployed\\nmodel having two endpoints -...\"],[\"# Querying the gRPC Endpoint\\n\\nWhile REST is quite popular in the API world, many applications often\\n...\"],[\"You can also fetch the key-values of our interest from the above results like so:\\n\\n```py\\ngrpc_predic...\"],[\"--\\ntitle: \\\"Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU\\\" \\nthumbnail: assets\\u002f133_trl_peft\\u002fth...\"],[\"The choice of the base LLM is quite crucial here. At this time of writing, the “best” open-source LL...\"],[\"Fine-tuning a language model with RL follows roughly the protocol detailed below. This requires havi...\"],[\"Many techniques have been adopted to tackle these challenges at scale. The most familiar paradigms a...\"],[\"Therefore, we asked ourselves the following question: how far can we go with just data parallelism? ...\"],[\"In a nutshell, you can reduce the size of a full-precision model by 4 (thus, by 2 for half-precision...\"],[\"The library is still under extensive and active development, with many upcoming features to be annou...\"],[\"The second step is to load adapters inside the model and make these adapters trainable. This enables...\"],[\"Overall there were three key steps and training scripts:\\n\\n1. **[Script](https:\\u002f\\u002fgithub.com\\u002flvwerra\\u002ft...\"],[\"Finally, we could then fine-tune another low-rank adapter, on top of the frozen imdb-finetuned model...\"],[\"## References\\n\\n- parallelism paradigms: [https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fv4.17.0\\u002fen\\u002fparalle...\"],[\"--\\ntitle: \\\"Sentiment Analysis on Encrypted Data with Homomorphic Encryption\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"Now we can install all the required libraries for the this blog with the following command.\\n```\\npip ...\"],[\"Now we can split our dataset into training and test sets. We will use a seed for this code to ensure...\"],[\"Using the hidden representation for some text can be tricky at first, mainly because we could tackle...\"],[\"# Send the model to the device\\n   transformer_model = transformer_model.to(device)\\n   output_hidden_...\"],[\"# A gridsearch to find the best parameters\\nparameters = {\\n    \\\"n_bits\\\": [2, 3],\\n    \\\"max_depth\\\": [1]...\"],[\"```python\\nimport time\\n# Compile the model to get the FHE inference engine\\n# (this may take a few min...\"],[\"```python\\n# Let's save the model to be pushed to a server later\\nfrom concrete.ml.deployment import F...\"],[\"1. train a machine learning model to classify tweets, and\\n2. predict over encrypted data using this ...\"],[\"--\\ntitle: \\\"How to host a Unity game in a Space\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002funity-in-sp...\"],[\"## Step 4: Switch the Build Target to WebGL\\n\\nNavigate to `File \\u003e Build Settings` and switch the Buil...\"],[\"## Step 8: Build your Project\\n\\nReturn to the Build Settings window and click the \\\"Build\\\" button. Cho...\"],[\"--\\ntitle: \\\"Llama 2 on Amazon SageMaker a Benchmark\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fllama_sagemaker_benchma...\"],[\"We hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Be...\"],[\"### What is GPTQ?\\n\\nGPTQ is a post-training quantziation method to compress LLMs, like GPT. GPTQ comp...\"],[\"You can find the full data of the benchmark in the [Amazon SageMaker Benchmark: TGI 1.0.3 Llama 2](h...\"],[\"| Model       | Quantization | Instance       | concurrent requests | Latency (ms\\u002ftoken) median | Th...\"],[\"| Model       | Quantization | Instance        | concurrent requests | Latency (ms\\u002ftoken) median | T...\"],[\"| Model       | Quantization | Instance        | concurrent requests | Latency (ms\\u002ftoken) median | T...\"],[\"---\\n\\nThanks for reading! If you have any questions, feel free to contact me on [Twitter](https:\\u002f\\u002ftwi...\"],[\"--\\ntitle: \\\"Announcing Evaluation on the Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f82_eval_on_the_hub\\u002fthumbnail.pn...\"],[\"However, it is no exaggeration to say that modern AI is in an evaluation crisis. Proper evaluation t...\"],[\"**Finding the best model for your task**\\u003cbr\\u002f\\u003e\\nSuppose you know exactly what your task is and you wan...\"],[\"## DogFood - Distinguishing Dogs, Muffins and Fried Chicken\\n\\nSo what does it look like in practice? ...\"],[\"Clicking on the \\u003cem\\u003eAdvanced configuration\\u003c\\u002fem\\u003e button will show you the various settings to choose ...\"],[\"Once a job is submitted, the models will be automatically evaluated and a Hub pull request will be o...\"],[\"Benchmarks are saturating, meaning that machines outperform humans on certain test sets, almost fast...\"],[\"--\\ntitle: \\\"Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models\\\"\\nthumbnail: ...\"],[\"In [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks\\n(2020)](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f1...\"],[\"It is highly recommended (probably even necessary) to have read [this\\nblog\\npost](https:\\u002f\\u002fcolab.resea...\"],[\"The capability of pre-trained language models to effectively transfer\\n*task-agnostic* knowledge to *...\"],[\"$$ f_{\\\\theta_{\\\\text{BERT}}}: \\\\mathbf{X}_{1:n} \\\\to \\\\mathbf{\\\\overline{X}}_{1:n}. $$\\n\\nBERT\\\\'s contextua...\"],[\"*Encoder-only* models can only map an input sequence to an output\\nsequence of *a priori* known outpu...\"],[\"$$ p_{\\\\theta_{\\\\text{gpt2}}}(\\\\mathbf{y}_i | \\\\mathbf{Y}_{0:i-1}) = \\\\textbf{Softmax}(\\\\mathbf{l}_i) = \\\\t...\"],[\"GPT2 is therefore well-suited for *language generation*, but less so for\\n*conditional* generation. B...\"],[\"Now, we know that freely available checkpoints of large pre-trained\\n*stand-alone* encoder and decode...\"],[\"\\\\\\\\({}^2\\\\\\\\) *Fine-tuning* is defined as the *task-specific* training of a\\nmodel that has been initial...\"],[\"### **Recap Encoder-Decoder Model**\\n\\nFirst, let\\\\'s do a quick recap of the encoder-decoder architect...\"],[\"$$ p_{\\\\theta_{\\\\text{dec}}}(\\\\mathbf{y}_i | \\\\mathbf{Y}_{0: i -1}, \\\\mathbf{\\\\overline{X}}_{1:n}) = \\\\text...\"],[\"Before fine-tuning, the encoder therefore behaves exactly like a\\npre-trained BERT model. Assuming th...\"],[\"2.  Second, BERT\\\\'s *bi-directional* self-attention layers have to be\\n    changed to *uni-directiona...\"],[\"3.  Third, the decoder outputs a sequence of logit vectors\\n    \\\\\\\\(\\\\mathbf{L}_{1:m}\\\\\\\\) in order to de...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder...\"],[\"### **Encoder-Decoder Weight Sharing**\\n\\nIn [Raffel et al. (2020)](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f1910.10683.p...\"],[\"In the same way, we can warm-start an encoder-decoder model by sharing\\nthe encoder weights with the ...\"],[\"To be more precise, the publicly available pre-trained checkpoints of\\n**BERT**, **RoBERTa**, and **G...\"],[\"The model *Rnd2Rnd*, which is based on the BERT2BERT architecture,\\ncontains 221M weight parameters -...\"],[\"|Seq2Seq Task               |Datasets                                                               ...\"],[\"|Gigaword                   |[Napoles et al. (2012)](http:\\u002f\\u002fdx.doi.org\\u002f10.18653\\u002fv1\\u002fD15-1044)        ...\"],[\"Depending on the task, a slightly different training regime was used.\\n*E.g.* according to the size o...\"],[\"As an example, the sentence:\\n\\n*Street Rod is the first in a series of two games released for the PC\\n...\"],[\"Let\\\\'s see how the models perform on sentence fusion and -splitting.\\n\\n  |Model                  | 10...\"],[\"The first two columns show the performance of the encoder-decoder models\\non the DiscoFuse evaluation...\"],[\"### Machine Translation (WMT14)\\n\\nNext, the authors evaluated warm-started encoder-decoder models on ...\"],[\"Again, we observe a significant performance boost by warm-starting the\\nencoder-part, with *BERT2Rnd*...\"],[\"### Summarization (CNN\\u002fDailymail, BBC XSum, Gigaword)\\n\\nFinally, the encoder-decoder models were eval...\"],[\"The models are evaluated using the [Rouge\\nmetric](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002fN03-1020\\u002f), where...\"],[\"Furthermore, the shared encoder-decoder models are the best performing\\nmodels for summarization. *Ro...\"],[\"-   Next, we noticed that it is often beneficial to share encoder and\\n    decoder weights, especiall...\"],[\"For each of the above tasks, the most performant models were ported to\\n🤗Transformers and can be acce...\"],[\"\\\\\\\\({}^2\\\\\\\\) Model capacity is an informal definition of how good the model is\\nat modeling complex pat...\"],[\"We will need datasets and transformers to be installed.\\n\\n```python\\n!pip install datasets==1.0.2\\n!pip...\"],[\"```python\\nOUTPUT:\\n-------\\nArticle:...\"],[\"\\\"\\\"\\\"It's official: U.S. President Barack Obama wants lawmakers to weigh in on whether to use military...\"],[\". \\\"The aim of the game here, the mandate, is very clear -- and that is to ascertain whether chemical...\"],[\". Some global leaders have expressed support, but the British Parliament's vote against military act...\"],[\".S. military officials said they remained at the ready. 5 key assertions: U.S. intelligence report o...\"],[\". In the United States, scattered groups of anti-war protesters around the country took to the stree...\"],[\". British intelligence had put the number of people killed in the attack at more than 350. On Saturd...\"],[\"Summary:\\n\\\"\\\"\\\"Syrian official: Obama climbed to the top of the tree, \\\"doesn't know how to get down\\\"\\\\nO...\"],[\"The input data seems to consist of short news articles. Interestingly,\\nthe labels appear to be bulle...\"],[\"We can define the `.map()` function as follows.\\n\\n```python\\n# map article and summary len to dict as ...\"],[\"`bert-base-cased` is limited to 512 tokens, which means we would have to\\ncut possibly important info...\"],[\"batch[\\\"input_ids\\\"] = inputs.input_ids\\n  batch[\\\"attention_mask\\\"] = inputs.attention_mask\\n  batch[\\\"lab...\"],[\"So far, the data was manipulated using Python\\\\'s `List` format. Let\\\\'s\\nconvert the data to PyTorch T...\"],[\"```python\\nfrom transformers import EncoderDecoderModel\\n```\\n\\nIn contrast to other model classes in 🤗T...\"],[\"```python\\nOUTPUT:\\n-------\\n\\\"\\\"\\\"Some weights of the model checkpoint at bert-base-uncased were not used...\"],[\"Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased ...\"],[\".layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weig...\"],[\".self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.cr...\"],[\".layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert...\"],[\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and ...\"],[\"For once, we should take a good look at the warning here. We can see\\nthat two weights corresponding ...\"],[\"```python\\nOUTPUT:\\n-------\\n    EncoderDecoderModel(\\n      (encoder): BertModel(\\n        (embeddings):...\"],[\"(query): Linear(in_features=768, out_features=768, bias=True)\\n                  (key): Linear(in_fea...\"],[\"(query): Linear(in_features=768, out_features=768, bias=True)\\n                    (key): Linear(in_f...\"],[\")\\n              ),\\n\\t\\t\\t\\t\\t\\t\\t...,\\n              (11): BertLayer(\\n                (attention): BertAtten...\"],[\"(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n                  (dropout): Drop...\"],[\"We see that `bert2bert.encoder` is an instance of `BertModel` and that\\n`bert2bert.decoder` one of `B...\"],[\"```python\\nOUTPUT:\\n-------\\n    EncoderDecoderConfig {\\n      \\\"_name_or_path\\\": \\\"bert2bert\\\",\\n      \\\"arch...\"],[\"\\\"tokenizer_class\\\": null,\\n        \\\"top_k\\\": 50,\\n        \\\"top_p\\\": 1.0,\\n        \\\"torchscript\\\": false,\\n  ...\"],[\"\\\"temperature\\\": 1.0,\\n        \\\"tie_encoder_decoder\\\": false,\\n        \\\"tie_word_embeddings\\\": true,\\n     ...\"],[\"The config is similarly composed of an encoder config and a decoder\\nconfig both of which are instanc...\"],[\"```python\\n# free memory\\ndel shared_bert2bert\\n```\\n\\nWe have warm-started a `bert2bert` model, but we h...\"],[\"In addition, we need a couple of python packages to make the\\n`Seq2SeqTrainer` work.\\n\\n```python\\n!pip ...\"],[\"Also, we need to define a function to correctly compute the ROUGE score\\nduring validation. Since we ...\"],[\"Awesome, we should now be fully equipped to finetune a warm-started\\nencoder-decoder model. To check ...\"],[\"outputs = bert2bert.generate(input_ids, attention_mask=attention_mask)\\n\\n    output_str = tokenizer.b...\"],[\"--\\ntitle: \\\"Introducing Hugging Face for Education 🤗\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_education\\u002fthumbnail....\"],[\"Some examples of our existing efforts:\\n- we describe in a very accessible way [different uses of ML ...\"],[\"## 🤗 **Education for Beginners**\\n\\n🗣️ We want to lower the barrier to becoming a machine learning eng...\"],[\"## 🤗 **Education for Instructors**\\n\\n🗣️ We want to empower educators with tools and offer collaborati...\"],[\"1️⃣ [A Tour through the Hugging Face Hub](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002feducation-toolkit\\u002fblob\\u002fmain...\"],[\"--\\ntitle: \\\"How Hugging Face Accelerated Development of Witty Works Writing Assistant\\\"\\nthumbnail: \\u002fbl...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f78_ml_director_insights\\u002fwittyworks.png\\\"\\u003e\\u003cbr\\u003e\\n    \\u003cem\\u003eE...\"],[\"### Solutions provided by the [Hugging Face Experts](https:\\u002f\\u002fhuggingface.co\\u002fsupport?utm_source=blog-...\"],[\"- #### **Get guidance on selecting the right ML library.**\\nThe Hugging Face Expert suggested using t...\"],[\"After a first test on Google Colab, the Hugging Face experts guided Witty Works on deploying the mod...\"],[\"--\\ntitle: Inference for PROs\\nthumbnail: \\u002fblog\\u002fassets\\u002finference_pro\\u002fthumbnail.png\\nauthors:\\n  - user: ...\"],[\"## Supported Models\\n\\nIn addition to thousands of public models available in the Hub, PRO users get f...\"],[\"Inference for PROs makes it easy to experiment and prototype with new models without having to deplo...\"],[\"You can also use many of the familiar transformers generation parameters, like `temperature` or `max...\"],[\"In addition to Python, you can also use JavaScript to integrate inference calls inside your JS or no...\"],[\"```\\n\\u003cs\\u003e[INST] \\u003c\\u003cSYS\\u003e\\u003e\\n{{ system_prompt }}\\n\\u003c\\u003c\\u002fSYS\\u003e\\u003e\\n\\n{{ user_msg_1 }} [\\u002fINST] {{ model_answer_1 }} \\u003c\\u002f...\"],[\"For more details on how this task works, please take a look at https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fcodellama...\"],[\"- `do_sample`: If set to `False` (the default), the generation method will be _greedy search_, which...\"],[\"In addition to the sampling parameters above, you can also control general aspects of the generation...\"],[\"If you are using `InferenceClient`, you can simply append it to the `headers` client property:\\n\\n```P...\"],[\"## Subscribe to PRO\\n\\nYou can sign up today for a PRO subscription [here](https:\\u002f\\u002fhuggingface.co\\u002fsubs...\"],[\"--\\ntitle: \\\"Towards Encrypted Large Language Models with FHE\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fencrypted-llm\\u002f...\"],[\"## Fully Homomorphic Encryption (FHE) Can Solve LLM Privacy Challenges\\n\\nZama’s solution to the chall...\"],[\"## Implementation of a LLM layer with FHE\\n\\nNext, you’ll see how to encrypt a single attention head o...\"],[\"![Single Quantized Head Attention Average Top-k Accuracy](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingfac...\"],[\"```python\\nclass SingleHeadAttention(QGPT2):\\n    \\\"\\\"\\\"Class representing a single attention head implem...\"],[\"```python\\nqgpt2_model = SingleHeadQGPT2Model.from_pretrained(\\n    \\\"gpt2_model\\\", n_bits=4, use_cache=...\"],[\"## Conclusion\\n\\nLarge Language Models are great assistance tools in a wide variety of use cases but t...\"],[\"--\\ntitle: \\\"Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny\\\"\\nthumbnail:...\"],[\"## Knowledge Distillation\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingf...\"],[\"In this particular type of knowledge distillation, the student model is trained to do the normal dif...\"],[\"Image taken from the [paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2305.15798)  “On Architectural Compression of Tex...\"],[\"We have observed that distilled models are up to 100% faster than the original base models. The Benc...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"--\\ntitle: \\\"The Hugging Face Hub for Galleries, Libraries, Archives and Museums\\\"\\nthumbnail: \\u002fblog\\u002fass...\"],[\"You can read the whole post or jump to the most relevant sections! \\n\\n- If you don't know what the Hu...\"],[\"Spaces make hosting and making your application accessible for others to use much more straightforwa...\"],[\"We can find NER models on the Hub by filtering models by task. In this case, we choose `token-classi...\"],[\"We can make datasets available via the Hugging Face hub in various ways. I'll walk through an exampl...\"],[\"![Choosing a name](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblo...\"],[\"![Example metadata](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fbl...\"],[\"## Why might Galleries, Libraries, Archives and Museums want to use the Hugging Face hub?\\n\\nThere are...\"],[\"## Example uses of the Hugging Face Hub\\nIndividuals and organizations already use the Hugging Face h...\"],[\"The Hub supports many features which help make machine learning more accessible. Some features which...\"],[\"If you require any assistance while using the Hugging Face Hub, there are several avenues you can ex...\"],[\"--\\ntitle: \\\"Putting ethical principles at the core of the research lifecycle\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"## Limitations of this ethical charter\\n\\nThis document is a work in progress and reflects a state of ...\"],[\"- Promotion of content and activities which are detrimental in nature, such as violence, harassment,...\"],[\"## Values for the project\\n\\n- **Be transparent:** We are transparent and open about the intent, sourc...\"],[\"We note that some of these values can sometimes be in conflict (for instance being fair and sharing ...\"],[\"--\\ntitle: \\\"StarCoder: A State-of-the-Art LLM for Code\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f141_starcoder\\u002fstarco...\"],[\"## Evaluation\\n\\nWe thoroughly evaluated StarCoder and several similar models and a variety of benchma...\"],[\"An interesting aspect of StarCoder is that it's multilingual and thus we evaluated it on MultiPL-E w...\"],[\"## About BigCode\\n\\nBigCode is an open scientific collaboration led jointly by Hugging Face and Servic...\"],[\"### Data & Governance\\n- [StarCoderData](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fbigcode\\u002fstarcoderdata): Pret...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 M...\"],[\"In keeping with our core value of *democratization*, we have also spent a lot of time speaking publi...\"],[\"- Comments from [Sasha](https:\\u002f\\u002fhuggingface.co\\u002fsasha) on **AI’s energy use and carbon emissions** ([...\"],[\".vox.com\\u002ftechnology\\u002f23738987\\u002fracism-ai-automated-bias-discrimination-algorithm)); addressing how **m...\"],[\"- Comments from [Nathan](https:\\u002f\\u002fhuggingface.co\\u002fnatolambert) on the state of the art on **language m...\"],[\"- Comments from [Irene](https:\\u002f\\u002fhuggingface.co\\u002firenesolaiman) on understanding the **regulatory land...\"],[\"Some of our talks released this summer include [Giada](https:\\u002f\\u002fhuggingface.co\\u002fgiadap)’s [TED present...\"],[\"Of course, we have also made progress on our regular work (our “work work”). The fundamental value o...\"],[\"We have also moved forward with our goals of *fairness* and *justice* with [bias and harm testing](h...\"],[\"Finally, we have been surprised and delighted by public recognition for many of the society & ethics...\"],[\"--\\ntitle: 'Convert Transformers to ONNX with Hugging Face Optimum'\\nthumbnail: \\u002fblog\\u002fassets\\u002f81_conver...\"],[\"Let's get started! 🚀\\n\\n---\\n\\nIf you are interested in optimizing your models to run with maximum effic...\"],[\"➡️[Learn more about ONNX.](https:\\u002f\\u002fonnx.ai\\u002fabout.html)\\n\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\u003chtml itemsc...\"],[\"A list of all supported Transformers architectures can be found in the [ONNX section of the Transfor...\"],[\"```python\\npip install transformers torch\\n```\\n\\nexporting our checkpoint with `export` \\n\\n```python\\nimp...\"],[\"# load config\\nmodel_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, ...\"],[\"Since you successfully convert your Transformers model to ONNX the whole set of optimization and qua...\"],[\"--\\ntitle: \\\"Comments on U.S. National AI Research Resource Interim Report\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f92...\"],[\"- Make ML Accessible to Interdisciplinary, Non-Technical Experts \\n    - NAIRR should provide educati...\"],[\"--\\ntitle: \\\"3D Asset Generation: AI for Game Development #3\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games...\"],[\"### The Current State of Text-to-3D\\n\\nAs discussed in [Part 1](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fml-for-gam...\"],[\"### Why It Isn't Useful (yet)\\n\\n**Note:** This section is intended for readers who are familiar with ...\"],[\"Things are changing rapidly in this area, though, and there may be a viable solution in the near fut...\"],[\"--\\ntitle: \\\"Opinion Classification with Kili and HuggingFace AutoTrain\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f59_op...\"],[\"## AutoTrain with HuggingFace\\n\\nAutomated Machine Learning is a term for automating a Machine Learnin...\"],[\"It is available either online or on-premise and it enables modern Machine Learning technics either o...\"],[\"We have defined 4 categories, \\n\\n- Subscription: Since medium has a subscription option, anything rel...\"],[\"![](assets\\u002f59_opinion-classification-with-kili\\u002f3.png)\\n\\nAs you can see from the menu at the left, it ...\"],[\"interface = {\\n    'jobs': {\\n        'JOB_0': {\\n            'mlTask': 'CLASSIFICATION',\\n            '...\"],[\"```python\\ndef import_dataframe(project_id:str, dataset:pd.DataFrame, text_data_column:str, external_...\"],[\"```python\\ndataset_path = '..\\u002fdata\\u002fprocessed\\u002flowercase_cleaned_dataset.csv'\\ndf = pd.read_csv(dataset_...\"],[\"# The changes should be given as an array that \\n# contains the change for every single sample. \\n# Th...\"],[\"API_KEY = os.getenv('KILI_API_KEY')\\ndataset_path = '..\\u002fdata\\u002fprocessed\\u002flowercase_cleaned_dataset.csv'...\"],[\"# we can drop the `labels` column now\\ndf_ns = df_ns.drop(columns=['labels'])\\n\\n# we'll remove the mul...\"],[\"2. We can select the dataset repository we created before or upload the dataset again. Then we need ...\"],[\"Ray tune is a popular library for hyper-parameter optimization which comes with many SOTA algorithms...\"],[\"# pre-defined evaluation metrics\\nfrom sklearn.metrics import (accuracy_score, f1_score,\\n            ...\"],[\"The metric to optimize is accuracy, we want this value to be as high as possible. Because of that, w...\"],[\"Now we can define the search algorithm and the scheduler for the hyper-parameter-search. \\n\\n```python...\"],[\"# tokenize the datasets\\n    tokenized_train_set = train_dataset.map(tokenize)\\n    tokenized_val_set ...\"],[\"## Final Analysis\\n\\nWe can use the fine-tuned model to conduct the final analysis now. All we have to...\"],[\"We won't do a detailed analysis of the reviews, a basic understanding of potential problems would su...\"],[\"--\\ntitle: \\\"AI for Game Development: Creating a Farming Game in 5 Days. Part 2\\\"\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"The short version is straightforward: ask [ChatGPT](https:\\u002f\\u002fchat.openai.com\\u002fchat) for advice, and fo...\"],[\"**Transformers**, [introduced in 2017](https:\\u002f\\u002fproceedings.neurips.cc\\u002fpaper\\u002f2017\\u002ffile\\u002f3f5ee243547dee...\"],[\"#### Limitations\\n\\nChatGPT often sounds very convincing, while being wrong. Here is an [archive of Ch...\"],[\"--\\ntitle: How to train a new language model from scratch using Transformers and Tokenizers\\nthumbnail...\"],[\"Our model is going to be called… wait for it… **EsperBERTo** 😂\\n\\n\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f01_how-to-tra...\"],[\"# Save files to disk\\ntokenizer.save_model(\\\".\\\", \\\"esperberto\\\")\\n```\\n\\nAnd here’s a slightly accelerated ...\"],[\"print(\\n    tokenizer.encode(\\\"Mi estas Julien.\\\")\\n)\\n# Encoding(num_tokens=7, ...)\\n# tokens: ['\\u003cs\\u003e', 'M...\"],[\"Here’s a simple version of our EsperantoDataset.\\n\\n```python\\nfrom torch.utils.data import Dataset\\n\\ncl...\"],[\"**🔥🔥🔥 Let’s start training!! 🔥🔥🔥**\\n\\nHere you can check our Tensorboard for [one particular set of hy...\"],[\"Ok, simple syntax\\u002fgrammar works. Let’s try a slightly more interesting prompt:\\n\\n```python\\nfill_mask(...\"],[\"Again, here’s the hosted **[Tensorboard](https:\\u002f\\u002ftensorboard.dev\\u002fexperiment\\u002flOZn2wOWQo6ixpwtWyyDfQ\\u002f#...\"],[\"### **TADA!**\\n\\n➡️ Your model has a page on https:\\u002f\\u002fhuggingface.co\\u002fmodels and everyone can load it us...\"],[\"--\\ntitle: \\\"Using & Mixing Hugging Face Models with Gradio 2.0\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f22_gradio\\u002fgra...\"],[\"Do you want to customize the demo? You can override any of the default parameters of the [Interface ...\"],[\"--\\ntitle: \\\"Hugging Face and Graphcore partner for IPU-optimized Transformers\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"This design delivers high performance and new levels of efficiency, whether running today’s most pop...\"],[\"Using Optimum, a new open-source library and toolkit, developers will be able to access hardware-opt...\"],[\"Now Graphcore users will be able to unlock such performance advantages, through the Hugging Face pla...\"],[\"--\\ntitle: \\\"AudioLDM 2, but faster ⚡️\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f161_audioldm2\\u002fthumbnail.png\\nauthors:\\n...\"],[\"The overall generation process is summarised as follows:\\n\\n1. Given a text input \\\\\\\\(\\\\boldsymbol{x}\\\\\\\\)...\"],[\"$$\\n\\\\boldsymbol{z}_{t} = \\\\text{LDM}\\\\left(\\\\boldsymbol{z}_{t-1} | \\\\tilde{\\\\boldsymbol{E}}_{1:N}, \\\\boldsy...\"],[\"AudioLDM 2 comes in three variants. Two of these checkpoints are applicable to the general task of t...\"],[\"The pipeline can be moved to the GPU in much the same way as a standard PyTorch nn module:\\n\\n```pytho...\"],[\"Sounds much like our text prompt! The quality is good, but still has artefacts of background noise. ...\"],[\"## Optimisation 1: Flash Attention\\n\\nPyTorch 2.0 and upwards includes an optimised and memory-efficie...\"],[\"We can load the weights in float16 precision by passing the [`torch_dtype`](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"```python\\naudio = pipe(prompt, negative_prompt=negative_prompt, generator=generator.manual_seed(0))....\"],[\"```python\\npipe.scheduler.compatibles\\n```\\n\\n**Output:**\\n```\\n[diffusers.schedulers.scheduling_lms_discr...\"],[\"Let's see how we can switch the AudioLDM 2 scheduler from DDIM to DPM Multistep. We'll use the [`Con...\"],[\"## What about memory?\\n\\nThe length of the audio sample we want to generate dictates the *width* of th...\"],[\"pipe.to(\\\"cuda\\\")\\n\\naudio = pipe(prompt, negative_prompt=negative_prompt, num_waveforms_per_prompt=4, a...\"],[\"```python\\npipe.enable_model_cpu_offload()\\n```\\n\\nRunning generation with CPU offload is then the same ...\"],[\"--\\ntitle: \\\"Accelerating Stable Diffusion Inference on Intel CPUs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f136_stable...\"],[\"First, let's create a virtual environment with the required libraries: Transformers, Diffusers, Acce...\"],[\"Optimum Intel supports [OpenVINO](https:\\u002f\\u002fdocs.openvino.ai\\u002flatest\\u002findex.html), an Intel open-source ...\"],[\"If you can't or don't want to use OpenVINO, the rest of this post will show you a series of other op...\"],[\"```\\nnumactl -C 0-31 python sd_blog_1.py\\n```\\n\\nThanks to these optimizations, our original Diffusers c...\"],[\"# optimize with IPEX\\npipe.unet = ipex.optimize(pipe.unet.eval(), dtype=torch.bfloat16, inplace=True,...\"],[\"With this final version, inference latency is now down to **5.05 seconds**. Compared to our initial ...\"],[\"--\\ntitle: \\\"A Complete Guide to Audio Datasets\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f116_audio_datasets\\u002fthumbnail...\"],[\"Let's head to the Hub and filter the datasets by task:\\n* [Speech Recognition Datasets on the Hub](ht...\"],[\"## Load an Audio Dataset\\n\\nOne of the key defining features of 🤗 Datasets is the ability to download ...\"],[\"```python\\nfrom datasets import load_dataset\\n\\ngigaspeech = load_dataset(\\\"speechcolab\\u002fgigaspeech\\\", \\\"xs...\"],[\"```python\\nprint(gigaspeech[\\\"train\\\"][0])\\n```\\n\\n**Print Output:**\\n```python\\n{'segment_id': 'YOU00000003...\"],[\"Let's check that we've successfully retained the `text` and `audio` columns:\\n\\n```python\\nprint(gigasp...\"],[\"We can set the audio inputs to our desired sampling rate using 🤗 Datasets' \\n[`cast_column`](https:\\u002f\\u002f...\"],[\"```python\\ngigaspeech = gigaspeech.cast_column(\\\"audio\\\", Audio(sampling_rate=16000))\\n\\nprint(gigaspeech...\"],[\"```python\\ndef prepare_dataset(batch):\\n    audio = batch[\\\"audio\\\"]\\n    batch = processor(audio[\\\"array\\\"...\"],[\"And with that, we have the GigaSpeech dataset fully prepared for our model! In total, this process r...\"],[\"This is analogous to _downloading_ a TV show versus _streaming_ it. When we download a TV show, we d...\"],[\"There is one caveat to streaming mode. When downloading a dataset, both the raw data and processed d...\"],[\"Streaming mode can take your research to the next level: not only are the biggest datasets accessibl...\"],[\"| Dataset                                                                                 | Domain  ...\"],[\"| [Earnings-22](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002frevdotcom\\u002fearnings22)                     | Fincanci...\"],[\"Refer to the [Google Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fsanchit-gandhi\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"```python\\ncommon_voice = load_dataset(\\\"mozilla-foundation\\u002fcommon_voice_11\\\", \\\"en\\\", use_auth_token=Tru...\"],[\"```python\\ngigaspeech = load_dataset(\\\"speechcolab\\u002fgigaspeech\\\", \\\"xs\\\", use_auth_token=True)\\n```\\n\\n#### [...\"],[\"```python\\nami = load_dataset(\\\"edinburghcstr\\u002fami\\\", \\\"ihm\\\")\\n```\\n\\n### Multilingual Speech Recognition\\n\\nM...\"],[\"#### [FLEURS](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgoogle\\u002ffleurs)\\nFLEURS (Few-shot Learning Evaluation of...\"],[\"#### [FLEURS](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgoogle\\u002ffleurs)\\nFLEURS (Few-shot Learning Evaluation of...\"],[\"#### [Multilingual Spoken Words](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fMLCommons\\u002fml_spoken_words)\\nMultilin...\"],[\"## Closing Remarks\\n\\nIn this blog post, we explored the Hugging Face Hub and experienced the Dataset ...\"],[\"--\\ntitle: The Annotated Diffusion Model\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_annotated-diffusion\\u002fthumbnail.png...\"],[\"Note that there are [several perspectives](https:\\u002f\\u002ftwitter.com\\u002fsedielem\\u002fstatus\\u002f1530894256168222722?s...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f78_annotated-diffusion\\u002fdiffusion_figure.png\\\" width=\\\"600\\\" \\u002f\\u003e\\n...\"],[\"Recall that a normal distribution (also called Gaussian distribution) is defined by 2 parameters: a ...\"],[\"Ok, so we need a neural network to represent a (conditional) probability distribution of the backwar...\"],[\"So we continue, assuming that our neural network only needs to learn\\u002frepresent the mean of this cond...\"],[\"with \\\\\\\\(\\\\alpha_t := 1 - \\\\beta_t\\\\\\\\) and \\\\\\\\(\\\\bar{\\\\alpha}_t := \\\\Pi_{s=1}^{t} \\\\alpha_s\\\\\\\\). Let's refer t...\"],[\"Here, \\\\\\\\(\\\\mathbf{x}_0\\\\\\\\) is the initial (real, uncorrupted) image, and we see the direct noise level...\"],[\"What is typically used here is very similar to that of an [Autoencoder](https:\\u002f\\u002fen.wikipedia.org\\u002fwik...\"],[\"```python\\ndef exists(x):\\n    return x is not None\\n\\ndef default(val, d):\\n    if exists(val):\\n        ...\"],[\"```python\\nclass SinusoidalPositionEmbeddings(nn.Module):\\n    def __init__(self, dim):\\n        super(...\"],[\"def forward(self, x, scale_shift=None):\\n        x = self.proj(x)\\n        x = self.norm(x)\\n\\n        i...\"],[\"h = self.block1(x, scale_shift=scale_shift)\\n        h = self.block2(h)\\n        return h + self.res_c...\"],[\"sim = einsum(\\\"b h d i, b h d j -\\u003e b h i j\\\", q, k)\\n        sim = sim - sim.amax(dim=-1, keepdim=True)...\"],[\"```python\\nclass PreNorm(nn.Module):\\n    def __init__(self, dim, fn):\\n        super().__init__()\\n    ...\"],[\"Ultimately, neural networks stack up layers as if they were lego blocks (but it's important to [unde...\"],[\"mid_dim = dims[-1]\\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\\n  ...\"],[\"x = torch.cat((x, h.pop()), dim=1)\\n            x = block2(x, t)\\n            x = attn(x)\\n\\n           ...\"],[\"To start with, let's use the linear schedule for \\\\\\\\(T=300\\\\\\\\) time steps and define the various varia...\"],[\"These transformations are fairly simple: we first normalize images by dividing by \\\\\\\\(255\\\\\\\\) (such th...\"],[\"```python\\n# forward diffusion (using the nice property)\\ndef q_sample(x_start, t, noise=None):\\n    if...\"],[\"plt.tight_layout()\\n```\\n\\n```python\\nplot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50,...\"],[\"```python\\nfrom datasets import load_dataset\\n\\n# load dataset from the hub\\ndataset = load_dataset(\\\"fas...\"],[\"\\u003cimg src=\\\"assets\\u002f78_annotated-diffusion\\u002fsampling.png\\\" width=\\\"500\\\" \\u002f\\u003e\\n\\nGenerating new images from a d...\"],[\"@torch.no_grad()\\ndef sample(model, image_size, batch_size=16, channels=3):\\n    return p_sample_loop(...\"],[\"loss = p_losses(model, batch, t, loss_type=\\\"huber\\\")\\n\\n      if step % 100 == 0:\\n        print(\\\"Loss:\\\"...\"],[\"\\u003cimg src=\\\"assets\\u002f78_annotated-diffusion\\u002foutput.png\\\" width=\\\"300\\\" \\u002f\\u003e\\n\\nSeems like the model is capable ...\"],[\"- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f210...\"],[\"Note that this list only includes important works until the time of writing, which is June 7th, 2022...\"],[\"--\\ntitle: \\\"How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML ro...\"],[\"## Transcription:\\n\\n### Introduction\\n\\nMy name is Swaraj. I'm the CTO and co-founder at Sempre Health....\"],[\"### How did you leverage the Expert Acceleration Program?\\n\\nThe Hugging Face team really helped us in...\"],[\"### For what type of AI problems should ML teams consider the Expert Acceleration Program?\\nHere at S...\"],[\"--\\ntitle: \\\"Databricks ❤️ Hugging Face: up to 40% faster training and tuning of Large Language Models...\"],[\"```swift\\nfrom datasets import load_dataset\\n\\ntrain_df = train.write.parquet(train_dbfs_path, mode=\\\"ov...\"],[\"## Why does this matter?\\n\\nAs we transition to this new AI paradigm, organizations will need to use t...\"],[\"In order to become the best platform for users to jump into the world of AI, we’re working hard to p...\"],[\"--\\ntitle: 'Introducing Snowball Fight ☃️, our first ML-Agents environment'\\nthumbnail: \\u002fblog\\u002fassets\\u002f3...\"],[\"## Be part of the conversation: join our discord server!\\n\\nIf you're using ML-Agents or interested in...\"],[\"--\\ntitle: \\\"Deep Learning with Proteins\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f119_deep_learning_with_proteins\\u002ffol...\"],[\"Let’s say that you want to train a DL model to take a sentence in English as input and decide if it’...\"],[\"| Text | Label |\\n| --- | --- |\\n| She’s the best director in the world! | 1 |\\n| I hate this movie. | ...\"],[\"This stage of affairs continued until 2018, when two huge papers landed, introducing the models [ULM...\"],[\"The reason this works is that in the process of solving any non-trivial task, neural networks learn ...\"],[\"Other proteins are critical in health and disease - everyone probably remembers endless news reports...\"],[\"![protein structure](assets\\u002f119_deep_learning_with_proteins\\u002fprotein_structure.png)\\n\\n*This figure sho...\"],[\"At this point, hopefully you have a vague idea of what a protein is and why biologists care about th...\"],[\"Like a lot of other fields, though, the arrival of deep learning changed everything. AlphaFold and e...\"],[\"The key takeaway, though, is that even though proteins are very different to language, they can be h...\"],[\"If you’re a biologist, on the other hand, you probably have several ideas for what you want to try, ...\"],[\"## Conclusion\\n\\nThe intersection of deep learning and biology is going to be an incredibly active and...\"],[\"--\\ntitle: \\\"Sentence Transformers in the Hugging Face Hub\\\"\\nauthors:\\n- user: osanseviero\\n- user: nreim...\"],[\"\\u003cdiv\\u003e\\u003ca class=\\\"text-xs block mb-3 text-gray-300\\\" href=\\\"\\u002fsentence-transformers\\u002fdistilbert-base-nli-ma...\"],[\".json&quot;},{&quot;rfilename&quot;:&quot;vocab.txt&quot;},{&quot;rfilename&quot;:&quot;1_Pooling\\u002fco...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"\\u002fdocs\\\"\\u003e\\u003csvg class=\\\"ml-1.5 text-sm text-gray-400 ...\"],[\".w3.org\\u002f2000\\u002fsvg\\\" xmlns:xlink=\\\"http:\\u002f\\u002fwww.w3.org\\u002f1999\\u002fxlink\\\" aria-hidden=\\\"true\\\" focusable=\\\"false\\\" ro...\"],[\"JSON Output\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"flex items-center ml-auto\\\"\\u003e\\u003csvg class=\\\"mr-1\\\" xmlns=\\\"http:\\u002f\\u002fwww.w...\"],[\"But seeing a bunch of numbers might not be very useful to you (unless you're able to understand the ...\"],[\"\\u003c!-- Hackiest hack ever for the draft --\\u003e\\n\\u003cdiv\\u003e\\u003ca class=\\\"text-xs block mb-3 text-gray-300\\\" href=\\\"\\u002fse...\"],[\"\\u003cdiv class=\\\"p-5 shadow-sm rounded-xl bg-white max-w-md\\\"\\u003e\\u003cdiv class=\\\"SVELTE_HYDRATER \\\" data-props=\\\"{&...\"],[\"\\\"\\u003e \\u003cdiv class=\\\"font-semibold flex items-center mb-2\\\"\\u003e\\u003cdiv class=\\\"text-lg flex items-center\\\"\\u003e\\u003csvg xml...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"\\u002fdocs\\\"\\u003e\\u003csvg class=\\\"ml-1.5 text-sm text-gray-400 ...\"],[\"...\\\" type=\\\"text\\\"\\u003e\\u003c\\u002flabel\\u003e \\u003clabel class=\\\"block \\\"\\u003e \\u003cspan class=\\\"text-sm text-gray-500\\\"\\u003eSentences to co...\"],[\"JSON Output\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"flex items-center ml-auto\\\"\\u003e\\u003csvg class=\\\"mr-1\\\" xmlns=\\\"http:\\u002f\\u002fwww.w...\"],[\"Of course, on top of the widgets, we also provide API endpoints in our Inference API that you can us...\"],[\"```yaml\\n---\\ntags:\\n  - sentence-transformers\\n  - sentence-similarity # Or feature-extraction!\\n---\\n```...\"],[\"--\\ntitle: \\\"Running IF with 🧨 diffusers on a Free Tier Google Colab\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fif\\u002fthumb...\"],[\"As a result, IF is better at generating images with high-frequency\\ndetails (*e.g.,* human faces and ...\"],[\"## Table of contents\\n\\n* [Accepting the license](#accepting-the-license)\\n* [Optimizing IF to run on m...\"],[\"The deep learning community has created world class tools to run\\nresource intensive models on consum...\"],[\"Therefore, we lower the precision of T5 even more by using\\n`bitsandbytes` 8bit quantization, which a...\"],[\"``` python\\n!grep MemTotal \\u002fproc\\u002fmeminfo\\n```\\n\\n```bash\\nMemTotal:       13297192 kB\\n```\\n\\nAnd an NVIDIA ...\"],[\"``` python\\n! pip install --upgrade \\\\\\n  diffusers~=0.16 \\\\\\n  transformers~=4.28 \\\\\\n  safetensors~=0.3 \\\\...\"],[\"### 1.2 Create text embeddings\\n\\nThe Diffusers API for accessing diffusion models is the\\n`DiffusionPi...\"],[\"First, use the Python keyword `del` to delete all Python objects\\nreferencing allocated GPU memory\\n\\n`...\"],[\"Let\\\\'s manually convert the raw tensors to PIL and have a sneak peek at\\nthe final result. The output...\"],[\"And again, we delete the Python pointer and free memory\\n\\n``` python\\ndel pipe\\nflush()\\n```\\n\\n### 1.6 St...\"],[\"Et voila! A beautiful 1024x1024 image in a free-tier Google Colab.\\n\\nWe have shown how 🧨 diffusers ma...\"],[\"![iv_sample](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers...\"],[\"First, remove the Python pointers\\n\\n``` python\\ndel text_encoder\\ndel pipe\\n```\\n\\nand then free the memor...\"],[\"You can also use the Stable Diffusion x4 upscaler on this image. Feel\\nfree to try it out using the c...\"],[\"💡 **Note**: You can create masks yourself by manually creating a\\ngreyscale image.\\n\\n``` python\\nfrom P...\"],[\"Now, we need to pass the input image, the mask image, and the prompt\\nembeddings.\\n\\n``` python\\nimage =...\"],[\"## Conclusion\\n\\nIF in 32-bit floating point precision uses 40 GB of weights in total. We\\nshowed how u...\"],[\"--\\ntitle: \\\"Hugging Face and IBM partner on watsonx.ai, the next-generation enterprise studio for AI ...\"],[\"1. **Model standardization**: the [Transformer](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1706.03762) architecture is no...\"],[\"IBM decided that open source should be at the core of watsonx.ai. We couldn't agree more! Built on [...\"],[\"Our joint team is hard at work at the moment. We can't wait to show you what we've been up to! The m...\"],[\"--\\ntitle: The State of Computer Vision at Hugging Face 🤗\\nthumbnail: \\u002fblog\\u002fassets\\u002fcv_state\\u002fthumbnail....\"],[\"- Image-to-text (image captioning, OCR)\\n- Text-to-image\\n- Document question-answering\\n- Visual quest...\"],[\"```py\\nfrom transformers import pipeline\\n\\ndepth_estimator = pipeline(task=\\\"depth-estimation\\\", model=\\\"...\"],[\"**Where do I find the code?**\\n\\n- [Model documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002finde...\"],[\"## Integrations with Datasets\\n\\n[Datasets](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets) provides easy access...\"],[\"We have over 200 models from `timm` on the Hub and more are on the way. Check out the [documentation...\"],[\"## Support for third-party libraries\\n\\nCentral to the Hugging Face ecosystem is the [Hugging Face Hub...\"],[\"- [Generate 3D voxels from a predicted depth map of an input image](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fra...\"],[\"In this section, we wanted to share our philosophy behind adding support for Computer Vision in 🤗 Tr...\"],[\"Even for a difficult task like object detection, the user experience doesn’t change very much:\\n\\n```p...\"],[\"- [CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002fclip) that enables zero-shot ima...\"],[\"The community can expect to see more zero-shot models for computer vision being supported from 🤗Tran...\"],[\"As always, we welcome your patches, PRs, model checkpoints, datasets, and other contributions! 🤗\\n\\n*A...\"],[\"--\\ntitle: \\\"Fine-tuning Stable Diffusion models on Intel CPUs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fstable-diffusi...\"],[\"This post will show you how to fine-tune a Stable Diffusion model on an Intel Sapphire Rapids CPU cl...\"],[\"```\\nArchitecture:            x86_64\\n  CPU op-mode(s):        32-bit, 64-bit\\n  Address sizes:        ...\"],[\"Let's first list the IP addresses of our servers in `nodefile.` The first line refers to the primary...\"],[\"```\\ndiff --git a\\u002fexamples\\u002ftextual_inversion\\u002ftextual_inversion.py b\\u002fexamples\\u002ftextual_inversion\\u002ftextua...\"],[\"Here are the images:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fsd-concepts-library\\u002fdicoo\\u002fresolve\\u002fmain\\u002fconcep...\"],[\"export MODEL_NAME=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport DATA_DIR=\\\"\\u002fhome\\u002fdevcloud\\u002fdicoo\\\"\\n```\\n\\nWe ca...\"],[\"You can quickly pinpoint the troublemaker by logging in to each node and training locally. First, se...\"],[\"Then, we load the optimized model, generate five different images and save them:\\n\\n```\\nfrom optimum.i...\"],[\"Here are some resources to help you get started:\\n\\n* Diffusers [documentation](https:\\u002f\\u002fhuggingface.co...\"],[\"--\\ntitle: \\\"Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"Note that you should generally use the latest version of `@gradio\\u002flite` that is available. You can s...\"],[\"## More Examples: Adding Additional Files and Requirements\\n\\nWhat if you want to create a Gradio app ...\"],[\"async def classify(text):\\n\\treturn await pipe(text)\\n\\ndemo = gr.Interface(classify, \\\"textbox\\\", \\\"json\\\")...\"],[\"## Try it out!\\n\\nYou can immediately try out `@gradio\\u002flite` by copying and pasting this code in a loc...\"],[\"--\\ntitle: \\\"BERT 101 - State Of The Art NLP Model Explained\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f52_bert_101\\u002fthum...\"],[\"In this guide, you'll learn what BERT is, why it’s different, and how to get started using BERT:\\n\\n1....\"],[\"---\\n\\n### 1.1 Example of BERT\\n\\n\\nBERT helps Google better surface (English) results for nearly all sea...\"],[\"### 2.2 What is a Masked Language Model?\\n\\nMLM enables\\u002fenforces bidirectional learning from text by m...\"],[\"\\u003cdiv class=\\\"bg-white pb-1\\\"\\u003e...\"],[\"\\u003cdiv class=\\\"SVELTE_HYDRATER contents\\\" data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.hu...\"],[\".04805&quot;,&quot;transformers&quot;,&quot;exbert&quot;,&quot;license:apache-2.0&quot;,&quot;autonl...\"],[\".04805&quot;,&quot;label&quot;:&quot;arxiv:1810.04805&quot;,&quot;type&quot;:&quot;arxiv&quot;},{&qu...\"],[\"\\u003cdiv class=\\\"flex flex-col w-full max-w-full\\\"\\u003e\\n            \\u003cdiv class=\\\"font-semibold flex items-cente...\"],[\"\\u003cpath d=\\\"M12.3625 13.85H10.1875V12.7625H12.3625V10.5875H13.45V12.7625C13.4497 13.0508 13.335 13.3272...\"],[\"\\u003csvg class=\\\"-mr-1 ml-2 h-5 w-5 transition ease-in-out transform false\\\" xmlns=\\\"http:\\u002f\\u002fwww.w3.org\\u002f2000...\"],[\"\\u003cpath d=\\\"M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z\\\" fill=\\\"currentColor\\\"\\u003e\\u003c\\u002fpath\\u003e\\n             ...\"],[\"**Fun Fact:** Masking has been around a long time - [1953 Paper on Cloze procedure (or ‘Masking’)](h...\"],[\"Timeline of popular Transformer model releases:\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e...\"],[\"Fun Fact: Google has been using your reCAPTCHA selections to label training data since 2011. The ent...\"],[\"ML Architecture Glossary:\\n\\n| ML Architecture Parts | Definition                                     ...\"],[\"Let’s take a look at how BERTlarge’s  additional layers, attention heads, and parameters have increa...\"],[\"#### 4.3 GLUE Benchmark\\n[GLUE](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fglue) (General Language Understanding...\"],[\"## 6. The open source power of BERT\\n\\nUnlike other large learning models like GPT-3, BERT’s source co...\"],[\"Note: Hugging Face's [pipeline class](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain_classes\\u002fpipeline...\"],[\"Let's see what jobs BERT suggests for a \\\"man\\\":\\n\\n```python\\nunmasker(\\\"The man worked as a [MASK].\\\")\\n``...\"],[\"BERT predicted the woman's job to be a Nurse, Waitress, Maid, Prostitute, or Cook displaying a clear...\"],[\"\\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fQuestion\\\"\\u003e\\n    \\u003ch3 itemprop=\\\"name\\\"...\"],[\"\\u003ch3 itemprop=\\\"name\\\"\\u003eHow long does it take to fine-tune BERT?\\u003c\\u002fh3\\u003e\\n  \\u003cdiv itemscope itemprop=\\\"accepte...\"],[\"## 9. Conclusion\\n\\nBERT is a highly complex and advanced language model that helps people automate la...\"],[\"--\\ntitle: \\\"We Raised $100 Million for Open & Collaborative Machine Learning 🚀\\\"\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f65_series_c\\u002fhome-of-machine-l...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f65_series_c\\u002fteam.png\\\" alt=\\\"Th...\"],[\"--\\ntitle: \\\"Efficient Table Pre-training without Real Data: An Introduction to TAPEX\\\"\\nthumbnail: \\u002fblo...\"],[\"### Pre-training\\n\\nThe following figure illustrates the pre-training process. At each step, we first ...\"],[\"outputs = model.generate(**encoding)\\n\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True...\"],[\"\\u003cdiv class=\\\"bg-white pb-1\\\"\\u003e\\u003cdiv class=\\\"SVELTE_HYDRATER contents\\\" data-props=\\\"{&quot;apiUrl&quot;:&qu...\"],[\".07653&quot;,&quot;label&quot;:&quot;arxiv:2107.07653&quot;,&quot;type&quot;:&quot;arxiv&quot;},{&qu...\"],[\"\\\"\\u003e \\u003cdiv class=\\\"font-semibold flex items-center mb-2\\\"\\u003e\\u003cdiv class=\\\"text-lg flex items-center\\\"\\u003e\\u003csvg xml...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"https:\\u002f\\u002fapi-inference.huggingface.co\\u002f\\\"\\u003e\\u003csvg clas...\"],[\".95 12.2187H2.775V9.49998H4.95V12.2187ZM10.3875 5.69373V8.41248H6.0375V5.69373H10.3875ZM11.475 9.499...\"],[\"false...\"],[\"false\\\"\\u003e\\u003cdiv class=\\\"inline-flex justify-between w-32 lg:w-44 rounded-md border border-gray-100 px-4 p...\"],[\".414z\\\" clip-rule=\\\"evenodd\\\"\\u003e\\u003c\\u002fpath\\u003e\\u003c\\u002fsvg\\u003e\\u003c\\u002fdiv\\u003e \\u003c\\u002fdiv\\u003e\\u003c\\u002fdiv\\u003e \\u003cform\\u003e\\u003cdiv class=\\\"flex h-10\\\"\\u003e\\u003cinput clas...\"],[\".5\\\" type=\\\"button\\\"\\u003e\\u003csvg class=\\\"mr-2\\\" xmlns=\\\"http:\\u002f\\u002fwww.w3.org\\u002f2000\\u002fsvg\\\" aria-hidden=\\\"true\\\" focusable=...\"],[\"Add row\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"btn-widget flex-1 lg:flex-none mt-2 lg:mr-1.5\\\" type=\\\"button\\\"\\u003e\\u003csvg cl...\"],[\"### Experiments\\n\\nWe evaluate TAPEX on four benchmark datasets, including [WikiSQL (Weak)](https:\\u002f\\u002fhu...\"],[\"![corpus](assets\\u002f74_tapex\\u002ftapex-performance.png)\\n\\n\\n### Comparison to Previous Table Pre-training\\n\\nTh...\"],[\"![efficiency](assets\\u002f74_tapex\\u002ftapex-efficiency.png)\\n\\nWhat about the efficiency? How efficient is suc...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #4: Bias in Text-to-Image Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"For example, if the training data are mainly in English they probably convey rather Western values. ...\"],[\"**Biases in training data:** Popular multimodal datasets such as [LAION-5B](https:\\u002f\\u002flaion.ai\\u002fblog\\u002fla...\"],[\"**Biases in inference:** The [CLIP model](https:\\u002f\\u002fhuggingface.co\\u002fopenai\\u002fclip-vit-large-patch14) used...\"],[\"## Detecting Bias\\n\\nMost of the issues that we describe above cannot be solved with a single solution...\"],[\"Other tools, like the [Face Clustering tool](https:\\u002f\\u002fhf.co\\u002fspaces\\u002fsociety-ethics\\u002fDiffusionFaceCluste...\"],[\"**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https:\\u002f...\"],[\"Also, as we have mentioned in a [previous newsletter](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fethics-soc-2#addre...\"],[\"## Other updates\\n\\nWe are also continuing work on other fronts of ethics and society, including:\\n\\n- *...\"],[\"--\\ntitle: \\\"Active Learning with AutoNLP and Prodigy\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f43_autonlp_prodigy\\u002fthum...\"],[\"## Prodigy\\n\\n[Prodigy](https:\\u002f\\u002fprodi.gy\\u002f) is an annotation tool developed by Explosion (the makers of...\"],[\"\\u003cimg src=\\\"assets\\u002f43_autonlp_prodigy\\u002fautonlp_data_multi_class.png\\\"\\u003e\\n\\nStep 4: Accept the pricing and t...\"],[\"Once you run the above command, you can go to the prodigy web interface (usually at localhost:8080) ...\"],[\"with open('data.jsonl', 'w') as outfile:\\n    for entry in dataset:\\n        json.dump(entry, outfile)...\"],[\"\\u003cimg src=\\\"assets\\u002f43_autonlp_prodigy\\u002fa3.png\\\"\\u003e\\n\\nLet's take a look at how this model performs on the sa...\"],[\"We have open-sourced the best model created using this process. You can try it [here](https:\\u002f\\u002fhuggin...\"],[\"--\\ntitle: \\\"AI Policy @🤗: Open ML Considerations in the EU AI Act\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002feu_ai_act_...\"],[\"Hugging Face is where it is today thanks to its community of developers, so we’ve seen firsthand wha...\"],[\"You can find more detail and context for those in the \\u003ca href=\\\"\\u002fblog\\u002fassets\\u002feu_ai_act_oss\\u002fsupporting...\"],[\"--\\ntitle: \\\"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\\\"\\nthu...\"],[\"“*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and...\"],[\"[Optimum Intel](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-intel) is part of Optimum and builds on top o...\"],[\"Let’s get started! All code is available in this [notebook](https:\\u002f\\u002fgitlab.com\\u002fjuliensimon\\u002fhuggingfa...\"],[\"```\\nimport evaluate\\n\\ndef eval_func(model):\\n    task_evaluator = evaluate.evaluator(\\\"text-classificat...\"],[\"The log tells us that Optimum Intel has quantized 38 ```Linear``` and 2 ```Embedding``` operators.\\n\\n...\"],[\"```\\n# Quantized model\\n\\nTransformerBlock(\\n  (attention): MultiHeadSelfAttention(\\n    (dropout): Dropo...\"],[\"```\\n[INFO] |**********************Tune Result Statistics**********************|\\n[INFO] +------------...\"],[\"--\\ntitle: \\\"CO2 Emissions and the 🤗 Hub: Leading the Charge\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f60_carbon_emissi...\"],[\"For example, we can search for all models that took a maximum of 100 grams to make:\\n\\n\\n```python\\nfrom...\"],[\"So, if you run something like this...\\n\\n\\n```python\\nfrom datasets import load_dataset\\nfrom transformer...\"],[\"For more references on the metadata format for `co2_eq_emissions ` see [the hub docs](https:\\u002f\\u002fhuggin...\"],[\"--\\ntitle: \\\"What's going on with the Open LLM Leaderboard?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fevaluating-mmlu-l...\"],[\"Ready? Then buckle up, we’re taking off 🚀.\\n\\n## What's the Open LLM Leaderboard?\\n\\nFirst, note that th...\"],[\"Both the EleutherAI Harness and Stanford HELM benchmarks are interesting because they gather many ev...\"],[\"## How we automatically evaluate a model in today’s LLM world\\n\\nMMLU is a multiple choice question te...\"],[\"![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fevaluating...\"],[\"\\u003cdiv\\u003e\\n\\u003ctable\\u003e\\u003cp\\u003e\\n  \\u003ctbody\\u003e\\n \\u003ctr style=\\\"text-align: left;\\\"\\u003e\\n  \\u003ctd\\u003eOriginal implementation \\u003ca href=\\\"ht...\"],[\"The differences between them can seem small, did you spot them all? Here they are:\\n- First sentence,...\"],[\"Here, the model has one example of the expected behavior and is thus less likely to predict answers ...\"],[\"![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fevaluating...\"],[\"Here is a table summary of the answers provided and generated by the model to summarize what we’ve s...\"],[\"We’ve covered them all!\\n\\nNow let’s compare the model scores on these three possible ways to evaluate...\"],[\"Now, is there a \\\"best way\\\" to evaluate a model among all the ones we've seen? It's a tricky question...\"],[\"This is why open, standardized, and reproducible benchmarks such as the [EleutherAI Eval Harness](ht...\"],[\"--\\ntitle: \\\"The Technology Behind BLOOM Training\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f86_bloom_megatron_deepspeed...\"],[\"There are 6 main groups of people to thank:\\n\\n1. The HuggingFace's BigScience team who dedicated more...\"],[\"## Overview\\n\\nBLOOM's architecture is very similar to [GPT3](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fGPT-3) wit...\"],[\"## Megatron-DeepSpeed\\n\\nThe 176B BLOOM model has been trained using [Megatron-DeepSpeed](https:\\u002f\\u002fgith...\"],[\"Please note that both Megatron-LM and DeepSpeed have Pipeline Parallelism and BF16 Optimizer impleme...\"],[\"## Data Parallelism\\n\\nMost users with just a few GPUs are likely to be familiar with `DistributedData...\"],[\"The main building block of any transformer is a fully connected `nn.Linear` followed by a nonlinear ...\"],[\"Special considerations: Due to the two all reduces per layer in both the forward and backward passes...\"],[\"Now while data travels from layer 0 to 1, 1 to 2 and 2 to 3 this is just like the forward pass of a ...\"],[\"The following illustration from the [GPipe paper](https:\\u002f\\u002fai.googleblog.com\\u002f2019\\u002f03\\u002fintroducing-gpip...\"],[\"To calculate the global batch size of the DP + PP setup we then do: `mbs*chunks*dp_degree` (`8*32*4=...\"],[\"While both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-Deep...\"],[\"![dp-pp-tp-3d](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fparalle...\"],[\"ZeRO stage 3 can also be used to train models at this scale, however, it requires more communication...\"],[\"So back in January as we knew we would be training on A100s which support the BF16 format Olatunji R...\"],[\"All PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no...\"],[\"If we were to fuse these two operations, i.e. put them into a single \\\"fused kernel\\\", and just launch...\"],[\"## Datasets\\n\\nAnother important feature from Megatron-LM is the efficient data loader. During start u...\"],[\"## Positional Encoding\\n\\nWe also replaced the usual positional embedding with an AliBi - based on the...\"],[\"We have run into a variety of other problems that led to 5-10h downtime several times, some related ...\"],[\"## Conclusion\\n\\nThe most difficult and intense part of the training was the 2 months leading to the s...\"],[\"Megatron-LM:\\n\\n- [Efficient Large-Scale Language Model Training on GPU Clusters](https:\\u002f\\u002farxiv.org\\u002fab...\"],[\"--\\ntitle: \\\"Happy 1st anniversary 🤗 Diffusers!\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fdiffusers-turns-1\\u002fdiffusers-...\"],[\"**Table of Contents**\\n\\n* [Striving for photorealism](#striving-for-photorealism)\\n* [Video pipelines]...\"],[\"## Video pipelines\\n\\nText-to-image pipelines are cool, but text-to-video is even cooler! We currently...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhf-internal-testing\\u002fdi...\"],[\"In addition to that, we also support specific hardware and formats like ONNX, the `mps` PyTorch devi...\"],[\"## Support for LoRA\\n\\nFine-tuning diffusion models is expensive and out of reach for most consumer GP...\"],[\"\\u003cdiv class=\\\"mx-auto max-w-screen-xl py-8\\\"\\u003e\\n  \\u003cdiv class=\\\"mb-8 sm:break-inside-avoid\\\"\\u003e\\n    \\u003cblockquot...\"],[\"\\u003cdiv class=\\\"text-sm\\\"\\u003e\\n        \\u003cp class=\\\"font-medium\\\"\\u003eSimo\\u003c\\u002fp\\u003e\\n      \\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n  \\u003cdi...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n  \\u003cdiv class=\\\"mb-8 sm:break-inside-avoid\\\"\\u003e\\n    \\u003cblockquote class=\\\"rounded...\"],[\"\\u003c\\u002fblockquote\\u003e\\n    \\u003cdiv class=\\\"flex items-center gap-4\\\"\\u003e\\n      \\u003cimg src=\\\"https:\\u002f\\u002favatars.githubuserco...\"],[\"\\u003cp class=\\\"font-medium\\\"\\u003emmagic\\u003c\\u002fp\\u003e\\n      \\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n  \\u003c\\u002fdiv\\u003e\\n  \\u003cdiv class=\\\"mb-8 sm:break-insid...\"],[\"We also collaborated with Google Cloud (who generously provided the compute) to provide technical gu...\"],[\"Besides these, a heartfelt shoutout to the following contributors who helped us ship some of the mos...\"],[\"Over the last year, we also saw many companies choosing to build their products on top of 🤗 Diffuser...\"],[\"## Looking forward\\n\\nAs we celebrate our first anniversary, we're grateful to our community and open-...\"],[\"--\\ntitle: \\\"Getting Started with Hugging Face Transformers for IPUs with Optimum\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"### Getting started with IPUs and Optimum\\n\\nLet’s use BERT as an example to help you get started with...\"],[\"#### Install Optimum Graphcore\\n\\nNow that your environment has all the Graphcore Poplar and PopTorch ...\"],[\"```\\n$ python3 run_qa.py \\\\\\n\\t--ipu_config_name=.\\u002f \\\\\\n\\t--model_name_or_path bert-base-uncased \\\\\\n\\t--datas...\"],[\"``` \\n\\t# Tokenizer check: this script requires a fast tokenizer.\\n\\tif not isinstance(tokenizer, PreTra...\"],[\"```\\n\\\"epoch\\\": 3.0,\\n\\\"train_loss\\\": 0.9465060763888888,\\n\\\"train_runtime\\\": 368.4015,\\n\\\"train_samples\\\": 8852...\"],[\"--\\ntitle: Deprecation of Git Authentication using password\\nthumbnail: \\u002fblog\\u002fassets\\u002fpassword-git-depr...\"],[\"After generating your access token, you can update your Git repository using the following commands:...\"],[\"--\\ntitle: \\\"Finetune Stable Diffusion Models with DDPO via TRL\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f166_trl_ddpo...\"],[\"## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to ...\"],[\"The two orders of approximation have a significant impact on both performance and the ability to han...\"],[\"## DDPO and RLHF: a mix to enforce aestheticness\\n\\nThe general training aspect of [RLHF](https:\\u002f\\u002fhugg...\"],[\"We keep these steps in mind while moving on to actually getting these running which is described in ...\"],[\"There is only one commandline flag argument that is required of the user to get things up and runnin...\"],[\"## Lessons learned\\n\\n1. The results seem to generalize over a wide variety of prompts despite the min...\"],[\"## Results\\n\\nThe following are pre-finetuned (left) and post-finetuned (right) outputs for the prompt...\"],[\"`trl` library's `DDPOTrainer` implements DDPO for finetuning SD models.\\n\\nOur experimental findings u...\"],[\"--\\ntitle: \\\"Fine-Tune Whisper For Multilingual ASR with 🤗 Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f111_...\"],[\"## Introduction\\n\\nWhisper is a pre-trained model for automatic speech recognition (ASR) \\npublished in...\"],[\"When scaled to 680,000 hours of labelled pre-training data, Whisper models \\ndemonstrate a strong abi...\"],[\"In a sequence-to-sequence model, the encoder transforms the audio inputs \\ninto a set of hidden state...\"],[\"| Size   | Layers | Width | Heads | Parameters | English-only                                       ...\"],[\"## Fine-tuning Whisper in a Google Colab\\n\\n### Prepare Environment\\n\\nWe'll employ several popular Pyth...\"],[\"```python\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()\\n```\\n\\n**Print Output:**\\n```bas...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"assets\\u002f111_fine_tune_whisper\\u002fselect_hi.jpg\\\" alt=\\\"Trulli\\\" style=\\\"width:100%\\\"\\u003e\\n\\u003c\\u002ffi...\"],[\"Most ASR datasets only provide input audio samples (`audio`) and the \\ncorresponding transcribed text...\"],[\"Since speech is continuous, it contains an infinite number of amplitude values.\\nThis poses problems ...\"],[\"The Whisper feature extractor performs two operations. It first pads\\u002ftruncates a batch of audio samp...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"assets\\u002f111_fine_tune_whisper\\u002fspectrogram.jpg\\\" alt=\\\"Trulli\\\" style=\\\"width:100%\\\"\\u003e\\n\\u003cf...\"],[\"The Whisper tokenizer is pre-trained on the transcriptions for the 96 pre-training languages.\\nConseq...\"],[\"print(f\\\"Input:                 {input_str}\\\")\\nprint(f\\\"Decoded w\\u002f special:    {decoded_with_special}\\\")...\"],[\"```python\\nfrom transformers import WhisperProcessor\\n\\nprocessor = WhisperProcessor.from_pretrained(\\\"o...\"],[\"common_voice = common_voice.cast_column(\\\"audio\\\", Audio(sampling_rate=16000))\\n```\\n\\nRe-loading the fir...\"],[\"We can apply the data preparation function to all of our training examples using dataset's `.map` me...\"],[\"Once we've fine-tuned the model, we will evaluate it on the test data to verify that we have correct...\"],[\"```python\\nimport torch\\n\\nfrom dataclasses import dataclass\\nfrom typing import Any, Dict, List, Union\\n...\"],[\"```python\\nimport evaluate\\n\\nmetric = evaluate.load(\\\"wer\\\")\\n```\\n\\nWe then simply have to define a functi...\"],[\"```python\\nfrom transformers import WhisperForConditionalGeneration\\n\\nmodel = WhisperForConditionalGen...\"],[\"```python\\nmodel.config.forced_decoder_ids = None\\nmodel.config.suppress_tokens = []\\n```\\n\\n### Define t...\"],[\"**Note**: if one does not want to upload the model checkpoints to the Hub, \\nset `push_to_hub=False`....\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"assets\\u002f111_fine_tune_whisper\\u002fhf_speech_bench.jpg\\\" alt=\\\"Trulli\\\" style=\\\"width:100%\\\"...\"],[\"model = WhisperForConditionalGeneration.from_pretrained(\\\"sanchit-gandhi\\u002fwhisper-small-hi\\\")\\nprocessor...\"],[\"iface.launch()\\n```\\n\\n## Closing Remarks\\n\\nIn this blog, we covered a step-by-step guide on fine-tuning...\"],[\"--\\ntitle: \\\"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\\\" \\nthumbnail...\"],[\"**\\u003cspan style=\\\"text-decoration:underline;\\\"\\u003eTable of contents:\\u003c\\u002fspan\\u003e**\\n* **\\u003cspan style=\\\"text-decorat...\"],[\"1. **lock in** behaviors in time and hinder social progress [from being reflected in technology](htt...\"],[\"**These issues are deeply personal** for many of us ML researchers and developers at Hugging Face an...\"],[\"While our own experiences do not come close to covering the myriad ways in which ML-mediated discrim...\"],[\"## Putting Bias in Context\\n\\nThe first and maybe most important concept to consider when dealing with...\"],[\"Now let’s dive deeper into the issue of linking biases in stand-alone\\u002fcontext-less ML artifacts to s...\"],[\"1. \\u003cspan style=\\\"text-decoration:underline;\\\"\\u003eThe model is integrated into a website creation service\\u003c...\"],[\"* In cases like this one, there may be no level of bias mitigation that makes the risk acceptable. I...\"],[\"So, who’s on the hook for machine biases in ML? These three cases illustrate one of the reasons why ...\"],[\"In the next section, we review these various stages along with some of the tools that can help us ad...\"],[\"For example, let’s go back to one of the first highly-publicized cases of a Machine Learning system ...\"],[\"So what does this have to do with bias? Doesn’t showing people content that they’re likely to enjoy ...\"],[\"This example serves to illustrate that the impact of machine biases in an ML-supported product depen...\"],[\"#### Task definition: recommendations\\n\\nThere are as many ways for the ML task definition and deploym...\"],[\"You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it come...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"#### Dataset selection\\u002fcuration: recommendations\\n\\nThese tools aren’t full solutions by themselves, r...\"],[\"Model cards were originally proposed by [(Mitchell et al., 2019)](https:\\u002f\\u002fdl.acm.org\\u002fdoi\\u002f10.1145\\u002f328...\"],[\"Documentation is a great first step for sharing general insights about a model’s behavior, but it is...\"],[\"Visualization of model outputs isn’t just for generative models though! For classification models, w...\"],[\"Finally, a few benchmarks exist that can measure bias-related phenomena in models. For language mode...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"## Conclusion and Overview of Bias Analysis and Documentation Tools from 🤗\\n\\nAs we learn to leverage ...\"],[\"Summary of linked tools:\\n* Tasks:\\n    * Explore our directory of [ML Tasks](https:\\u002f\\u002fhuggingface.co\\u002ft...\"],[\"Thanks for reading! 🤗\\n\\n~ Yacine, on behalf of the Ethics and Society regulars\\n    \\nIf you want to ci...\"],[\"--\\ntitle: 'Distributed Training: Train BART\\u002fT5 for Summarization using 🤗 Transformers and Amazon Sag...\"],[\"listed again here:\\n\\n- [🤗 Transformers Documentation: Amazon SageMaker](https:\\u002f\\u002fhuggingface.co\\u002ftransf...\"],[\"As [distributed training strategy](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002fsagemaker.html#distributed-tr...\"],[\"---\\n\\n## Set up a development environment and install sagemaker\\n\\nAfter our SageMaker Notebook Instanc...\"],[\"***Note**: you can use this tutorial as-is to train your model on a different examples script.*\\n\\nSin...\"],[\"# configuration for running training on smdistributed Data Parallel\\ndistribution = {'smdistributed':...\"],[\"```python\\n# starting the training job\\nhuggingface_estimator.fit()\\n```\\n```bash\\n2021-04-01 13:00:35 St...\"],[\"# unzip model\\ntar = tarfile.open(f\\\"{local_path}\\u002fmodel.tar.gz\\\", \\\"r:gz\\\")\\ntar.extractall(path=local_pat...\"],[\"```python\\nimport json\\n\\nMODEL_CARD_TEMPLATE = \\\"\\\"\\\"\\n---\\nlanguage: en\\ntags:\\n- sagemaker\\n- bart\\n- summari...\"],[\"## `{model_name}`\\n\\nThis model was trained using Amazon SageMaker and the new Hugging Face Deep Learn...\"],[\"with open(f\\\"{local_path}\\u002fREADME.md\\\", \\\"w\\\") as f:\\n    f.write(model_card)\\n```\\n\\nAfter we have our unzip...\"],[\"--\\ntitle: \\\"SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"\\u003cp\\u003e🏎 \\u003cstrong\\u003eFast to train:\\u003c\\u002fstrong\\u003e SetFitABSA requires only a handful of labeled training samples;...\"],[\"* **Training sentence:** \\\"Waiters aren't friendly but the cream pasta is out of this world.\\\"\\n* **Tok...\"],[\"| Text                                                                          | Label |\\n|:--------...\"],[\"Note that as opposed to the aspect extraction model, we don't include non-aspects in this training s...\"],[\"Note that for the SB1 task, SetFitABSA is 110M parameters, for SB2 it is 110M parameters, and for SB...\"],[\"We continue by preparing the training set. The format of the training set is a `Dataset` with the co...\"],[\"```python\\nfrom datasets import load_dataset\\nfrom setfit import AbsaTrainer, AbsaModel\\n\\n# Create a tr...\"],[\"print(preds)\\n# [\\n#     [{'span': 'pizza', 'polarity': 'positive'}],\\n#     [{'span': 'food variations...\"],[\"--\\ntitle: \\\"Boosting Wav2Vec2 with n-grams in 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f44_boost_wav2ve...\"],[\"Previously audio classification models required an additional language\\nmodel (LM) and a dictionary t...\"],[\"We start by:\\n\\n1.  How does decoding audio with an LM differ from decoding audio\\n    without an LM?\\n2...\"],[\"We can pick one of the 73 audio samples and listen to it.\\n\\n```python\\naudio_sample = dataset[2]\\naudio...\"],[\"First, we need to install `pyctcdecode` and `kenlm`.\\n\\n```bash\\npip install https:\\u002f\\u002fgithub.com\\u002fkpu\\u002fken...\"],[\"OK, let's run the decoding step again. `pyctcdecode` language model\\ndecoder does not automatically c...\"],[\"The language model on its own most likely does favor the correct word\\n*roast* since the word sequenc...\"],[\"As always a language model is only as good as the data it is trained on.\\nIn the case of speech recog...\"],[\"A dataset that seems sensible here and which is relatively clean and\\neasy to pre-process is\\n[europar...\"],[\"```python\\ndataset = dataset.map(extract_text, remove_columns=dataset.column_names)\\n```\\n\\nGreat. Let's...\"],[\"Looking again at Table 9 of Appendix C of the [official Wav2Vec2 paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2006.1...\"],[\"Great, let's see step-by-step how to build an *n-gram*. We will use the\\npopular [KenLM library](http...\"],[\"**Output:**\\n```bash\\n    === 1\\u002f5 Counting and sorting n-grams ===\\n    Reading \\u002fcontent\\u002fswedish_text.t...\"],[\"probing 2195 assuming -r models -p 1.5\\n    trie     922 without quantization\\n    trie     518 assumi...\"],[\"Great, we have built a *5-gram* LM! Let's inspect the first couple of\\nlines.\\n\\n```bash\\nhead -20 5gram...\"],[\"```bash\\nhead -20 5gram_correct.arpa\\n```\\n\\n**Output:**\\n```bash\\n    \\\\data\\\\\\n    ngram 1=360209\\n    ngram...\"],[\"```python\\nfrom pyctcdecode import build_ctcdecoder\\n\\ndecoder = build_ctcdecoder(\\n    labels=list(sort...\"],[\"Let's inspect the local repository. The `tree` command conveniently can\\nalso show the size of the di...\"],[\"9 directories, 34 files\\n```\\n\\nAs can be seen the *5-gram* LM is quite large - it amounts to more than...\"],[\"```bash\\nrm xls-r-300m-sv\\u002flanguage_model\\u002f5gram_correct.arpa && tree -h xls-r-300m-sv\\u002f\\n```\\n\\n**Output:*...\"],[\"9 directories, 34 files\\n```\\n\\nNice, we reduced the *n-gram* by more than half to less than 2GB now. I...\"],[\"--\\ntitle: \\\"Hugging Face Machine Learning Demos on arXiv\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002farxiv\\u002fthumbnail.pn...\"],[\"![An interactive demo of a protein structure model, available on Hugging Face Spaces](\\u002fblog\\u002fassets\\u002fa...\"],[\"--\\ntitle: \\\"Illustrating Reinforcement Learning from Human Feedback (RLHF)\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"Writing a loss function to capture these attributes seems intractable and most language models are s...\"],[\"1. Pretraining a language model (LM),\\n2. gathering data and training a reward model, and\\n3. fine-tun...\"],[\"Next, with a language model, one needs to generate data to train a **reward model**, which is how hu...\"],[\"Human annotators are used to rank the generated text outputs from the LM. One may initially think th...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"Let's first formulate this fine-tuning task as a RL problem. First, the **policy** is a language mod...\"],[\"The reward function is where the system combines all of the models we have discussed into one RLHF p...\"],[\"Finally, the **update rule** is the parameter update from PPO that maximizes the reward metrics in t...\"],[\"# Open-source tools for RLHF\\n\\nThe first [code](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002flm-human-preferences) relea...\"],[\"Both TRLX and RL4LMs are under heavy further development, so expect more features beyond these soon....\"],[\"Generating well-written human text answering specific prompts is very costly, as it often requires h...\"],[\"With these limitations, huge swaths of unexplored design options could still enable RLHF to take sub...\"],[\"We hosted a lecture on Tuesday 13 December 2022 that expanded on this post; you can watch it [here](...\"],[\"And here is a snapshot of the growing set of \\\"key\\\" papers that show RLHF's performance for LMs:\\n- [F...\"],[\"- [Scaling Laws for Reward Model Overoptimization](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2210.10760) (Gao et al. 202...\"],[\"The field is the convergence of multiple fields, so you can also find resources in other areas:\\n* Co...\"],[\"--\\ntitle: 🧨 Stable Diffusion  in JAX \\u002f Flax !\\nthumbnail: \\u002fblog\\u002fassets\\u002f108_stable_diffusion_jax\\u002fthumb...\"],[\"*Output*:\\n```bash \\n    Found 8 JAX devices of type TPU v2.\\n```\\n\\n\\n\\nMake sure `diffusers` is installed...\"],[\"Flax weights are available in Hugging Face Hub as part of the Stable Diffusion repo. The Stable Diff...\"],[\"``` python\\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\\n    \\\"CompVis\\u002fstable-diffu...\"],[\"We are almost ready to generate images! We just need to create a random number generator to pass to ...\"],[\"``` python\\nimages = pipeline(prompt_ids, p_params, rng, jit=True)[0]\\n```\\n\\n*Output*:\\n```bash \\n    CPU...\"],[\"![png](assets\\u002f108_stable_diffusion_jax\\u002fjax_stable_diffusion_2.png)\\n\\n\\n-------------------------------...\"],[\"We can code `_generate` completely ignoring the fact that it will be invoked in parallel. We just ca...\"],[\"--\\ntitle: 'Pre-Train BERT with Hugging Face Transformers and Habana Gaudi'\\nthumbnail: \\u002fblog\\u002fassets\\u002f9...\"],[\"**Requirements**\\n\\nBefore we start, make sure you have met the following requirements\\n\\n* AWS Account ...\"],[\"```bash\\n“Dang! I’m out fishing and a huge trout just [MASK] my line!”\\n```\\nRead more about Masked Lan...\"],[\"```python\\nfrom huggingface_hub import HfApi\\n\\nuser_id = HfApi().whoami()[\\\"name\\\"]\\n\\nprint(f\\\"user id '{u...\"],[\"```python\\nfrom tqdm import tqdm\\nfrom transformers import BertTokenizerFast\\n\\n# repositor id for savin...\"],[\"# preprocess dataset\\ntokenized_datasets = raw_datasets.map(group_texts, batched=True, remove_columns...\"],[\"```python\\n# push dataset to hugging face\\ndataset_id=f\\\"{user_id}\\u002fprocessed_bert_dataset\\\"\\ntokenized_da...\"],[\"# Initialize our Trainer\\n-trainer = Trainer(\\n+trainer = GaudiTrainer(\\n    model=model,\\n    args=trai...\"],[\"```\\n\\nWe can start our training by creating a `EC2RemoteRunner` and then `launch` it. This will then ...\"],[\"Meaning if we want to do a full pre-training it would take around 125h hours (12,5 hours * 10) and w...\"],[\"We compared our implementation with the [fastest BERT-pretraining](https:\\u002f\\u002fwww.deepspeed.ai\\u002fTutorial...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Sasha Luccioni\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f69_sasha_luccioni_inte...\"],[\"### Thank you so much for joining us today, we are so excited to have you on!\\n\\n**Sasha:** I'm really...\"],[\"### What are you most excited about that you're working on now?\\n\\n**Sasha:** I think the [Big Science...\"],[\"### Love the idea of evaluating the carbon footprint of an email!?\\n\\n**Sasha:** Yeah, people did it, ...\"],[\"So first we just [created this online calculator](https:\\u002f\\u002fmlco2.github.io\\u002fimpact\\u002f) where someone cou...\"],[\"For example; France is mostly nuclear, mostly energy, and Canada has a lot of hydroelectric energy. ...\"],[\"### What are some of the ways that machine learning teams and engineers could be a bit more proactiv...\"],[\"**Sasha:** Yeah, we wrote a paper a couple of years ago that was a cool experience. It's almost a hu...\"],[\"Then instead of powering up a diesel generator which is cool because you can just power them up, and...\"],[\"### For people listening that are interested in this effort, but perhaps work at an organization whe...\"],[\"**Sasha:** Actually, machine learning people or AI people, in general, have this stereotype from oth...\"],[\"And I've participated in organizing workshops where people submit ideas that are super great on pape...\"],[\"### So sad. That's such a great story though and how there are opportunities like that.\\n\\n**Sasha:** ...\"],[\"**Sasha:** Yeah, there's this concept that my mom read about in some magazine ages ago when I was a ...\"],[\"For example, what I feel like we're doing at Hugging Face is really that machine learning needs more...\"],[\"### What other examples or applications do you find and see potential meaning in AI machine learning...\"],[\"### And you've talked before about the power of data and how it's not talked about enough.\\n\\n**Sasha:...\"],[\"### Wow. That is so interesting!\\n\\n**Sasha:** It's actually really, camera trap data is a really huge...\"],[\"**Sasha:** Yeah, I guess another anecdote, I have a lot of these anecdotes, but at some point we wan...\"],[\"So we completely canceled the hackathon, then instead we did, I think we call them data literacy or ...\"],[\"### Exactly, that's so interesting. That's so amazing that you were able to jump in there and provid...\"],[\"For example, what I was working on during my Ph.D. was how to help pick activities, like learning ac...\"],[\"I remember in grade 12, my final year of high school, my parents made me sign up for a math competit...\"],[\"And so I remember my little brother also was thinking of going to machine learning, and my dad was l...\"],[\"Then I remember I got pictures of flowers or something, and I got super into it. I was like yeah, se...\"],[\"They also invited me and my family to the opening, and I'm so excited to go there. You could actuall...\"],[\"Technology or events in general, we have more influence on them than we think by using Alexa, for ex...\"],[\"### What are you interested in right now? It could be anything, a movie, a recipe, a podcast, etc.?\\n...\"],[\"**Sasha:** My favorite currently, papers by a researcher or by [Abeba Birhane](https:\\u002f\\u002ftwitter.com\\u002fA...\"],[\"### Wow, we'll definitely be linking to that paper as well, so people can check that out. Yeah, very...\"],[\"**Sasha:** Yeah, something I'm working on outside of Big Science is on evaluation and how we evaluat...\"],[\"### Where can people find you online?\\n\\n**Sasha:** I'm on [Twitter @SashaMTL](https:\\u002f\\u002ftwitter.com\\u002fSas...\"],[\"--\\ntitle: Simple considerations for simple people building fancy neural networks\\nthumbnail: \\u002fblog\\u002fas...\"],[\"In reality, **building and training neural networks can often be an extremely frustrating experience...\"],[\"It is important to get a **high-level feeling (qualitative) of your dataset along with a fine-graine...\"],[\"*   How would a random predictor perform (especially in classification problems)? Dataset can be unb...\"],[\"\\u003e Pro-tip: in my experience working with pre-trained language models, freezing the embeddings module...\"],[\"As the loss decreases, you also want to look at the model’s predictions: either by evaluating on you...\"],[\"On average, experts use fewer resources to find better solutions.\\n\\nTo conclude, a piece of general a...\"],[\"--\\ntitle: \\\"Fine-tuning Llama 2 70B using PyTorch FSDP\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f160_fsdp_llama\\u002fthumb...\"],[\"## Challenges with fine-tuning LLaMa 70B\\n\\nWe encountered three main challenges when trying to fine-t...\"],[\"### Pre-requisites\\n\\nFirst follow these steps to install Flash Attention V2:  Dao-AILab\\u002fflash-attenti...\"],[\"```bash\\naccelerator.process_index=0 GPU Memory before entering the loading : 0\\naccelerator.process_i...\"],[\"Let’s create the accelerate config via below command:\\n```\\naccelerate config --config_file \\\"fsdp_conf...\"],[\"[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f...\"],[\"(Source: [link](https:\\u002f\\u002fgordicaleksa.medium.com\\u002feli5-flash-attention-5c44017022ad))\\n\\n **Tiling** is ...\"],[\"```\\naccelerate launch \\\\\\n    --config_file configs\\u002ffsdp_config.yaml \\\\\\n    --main_process_ip $MASTER_A...\"],[\"- Human: What is Deep Learning? Explain like a Pirate.\\n\\n+ Assistant: Arrr Grumete! Are ye lookin' fe...\"],[\"- Human: Now explain it like a chef.\\n\\n+ Assistant: Certainly! Here's an explanation of deep learning...\"],[\"The whole conversation is formatted as below: \\n```\\n\\u003c|system|\\u003e system message \\u003c|endoftext|\\u003e \\u003c|prompte...\"],[\"--\\ntitle: Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"In this blog post, we will outline the problems of optimizing Stable Diffusion models and propose a ...\"],[\"However, it turns out that the traditional model optimization methods, such as post-training 8-bit q...\"],[\"We used the [text-to-image fine-tuning example](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002ftraining\\u002ftext2...\"],[\"Recently, Facebook Research introduced a [Token Merging](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2210.09461) method fo...\"],[\"\\u003cdiv class=\\\"flex flex-row\\\"\\u003e\\n\\u003cdiv class=\\\"grid grid-cols-2 gap-4\\\"\\u003e\\n\\u003cfigure\\u003e\\n\\u003cimg class=\\\"max-w-full rou...\"],[\"Results of image generation [demo](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhelenai\\u002fstable_diffusion) using dif...\"],[\"Below we show how to perform inference with the final pipeline optimized to run on Intel CPUs:\\n\\n```p...\"],[\"## What about the general-purpose Stable Diffusion model?\\n\\nAs we showed with the Pokemon image gener...\"],[\"--\\ntitle: \\\"A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using tran...\"],[\"After completing the training of BLOOM-176B, we at HuggingFace and BigScience were looking for ways ...\"],[\"![Summary](assets\\u002f96_hf_bitsandbytes_integration\\u002ftf32-Mantissa-chart-hi-res-FINAL.png)\\n\\nFloat32 (FP3...\"],[\"In the machine learning jargon FP32 is called full precision (4 bytes), while BF16 and FP16 are refe...\"],[\"To remediate that, we introduce 8-bit quantization. This method uses a quarter precision, thus needi...\"],[\"![quantization](assets\\u002f96_hf_bitsandbytes_integration\\u002fquantization.png)\\n\\n(Image taken from: [this bl...\"],[\"These tricks can be combined in several ways, for example, row-wise or vector-wise quantization, whe...\"],[\"In essence, LLM.int8() seeks to complete the matrix multiplication computation in three steps:\\n1. Fr...\"],[\"### Inside the MatMul\\n\\nOnce the hidden states are computed we extract the outliers using a custom th...\"],[\"We ran several common benchmarks with the 8-bit and native models using lm-eval-harness and reported...\"],[\"We indeed observe 0 performance degradation for those models since the absolute difference of the me...\"],[\"| Precision      | Number of parameters | Hardware     | Time per token in milliseconds for Batch Si...\"],[\"| int8           | 3B                   | 1xT4 15GB    |                                            ...\"],[\"The 3 models are BLOOM-176B, T5-11B and T5-3B.\\n\\n### Hugging Face `transformers` integration nuances\\n...\"],[\"5. Now time to load your model in 8-bit!\\n\\n```py\\nint8_model.load_state_dict(torch.load(\\\"model.pt\\\"))\\ni...\"],[\"```py\\n(int8_model[0].weight.CB * int8_model[0].weight.SCB) \\u002f 127\\n```\\n\\nAnd you will get:\\n\\n```\\ntensor(...\"],[\"with init_empty_weights():\\n    model = nn.Sequential([nn.Linear(100000, 100000) for _ in range(1000)...\"],[\"This function recursively replaces all `nn.Linear` layers of a given model initialized on the `meta`...\"],[\"### Be very careful on how to set devices with `accelerate`\\n\\nHere we played a very delicate balancin...\"],[\"Now time to see how to benefit from this integration and how to successfully use it in `transformers...\"],[\"### Faster inference speed for smaller models\\n\\nAs we have seen in the [the benchmarking section](#is...\"],[\"### Scaling up on other modalities\\n\\nCurrently, language models dominate very large models. Leveragin...\"],[\"--\\ntitle: \\\"Speech Synthesis, Recognition, and More With SpeechT5\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fspeecht5\\u002ft...\"],[\"At the heart of SpeechT5 is a regular **Transformer encoder-decoder** model. Just like any other Tra...\"],[\"For the TTS task, the model uses the following pre-nets and post-nets:\\n\\n- **Text encoder pre-net.** ...\"],[\"processor = SpeechT5Processor.from_pretrained(\\\"microsoft\\u002fspeecht5_tts\\\")\\nmodel = SpeechT5ForTextToSpe...\"],[\"To convert the predicted log mel spectrogram into an actual speech waveform, we need a **vocoder**. ...\"],[\"You can play with an [interactive demo](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fMatthijs\\u002fspeecht5-tts-demo) on...\"],[\"```python\\nfrom datasets import load_dataset\\ndataset = load_dataset(\\\"hf-internal-testing\\u002flibrispeech_...\"],[\"The converted voice ([download](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fres...\"],[\"If you’ve tried any of the other 🤗 Transformers speech recognition models before, you’ll find Speech...\"],[\"Finally, tell the model to generate text tokens from the speech input, and then use the processor’s ...\"],[\"--\\ntitle: \\\"Code Llama: Llama 2 learns to code\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f160_codellama\\u002fthumbnail.jpg\\n...\"],[\"## What’s Code Llama?\\n\\nThe Code Llama release introduces a family of models of 7, 13, and 34 billion...\"],[\"For the instruction model, they used two datasets: the instruction tuning dataset collected for Llam...\"],[\"```bash\\n!pip install --upgrade transformers\\n```\\n#### A Note on dtypes\\n\\nWhen using models like Code L...\"],[\"sequences = pipeline(\\n    'def fibonacci(',\\n    do_sample=True,\\n    temperature=0.2,\\n    top_p=0.9,\\n...\"],[\"prompt = '''def remove_non_ascii(s: str) -\\u003e str:\\n    \\\"\\\"\\\" \\u003cFILL_ME\\u003e\\n    return result\\n'''\\n\\ninput_ids ...\"],[\"```\\n\\u003cs\\u003e[INST] \\u003c\\u003cSYS\\u003e\\u003e\\n{{ system_prompt }}\\n\\u003c\\u003c\\u002fSYS\\u003e\\u003e\\n\\n{{ user_msg_1 }} [\\u002fINST] {{ model_answer_1 }} \\u003c\\u002f...\"],[\"```python\\nsystem = \\\"System prompt\\\"\\nuser_1 = \\\"user_prompt_1\\\"\\nanswer_1 = \\\"answer_1\\\"\\nuser_2 = \\\"user_pro...\"],[\"You can try out Text Generation Inference on your own infrastructure, or you can use Hugging Face's ...\"],[\"![VS Code extension](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fb...\"],[\"| Model                  | License            | Dataset known | Commercial use? | Pretraining length...\"],[\"| StarCoder-15B          | BigCode-OpenRail-M | ✅             | ✅               | 1,035B            ...\"],[\"**Note:** The scores presented in the table above are sourced from our code leaderboard, where we ev...\"],[\"--\\ntitle: \\\"Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU plat...\"],[\"## Supported hardware platforms\\n\\nOn the GPU side, AMD and Hugging Face will first collaborate on the...\"],[\"## The road ahead\\n\\nOur initial focus will be ensuring the models most important to our community wor...\"],[\"--\\ntitle: \\\"Accelerating over 130,000 Hugging Face models with ONNX Runtime\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"| Model Architecture | Approximate No. of Models |\\n|:------------------:|:-------------------------:...\"],[\"--\\ntitle: \\\"AI Speech Recognition in Unity\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002funity-asr-thumbn...\"],[\"Then, use the `Start()` method to set up listeners for the start and stop buttons:\\n```\\nprivate void ...\"],[\"Finally, we'll need to implement the `EncodeAsWAV()` method, to prepare the audio data for the Huggi...\"],[\"private void StartRecording() {\\n        clip = Microphone.Start(null, false, 10, 44100);\\n        rec...\"],[\"### 4. Speech Recognition\\n\\nNext, we'll want to use the Hugging Face Unity API to run speech recognit...\"],[\"private void Update() {\\n        if (recording && Microphone.GetPosition(null) \\u003e= clip.samples) {\\n   ...\"],[\"private byte[] EncodeAsWAV(float[] samples, int frequency, int channels) {\\n        using (var memory...\"],[\"--\\ntitle: 'Building a Playlist Generator with Sentence Transformers'\\nthumbnail: \\u002fblog\\u002fassets\\u002f87_play...\"],[\"As we’ve explored in [previous posts on the Hugging Face blog](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fgetting-s...\"],[\"[The ST documentation highlights many of the choices](https:\\u002f\\u002fwww.sbert.net\\u002fdocs\\u002fpretrained_models.h...\"],[\"```python\\nfrom sentence_transformers import SentenceTransformer\\nimport pickle\\n\\nembedder = SentenceTr...\"],[\"Since we’re searching for any verse that matches the text prompt, there’s a good chance that the sem...\"],[\"The Gradio Blocks API lets you build *multi-step* interfaces, which means that you’re free to create...\"],[\"In that function, we use the text prompt to conduct the semantic search. As seen above, to push upda...\"],[\"While the song *lyrics* aren’t being released, I’ve **[published the verse embeddings along with the...\"],[\"--\\ntitle: \\\"Accelerating Vision-Language Models: BridgeTower on Habana Gaudi2\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"## BridgeTower\\n\\nIn the recent past, [Vision-Language (VL) models](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fvision...\"],[\"[Nvidia A100 Tensor Core GPU](https:\\u002f\\u002fwww.nvidia.com\\u002fen-us\\u002fdata-center\\u002fa100\\u002f) includes the 3rd gener...\"],[\"Hyperparameters are the same for all accelerators. We used a batch size of 48 samples for each devic...\"],[\"Let's run the three following experiments:\\n- a mixed-precision (*bfloat16*\\u002f*float32*) run distribute...\"],[\"Tensorboard logs can be visualized [here](https:\\u002f\\u002fhuggingface.co\\u002fregisss\\u002fbridgetower-newyorker-gaudi...\"],[\"### Hardware-accelerated data loading with Optimum Habana\\n\\nFor even larger speedups, we are now goin...\"],[\"\\u003c!-- To achieve this on Gaudi2, Habana's media pipeline enables us to:\\n- Initialize a media pipeline...\"],[\"We got an additional x1.10 speedup compared to the previous run with `dataloader_num_workers=2` only...\"],[\"```bash\\npip install optimum[habana]\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-habana.git\\ncd o...\"],[\"Note that `--mediapipe_dataloader` is compatible with Gaudi2 only and will not work with A100\\u002fH100.\\n...\"],[\"### Related Topics\\n\\n- [Faster Training and Inference: Habana Gaudi-2 vs Nvidia A100 80GB](https:\\u002f\\u002fhu...\"],[\"--\\ntitle: \\\"Introducing 🤗 Accelerate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f20_accelerate_library\\u002faccelerate_diff.p...\"],[\"output = model(source)\\n          loss = F.cross_entropy(output, targets)\\n\\n-         loss.backward()\\n...\"],[\"model = torch.nn.Transformer().to(device)\\n+ model = DistributedDataParallel(model)  \\n  optim = torch...\"],[\"```python\\nmodel, optim, data = accelerator.prepare(model, optim, data)\\n```\\n\\nThis is the main bulk of...\"],[\"```python\\naccelerator.backward(loss)\\n```\\n\\nThis last line adds the necessary steps for the backward p...\"],[\"### One launcher to rule them all\\n\\nThe scripts using Accelerate will be completely compatible with y...\"],[\"--\\ntitle: \\\"Interactively explore your Huggingface dataset with one line of code\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"Data inspection is a very important task in almost all ML development stages, but it can also be ver...\"],[\"## **Leveraging model results for data inspection**\\n\\nExploring raw unstructured datasets often yield...\"],[\"features = datasets.Features({**ds.features, \\\"prediction\\\": ds.features[\\\"fine_label\\\"], \\\"embedding\\\": d...\"],[\"## Using Spotlight on the Hugging Face hub\\n\\nYou can use Spotlight directly on your local NLP, audio,...\"],[\"--\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthu...\"],[\"[Hosting over 200,000 open-source models](https:\\u002f\\u002fhuggingface.co\\u002fmodels), and serving over 1 million...\"],[\"Deploying Hugging Face models on Azure Machine Learning has never been easier:\\n* Open the Hugging Fa...\"],[\"The Hugging Face Blog Repository 🤗\\nThis is the official repository of the [Hugging Face Blog](https:...\"],[\"# Train your first Decision Transformer\\n\\nYour content here [...]\\n```\\n\\nWhen published, the Hub will i...\"],[\"--\\ntitle: Training CodeParrot 🦜 from Scratch\\nthumbnail: \\u002fblog\\u002fassets\\u002f40_codeparrot\\u002fthumbnail.png\\naut...\"],[\"## Initializing the Tokenizer and Model\\n\\nFirst we need a tokenizer. Let's train one specifically on ...\"],[\"Now, we initialize a new model. We’ll use the same hyperparameters as GPT-2 large (1.5B parameters) ...\"],[\"We are now ready to train! Let's use the `huggingface_hub` client library to clone the repository wi...\"],[\"```Python\\nclass ConstantLengthDataset(IterableDataset):\\n    def __init__(\\n        self, tokenizer, d...\"],[\"```Python\\ndef create_dataloaders(args):\\n    ds_kwargs = {\\\"streaming\\\": True}\\n    train_data = load_da...\"],[\"```Python\\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\\n    model, opti...\"],[\"```Python\\n# Train model\\nmodel.train()\\ncompleted_steps = 0\\nfor step, batch in enumerate(train_dataloa...\"],[\"```Python\\n# Evaluate and save the last checkpoint\\nlogger.info(\\\"Evaluating and saving model after tra...\"],[\"But what is this _pass@k_ metric exactly? Simply put it measures the probability of at least one pro...\"],[\"# setup unit tests for is_even\\nimport unittest\\n```\\n\\n**Completion:**\\n```Python\\nclass TestIsEven(unitt...\"],[\"pipe = pipeline('text-generation', model='lvwerra\\u002fcodeparrot')\\npipe('def hello_world():')\\n```\\n\\n## Su...\"],[\"--\\ntitle: \\\"Optimizing Bark using 🤗 Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fbark_optimization\\u002fthumbnai...\"],[\"This blog post is organized as follows:\\n\\n## Table of Contents\\n\\n1.   A [reminder](#bark-architecture)...\"],[\"## Load the Model and its Processor\\n\\nThe pre-trained Bark small and large checkpoints can be loaded ...\"],[\"```python\\nimport torch\\nfrom transformers import set_seed\\n\\n\\ndef measure_latency_and_memory_use(model,...\"],[\"The output sounds like this ([download audio](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentat...\"],[\"The naive attention technique can be greatly optimized via a technique called [Flash Attention](http...\"],[\"## 2. Half-precision\\n\\nMost AI models typically use a storage format called single-precision floating...\"],[\"Why is this a problem? GPU memory is precious in AI, because it's where operations are fastest, and ...\"],[\"# convert to bettertransformer\\nmodel = BetterTransformer.transform(model, keep_original_model=False)...\"],[\"with torch.inference_mode():\\n  # samples are generated all at once\\n  speech_output = model.generate(...\"],[\"## How to read the results?\\n\\n### Latency\\n\\nIt measures the duration of a single call to the generatio...\"],[\"| absolute values               | Latency | Memory  | Throghput |\\n|-------------------------------|-...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights [Part 4]\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_ml_director_in...\"],[\"### [Javier Mansilla](https:\\u002f\\u002fwww.linkedin.com\\u002fin\\u002fjavimansilla\\u002f?originalSubdomain=ar) - Director of ...\"],[\"We even use ML to optimize the way we reserve and use infrastructure.\\n\\n\\n#### **2. What are the bigge...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f78_ml_director...\"],[\"Engineering is vast in its applications and can encompass a great many areas.  That said, more recen...\"],[\"2. Another critical challenge regarding ML in engineering is that the field is mainly categorized by...\"],[\"#### **3. What’s a common mistake you see people make when trying to integrate ML into Engineering?*...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f78_ml_director...\"],[\"#### **2. What are the biggest ML challenges within Education?**\\nI see MLOps technology as a key opp...\"],[\"#### **4. What excites you most about the future of ML?**\\nI’m excited about the opportunity to mento...\"],[\"#### **1. How has ML made a positive impact on SaaS?**\\nMachine learning has become truly operational...\"],[\"#### **3. What’s a common mistake you see people make trying to integrate ML into SaaS?**\\nTo get it ...\"],[\"---\\n\\n🤗   Thank you for joining us in this fourth installment of ML Director Insights. \\n \\nBig thanks ...\"],[\"--\\ntitle: \\\"Panel on Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fpanel-on-hugging-face\\u002fthumbnail.png\\nautho...\"],[\"Here are some notable features of Panel that our users find valuable. \\n\\n- Panel provides extensive s...\"],[\"--\\ntitle: \\\"Diffusion Models Live Event\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fdiffusion-models-event\\u002fthumbnail.png...\"],[\"\\u003cdiv\\n    class=\\\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\\\"\\n\\u003e\\n    \\u003cdiv class=\\\"text-center flex...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003cdiv class=\\\"text-center flex flex-col items-center\\\"\\u003e\\n        \\u003cimg src=\\\"https:\\u002f\\u002fhuggingfac...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003eApolinário Passos: \\u003cem\\u003eDALL-E 2 is cool but... what will come after the generative media ...\"],[\"--\\ntitle: \\\"Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\\\"\\nthumbnail:...\"],[\"[Habana Gaudi](https:\\u002f\\u002fhabana.ai\\u002ftraining\\u002f) training solutions, which power Amazon’s EC2 DL1 instanc...\"],[\"To learn how to get started training with Habana Gaudi, please visit [https:\\u002f\\u002fdeveloper.habana.ai](h...\"],[\"--\\ntitle: \\\"Fine-tune Llama 2 with DPO\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f157_dpo_trl\\u002fdpo_thumbnail.png\\nauthor...\"],[\"## DPO vs PPO\\n\\nIn the traditional model of optimising human derived preferences via RL, the goto met...\"],[\"1. a supervised fine-tuning (SFT) step\\n2. the process of annotating data with preference labels\\n3. t...\"],[\"dataset.map(\\n    return_prompt_and_responses,\\n    batched=True,\\n    remove_columns=original_columns\\n...\"],[\"The process as introduced above involves the supervised fine-tuning step using [QLoRA](https:\\u002f\\u002farxiv...\"],[\"```python\\nmodel = AutoPeftModelForCausalLM.from_pretrained(\\n    script_args.model_name_or_path, # lo...\"],[\"The WandB logs for the DPO training run can be found [here](https:\\u002f\\u002fwandb.ai\\u002fkrasul\\u002fhuggingface\\u002fruns...\"],[\"--\\ntitle: \\\"Welcome fastText to the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f147_fasttext\\u002fthumbnail....\"],[\"![text_classification_widget](assets\\u002f147_fasttext\\u002ffasttext_text_classification_widget.png)\\n![feature...\"],[\"## Would you like to integrate your library to the Hub?\\n\\nThis integration is possible thanks to our ...\"],[\"--\\ntitle: \\\"Introducing Decision Transformers on Hugging Face 🤗\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f58_decision-...\"],[\"Deep Reinforcement Learning agents **learn with batches of experience.** The question is, how do the...\"],[\"## Introducing Decision Transformers\\n\\nThe Decision Transformer model was introduced by [“Decision Tr...\"],[\"## Using the Decision Transformer in 🤗 Transformers\\n\\nThe Decision Transformer model is now available...\"],[\"The model performs an [autoregressive prediction](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fAutoregressive_model...\"],[\"`````python\\n# Function that gets an action from the model using autoregressive prediction \\n# with a ...\"],[\"In order to evaluate the model, we need some additional information; the mean and standard deviation...\"],[\"# take steps in the environment\\nfor t in range(max_ep_len):\\n\\t# add zeros for actions as input for th...\"],[\"## What’s next?\\n\\nIn the coming weeks and months, we plan on supporting other tools from the ecosyste...\"],[\"--\\ntitle: \\\"Course Launch Community Event\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f34_course_launch\\u002fspeakers_day1_thu...\"],[\"## Day 1 (November 15th): A high-level view of Transformers and how to train them\\n\\nThe first day of ...\"],[\"\\u003cdiv\\n    class=\\\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\\\"\\n\\u003e\\n    \\u003cdiv class=\\\"text-center flex...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f34_course_launch\\u002fmeg_mitchell.png\\\" width=50% style=\\\"border-radius: 50%;\\\"\\u003e\\n   ...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003eJakob Uszkoreit: \\u003cem\\u003eIt Ain&#39;t Broke So \\u003cdel\\u003eDon&#39;t Fix\\u003c\\u002fdel\\u003e Let&#39;s Break It\\u003c\\u002fe...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003cdiv class=\\\"text-center flex flex-col items-center\\\"\\u003e\\n        \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f34_co...\"],[\"## Day 2 (November 16th): The tools you will use\\n\\nDay 2 will be focused on talks by the Hugging Face...\"],[\"\\u003cdiv\\n    class=\\\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\\\"\\n\\u003e\\n    \\u003cdiv class=\\\"text-center flex...\"],[\"\\u003cp\\u003e\\u003cstrong\\u003eSylvain Gugger: \\u003cem\\u003eSupercharge your PyTorch training loop with 🤗 Accelerate\\u003c\\u002fem\\u003e\\u003c\\u002fstrong...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003cdiv class=\\\"text-center flex flex-col items-center\\\"\\u003e\\n        \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f34_co...\"],[\"--\\ntitle: 'Accelerated Inference with Optimum and Transformers Pipelines'\\nthumbnail: \\u002fblog\\u002fassets\\u002f66...\"],[\"Let's get started! 🚀\\n\\n## 1. What is Optimum? An ELI5\\n\\n[Hugging Face Optimum](https:\\u002f\\u002fgithub.com\\u002fhugg...\"],[\"```diff\\nfrom transformers import AutoTokenizer, pipeline\\n-from transformers import AutoModelForQuest...\"],[\"In this End-to-End tutorial on accelerating RoBERTa for question-answering, you will learn how to:\\n\\n...\"],[\"# load vanilla transformers and convert to onnx\\nmodel = ORTModelForQuestionAnswering.from_pretrained...\"],[\"To test performance we can use the `ORTModelForQuestionAnswering` class again and provide an additio...\"],[\"We can now compare this model size as well as some latency performance\\n\\n```python\\nimport os\\n# get mo...\"],[\"Nice! The model predicted the same answer.\\n\\n### 3.5 Run accelerated inference using Transformers pip...\"],[\"tokenizer = AutoTokenizer.from_pretrained(onnx_path)\\nmodel = ORTModelForQuestionAnswering.from_pretr...\"],[\"metric = load_metric(\\\"squad_v2\\\")\\ndataset = load_dataset(\\\"squad_v2\\\")[\\\"validation\\\"]\\n\\nprint(f\\\"length of...\"],[\"print(f\\\"vanilla model: exact={default_acc['exact']}% f1={default_acc['f1']}%\\\")\\nprint(f\\\"optimized mod...\"],[\"```python\\nfrom time import perf_counter\\nimport numpy as np\\n\\ndef measure_latency(pipe):\\n    latencies...\"],[\"- **Remote Models \\u003e 2GB:** Currently, only models smaller than 2GB can be loaded from the [Hugging F...\"],[\"You can find an example and instructions in our [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002f...\"],[\"---\\n\\nThanks for reading! If you are as excited as I am about accelerating Transformers, make them ef...\"],[\"--\\ntitle: \\\"Hugging Face Selected for the French Data Protection Agency Enhanced Support Program\\\"\\nthu...\"],[\"When it comes to respecting people’s privacy rights, the recent developments in ML and AI pose new q...\"],[\"--\\ntitle: Using Stable Diffusion with Core ML on Apple Silicon\\nthumbnail: \\u002fblog\\u002fassets\\u002fdiffusers_cor...\"],[\"Core ML supports all the compute units available in your device: CPU, GPU and Apple's Neural Engine ...\"],[\"Each model repo is organized in a tree structure that provides these different variants:\\n\\n```\\ncoreml...\"],[\"`\\u003coutput-mlpackages-directory\\u003e` should point to the checkpoint you downloaded in the step above, and...\"],[\"```Python\\nfrom huggingface_hub import snapshot_download\\nfrom pathlib import Path\\n\\nrepo_id = \\\"apple\\u002fc...\"],[\"## Next Steps\\n\\nWe are really excited about the opportunities this brings and can't wait to see what ...\"],[\"--\\ntitle: \\\"Exploring simple optimizations for SDXL\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fsimple_sdxl_optimization...\"],[\"pipe = StableDiffusionXLPipeline.from_pretrained(\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\").to(\\\"cud...\"],[\"With 🤗 Diffusers, you can use fp16 for inference by specifying the `torch.dtype` parameter to conver...\"],[\"pipe = StableDiffusionXLPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\",\\n  ...\"],[\"### Model CPU offloading\\n\\nModel offloading saves memory by loading the UNet into the GPU memory whil...\"],[\"This is where “slicing” is useful. The input tensor to be decoded is split into slices and the compu...\"],[\"call_args = dict(\\n\\t\\tprompt_embeds=prompt_embeds,\\n\\t\\tnegative_prompt_embeds=negative_prompt_embeds,\\n\\t\\t...\"],[\"## Conclusion\\n\\nTo conclude and summarize the savings from our optimizations:\\n\\n\\u003cdiv style=\\\"background...\"],[\"--\\ntitle: \\\"Fine-Tune ViT for Image Classification with 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f51_fi...\"],[\"To get started, let's first install both those packages.\\n\\n\\n```bash\\npip install datasets transformers...\"],[\"```python\\nlabels = ds['train'].features['labels']\\nlabels\\n```\\n\\n\\n\\n\\n    ClassLabel(num_classes=3, names...\"],[\"return grid\\n\\nshow_examples(ds, seed=random.randint(0, 1337), examples_per_class=3)\\n```\\n\\n\\n\\n\\u003cfigure cl...\"],[\"To process an image, simply pass it to the image processor's call function. This will return a dict ...\"],[\"You can directly apply this to the dataset using `ds.with_transform(transform)`.\\n\\n\\n```python\\nprepare...\"],[\"```python\\nimport numpy as np\\nfrom datasets import load_metric\\n\\nmetric = load_metric(\\\"accuracy\\\")\\ndef ...\"],[\"What I'm trying to say is that you'll have a bad time if you forget to set `remove_unused_columns=Fa...\"],[\"```python\\nkwargs = {\\n    \\\"finetuned_from\\\": model.config._name_or_path,\\n    \\\"tasks\\\": \\\"image-classific...\"],[\"--\\ntitle: \\\"~Don't~ Repeat Yourself\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f59_transformers_philosophy\\u002ftransformers....\"],[\"No, we are not lazy - it's a very conscious decision not to apply the DRY design principle to the Tr...\"],[\"### 2. Modeling code is our product\\nWe assume that a significant amount of users of the Transformers...\"],[\"As an example, two years ago, one might have defined BERT's self attention layer as the standard att...\"],[\"Instead, research teams tend to publish a new model built upon previous models but rarely make signi...\"],[\"On the other hand, the modeling code of successor models can very well logically depend on its prede...\"],[\"### Drawbacks\\nClearly, there are also drawbacks to the single file policy two of which we quickly wa...\"],[\"### Conclusion\\nAll in all, at 🤗 Hugging Face we are convinced that the *single file policy* is the r...\"],[\"--\\ntitle: \\\"Introducing Würstchen: Fast Diffusion for Image Generation\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fwuer...\"],[\"![Würstchen images with Prompts](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"## How to use Würstchen?\\nYou can either try it using the Demo here:\\n\\n\\u003cscript type=\\\"module\\\" src=\\\"http...\"],[\"### Models on the Hub\\nAll checkpoints can also be seen on the [Huggingface Hub](https:\\u002f\\u002fhuggingface....\"],[\"## Optimisation Technique 1: Flash Attention\\n\\nStarting from version 2.0, PyTorch has integrated a hi...\"],[\"```python\\npipeline.prior_prior = torch.compile(pipeline.prior_prior , mode=\\\"reduce-overhead\\\", fullgr...\"],[\"--\\ntitle: Faster TensorFlow models in Hugging Face Transformers\\nthumbnail: \\u002fblog\\u002fassets\\u002f10_tf-servin...\"],[\"| Batch size | Google implementation | v4.2.0 implementation | Relative difference Google\\u002fv4.2.0 imp...\"],[\"### What is a SavedModel?\\n\\nA SavedModel contains a standalone TensorFlow model, including its weight...\"],[\"Below, you can find the inputs and outputs representations of a `TFBertForSequenceClassification` sa...\"],[\"# return the formated output\\n        return self.serving_output(output)\\n\\n# Instantiate the model wit...\"],[\"```python\\nfrom transformers import TFBertForSequenceClassification\\n\\nmodel = TFBertForSequenceClassif...\"],[\"# Load the model config of our SavedModel\\nconfig = BertConfig.from_pretrained(\\\"nateraw\\u002fbert-base-unc...\"],[\"# Tokenize the sentence but this time with TensorFlow tensors as output already batch sized to 1. Ex...\"],[\"# Print the proper LABEL with its index\\nprint(config.id2label[np.argmax(np.abs(output))])\\n```\\n\\n## Co...\"],[\"--\\ntitle: \\\"Object Detection Leaderboard\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fobject-detection-leaderboard\\u002fthumbn...\"],[\"So, let's embark on this exploration together and unlock the secrets of the Object Detection Leaderb...\"],[\"## What's Object Detection?\\n\\nIn the field of Computer Vision, Object Detection refers to the task of...\"],[\"The diversity of detectors goes beyond the range of output classes they can recognize. They vary in ...\"],[\"### What's Average Precision and how to compute it?\\n\\nAverage Precision (AP) is a single-number that ...\"],[\"Every box predicted by the model is considered a “positive” detection. The Intersection over Union (...\"],[\"\\u003cp style=\\\"text-align: center;\\\"\\u003e\\n\\\\\\\\( \\\\text{Recall} = \\\\frac{TP}{(TP + FN)} = \\\\frac{TP}{\\\\text{all groun...\"],[\"To demonstrate how to calculate the Average Precision plot, we'll use a practical example from one o...\"],[\"Note that by rule 2, in image 1, \\\"E\\\" is TP while \\\"D\\\" is FP because IoU between \\\"E\\\" and the ground-tr...\"],[\"Now, we can plot the Precision x Recall curve with the values, as shown in Figure 6:\\n\\n\\u003cdiv display=\\\"...\"],[\"The COCO approach, for instance, uses 101-interpolation, which computes 101 points for equally space...\"],[\"### What's Average Recall and how to compute it?\\n\\nAverage Recall (AR) is a metric that's often used ...\"],[\"\\u003cp style=\\\"text-align: center;\\\"\\u003e\\n\\\\\\\\( \\\\text{AP@[.5:.05:0.95} = \\\\frac{\\\\text{AP}_{0.5} + \\\\text{AP}_{0.55...\"],[\"To measure accuracy, we used 12 metrics involving Average Precision and Average Recall using [COCO s...\"],[\"If you want a model with good object recognition and objects generally in the right place, you can l...\"],[\"```python \\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL...\"],[\"To illustrate this process, let's consider the examples in Figure 9 and Figure 10. In Figure 9, we c...\"],[\"#### Ported models should output the same logits as the original models\\n\\nAt Hugging Face, we are ver...\"],[\"#### Calculating the IoU requires careful consideration\\n\\nIoU might seem straightforward to calculate...\"],[\"| Use Case                                     | Real-world Scenarios                  | Recommended...\"],[\"The results shown in our 🤗 [Object Detection Leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhf-vision\\u002fob...\"],[\"--\\ntitle: \\\"The Falcon has landed in the Hugging Face ecosystem\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f147_falcon\\u002f...\"],[\"## The Falcon models\\n\\nThe Falcon family is composed of two base models: [Falcon-40B](https:\\u002f\\u002fhugging...\"],[\"Falcon-7B and Falcon-40B have been trained on 1.5 trillion and 1 trillion tokens respectively, in li...\"],[\"| Model | License | Commercial use? | Pretraining length [tokens] | Pretraining compute [PF-days] | ...\"],[\"The video shows a lightweight app that leverages a Swift library for the heavy lifting: model loadin...\"],[\"And then, you'd run text generation using code like the following:\\n\\n```python\\nsequences = pipeline(\\n...\"],[\"There's also the possibility to use [4-bit loading](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002f4bit-transformers-bi...\"],[\"For 7B models, we advise you to select \\\"GPU [medium] - 1x Nvidia A10G\\\". \\n\\nFor 40B models, you will n...\"],[\"For the 7B models, we see that the base model is better than `llama-7b` and edges out MosaicML's `mp...\"],[\"Let's now take a look at how you can fine-tune your very own Falcon models - perhaps one of yours wi...\"],[\"More specifically, after selecting the target modules to adapt (in practice the query \\u002f key layers o...\"],[\"dataset = load_dataset(\\\"imdb\\\", split=\\\"train\\\")\\n\\nmodel_id = \\\"tiiuae\\u002ffalcon-7b\\\"\\n\\ntokenizer = AutoTokeni...\"],[\"--\\ntitle: \\\"Gradio 3.0 is Out!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f68_gradio_blocks\\u002fblock-party.png\\nauthors:\\n- u...\"],[\"Check out all the components you can use [on our (redesigned) docs](http:\\u002f\\u002fwww.gradio.app\\u002fdocs) 🤗!\\n\\n...\"],[\"Once you run `launch()`, the following demo will appear:\\n\\n\\u003cimg class=\\\"max-w-full mx-auto my-6\\\" style...\"],[\"--\\ntitle: \\\"Releasing Swift Transformers: Run On-Device LLMs in Apple Devices\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"Let's go!\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cvideo controls title=\\\"Llama 2 (7B) chat model running on an M1 MacB...\"],[\"## Tasks Overview\\n\\nWhen I published tweets showing [Falcon](https:\\u002f\\u002ftwitter.com\\u002fpcuenq\\u002fstatus\\u002f166460...\"],[\"- [Conversion to Core ML](#conversion-to-core-ml). We'll use Llama 2 as a real-life example.\\n- [Opti...\"],[\"## Conversion to Core ML\\n\\nCore ML is Apple's native framework for Machine Learning, and also the nam...\"],[\"2. Use [`exporters`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fexporters), a Python conversion package built on...\"],[\"- If you have to use `coremltools`, use the latest version: `7.0b1`. Despite technically being a bet...\"],[\"There are a few key optimization areas we've identified. They are a very important topic for us and ...\"],[\"### Tokenizers\\n\\nTokenization solves two complementary tasks: adapt text input to the tensor format u...\"],[\"In contrast, projects that use language models in other domains, such as Swift apps, usually resort ...\"],[\"Regarding models, we created a simple `LanguageModel` type as a wrapper for a Core ML model, focusin...\"],[\"Additional methods such as \\\"nucleus sampling\\\" will come later. We recommend [this blog post](https:\\u002f...\"],[\"## Missing Parts \\u002f Coming Next\\n\\nAs stated, we are just getting started! Our upcoming priorities incl...\"],[\"- Edge cases and type mismatches\\n\\nEven for supported PyTorch operations, it's very difficult to ensu...\"],[\"What the error screenshot is telling us is that there's a type mismatch trying to fill the mask tens...\"],[\"---\\ntitle: \\\"Making ML-powered web games with Transformers.js\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fml-web-games\\u002f...\"],[\"In our version, you'll have one minute to draw as many items as you can, one prompt at a time. If th...\"],[\"![MobileViT archtecture](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fma...\"],[\"## 2. Running in the browser with Transformers.js\\n\\n### What is Transformers.js?\\n\\n[Transformers.js](h...\"],[\"```bash\\ncd doodle-dash\\nnpm install\\nnpm install @xenova\\u002ftransformers\\n```\\n\\nYou can then start the deve...\"],[\"\\u002f\\u002f Attach the callback function as an event listener.\\n        worker.current.addEventListener('messa...\"],[\"### Taking advantage of real-time performance\\n\\nOne of the main advantages of performing in-browser i...\"],[\"In the spirit of open-source development, I decided to ask [Hugging Chat](https:\\u002f\\u002fhuggingface.co\\u002fcha...\"],[\"--\\ntitle: \\\"Deploy MusicGen in no time with Inference Endpoints\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002frun-musicge...\"],[\"Or simply use the final result and deploy our [custom MusicGen model repo](https:\\u002f\\u002fhuggingface.co\\u002fre...\"],[\"# take the first half of the audio sample\\nsample[\\\"array\\\"] = sample[\\\"array\\\"][: len(sample[\\\"array\\\"]) \\u002f...\"],[\"def __call__(self, data: Dict[str, Any]) -\\u003e Dict[str, str]:\\n        \\\"\\\"\\\"\\n        Args:\\n            da...\"],[\"![Create Endpoint](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblo...\"],[\"And voila! \\n\\nPlay with the demo below to try the endpoint out.\\n\\n\\u003cgradio-app theme_mode=\\\"light\\\" space...\"],[\"--\\n\\ntitle: \\\"Results of the Open Source AI Game Jam\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fgame-jam-first-edition-r...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fgame-ja...\"],[\"🤖 Backgrounds visuals were generated using Stable Diffusion\\n\\n🎮👉 https:\\u002f\\u002fvisionistx.itch.io\\u002fyabbit-at...\"],[\"🤖 Used embeddings from all-MiniLM-L6-v2 model and GloVe to generate the map.\\n\\n🎮👉 https:\\u002f\\u002fdanielquela...\"],[\"🤖 Soundful - Music generator\\n\\n🤖 Elevenlabs - Voice generator\\n\\n🤖 Scenario - Image generator\\n\\n🎮👉 https...\"],[\"---\\n\\nThe first-ever Open Source AI Game Jam proved to be an astounding success, exceeding our expect...\"],[\"Contrastive Search\\n\\nThis is a companion notebook to the [Hugging Face guest blog post entry about co...\"],[\"# prepare the prefix\\nprefix_text = r'DeepMind Company is'\\ninputs = tokenizer(prefix_text, return_ten...\"],[\"### 4.2. Example Two - OPT:\\n\\n\\n```python\\n# Load the language model and prepare the prefix text:\\nimpor...\"],[\"## 1. Environment Installation:\\n\\n\\n```python\\n! pip install tensorflow\\n# Constrastive Search for Tenso...\"],[\"# prepare the prefix\\nprefix_text = r'DeepMind Company is'\\ninputs = tokenizer(prefix_text, return_ten...\"],[\"### 4.2. Example Two - OPT:\\n\\n\\n```python\\n# Load the language model and prepare the prefix text:\\nimpor...\"],[\"--\\ntitle: 🧨 Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e\\nthumbnail: \\u002fblog\\u002fas...\"],[\"🧨 Diffusers JAX integration offers a convenient way to run SDXL on TPU via [XLA](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"#### JIT compilation\\n\\nA notable feature of JAX is its [just-in-time (jit) compilation](https:\\u002f\\u002fjax.r...\"],[\"TPU v5e instances come in multiple shapes, including 1, 4 and 8-chip shapes, all the way up to 256 c...\"],[\"We are now ready to set up our prompt and the rest of the pipeline inputs.\\n\\n```python\\ndefault_prompt...\"],[\"We are now ready to put everything together in a generate function:\\n\\n```python\\ndef generate(\\n    pro...\"],[\"It now took about 2s to generate the 4 images!\\n\\n## Benchmark\\n\\nThe following measures were obtained r...\"],[\"## How does the demo work?\\n\\nThe [demo we showed before](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgoogle\\u002fsdxl) w...\"],[\"--\\ntitle: \\\"Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia\\\"\\nthumbnail: \\u002f...\"],[\"The real value of AWS Inferentia instances compared to GPU comes through the multiple Neuron Cores a...\"],[\"## 1. Convert your Hugging Face Transformer to AWS Neuron\\n\\nWe are going to use the [AWS Neuron SDK f...\"],[\"```python\\nmodel_id = \\\"distilbert-base-uncased-finetuned-sst-2-english\\\"\\n```\\n\\nAt the time of writing, ...\"],[\"## 2. Create a custom `inference.py` script for `text-classification`\\n\\nThe [Hugging Face Inference T...\"],[\"# To use one neuron core per worker\\nos.environ[\\\"NEURON_RT_NUM_CORES\\\"] = \\\"1\\\"\\n\\n# saved weights name\\nAW...\"],[\"To do this we need to set up our permissions.\\n\\n```python\\nimport sagemaker\\nimport boto3\\nsess = sagema...\"],[\"```python\\nfrom sagemaker.huggingface.model import HuggingFaceModel\\n\\n# create Hugging Face Model Clas...\"],[\"The average latency for our BERT model is `5-6ms` for a sequence length of 128.\\n\\u003cbr\\u003e\\n\\u003cfigure class=\\\"...\"],[\"--\\ntitle: \\\"How to train your model dynamically using adversarial data\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f88_mn...\"],[\"![](https:\\u002f\\u002fi.imgur.com\\u002f1OiMHhE.png)\\n\\n\\u003e Image source: [mnist | Tensorflow Datasets](https:\\u002f\\u002fwww.tens...\"],[\"def forward(self, x):\\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\\n        x = F.relu(F.max_po...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fchrisjay-simple-mnist-classification.hf.space\\\" frameBorder=\\\"0\\\" width=\\\"100%\\\" hei...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fchrisjay-mnist-adversarial.hf.space\\\" frameBorder=\\\"0\\\" width=\\\"100%\\\" height=\\\"1400p...\"],[\"--\\ntitle: \\\"Rocket Money x Hugging Face: Scaling Volatile ML Models in Production​\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"## The Journey Toward a New System\\n\\nWe first extracted brands and products from transactions using r...\"],[\"## Solving Domain Challenges and Constraints by Partnering with Hugging Face\\n\\nThere are a number of ...\"],[\"In the beginning, we auditioned a hand-rolled, in-house model hosting solution we had been using for...\"],[\"Once the contract was signed, we began the migration of moving off our regex based system to direct ...\"],[\"The inaugural year of collaboration between Rocket Money and Hugging Face was not without its challe...\"],[\"Post launch, the internal Rocket Money team is now focusing on both class and performance tuning of ...\"],[\"--\\ntitle: \\\"Leveraging Hugging Face for complex generative AI use cases\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_m...\"],[\"--\\ntitle: \\\"ControlNet in 🧨 Diffusers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fcontrolnet\\u002fthumbnail.png \\nauthors:\\n- ...\"],[\"You can turn your sketch scribble into an artistic drawing.\\n\\n\\u003ctable\\u003e\\n\\u003ctr style=\\\"text-align: center;\\\"...\"],[\"Training ControlNet is comprised of the following steps:\\n\\n1. Cloning the pre-trained parameters of a...\"],[\"Similarly, if we were to condition ControlNet with semantic segmentation maps, a training sample wou...\"],[\"Before we begin, we want to give a huge shout-out to the community contributor [Takuma Mori](https:\\u002f...\"],[\"```bash\\npip install opencv-contrib-python\\npip install controlnet_aux\\n```\\n\\nWe will use the famous pai...\"],[\"Instead of using Stable Diffusion's default [PNDMScheduler](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fma...\"],[\"```py\\npipe.enable_model_cpu_offload()\\n```\\n\\nFinally, we want to take full advantage of the amazing [F...\"],[\"Finally, we can run the pipeline and display the image!\\n\\n```py\\noutput = pipe(\\n    prompt,\\n    canny_...\"],[\"It is noticeable that Mr Potato Head is not the best candidate but he tried his best and did a prett...\"],[\"```python\\ncontrolnet = ControlNetModel.from_pretrained(\\n    \\\"fusing\\u002fstable-diffusion-v1-5-controlnet...\"],[\"Prepare the conditioning\\n\\n```python\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimp...\"],[\"#### Running ControlNet with multiple conditionings\\n\\n```python\\nfrom diffusers import StableDiffusion...\"],[\"* [lllyasviel\\u002fsd-controlnet-depth](https:\\u002f\\u002fhuggingface.co\\u002flllyasviel\\u002fsd-controlnet-depth)\\n* [lllyasv...\"],[\"## Conclusion\\n\\nWe have been playing a lot with [`StableDiffusionControlNetPipeline`](https:\\u002f\\u002fhugging...\"],[\"--\\ntitle: Universal Image Segmentation with Mask2Former and OneFormer\\nthumbnail: \\u002fblog\\u002fassets\\u002f127_ma...\"],[\"Image segmentation can largely be split into 3 subtasks - instance, semantic and panoptic segmentati...\"],[\"Over the last years, researchers have come up with several architectures that were typically very ta...\"],[\"[Mask2Former](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fmodel_doc\\u002fmask2former) extends this to i...\"],[\"Note that Mask2Former still needs to be trained on each task separately to obtain state-of-the-art r...\"],[\"Next, let's load the familiar cats image from the COCO dataset, on which we'll perform inference.\\n\\n`...\"],[\"Let's visualize the results:\\n\\n```\\nfrom collections import defaultdict\\nimport matplotlib.pyplot as pl...\"],[\"The demo notebooks make use of `MaskFormerForInstanceSegmentation` to load the model whereas you'll ...\"],[\"--\\ntitle: \\\"A Dive into Text-to-Video Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f140_text-to-video\\u002fthumbnail.png...\"],[\"## Text-to-Video vs. Text-to-Image\\nWith so many recent developments, it can be difficult to keep up ...\"],[\"The text-to-video task faces unique challenges on multiple fronts. Some of these main challenges inc...\"],[\"Like the text-to-image task, early work on text-to-video generation dates back only a few years. Ear...\"],[\"Taking inspiration from the success of large-scale pretrained transformer models in text (GPT-3) and...\"],[\"The third and current wave of text-to-video models features predominantly diffusion-based architectu...\"],[\"Text2Video-Zero is a text-guided video generation and manipulation framework that works in a fashion...\"],[\"## Datasets\\nLike other vision-language models, text-to-video models are typically trained on large p...\"],[\"These large datasets experience similar issues to those found in text-to-image datasets. The most co...\"],[\"### Hugging Face Demos\\nAt Hugging Face, our goal is to make it easier to use and build upon state-of...\"],[\"\\u003cgradio-app theme_mode=\\\"light\\\" space=\\\"Tune-A-Video-library\\u002fTune-A-Video-Training-UI\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\n...\"],[\"## Conclusion\\nText-to-video research is progressing exponentially, but existing work is still limite...\"],[\"--\\ntitle: \\\"Stable Diffusion XL on Mac with Advanced Core ML Quantization\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fst...\"],[\"For Stable Diffusion XL we’ve done a few things:\\n* Ported the [base model to Core ML](https:\\u002f\\u002fhuggin...\"],[\"## Using SD XL Models from the Hugging Face Hub\\n\\nAs part of this release, we published two different...\"],[\"For reference, these are the performance figures we achieved on different devices:\\n\\n|        Device ...\"],[\"We explored a different alternative instead: **mixed-bit palettization**. Instead of using 6 bits pe...\"],[\"For visual examples, these are the results on prompt `a high quality photo of a surfing dog` running...\"],[\"## How are Mixed-Bit Recipes Created?\\n\\nThe following plot shows the signal strength (PSNR in dB) ver...\"],[\"The application phase simply goes over the recipe and applies palettization with the number of bits ...\"],[\"### Running Mixed-Bit Palettization\\n\\nAfter converting Stable Diffusion or Stable Diffusion XL models...\"],[\"### Published Resources\\n\\n* [`apple\\u002fml-stable-diffusion`](https:\\u002f\\u002fgithub.com\\u002fapple\\u002fml-stable-diffusio...\"],[\"--\\ntitle: \\\"An Introduction to Deep Reinforcement Learning\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f63_deep_rl_intro\\u002f...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f63_deep_rl_intro\\u002fOpenAIFive.j...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo\\n        alt=\\\"LunarLander\\\"\\n        sty...\"],[\"## **What is Reinforcement Learning?**\\n\\nTo understand Reinforcement Learning, let’s start with the b...\"],[\"### **A formal definition**\\n\\nIf we take now a formal definition:\\n\\n\\u003e Reinforcement learning is a fram...\"],[\"### **The reward hypothesis: the central idea of Reinforcement Learning**\\n\\n⇒ Why is the goal of the ...\"],[\"- *Observation o*: is a **partial description of the state.** In a partially observed environment.\\n\\n...\"],[\"To recap:\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f63_deep_rl_intro\\u002fac...\"],[\"Consequently, **the reward near the cat, even if it is bigger (more cheese), will be more discounted...\"],[\"These are tasks that continue forever (no terminal state). In this case, the agent must **learn how ...\"],[\"This is what we call the exploration\\u002fexploitation trade-off. We need to balance how much we **explor...\"],[\"Think of policy as the brain of our agent, the function that will tells us the action to take given ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg class=\\\"center\\\" src=\\\"assets\\u002f63_deep_rl_int...\"],[\"Thanks to our value function, at each step our policy will select the state with the biggest value d...\"],[\"That was a lot of information, if we summarize:\\n\\n- Reinforcement Learning is a computational approac...\"],[\"---\\nNow that you've studied the bases of Reinforcement Learning, you’re ready to train your first la...\"],[\"And don't forget to share with your friends who want to learn 🤗 !\\n\\nFinally, we want **to improve and...\"],[\"--\\ntitle: \\\"Scaling up BERT-like model Inference on modern CPU  - Part 2\\\"\\nauthors:\\n- user: echarlaix\\n...\"],[\"Back in April, Intel launched its [latest generation of Intel Xeon processors](https:\\u002f\\u002fwww.intel.com...\"],[\"In this area, Intel plays an essential role by providing software components under the oneAPI umbrel...\"],[\"## Deep Dive: Leveraging advanced Intel features to improve AI performances\\n\\n### Performance tuning ...\"],[\"While it’s very complex to apply generic optimizations to different object structures and layouts, t...\"],[\"Still, implementing parallel algorithms might not be as simple as throwing more cores to do the work...\"],[\"In every programmer toolkit, there are multiple levels which can bring mathematical operations suppo...\"],[\"## More Efficient AI Processing on latest Intel Ice Lake CPUs\\n\\nIn order to report the performances o...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"Last but not least, it is usually harder through these frameworks to enable graph optimizations such...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"The global trend highlights the positive impact of the number of cores on the observed latencies. \\nI...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"This is often referred to as “tracing” the graph and, as you can see here, the results are not that ...\"],[\"The default memory allocator strategies often rely on global memory pools which require the usage of...\"],[\"#### Memory allocator benchmarks\\n\\nAgain, we first compare performance against frameworks executing i...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"As per the graph above, you can notice that the standard library allocator (glibc) is often behind p...\"],[\"Now, back to the graph mode where we benchmark framework having an omniscient representation of the ...\"],[\"This time, by knowing the underlying structure of the operator flows and matrix shapes involved then...\"],[\"There are many libraries available which provide such higher-level features to accelerate the develo...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" ...\"],[\"- The number of cores: although using as many cores as you have is often a good idea, it does not al...\"],[\"\\u003ctable class=\\\"block mx-auto\\\"\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e\\n        \\u003cfigure class=\\\"image table text-center m-0 w-f...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e\\n        \\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n            \\u003cmed...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e\\n        \\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n            \\u003cmed...\"],[\"At Hugging Face, we are on a mission to democratize state-of-the-art Machine Learning, and a critica...\"],[\"--\\ntitle: \\\"Supercharged Searching on the 🤗 Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f48_hubsearch\\u002fthumbnail.png\\na...\"],[\"This is where the `huggingface_hub` comes in. \\n\\nFor those familiar with the library, you may already...\"],[\"This is key because without this \\\"cheat sheet\\\" of knowing how certain parameters should be written, ...\"],[\"## Taking it up a Notch\\n\\nWe saw how we could use the `ModelSearchArguments` and `DatasetSearchArgume...\"],[\"Very quickly we see that it's a much more coordinated approach for searching through the API, with n...\"],[\"```python\\n\\u003e\\u003e\\u003e # As a dictionary key\\n\\u003e\\u003e\\u003e ad[\\\"3_c\\\"]\\n```\\n    4\\n\\n## Concluding thoughts\\n\\nHopefully by no...\"],[\"--\\ntitle: \\\"Optimization story: Bloom inference\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fbloom-inference-pytorch-scri...\"],[\"# Porting to transformers\\n\\nBecause of the original training code, we set out to do something which w...\"],[\"The first model (small-testing) is in `bfloat16` like the big bloom so \\neverything should be very si...\"],[\"However `pipelines` are not distributed aware (it's not their goal). After briefly\\ndiscussing option...\"],[\"@task\\n    def bloom_small(self):\\n        sentence = \\\"Translate to chinese. EN: I like soup. CN: \\\"\\n  ...\"],[\"Let's do the math and we are getting `17 TFlop` for a single forward pass.\\nLooking at the [specs](ht...\"],[\"Now that we have a good understanding of where we stand it's time to get to work.\\n\\nWe tried many dif...\"],[\"- Porting was not an easy task as some conditions and kernels were hard to\\n    reproduce correctly e...\"],[\"So we could have 16x more throughput at a 5x latency cost. Not bad, but looking\\n    at the numbers w...\"],[\"## Using ONNX\\u002fTRT or other compiled approaches\\n  - They are supposed to handle most of the optimizat...\"],[\"Results:\\n\\n  - We had really impressive results fast which are roughly the same as\\n  the last iterati...\"],[\"## Webserver ideas\\n  - Given that we are going to run a free server where users are going to \\n    se...\"],[\"Results\\n\\n  - Next chapter.\\n\\n\\n\\n# Final route: PyTorch + TP + 1 custom kernel + torch.jit.script\\n\\n## W...\"],[\"\\u003cimg src=\\\"assets\\u002fbloom-inference-optimization\\u002fprofiler.png\\\"\\u003e\\nWe see many  `cat` operations before `b...\"],[\"## Low-hanging fruits\\n\\nNow that we had a TP implementation, we could start profiling and optimizing ...\"],[\"We started a 3-day job to reimplement the necessary parts of `torch.distributed`\\nTo get up and runni...\"],[\"```python\\nattn_weights = attention_scores.masked_fill_(attention_mask, torch.finfo(attention_scores....\"],[\"Then we had to drop the use [generate](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fv4.22.2\\u002fen\\u002fmain_clas...\"],[\"## Have you tried ...?\\n\\nStuff we know exists and haven't used because of various reasons. It \\ncould ...\"],[\"## Padding and Reshapes\\n\\nAs mentioned throughout this article, every tensor copy has a cost and anot...\"],[\"--\\ntitle: \\\"Welcome spaCy to the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f23_spacy\\u002fthumbnail.png\\n\\nau...\"],[\"\\u003cdiv\\u003e\\u003ca class=\\\"text-xs block mb-3 text-gray-300\\\" href=\\\"\\u002fspacy\\u002fen_core_web_sm\\\"\\u003e\\u003ccode\\u003espacy\\u002fen_core_we...\"],[\"\\u003cdiv class=\\\"SVELTE_HYDRATER \\\" data-props=\\\"{&quot;apiUrl&quot;:&quot;https:\\u002f\\u002fapi-inference.huggingfac...\"],[\".9185392711}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;LABELED_DEPENDENCIES&quot;,&quot;type&quo...\"],[\".bin&quot;},{&quot;rfilename&quot;:&quot;vocab\\u002fstrings.json&quot;},{&quot;rfilename&quot;:&quot;voca...\"],[\"\\\"\\u003e \\u003cdiv class=\\\"font-semibold flex items-center mb-2\\\"\\u003e\\u003cdiv class=\\\"text-lg flex items-center\\\"\\u003e\\u003csvg xml...\"],[\"Hosted inference API\\u003c\\u002fdiv\\u003e \\u003ca target=\\\"_blank\\\" href=\\\"\\u002fdocs\\\"\\u003e\\u003csvg class=\\\"ml-1.5 text-sm text-gray-400 ...\"],[\".90001 12.3625V12.3625ZM2.37501 12.3625V9.09998H5.63751V12.3625H2.37501ZM6.72501 15.625V13.45H8.9000...\"],[\".52255 2.01024 5.63722 2.28668 5.63751 2.57502V4.75002C5.63722 5.03835 5.52255 5.3148 5.31867 5.5186...\"],[\"JSON Output\\u003c\\u002fbutton\\u003e \\u003cbutton class=\\\"flex items-center ml-auto\\\"\\u003e\\u003csvg class=\\\"mr-1\\\" xmlns=\\\"http:\\u002f\\u002fwww.w...\"],[\"### Using existing models\\n\\nAll models from the Hub can be directly installed using `pip install`. \\n\\n...\"],[\"Try it out and share your models with the community!\\n\\n## Would you like to integrate your library to...\"],[\"--\\ntitle: \\\"Porting fairseq wmt19 translation system to transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f07_port...\"],[\"## Let's cheat\\n\\nThe first step was to cheat, of course. Why make a big effort when one can make a li...\"],[\"# install transformers\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\npip install -e .[dev]\\n...\"],[\"There are other files that needed to be modified as well, we will talk about those towards the end.\\n...\"],[\"To see what's inside the downloaded files, we have to first hunt down the right folder under `~\\u002f.cac...\"],[\"We are going to investigate each of these files in the following sections.\\n\\n## How translation syste...\"],[\"If we combine the first two and the last two steps we get 3 stages:\\n\\n1. **Encode input**: break inpu...\"],[\"Let's see how this approach helps to reduce memory and computation requirements. If we have an input...\"],[\"### fairseq's tokenizer workings\\n\\nLet's understand how `fairseq`'s tokenizer works.\\n\\n`fairseq` (*) u...\"],[\"You can refer to [this notebook](https:\\u002f\\u002fgithub.com\\u002fstas00\\u002fporting\\u002ftree\\u002fmaster\\u002ftransformers\\u002ffairseq-...\"],[\"These huge numbers also indicate to us that this tokenizer was trained on an enormous amount of text...\"],[\"Hopefully, you can now see how this works.\\n\\nOne confusing thing is that if you remember the `apply_b...\"],[\"* footnote: the more I work on porting models and datasets, the more I realize that putting the orig...\"],[\"After running the conversion script, let's check the converted dictionary:\\n\\n```\\n$ grep '\\\"Mach\\\"' \\u002fcod...\"],[\"So I just copied it to `src\\u002ftransformers\\u002ftokenization_fsmt.py` and renamed the class names:\\n```\\ncp t...\"],[\"```\\n-        [...].tokenize(text, return_str=False, escape=False)\\n+        [...].tokenize(text, retu...\"],[\"I first researched which of the existing architectures were the closest to my needs. It was BART tha...\"],[\"To accomplish that I used a slightly different `fairseq` API:\\n```\\nfrom fairseq import hub_utils\\n#che...\"],[\"Note that in the code sample above I'm not using `torch.load()` to load `state_dict`. This is what I...\"],[\"```\\n    model_conf = {\\n        \\\"architectures\\\": [\\\"FSMTForConditionalGeneration\\\"],\\n        \\\"model_typ...\"],[\"The first step was to take a sentence, encode it and then feed to the `generate` function - for `fai...\"],[\"(without the `decode` part first)\\n\\nrunning both side by side, stepping through with debugger on each...\"],[\"Similar to the encoding process, this one was done in reverse.\\n\\nThe steps were:\\n1. convert output id...\"],[\"Several times I had to update just the config files, and I didn't want to re-upload the large models...\"],[\"## AutoConfig, AutoTokenizer, etc.\\n\\nOne other change I needed to do is to plug the newly ported mode...\"],[\"To find all the places I needed to plug FSMT in, I mimicked `BartConfig`, `BartForConditionalGenerat...\"],[\"With the help of these scripts I found some more problems with the detokenizer, stepped through with...\"],[\"## Test Coverage\\n\\nThis next step was very important - I needed to prepare an extensive testing for t...\"],[\"I added one more test that performs a light BLEU evaluation - I used just 8 text inputs for each of ...\"],[\"For the task of translation [BLEU score](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fBLEU) is used as an evaluatio...\"],[\"I wasn't getting the same BLEU scores as the ones reported in the original paper, so I next needed t...\"],[\"This discovery lead me to write a new script: [run_eval_search.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftr...\"],[\"You will find the 5 ported AllenAI models [here](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=allenai&tag=fs...\"],[\"![break point group](.\\u002fassets\\u002f07_porting_fsmt\\u002finference_api.png)\\n\\n\\n\\n\\n## Documentation\\n\\nFinally, the ...\"],[\"It took two weeks of several cycles of feedback, followed by modifications, and more such cycles. Ev...\"],[\"- The PR merging process took a good couple of weeks before it was accepted. During this stage, besi...\"],[\"Thank you for reading!...\"],[\"--\\ntitle: \\\"Understanding BigBird's Block Sparse Attention\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f18_big_bird\\u002fattn....\"],[\"**BigBird RoBERTa-like** model is now available in 🤗Transformers. The goal of this post is to give t...\"],[\"---\\n\\nIn this blog post, we will try to answer those questions.\\n\\n### What tokens should be attended t...\"],[\"\\u003e\\u003e\\u003e # let's update our collection with the above tokens\\n\\u003e\\u003e\\u003e key_tokens.append(sliding_tokens)\\n```\\n\\n*...\"],[\"\\u003e\\u003e\\u003e # fill random tokens to our collection\\n\\u003e\\u003e\\u003e key_tokens.append(random_tokens)\\n\\n\\u003e\\u003e\\u003e # it's time to ...\"],[\"![](assets\\u002f18_big_bird\\u002fgraph.gif)\\n\\u003cimg src=\\\"assets\\u002f18_big_bird\\u002ffull.png\\\" width=230 height=230\\u003e\\n\\n**Bi...\"],[\"In case, we have many global tokens, then we may not need random connections since there will be mul...\"],[\"![BigBird block sparse attention](assets\\u002f18_big_bird\\u002fattn.png)\\n*Note: on the top, we have 2 extra se...\"],[\"```python\\n# what we want to do\\nQ[i] x [K[i-1], K[i], K[i+1]] for i = 1:-1\\n\\n# efficient implementatio...\"],[\"\\u003cimg src=\\\"assets\\u002f18_big_bird\\u002fintro.png\\\" width=500 height=250\\u003e\\n\\nAttention scores for \\\\\\\\({q}_{1}, {q}_...\"],[\"![BigBird block sparse attention](assets\\u002f18_big_bird\\u002fqlast_sec.png)\\n\\n---\\n\\nAttention score for \\\\\\\\(\\\\ma...\"],[\"\\u003cdetails\\u003e\\n\\n\\u003csummary\\u003eExpand this snippet in case you wanna see the calculations\\u003c\\u002fsummary\\u003e\\n\\n```md\\nBigB...\"],[\"The table below summarizes ITC & ETC:\\n\\n|                                              | ITC         ...\"],[\"# By setting attention_type to `original_full`, BigBird will be relying on the full attention of n^2...\"],[\"# forward pass\\n        output = model(**batch)\\n\\n        # back-propogation\\n        output[\\\"loss\\\"].ba...\"],[\"It's important to keep the following points in mind while working with big bird:\\n\\n* Sequence length ...\"],[\"You will soon find **BigBird Pegasus-like** model in the library for **long document summarization**...\"],[\"--\\ntitle: \\\"Train your first Decision Transformer\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f101_train-decision-transfo...\"],[\"The main idea is that instead of training a policy using RL methods, such as fitting a value functio...\"],[\"There are different types of Decision Transformers, but today, we’re going to train an offline Decis...\"],[\"First we need to import the `load_dataset` function from the 🤗 datasets package and download the dat...\"],[\"def __init__(self, dataset) -\\u003e None:\\n        self.act_dim = len(dataset[0][\\\"actions\\\"][0])\\n        se...\"],[\"d.append(np.array(feature[\\\"dones\\\"][si : si + self.max_len]).reshape(1, -1))\\n            timesteps.ap...\"],[\"s = torch.from_numpy(np.concatenate(s, axis=0)).float()\\n        a = torch.from_numpy(np.concatenate(...\"],[\"```python\\nclass TrainableDT(DecisionTransformerModel):\\n    def __init__(self, config):\\n        super...\"],[\"trainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=dataset[\\\"train\\\"],\\n    ...\"],[\"## References\\n[1] Chen, Lili, et al. \\\"Decision transformer: Reinforcement learning via sequence mode...\"],[\"--\\ntitle: Goodbye cold boot - how we made LoRA Inference 300% faster\\nthumbnail: \\u002fblog\\u002fassets\\u002f171_loa...\"],[\"Instead of fine-tuning the model by performing tiny changes to all its weights, we freeze most of th...\"],[\"There are far less blue base models than there are yellow ones on the Hub. If we can go quickly from...\"],[\"Now: request time has decreased from 35s to 13s since adapters will use only a few distinct \\\"blue\\\" b...\"],[\"A LoRA will have a ```base_model``` attribute. This is simply the model which the LoRA was built for...\"],[\"model = DiffusionPipeline.from_pretrained(\\n    base, **kwargs\\n)\\n\\nif torch.cuda.is_available():\\n    m...\"],[\"With 2 to 4 additional seconds per inference, we can serve many distinct LoRAs. However, on an A10G ...\"],[\"# Request another one\\n$ curl -H 'lora: nerijs\\u002fpixel-art-xl' 0:8888 -d '{\\\"inputs\\\": \\\"elephant\\\", \\\"param...\"],[\"--\\ntitle: \\\"Open-Source Text Generation & LLM Ecosystem at Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fos_l...\"],[\"![Causal LM Output](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fbl...\"],[\"On the Hugging Face Hub, you can find both causal language models and causal language models fine-tu...\"],[\"The second type of text generation model is commonly referred to as the text-to-text generation mode...\"],[\"### Models created with love by Hugging Face with BigScience and BigCode 💗\\n\\nHugging Face has co-led ...\"],[\"- [Falcon 40B](https:\\u002f\\u002fhuggingface.co\\u002ftiiuae\\u002ffalcon-40b)\\n- [XGen](https:\\u002f\\u002fhuggingface.co\\u002ftiiuae\\u002ffalc...\"],[\"There are two code generation models, [StarCoder by BigCode](https:\\u002f\\u002fhuggingface.co\\u002fmodels?sort=tren...\"],[\"If you're looking to fine-tune a model on an existing instruction dataset, you need to know how a da...\"],[\"| Model                                                                                    | Dataset...\"],[\"| [Pythia-12B](https:\\u002f\\u002fhuggingface.co\\u002fEleutherAI\\u002fpythia-12b)                               | [Pile](...\"],[\"| [FLAN-T5-XXL](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fflan-t5-xxl)                                 | [gsm8k]...\"],[\"| [StarChat-β](https:\\u002f\\u002fhuggingface.co\\u002fHuggingFaceH4\\u002fstarchat-beta)                     | [OpenAssist...\"],[\"### Text Generation Inference\\n\\nResponse time and latency for concurrent users are a big challenge fo...\"],[\"![HuggingChat Search](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002f...\"],[\"## Parameter Efficient Fine Tuning (PEFT)\\n\\nIf you’d like to fine-tune one of the existing large mode...\"],[\"--\\ntitle: \\\"Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action\\\" \\nthumbnail: \\u002fblog\\u002fass...\"],[\"1. **Define a model**: Before you can use BentoML, you need a machine learning model (or multiple mo...\"],[\"## A brief introduction to DeepFloyd IF\\n\\nDeepFloyd IF is a state-of-the-art, open-source text-to-ima...\"],[\"Once the prerequisites are met, clone the project repository to your local machine and navigate to t...\"],[\"```bash\\npython import_models.py\\n```\\n\\nOnce the downloads are complete, view the models in the Model s...\"],[\"Prompt:\\n\\n\\u003e orange and black, head shot of a woman standing under street lights, dark theme, Frank Mi...\"],[\"View the Bento in the local Bento Store.\\n\\n```bash\\n$ bentoml list\\n\\nTag                               ...\"],[\"--\\ntitle: \\\"Deep Q-Learning with Space Invaders\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_deep_rl_dqn\\u002fthumbnail.gif...\"],[\"And **we'll train it to play Space Invaders and other Atari environments using [RL-Zoo](https:\\u002f\\u002fgith...\"],[\"Internally, our Q-function has **a Q-table, a table where each cell corresponds to a state-action pa...\"],[\"\\u003cimg src=\\\"assets\\u002f63_deep_rl_intro\\u002fdeep.jpg\\\" alt=\\\"Deep Q Learning\\\"\\u002f\\u003e\\n\\n\\nNow that we understand Deep Q-...\"],[\"\\u003cimg src=\\\"assets\\u002f78_deep_rl_dqn\\u002ftemporal-limitation-2.jpg\\\" alt=\\\"Temporal Limitation\\\"\\u002f\\u003e\\nThat’s why, t...\"],[\"But, this is not the only change compared with Q-Learning. Deep Q-Learning training **might suffer f...\"],[\"The solution is to create a Replay Buffer that stores experience tuples while interacting with the e...\"],[\"It’s like if you were a cowboy (the Q estimation) and you want to catch the cow (the Q-target), you ...\"],[\"The solution is: when we compute the Q target, we use two networks to decouple the action selection ...\"],[\"That’s **normal if you still feel confused** with all these elements. **This was the same for me and...\"],[\"--\\ntitle: \\\"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"\\u003cp\\u003eIn 2017 a group of Google AI researchers published a paper introducing the transformer model arch...\"],[\"\\u003cp\\u003eA timeline showing releases of prominent transformer language models (credit: Hugging Face)\\u003c\\u002fp\\u003e\\n\\u003c...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fvit%20diag.png?width=1024&amp;name=vit%20diag.png\\\"...\"],[\"\\u003cp\\u003eAn overview of the ViT model structure as introduced in \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2010.11929...\"],[\"\\u003cp\\u003eThanks to the addition of a range of pre-optimized transformer models to the open-source Hugging ...\"],[\"\\u003cp\\u003eAt the same time, developing any model for X-ray classification (ViT or otherwise) will entail it...\"],[\"\\u003cp\\u003eIf this is your first time using IPUs, read the \\u003ca href=\\\"https:\\u002f\\u002fdocs.graphcore.ai\\u002fprojects\\u002fipu-p...\"],[\"\\u003cp\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fwww.graphcore.ai\\u002fhs-fs\\u002fhubfs\\u002fchest%20x-ray%20examples.png?width=700&amp;name=ch...\"],[\"\\u003cli\\u003eThe ViT Training Notebook from the \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fgraphcore\\u002ftutorials\\\" rel=\\\"noopene...\"],[\"\\u003cp style=\\\"font-weight: bold;\\\"\\u003eWe’ve even made it easier and created the HF Optimum Gradient so you c...\"],[\"\\u003cp\\u003eNext, download the \\u003ccode\\u003eData_Entry_2017_v2020.csv\\u003c\\u002fcode\\u003e file, which contains the labels. By def...\"],[\"\\u003cdiv style=\\\"font-size: 14px; line-height: 1.3;\\\"\\u003e\\n\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmaxfield\\u002fc...\"],[\"\\u003cdiv style=\\\"font-size: 14px; line-height: 1.3;\\\"\\u003e\\n\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmaxfield\\u002fc...\"],[\"\\u003cdiv style=\\\"font-size: 14px; line-height: 1.3;\\\"\\u003e\\n\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmaxfield\\u002f1...\"],[\"\\u003cdiv style=\\\"font-size: 14px; line-height: 1.3;\\\"\\u003e\\n\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmaxfield\\u002f2...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\u003cp\\u003eOur dataset is now ready to be used.\\u003c\\u002fp\\u003e\\n\\u003ch2\\u003ePreparing the model\\u003c\\u002fh2\\u003e\\n\\u003cp\\u003eTo train a model ...\"],[\"\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmaxfield\\u002faaad87d4b2560cc288913b9ec85ed312.js\\\"\\u003e\\u003c\\u002fscript\\u003e\\n\\u003c\\u002fd...\"],[\"\\u003cdiv style=\\\"font-size: 14px; line-height: 1.3;\\\"\\u003e\\n\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmaxfield\\u002f1...\"],[\"\\u003cdiv style=\\\"font-size: 14px; line-height: 1.3;\\\"\\u003e\\n\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmaxfield\\u002f3...\"],[\"\\u003cscript src=\\\"https:\\u002f\\u002fgist.github.com\\u002fnickmaxfield\\u002f562ceec321a9f4ac16483c11cb3694c2.js\\\"\\u003e\\u003c\\u002fscript\\u003e\\n\\u003c\\u002fd...\"],[\"\\u003cp\\u003e\\u003ca href=\\\"https:\\u002f\\u002fconsole.paperspace.com\\u002fgithub\\u002fgradient-ai\\u002fGraphcore-HuggingFace?machine=Free-IPU...\"],[\"\\u003ch2\\u003eMore Resources for Hugging Face Optimum on IPUs\\u003c\\u002fh2\\u003e\\n\\u003cul\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fgraphc...\"],[\"--\\ntitle: \\\"Nyströmformer: Approximating self-attention in linear time and memory via the Nyström met...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"But, what does it mean to sample a column from \\\\\\\\(S\\\\\\\\)? It means we select one element from each row...\"],[\"## How do we select landmarks?\\n\\nInstead of sampling \\\\\\\\(m\\\\\\\\) rows from \\\\\\\\(Q\\\\\\\\) and \\\\\\\\(K\\\\\\\\), the autho...\"],[\"q_landmarks = query_layer.reshape(\\n    -1,\\n    self.num_attention_heads,\\n    self.num_landmarks,\\n   ...\"],[\"```python\\nfrom transformers import AutoTokenizer, NystromformerForMaskedLM\\nimport torch\\n\\ntokenizer =...\"],[\"\\u003c\\u002fdiv\\u003e\\n\\n## Conclusion\\n\\nNyströmformer offers an efficient approximation to the standard self-attentio...\"],[\"--\\ntitle: \\\"Llama 2 is here - get it on Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fllama2\\u002fthumbnail.jpg\\na...\"],[\"## Why Llama 2?\\n\\nThe Llama 2 release introduces a family of pretrained and fine-tuned LLMs, ranging ...\"],[\"If you’ve been waiting for an open alternative to closed-source chatbots, Llama 2-Chat is likely you...\"],[\"## Inference\\nIn this section, we’ll go through different approaches to running inference of the Llam...\"],[\"```\\nResult: I liked \\\"Breaking Bad\\\" and \\\"Band of Brothers\\\". Do you have any recommendations of other ...\"],[\"- For 7B models, we advise you to select \\\"GPU [medium] - 1x Nvidia A10G\\\".\\n- For 13B models, we advis...\"],[\"First pip install `trl` and clone the script:\\n```bash\\npip install trl\\ngit clone https:\\u002f\\u002fgithub.com\\u002fl...\"],[\"If a question does not make any sense, or is not factually coherent, explain why instead of answerin...\"],[\"### Ignore previous instructions\\n\\nIn API-based models, people resort to tricks in an attempt to over...\"],[\"## Conclusion\\n\\nWe're very excited about Llama 2 being out! In the incoming days, be ready to learn m...\"],[\"--\\ntitle: \\\"Introducing The World's Largest Open Multilingual Language Model: BLOOM\\\"\\nthumbnail: \\u002fblog...\"],[\"Researchers can [now download, run and study BLOOM](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fbloom) to inve...\"],[\"--\\ntitle: \\\"Optimum-NVIDIA Unlocking blazingly fast LLM inference in just 1 line of code\\\" \\nthumbnail:...\"],[\"```diff\\n- from transformers.pipelines import pipeline\\n+ from optimum.nvidia.pipelines import pipelin...\"],[\"tokenizer.batch_decode(generated_ids[0], skip_special_tokens=True)\\n```\\n\\nFor more details, check out ...\"],[\"### Next steps\\n\\nOptimum-NVIDIA currently provides peak performance for the LLaMAForCausalLM architec...\"],[\"--\\ntitle: \\\"Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\\\" \\nthumbn...\"],[\"\\u003e We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a ...\"],[\"## Resources\\n\\nThis blogpost and release come with several resources to get started with 4bit models ...\"],[\"The recent QLoRA paper explores different data types, 4-bit Float and 4-bit NormalFloat. We will dis...\"],[\"The potential floating points that can be represented in the E4M3 format are in the range -448 to 44...\"],[\"More specifically, QLoRA uses 4-bit quantization to compress a pretrained language model. The LM par...\"],[\"```bash\\npip install -q -U bitsandbytes\\npip install -q -U git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransfor...\"],[\"The matrix multiplication and training will be faster if one uses a 16-bit compute dtype (default to...\"],[\"model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant...\"],[\"For text models, at this time of writing, this would include most used architectures such as Llama, ...\"],[\"#### What other consequences are there?\\n\\nThis integration can open up several positive consequences ...\"],[\"| Model name                          | Half precision model size (in GB) | Hardware type \\u002f total VR...\"],[\"| decapoda-research\\u002fllama-7b-hf       | 14GB                              | 1xNVIDIA-T4 \\u002f 16GB      ...\"],[\"| decapoda-research\\u002fllama-13b-hf      | 27GB                              | 1xNVIDIA-T4 \\u002f 16GB      ...\"],[\"We have used the recent `SFTTrainer` from TRL library, and the benchmarking script can be found [her...\"],[\"--\\ntitle: \\\"Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models\\\"\\nthumbnail: \\u002f...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002foptimum_onnxruntime-training\\u002f...\"],[\"To target these needs, Hugging Face built two open-sourced libraries: __Accelerate__ and __Optimum__...\"],[\"ONNX Runtime Training achieves such throughput improvements via several memory and compute optimizat...\"],[\"## ONNX Runtime Training in Optimum\\n\\nOptimum provides an `ORTTrainer` API that extends the `Trainer`...\"],[\"So concretely, what should users do with Optimum to take advantage of the ONNX Runtime acceleration ...\"],[\"# Step 3: Use ONNX Runtime for training!🤗\\ntrainer.train()\\n```\\n\\n## Looking Forward\\n\\nThe Hugging Face ...\"],[\"## Getting Started\\n\\nWe invite you to check out the links below to learn more about, and get started ...\"],[\"--\\ntitle: \\\"SafeCoder vs. Closed-source Code Assistants\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fsafecoder-vs-closed-...\"],[\"StarCoder is a 15.5 billion parameter model trained for code generation in over 80 programming langu...\"],[\"## Customization\\n\\nThe StarCoder models have been specifically designed to be customizable, and we ha...\"],[\"## Security and privacy\\n\\nSecurity is always a top concern, all the more when source code is involved...\"],[\"--\\ntitle: \\\"Optimizing your LLM in production\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f163_optimize_llm\\u002foptimize_llm....\"],[\"2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only pro...\"],[\"\\u003e *Loading the weights of a model having X billion parameters requires roughly 4 * X GB of VRAM in f...\"],[\"🤗 Transformers does not support tensor parallelism out of the box as it requires the model architect...\"],[\"We first load the model and tokenizer and then pass both to Transformers' [pipeline](https:\\u002f\\u002fhugging...\"],[\"\\u003e Almost all models are trained in bfloat16 nowadays, there is no reason to run the model in full fl...\"],[\"Without going into too many details, quantization schemes aim at reducing the precision of weights w...\"],[\"```bash\\n!pip install bitsandbytes\\n```\\n\\nWe can then load models in 8-bit quantization by simply addin...\"],[\"pipe = pipeline(\\\"text-generation\\\", model=model, tokenizer=tokenizer)\\n\\nresult = pipe(prompt, max_new_...\"],[\"\\u003e As a conclusion, it is important to remember that model quantization trades improved memory effici...\"],[\"\\\\\\\\(  \\\\mathbf{X} = (\\\\mathbf{x}_1, ... \\\\mathbf{x}_{N}) \\\\\\\\) is thereby the input sequence to the attent...\"],[\"$$ \\\\textbf{O}_i \\\\leftarrow s^a_{ij} * \\\\textbf{O}_i + s^b_{ij} * \\\\mathbf{V}_{j} \\\\times \\\\text{Softmax}...\"],[\"In practice, there is currently absolutely no reason to **not** use Flash Attention if available. Th...\"],[\"Question: Can you write some test cases for this function?\\n\\nAnswer: Sure, here are some tests.\\n\\nasse...\"],[\"We're getting the same output as before, however this time, the model repeats the answer multiple ti...\"],[\"We're getting the exact same result as before, but can observe a very significant speed-up thanks to...\"],[\"-   The positional embeddings\\n-   The key-value cache\\n\\nLet's go over each component in more detail\\n\\n...\"],[\"The authors of the [*Attention Is All You Need*](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1706.03762) paper introduced ...\"],[\"-   [Rotary Position Embedding (RoPE)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2104.09864)\\n-   [ALiBi](https:\\u002f\\u002farxiv.o...\"],[\"![](\\u002fblog\\u002fassets\\u002f163_optimize_llm\\u002falibi.png)\\n\\nAs shown in the [ALiBi](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2108.124...\"],[\"In conclusion, LLMs that are intended to be deployed in tasks that require handling large text input...\"],[\"As we can see every time we increase the text input tokens by the just sampled token.\\n\\nWith very few...\"],[\"print(\\\"shape of input_ids\\\", next_token_id.shape)\\n  print(\\\"length of key-value cache\\\", len(past_key_v...\"],[\"Note that the key-value cache is especially useful for applications such as chat where multiple pass...\"],[\"Two things should be noted here:\\n  1. Keeping all the context is crucial for LLMs deployed in chat s...\"],[\"1.  [Multi-Query-Attention (MQA)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1911.02150)\\n\\nMulti-Query-Attention was propo...\"],[\"The important part to understand here is that reducing the number of key-value attention heads to 1 ...\"],[\"GQA was only recently proposed which is why there is less adoption at the time of writing this noteb...\"],[\"--\\ntitle: \\\"Deploy GPT-J 6B for inference using  Hugging Face Transformers and Amazon SageMaker\\\"\\nthum...\"],[\"But before we get into it, I want to explain why deploying `GPT-J` into production is challenging. \\n...\"],[\"In [Transformers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers) the models loaded with the `from_pret...\"],[\"\\u003e Align PyTorch and Transformers version when saving the model with `torch.save(model,PATH)` and loa...\"],[\"For this, we need to create a `model.tar.gz` artifact containing our model weights and additional fi...\"],[\"```python\\nfrom sagemaker.huggingface import HuggingFaceModel\\nimport sagemaker\\n\\n# IAM role with permi...\"],[\"### Default request\\n\\nThis is an example of a default request using `greedy` search.\\n\\nInference-time ...\"],[\"---\\n\\nTo delete your endpoint you can run. \\n\\n```python\\npredictor.delete_endpoint()\\n```\\n\\n## Conclusion...\"],[\"--\\ntitle: \\\"Visualize proteins on Hugging Face Spaces\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f98_spaces_3dmoljs\\u002fthum...\"],[\"## Taking a Look at the Code\\n\\nLet's take a look at how to create the minimal working demo of our int...\"],[\"Our `molecule` function which returns the `iframe` conceptually looks like this: \\n\\n```python\\ndef mol...\"],[\"The `body` looks as follows:\\n\\n```html\\n\\u003cbody\\u003e\\n    \\u003cdiv id=\\\"container\\\" class=\\\"mol-container\\\"\\u003e\\u003c\\u002fdiv\\u003e\\n  ...\"],[\"\\u003cgradio-app theme_mode=\\\"light\\\" space=\\\"simonduerr\\u002fProteinMPNN\\\"\\u003e\\u003c\\u002fgradio-app\\u003e\\n\\n# Issues\\n\\nIf you encoun...\"],[\"--\\ntitle: \\\"Announcing our new Content Guidelines and Policy\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fcontent-guideli...\"],[\"Furthermore, as the field of AI and machine learning continues to expand, the variety of use cases a...\"],[\"This consideration for people's consent and experiences on the platform extends to Community Content...\"],[\"--\\ntitle: \\\"Fine-Tune MMS Adapter Models for low-resource ASR\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f151_mms\\u002fmms_ma...\"],[\"Meta AI's most recent release, [**Massive Multilingual Speech (MMS)**](https:\\u002f\\u002fai.facebook.com\\u002fblog\\u002f...\"],[\"Adapter layers act like linguistic bridges, enabling the model to leverage knowledge from one langua...\"],[\"You can see that the base models are saved (as usual) as a [`model.safetensors` file](https:\\u002f\\u002fhuggin...\"],[\"The work done in **MMS** leverages this idea of adapters for speech recognition across different lan...\"],[\"We strongly suggest to upload your training checkpoints directly to the [🤗 Hub](https:\\u002f\\u002fhuggingface....\"],[\"Wav2Vec2-like models fine-tuned on CTC transcribe an audio file with a single forward pass by first ...\"],[\"```python\\nfrom datasets import load_dataset, load_metric, Audio\\n\\ncommon_voice_train = load_dataset(\\\"...\"],[\"df = pd.DataFrame(dataset[picks])\\n    display(HTML(df.to_html()))\\n```\\n\\n```python\\nshow_random_element...\"],[\"```python\\nshow_random_elements(common_voice_train.remove_columns([\\\"path\\\",\\\"audio\\\"]))\\n```\\n\\n```bash\\ni̇k...\"],[\"```python\\ncommon_voice_train = common_voice_train.map(replace_hatted_characters)\\ncommon_voice_test =...\"],[\"Cool, we see that all letters of the alphabet occur in the dataset (which is not really surprising) ...\"],[\"Since a single MMS checkpoint can provide customized weights for multiple languages, the tokenizer c...\"],[\"In a final step, we use the json file to load the vocabulary into an instance of the `Wav2Vec2CTCTok...\"],[\"A pretrained checkpoint expects its input data to have been sampled more or less from the same distr...\"],[\"```python\\nfrom transformers import Wav2Vec2Processor\\n\\nprocessor = Wav2Vec2Processor(feature_extracto...\"],[\"Let's take a look at `\\\"audio\\\"` again.\\n\\n```python\\ncommon_voice_train[0][\\\"audio\\\"]\\n```\\n\\n\\n    {'path': '...\"],[\"**Note**: This mapping function is a good example of how the `Wav2Vec2Processor` class should be use...\"],[\"-   Define a data collator. In contrast to most NLP models, MMS has a much larger input length than ...\"],[\"```python\\nimport torch\\n\\nfrom dataclasses import dataclass, field\\nfrom typing import Any, Dict, List,...\"],[\"labels_batch = self.processor.pad(\\n            labels=label_features,\\n            padding=self.paddi...\"],[\"wer = wer_metric.compute(predictions=pred_str, references=label_str)\\n\\n    return {\\\"wer\\\": wer}\\n```\\n\\nN...\"],[\"We now want to make sure that only the adapter weights will be trained and that the rest of the mode...\"],[\"For more explanations on other parameters, one can take a look at the [docs](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"------------------------------------------------------------------------\\n\\n\\\\\\\\( {}^1 \\\\\\\\) To allow mode...\"],[\"From the [official paper](https:\\u002f\\u002fscontent-cdg4-3.xx.fbcdn.net\\u002fv\\u002ft39.8562-6\\u002f348827959_69675341899279...\"],[\"This makes it extremely simple to train additional adapter layers and add them to your repository.\\n\\n...\"],[\"```python\\ninput_dict = processor(common_voice_test_tr[0][\\\"audio\\\"][\\\"array\\\"], sampling_rate=16_000, re...\"],[\"**Output**:\\n\\n```bash\\n    Prediction:\\n    jag lämnade grovjobbet åt honom\\n\\n    Reference:\\n    jag läm...\"],[\"--\\ntitle: \\\"Policy Gradient with PyTorch\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f85_policy_gradient\\u002fthumbnail.gif\\nau...\"],[\"But, with policy-based methods, we want to optimize the policy directly **without having an intermed...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fassets\\u002f6...\"],[\"Let's take an example: we have an intelligent vacuum cleaner whose goal is to suck the dust and avoi...\"],[\"Instead, with a policy gradient, we output a **probability distribution over actions.**\\n\\n### The Dis...\"],[\"To do that we’re going to use the [Policy Gradient Theorem](https:\\u002f\\u002fwww.youtube.com\\u002fwatch?v=AKbX1Zvo...\"],[\"Now that we studied the theory behind Reinforce, **you’re ready to code your Reinforce agent with Py...\"],[\"--\\ntitle: \\\"Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\\\"\\nthumbnail: \\u002fblog...\"],[\"## Getting Started\\n\\nBefore you can connect your AWS Account with your Hugging Face account, you need...\"],[\"![Connect Account](assets\\u002f158_aws_marketplace\\u002f04_connect.jpg \\\"Connect Account\\\")\\n\\nAfter clicking \\\"Sub...\"],[\"---\\n\\nThanks for reading! If you have any questions, feel free to contact us at [api-enterprise@huggi...\"],[\"--\\ntitle: \\\"Practical 3D Asset Generation: A Step-by-Step Guide\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-g...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f124_ml-...\"],[\"Under 'subject', type the texture you want, like 'Wood Wall', and click 'Generate'. When you're happ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f124_ml-...\"],[\"--\\ntitle: \\\"Making LLMs lighter with AutoGPTQ and transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f159_autogptq...\"],[\"This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs.\\n\\n## Table of contents...\"],[\"## **A gentle summary of the GPTQ paper**\\n\\nQuantization methods usually belong to one of two categor...\"],[\"\\\\\\\\( \\\\sum_{i=0}^{d_{row}} \\\\|W_{l[i,:]}X-\\\\hat{W}_{l[i,:]}X\\\\|^{2}_{2} \\\\\\\\)\\n\\nThis means that we can quant...\"],[\"Since the AutoGPTQ library has a larger coverage of transformers models, we decided to provide an in...\"],[\"This is a benchmark sample for the batch size = 1 case. The benchmark was run on a single NVIDIA A10...\"],[\"Quantizing 🤗 Transformers models with the GPTQ method can be done in a few lines:\\n\\n```python\\nfrom tr...\"],[\"## **Fine-tune quantized models with PEFT**\\n\\nYou can not further train a quantized model using the r...\"],[\"On the quantization side, let’s emphasize again that this method only quantizes the weights. There h...\"],[\"This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step ...\"],[\"Finally, we would like to thank [Pedro Cuenca](https:\\u002f\\u002fgithub.com\\u002fpcuenca) for his help with the wri...\"],[\"--\\ntitle: \\\"Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets\\\"\\nthum...\"],[\"## Why have we created this tool?\\nThoughtful curation and analysis of Machine Learning datasets is o...\"],[\"A new wave of research in AI has called for a fundamental paradigm shift in how the field approaches...\"],[\"## When can I use the 🤗 Data Measurements Tool?\\nThe 🤗 Data Measurements Tool can be used iteratively...\"],[\"### Distributional Statistics\\n**To measure the language patterns in the dataset**\\n\\n*This begins to a...\"],[\"### Comparison statistics\\n*This begins to answer questions like “What kinds of topics, biases, and a...\"],[\"![image](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f14205986\\u002f143929481-0577cf78-38b0-4418-9a22-946630...\"],[\"--\\ntitle: \\\"New ViT and ALIGN Models From Kakao Brain\\\" \\nthumbnail: \\u002fblog\\u002f\\u002fassets\\u002f132_vit_align\\u002fthumbn...\"],[\"This blog will introduce the new [COYO](https:\\u002f\\u002fgithub.com\\u002fkakaobrain\\u002fcoyo-dataset) dataset, Kakao B...\"],[\"## COYO DATASET\\n\\n\\u003cp\\u003e\\n\\u003ccenter\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-im...\"],[\"| COYO | LAION 2B| ALIGN 1.8B |\\n| :----: | :----: | :----: |\\n| Image-text similarity score calculate...\"],[\"## How ViT and ALIGN work\\n\\nSo what do these models do? Let's breifly discuss how the ViT and ALIGN m...\"],[\"[Google then introduced ALIGN](https:\\u002f\\u002fai.googleblog.com\\u002f2021\\u002f05\\u002falign-scaling-up-visual-and-vision....\"],[\"```shell\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e dataset = load_dataset('kakaobrain\\u002fcoyo-700m')\\n\\u003e...\"],[\"```py\\nimport requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import ViTImageProcessor,...\"],[\"for c in top_class_preds:\\n    print(f\\\"{model.config.id2label[c.item()]} with probability {round(pred...\"],[\"Let's move on to experimenting with ALIGN, which can be used to retrieve multi-modal embeddings of t...\"],[\"# we can take the softmax to get the label probabilities\\nprobs = logits_per_image.softmax(dim=1)  \\np...\"],[\"with torch.no_grad():\\n    outputs = model(**inputs)\\n\\n# get the last hidden state and the final poole...\"],[\"```shell\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n\\u003e\\u003e\\u003e classifier = pipeline(task='zero-shot-image-clas...\"],[\"--\\ntitle: \\\"Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon\\\"\\nthumbnail: \\u002fbl...\"],[\"Quantization is a model compression technique that aims to solve both problems by reducing the range...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog...\"],[\"Now, let’s see how SmoothQuant works when applied to popular LLMs.\\n\\n## Quantizing LLMs with SmoothQu...\"],[\"The obvious benefit of working with smaller models is a significant reduction in inference latency. ...\"],[\"Together with Intel, we're hosting a new exciting demo in Spaces called [Q8-Chat](https:\\u002f\\u002fhuggingfac...\"],[\"--\\ntitle: \\\"An overview of inference solutions on Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f116_inference...\"],[\"Here's a sentence similarity example with the `sentence-transformers\\u002fall-MiniLM-L6-v2` [model](https...\"],[\"As rate limiting is enforced, we don't recommend using the Inference API for production. Instead, yo...\"],[\"## Spaces\\n\\nFinally, Spaces is another production-ready option to deploy your model for inference on ...\"],[\"--\\ntitle: \\\"Hugging Face's TensorFlow Philosophy\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f96_tensorflow_philosophy\\u002fth...\"],[\"```py\\nfrom transformers import TFAutoModel\\n\\nmodel = TFAutoModel.from_pretrained(\\\"bert-base-cased\\\")\\n`...\"],[\"It turns out that large models pretrained on lots of data are much, much better starting points for ...\"],[\"# Now our data is tokenized, we can pass it to our model, or use it in fit()!\\noutputs = model(tokeni...\"],[\"And if you want to train that model instead, it's just:\\n\\n```py\\nmodel.fit(my_data, my_labels)\\n```\\n\\nHo...\"],[\"Our solution to that is simple: If you `compile()` without a loss argument, we’ll give you the one y...\"],[\"```py\\nmodel.fit(inputs, labels)\\n```\\n\\nIn the past, we instead asked users to pass labels in the input...\"],[\"# Load and compile our model\\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\\\"bert-base...\"],[\"```py\\ntf_dataset = model.prepare_tf_dataset(\\n\\tdataset,\\n\\tbatch_size=16,\\n\\tshuffle=True\\n)\\n\\nmodel.fit(tf...\"],[\"XLA is useful for things besides generation too, though! We’ve also made a number of fixes to ensure...\"],[\"```py\\n# This is a new feature, so make sure to update to the latest version of transformers!\\n# You w...\"],[\"```py\\nmodel_name = \\\"your-username\\u002fmy-new-model\\\"\\nmodel = TFAutoModelForImageClassification.from_pretr...\"],[\"A lot of these things are small details, sure, but to coin a (rather clunky) phrase, great software ...\"],[\"--\\ntitle: The Age of Machine Learning As Code Has Arrived\\nthumbnail: \\u002fblog\\u002fassets\\u002f31_age_of_ml_as_co...\"],[\"Well, here's what I think.\\n\\n\\n### Machine Learning For The Masses!\\n\\nMachine Learning is everywhere, o...\"],[\"There's no need to reinvent the wheel either. The DevOps movement solved these problems over 10 year...\"],[\"Now, let's talk about Transformers.\\n\\n---\\n\\n### Transformers! Transformers! Transformers! ([Ballmer st...\"],[\"It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can k...\"],[\"\\u003ckbd\\u003e\\n  \\u003cimg src=\\\"assets\\u002f31_age_of_ml_as_code\\u002f07_kaggle.png\\\"\\u003e\\n\\u003c\\u002fkbd\\u003e\\n\\n\\n**Thank you all**. And yeah, ...\"],[\"--\\ntitle: \\\"Introducing Storage Regions on the HF Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f172_regions\\u002fthumbnail....\"],[\"## Regulatory and legal compliance\\n\\nIn many regulated industries, you may have a requirement to stor...\"],[\"--\\ntitle: \\\"Announcing the 🤗 AI Research Residency Program\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f57_ai_residency\\u002fr...\"],[\"More importantly, your application needs to present interest in effecting positive change through AI...\"],[\"--\\ntitle: \\\"Hugging Face Reads, Feb. 2021 - Long-range Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f14_long_...\"],[\"In particular, one issue has been at the center of the efforts: the quadratic cost in memory and tim...\"],[\"## Summaries\\n\\n### [Longformer - The Long-Document Transformer](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2004.05150)\\n\\nIz...\"],[\"\\u003cfigure\\u003e\\n  \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f14_long_range_transformers\\u002fLongformer.png\\\" alt=\\\"Longformer attenti...\"],[\"#### Main findings\\n\\n* The authors proposed the dilated windowed self-attention (Figure c) and showed...\"],[\"### [Compressive Transformers for Long-Range Sequence Modelling](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1911.05507)\\n\\n...\"],[\"A compression factor \\\\\\\\(c\\\\\\\\) (equal to 3 in the illustration) is chosen to decide the rate at which ...\"],[\"#### Follow-up questions\\n\\n* Compressive Transformer requires a special optimization schedule in whic...\"],[\"The theoretical foundations of the proposed approach are based on the Johnson-Lindenstrauss lemma. L...\"],[\"\\u003cfigure\\u003e\\n  \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f14_long_range_transformers\\u002fLinformer.png\\\" alt=\\\"Linformer performan...\"],[\"$$\\\\text{softmax}(Q * K) \\\\sim Q’ * K’ = \\\\phi(Q) * \\\\phi(K)$$\\n\\n, where \\\\\\\\(phi\\\\\\\\) is a non-linear suitab...\"],[\"## Reading group discussion\\n\\nThe developments in pre-trained transformer-based language models for n...\"],[\"These different inductive biases have implications in terms of computational speed and generalizatio...\"],[\"Modeling long inputs is not antithetical to modeling short inputs but instead should be thought from...\"],[\"## @Hugging Face 🤗: Long-range modeling\\n\\nThe Longformer implementation and the associated open-sourc...\"],[\"--\\ntitle: \\\"VQ-Diffusion\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f117_vq_diffusion\\u002fthumbnail.png\\nauthors:\\n- user: wi...\"],[\"image = pipe(prompt).images[0]\\n```\\n\\n![png](assets\\u002f117_vq_diffusion\\u002fvq_diffusion_teddy_bear_pool.png)...\"],[\"#### Approximating the reverse process\\n\\nAn encoder-decoder transformer approximates the classes of t...\"],[\"There is a smaller amount of literature covering discrete diffusion models than continuous diffusion...\"],[\"AR image generative models have evolved architecturally with much work towards making transformers c...\"],[\"[Image Transformer](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1802.05751) uses transformers by restricting self attentio...\"],[\"Despite having made tremendous strides, AR models still suffer from linear decreases in inference sp...\"],[\"[Improved Vector Quantized Diffusion Models](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2205.16007) improves upon VQ-Diff...\"],[\"--\\ntitle: Zero-shot image segmentation with CLIPSeg\\nthumbnail: \\u002fblog\\u002fassets\\u002f123_clipseg-zero-shot\\u002fth...\"],[\"One limitation of most image segmentation models is that they only work with a fixed list of categor...\"],[\"## CLIP: the magic model behind CLIPSeg\\n\\n[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmod...\"],[\"What’s more, CLIP is not only useful for classification, but it can also be used for [image search](...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"## Using CLIPSeg with Hugging Face Transformers\\n\\nUsing Hugging Face Transformers, you can easily dow...\"],[\"Finally, let's visualize the output.\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\n_, ax = plt.subplot...\"],[\"We can now process the input image and prompt image and input them to\\nthe model.\\n\\n```python\\nencoded_...\"],[\"```python\\n_, ax = plt.subplots(1, 2, figsize=(6, 4))\\n[a.axis('off') for a in ax.flatten()]\\nax[0].ims...\"],[\"```python\\nfrom segments import SegmentsClient\\nfrom getpass import getpass\\n\\napi_key = getpass('Enter ...\"],[\"# resize the outputs\\npreds = nn.functional.interpolate(\\n    outputs.logits.unsqueeze(1),\\n    size=(i...\"],[\"Lastly, we can upload the prediction to Segments.ai. To do that, we\\\\'ll\\nfirst convert the bitmap to ...\"],[\"Note that there's more research on zero-shot segmentation currently being conducted, so you can expe...\"],[\"--\\ntitle: \\\"Getting Started with Sentiment Analysis on Twitter\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f85_sentiment_...\"],[\"Buckle up and enjoy the ride! 🤗\\n\\n## What is Sentiment Analysis?\\n\\nSentiment analysis uses [machine le...\"],[\"Luckily, recent advancements in AI allowed companies to use machine learning models for sentiment an...\"],[\"Now that we covered what is sentiment analysis and why it's useful, let's get our hands dirty and ac...\"],[\"Then, you need to set up the [Twitter API credentials](https:\\u002f\\u002fdeveloper.twitter.com\\u002fen\\u002fdocs\\u002ftwitter...\"],[\"4. Analyzing tweets with sentiment analysis\\n\\nNow that you have data, you are ready to analyze the tw...\"],[\"```python\\nAPI_URL = \\\"https:\\u002f\\u002fapi-inference.huggingface.co\\u002fmodels\\u002f\\\" + model\\nheaders = {\\\"Authorization...\"],[\"@itskeeplearning @NotionHQ How you've linked gallery cards? Sentiment: Neutral\\n\\n@NotionHQ Running in...\"],[\"As a last step, let's create some wordclouds to see which words are the most used for each sentiment...\"],[\"That was fun, right? \\n\\nWith just a few lines of code, you were able to automatically gather tweets m...\"],[\"Let's get started! 🚀\\n\\n### Step 1: Getting the Tweets\\n\\nTo get started, you'll need to [create a Zap](...\"],[\"- The `model ID` is to tell the Inference API which model you want to use for making predictions. Fo...\"],[\"Once you have your model ID and your Hugging Face token ID, go back to your Zap and follow these ins...\"],[\"### Step 3: Save the results on Google Sheets\\n\\nAs the last step to your Zap, you will save the resul...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"Luckily, tools like the [Inference API](https:\\u002f\\u002fhuggingface.co\\u002finference-api) makes it super easy to...\"],[\"--\\ntitle: \\\"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\\"...\"],[\"![Screenshot of YAML metadata](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"#### Why is language metadata important?\\n\\nLanguage metadata can be a vital tool for finding relevant...\"],[\"```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"biglam\\u002fon_the_books\\\")\\n```\\n\\nHowe...\"],[\"#### Predicting the language of a dataset \\n\\nOnce we have some examples of text from a dataset, we ne...\"],[\"Once we’ve done this filtering, we have a further step of deciding how to use these predictions. The...\"],[\"### Using Librarian-Bot to Update Metadata\\n\\nTo ensure this valuable language metadata is incorporate...\"],[\"--\\ntitle: \\\"Jupyter X Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f135_notebooks-hub\\u002fbefore_after_notebook_...\"],[\"\\u003cfigure\\u003e\\n  \\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f135_notebooks-hub\\u002fbefore_after_notebook_rendering.png\\\" alt=\\\"A side...\"],[\"--\\ntitle: \\\"Gradio is joining Hugging Face!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f42_gradio_joins_hf\\u002fthumbnail.png...\"],[\"I recruited my talented housemates Ali Abdalla, Ali Abid, and Dawood Khan to release the first versi...\"],[\"Also: [we are hiring!!](https:\\u002f\\u002fapply.workable.com\\u002fhuggingface\\u002f) ❤️...\"],[\"--\\ntitle: \\\"Generating Stories: AI for Game Development #5\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002f...\"],[\"### Process\\n\\n**Requirements:** I'm using [ChatGPT](https:\\u002f\\u002fopenai.com\\u002fblog\\u002fchatgpt\\u002f) throughout this...\"],[\"3. **Write the content.** Once I'm happy with the story summary, I ask ChatGPT to write the in-game ...\"],[\"\\u003e ⚠️ **Limitation:** Using outputs from language models directly may have unintended legal, ethical,...\"],[\"\\u003e ⚠️ **Limitation:** Language models are susceptible to repetition.\\n\\nTo wrap up this section, here a...\"],[\"While many prevalent contenders are closed-source, there are also open-source dialog agent efforts, ...\"],[\"### Conclusion\\n\\nWant to play the final farming game? Check it out [here](https:\\u002f\\u002fhuggingface.co\\u002fspac...\"],[\"--\\ntitle: \\\"Making automatic speech recognition work on large files with Wav2Vec2 in 🤗 Transformers\\\"\\n...\"],[\"```bash\\npip install transformers\\n```\\n\\n```python\\nfrom transformers import pipeline\\n\\n# This will work ...\"],[\"Chunking with stride\\n--------------------\\n\\nWav2Vec2 uses the [CTC algorithm](https:\\u002f\\u002fdistill.pub\\u002f201...\"],[\"Chunking with stride on LM augmented models\\n-------------------------------------------\\n\\nIn [transfo...\"],[\"--\\ntitle: \\\"Red-Teaming Large Language Models\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fred-teaming\\u002fthumbnail.png\\naut...\"],[\"**Red-teaming** *is a form of evaluation that elicits model vulnerabilities that might lead to undes...\"],[\"Since red-teaming requires creative thinking of possible model failures, it is a problem with a larg...\"],[\"See [this](https:\\u002f\\u002ftwitter.com\\u002fspiantado\\u002fstatus\\u002f1599462375887114240) tweet thread for more examples....\"],[\"**Open source datasets for Red-teaming:**\\n\\n1. Meta’s [Bot Adversarial Dialog dataset](https:\\u002f\\u002fgithub...\"],[\"**Future directions:**\\n\\n1. There is no open-source red-teaming dataset for code generation that atte...\"],[\"--\\ntitle: \\\"2023, year of open LLMs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fcv_state\\u002fthumbnail.png\\nauthors:\\n- user: ...\"],[\"A **tokenizer** defines how the text from the training dataset is converted to numbers (as a model i...\"],[\"Pretrained LLMs can also be specialized or adapted for a specific task after pretraining, particular...\"],[\"## 🗝️ 2022, from a race for size to a race for data\\nWhat open models were available to the community...\"],[\"2. [OPT](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2205.01068) (Open Pre-trained Transformer)\\nThe OPT [model](ht...\"],[\"These huge models were exciting but also very expensive to run! When performing inference (computing...\"],[\"All these releases a) included model weights (under varyingly open licenses) and b) had good perform...\"],[\"The [Pythia](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2304.01373) models were released by the open-source non-p...\"],[\"Early in the summer came the [X-Gen](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2309.03450) [models](https:\\u002f\\u002fhugg...\"],[\"In parallel, a notable event of the end of the year 2023 was the rise of performances and a number o...\"],[\"**Chat-based fine-tuning** is a variant of supervised fine-tuning, where the annotated data is chat ...\"],[\"Both these methods are relatively easy to implement: you just need to find or generate related datas...\"],[\"**Direct preference optimization** (DPO) is another variation of RLHF, but does not require the trai...\"],[\"At the beginning of 2023, a few datasets for instruction\\u002fchat finetuning were already released. For ...\"],[\"❄️ Winter 2022\\u002f2023: In January this year, the [Human ChatGPT Instruction corpus](https:\\u002f\\u002fhuggingfac...\"],[\"🌱 Spring: In April, BAIR (Berkeley AI Research lab) released [Koala](https:\\u002f\\u002fbair.berkeley.edu\\u002fblog\\u002f...\"],[\"🌻Summer: In August, [UltraLM](https:\\u002f\\u002fgithub.com\\u002fthunlp\\u002fUltraChat) (a high-performing chat fine-tune...\"],[\"As we can see, this whole year's development relies both on the creation of new datasets through the...\"],[\"But what does it mean to merge a model?\\n\\n**Model merging** is a way to fuse the weights of different...\"],[\"You might want to use what is called **parameter efficient fine-tuning** (PEFT).\\nThis technique firs...\"],[\"So, if you reduce the precision, you reduce the memory each model parameter takes in storage, theref...\"],[\"New releases include\\n- A mixture of experts:\\n\\t- [Mixtral](https:\\u002f\\u002fhuggingface.co\\u002fmistralai\\u002fMixtral-8...\"],[\"That's it folks! \\nI hope you enjoyed this year's review, learned a thing or two, and feel as enthusi...\"],[\"--\\ntitle: \\\"The N Implementation Details of RLHF with PPO\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f167_the_n_implemen...\"],[\"- In [Matching Learning Curves](#matching-learning-curves), we show our main contribution: creating ...\"],[\"# Matching Learning Curves\\n\\nOur main contribution is to reproduce OAI’s results in stylistic tasks, ...\"],[\"# General Implementation Details\\n\\nWe now take a technical deep dive into the implementation details ...\"],[\"1. **The reward model and policy’s value head take input as the concatenation of `query` and `respon...\"],[\"2. **Pad with a special padding token and truncate inputs.** \\n    1. OAI sets a fixed input length f...\"],[\"2. When putting everything together, here is an example\\n    \\n    ```python\\n    import transformers\\n ...\"],[\"2. For example, if the `query=[23073, 50259, 50259]` and `response=[11, 339, 561]`, where (`50259` i...\"],[\"context_length = query.shape[1]\\n        query_response = torch.cat((query, response), 1)\\n        pre...\"],[\"Usually, we almost never pass `position_ids` in transformers. All the masking and shifting logic are...\"],[\"pad_id = tokenizer.pad_token_id\\n        query = torch.tensor([\\n            [pad_id, pad_id, 23073],\\n...\"],[\")\\n        print(output.sequences)\\n        \\n        \\\"\\\"\\\"\\n        tensor([[    0,     0, 23073, 16851, ...\"],[\"1. Note: I believe the dataset shuffling has a bug — the dataset is shuffled using the same seed for...\"],[\"# Reward Model Implementation Details\\n\\nIn this section, we discuss reward-model-specific implementat...\"],[\"1. **The reward model only outputs the value at the last token.**\\n    1. Notice that the rewards obt...\"],[\"2. **Reward head layer initialization**\\n    1. The weight of the reward head is initialized accordin...\"],[\"2. When performing the normalization process, the code first sets `reward_gain=1, reward_bias=0` ([l...\"],[\"5. Note that responses  \\\\\\\\( y \\\\sim \\\\rho(·|x) \\\\\\\\) we generated for the normalization purpose are from...\"],[\"# Policy Training Implementation Details\\n\\nIn this section, we will delve into details, such as layer...\"],[\"1. **Scale the logits by sampling temperature.** \\n    1. When calculating the log probability of res...\"],[\"3. Then pad the text ([lm_human_preferences\\u002flanguage\\u002fdatasets.py#L66-L67](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002f...\"],[\"2. Specifically, this is achieved with the following steps:\\n        1. **Token truncation**: We want...\"],[\"Samples extracted from our reproduction [https:\\u002f\\u002fwandb.ai\\u002fopenrlbenchmark\\u002flm_human_preference_detail...\"],[\"for mini_batch_start in range(0, batch_size, mini_batch_size):\\n                mini_batch_end = mini...\"],[\"# epoch: 3 batch_inds: [7 2 4 1 3 0 6 5]\\n        # ____⏩ a forward pass on [7. 2.]\\n        # ____⏩ a...\"],[\"9. **Per-minibatch reward and advantage whitening, with optional mean shifting**\\n    1. OAI implemen...\"],[\"mean = tf.Print(mean, [mean], 'mean', summarize=100)\\n            var = tf.Print(var, [var], 'var', s...\"],[\"[1.2127, 1.6000, 1.9873],\\n                [2.3746, 2.7619, 3.1492]], dtype=torch.float64)\\n        \\n ...\"],[\"## **PyTorch Adam optimizer numerical issues w.r.t RLHF**\\n\\n- This implementation detail is so intere...\"],[\"$$\\\\begin{aligned}\\\\text{pytorch adam :}\\\\quad \\\\theta_t & =\\\\theta_{t-1}-\\\\alpha \\\\cdot \\\\hat{m}_t \\u002f\\\\left(\\\\...\"],[\"- The equations above highlight that the distinction between pytorch and tensorflow implementation i...\"],[\"- How does this impact reproducibility and performance? To align settings, we record the original qu...\"],[\"- **PyTorch’s `Adam` presents a more extreme ratio max and min.** Here `ratio = torch.exp(logprobs_d...\"],[\"![adam_gpt2.png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002ftrl-internal-testing\\u002fexample-images\\u002fresolve\\u002fmain\\u002frl...\"],[\"# Acknowledgement\\n\\nThis work is supported by Hugging Face’s Big Science cluster 🤗. We also thank the...\"],[\"--\\ntitle: \\\"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\\\"\\nthumbnail: \\u002fbl...\"],[\"Distributed training is the key to enable training such large ML models. There have been major recen...\"],[\"In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch fe...\"],[\"## Multi-GPU FSDP\\n\\nHere, we experiment on the Single-Node Multi-GPU setting. We compare the performa...\"],[\"Table 1: Benchmarking FSDP on GPT-2 Large (762M) model\\n\\nWith respect to DDP, from Table 1 we can obs...\"],[\"| Method | Batch Size Max ($BS) | Num GPUs | Approx Train Time (Hours) | Notes |\\n| --- | --- | --- |...\"],[\"**Configuration through CLI:**\\n\\n1. **Sharding Strategy**: [1] FULL_SHARD, [2] SHARD_GRAD_OP\\n2. **Min...\"],[\"We can observe that the DDP takes twice as much memory as FSDP with auto wrap.  FSDP without auto wr...\"],[\"+ optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\\n+         optimi...\"],[\"- Mixed precision is currently not supported with FSDP as we wait for PyTorch to fix support for it....\"],[\"FSDP precisely addresses this by sharding the optimizer states, gradients and model parameters acros...\"],[\"# References\\n\\n[1] [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Infe...\"],[\"--\\ntitle: \\\"Hosting your Models and Datasets on Hugging Face Spaces using Streamlit\\\"\\nthumbnail: \\u002fblog...\"],[\"``` python\\nimport streamlit as st\\n\\n# adding the text that will show in the text box as default\\ndefau...\"],[\"Let's start by loading a dataset. A new feature in `Datasets`, called [streaming](https:\\u002f\\u002fhuggingfac...\"],[\"![spaces-streamlit](assets\\u002f29_streamlit-spaces\\u002fstreamlit.gif)\\n\\nThere are so many components and [pac...\"],[\"--\\ntitle: \\\"Mixture of Experts Explained\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fmoe\\u002fthumbnail.png\\nauthors:\\n- user: ...\"],[\"## TL;DR\\n\\nMoEs:\\n- Are **pretrained much faster** vs. dense models\\n- Have **faster inference** compar...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"Now that we have a rough idea of what a MoE is, let’s take a look at the research developments that ...\"],[\"These works led to exploring a mixture of experts in the context of NLP. Concretely, [Shazeer et al....\"],[\"This setup introduces some challenges. For example, although large batch sizes are usually better fo...\"],[\"3. We apply the softmax.\\n\\n$$\\nG(x) = \\\\text{Softmax}(\\\\text{KeepTopK}(H(x), k))\\n$$\\n\\nThis sparsity intro...\"],[\"GShard replaces every other FFN layer with an MoE layer using top-2 gating in both the encoder and t...\"],[\"The GShard paper has contributions by expressing parallel computation patterns that work well for Mo...\"],[\"Switch Transformers also explores the concept of expert capacity. \\n\\n$$\\n\\\\text{Expert Capacity} = \\\\lef...\"],[\"This [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1aGGVHZmtKmcNBbAwa9hbu58DDpIuB5O4?usp=sharin...\"],[\"## What does an expert learn?\\n\\nThe ST-MoE authors observed that encoder experts specialize in a grou...\"],[\"One question is whether to use the auxiliary loss for fine-tuning. The ST-MoE authors experimented w...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"With expert parallelism, experts are placed on different workers, and each worker takes a different ...\"],[\"A big downside of MoEs is the large number of parameters. For local use cases, one might want to use...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"So, TL;DR, some interesting areas to explore:\\n\\n* Distilling Mixtral into a dense model\\n* Explore mod...\"],[\"## Citation\\n\\n```bibtex\\n@misc {sanseviero2023moe,\\n    author       = { Omar Sanseviero and\\n          ...\"],[\"--\\ntitle: \\\"An Introduction to Q-Learning Part 1\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f70_deep_rl_q_part1\\u002fthumbnai...\"],[\"We'll also **implement our first RL agent from scratch**: a Q-Learning agent and will train it in tw...\"],[\"## **What is RL? A short recap**\\n\\nIn RL, we build an agent that can **make smart decisions**. For in...\"],[\"And in this chapter, **we'll dive deeper into the Value-based methods.**\\n\\n## **The two types of valu...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002ftwo-approa...\"],[\"For each state, the state-value function outputs the expected return if the agent **starts at that s...\"],[\"In either case, whatever value function we choose (state-value or action-value function), **the valu...\"],[\"So you see, that's a pretty tedious process if you need to do it for each state value or state-actio...\"],[\"## **Monte Carlo vs Temporal Difference Learning**\\n\\nThe last thing we need to talk about before divi...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fMC-3.jpg\\\" ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fMC-5p.jpg\\\"...\"],[\"We can now update  \\\\\\\\(V(S_0)\\\\\\\\):\\n\\nNew  \\\\\\\\(V(S_0) = V(S_0) + lr * [R_1 + gamma * V(S_1) - V(S_0)]\\\\\\\\)\\n...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f70_deep_rl_q_part1\\u002fsummary-le...\"],[\"--\\ntitle:  Introducing the Hugging Face LLM Inference Container for Amazon SageMaker\\nthumbnail: \\u002fblo...\"],[\"## What is Hugging Face LLM Inference DLC?\\n\\nHugging Face LLM DLC is a new purpose-built Inference Co...\"],[\"Officially supported model architectures are currently:\\n\\n- [BLOOM](https:\\u002f\\u002fhuggingface.co\\u002fbigscience...\"],[\"```python\\nimport sagemaker\\nimport boto3\\nsess = sagemaker.Session()\\n# sagemaker session bucket -\\u003e use...\"],[\"# print ecr image uri\\nprint(f\\\"llm image uri: {llm_image}\\\")\\n```\\n\\n## 3. Deploy Open Assistant 12B to A...\"],[\"```python\\n# Deploy model to an endpoint\\n# https:\\u002f\\u002fsagemaker.readthedocs.io\\u002fen\\u002fstable\\u002fapi\\u002finference\\u002fm...\"],[\"- `temperature`: Controls randomness in the model. Lower values will make the model more determinist...\"],[\"```\\n\\u003c|prompter|\\u003e[Instruction]\\u003c|endoftext|\\u003e\\n\\u003c|assistant|\\u003e\\n```\\n\\nlets give it a first try and ask about...\"],[\"with gr.Blocks() as demo:\\n    gr.Markdown(\\\"## Chat with Amazon SageMaker\\\")\\n    with gr.Column():\\n   ...\"],[\"To clean up, we can delete the model and endpoint.\\n\\n```python\\nllm.delete_model()\\nllm.delete_endpoint...\"],[\"--\\ntitle: \\\"Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face\\\"\\nthumbnail:...\"],[\"In 2021, Fetch set out to optimize its app’s scanning functionality. Fetch is an AWS-native company,...\"],[\"Sam Corzine, Machine Learning Engineer, Fetch\\n\\n\\nFetch’s ML pipeline is powered by several Amazon Sag...\"],[\"Since adopting Amazon SageMaker for ML tuning, training, and retraining, Fetch has enhanced the accu...\"],[\"For every metric that Fetch measures, performance has improved since adopting Amazon SageMaker. The ...\"],[\"## Outcome | Expanding ML to New Use Cases\\n\\nThe ML team at Fetch is continually working on new model...\"],[\"--\\ntitle: \\\"Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\\\"\\nthumbnail: ...\"],[\"Such large models raise new challenges in terms of memory and speed for both [training](https:\\u002f\\u002fhugg...\"],[\"Moreover, support for [HPU graphs](https:\\u002f\\u002fdocs.habana.ai\\u002fen\\u002flatest\\u002fPyTorch\\u002fInference_on_PyTorch\\u002fInf...\"],[\"### Latency\\n\\nWe measured latencies (batch of one sample) for two different sizes of BLOOMZ, both wit...\"],[\"*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, ...\"],[\"The script we wrote enables using your model to complete sentences over a whole dataset. This is use...\"],[\"```\\nBatch n°1\\nInput: ['Facebook has released a report that shows what content was most widely viewed...\"],[\"--------------------------------------------------------------------------------------------------\\nB...\"],[\"In the next section, we explain how to use the script we wrote to perform this benchmark or to apply...\"],[\"This benchmark was performed with Transformers v4.28.1, SynapseAI v1.9.0 and Optimum Habana v1.5.0.\\n...\"],[\"If you are interested in accelerating your Machine Learning training and inference workflows using t...\"],[\"--\\ntitle: Deploying 🤗 ViT on Kubernetes with TF Serving\\nthumbnail: \\u002fblog\\u002fassets\\u002f94_tf_serving_kubern...\"],[\"- **Deploying the Docker container**: You have various options here. The most\\n  widely used option i...\"],[\"# Containerization with Docker \\n\\nThe serving model can handle raw image inputs as bytes and is capab...\"],[\"The custom TensorFlow Serving image should be built on top of the [base one](http:\\u002f\\u002fhub.docker.com\\u002fr...\"],[\"```bash\\n$ NEW_IMAGE=tfserving:$MODEL_NAME\\n\\n$ docker commit \\\\ \\n    --change \\\"ENV MODEL_NAME $MODEL_NA...\"],[\"# Deploying on a Kubernetes cluster\\n\\nDeployment on a Kubernetes cluster requires the following:\\n\\n- P...\"],[\"## Writing Kubernetes manifests\\n\\nKubernetes manifests are written in [\\u003cu\\u003eYAML\\u003c\\u002fu\\u003e](https:\\u002f\\u002fyaml.org\\u002f...\"],[\"```\\n\\nYou can configure the names like `tfs-server`, `tfs-k8s` any way you\\nwant. Under `containers`, ...\"],[\"We made the service type ‘LoadBalancer’ so the endpoints are\\nexposed externally to the Kubernetes cl...\"],[\"You can experiment and set these to the required numbers based on your\\nrequirements. Note, however, ...\"],[\"```\\n\\nNote down the external IP when it becomes available.\\n\\nAnd that sums up all the steps you need t...\"],[\"**`enable_batching`** enables the batch inference capability that\\ncollects incoming requests with a ...\"],[\"In the next post, we’ll show you how to perform these deployments with\\nsignificantly less code with ...\"],[\"--\\ntitle: 'Welcome Stable-baselines3 to the Hugging Face Hub 🤗'\\nthumbnail: \\u002fblog\\u002fassets\\u002f47_sb3\\u002fthumb...\"],[\"```python\\nimport gym\\n\\nfrom huggingface_sb3 import load_from_hub\\nfrom stable_baselines3 import PPO\\nfr...\"],[\"# Train it for 10000 timesteps\\nmodel.learn(total_timesteps=10_000)\\n\\n# Save the model\\nmodel.save(\\\"ppo...\"],[\"And we would love to hear your feedback 💖. 📧 Feel free to [reach us](mailto:thomas.simonini@huggingf...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights [Part 2: SaaS Edition]\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f67_...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f67_ml_director...\"],[\"#### **2. What are the biggest ML challenges within SaaS?**\\na. Productizing ML applications require ...\"],[\"### [Cao (Danica) Xiao](https:\\u002f\\u002fwww.linkedin.com\\u002fin\\u002fcaoxiao\\u002f) - Senior Director of Machine Learning ...\"],[\"#### **2. What are the biggest ML challenges within SaaS?**\\nLack of data for ML model training that ...\"],[\"**Fun Fact:** Raphael has 4 kids and enjoys seeing them learn and make the same mistakes as some of ...\"],[\"This helps make people much better at their jobs.\\n\\n\\n#### **2. What are the biggest ML challenges wit...\"],[\"When we started identifying speakers we went directly with an ML method and this wasn’t as accurate ...\"],[\"We are also seeing a lot of tech from NLP entering other domains like speech and vision and being ab...\"],[\"**Fun Fact:**  The first application of ML I used was for Barbie toys. My professor at Schulich Busi...\"],[\"The sentiment mining tool needs to read data directly in Arabic if you want accurate insights from A...\"],[\"**Networked hospitals and connected care:**\\n\\nWith predictive care, command centers are all set to an...\"],[\"--\\ntitle: \\\"From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community\\\" \\nthumbnail: ...\"],[\"## Background\\n\\nThe efforts to bring Machine Learning to Elixir started almost 2 years ago with [the ...\"],[\"## Your turn\\n\\nIf you want to give Bumblebee a try, you can:\\n\\n  * Download [Livebook v0.8](https:\\u002f\\u002fli...\"],[\"--\\ntitle: \\\"🐶Safetensors audited as really safe and becoming the default\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f142...\"],[\"```\\npip install safetensors\\n```\\n\\nis likely to be the only thing needed to run `safetensors` files sa...\"],[\"Of course, there are other file formats out there, but\\nnone seemed to meet the full set of [ideal re...\"],[\"While it is impossible to \\nprove the absence of flaws, this is a major step in giving reassurance th...\"],[\"--\\ntitle: \\\"Federated Learning using Hugging Face and Flower\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002ffl-with-flower...\"],[\"```python\\nimport random\\n\\nimport torch\\nfrom datasets import load_dataset\\nfrom torch.utils.data import...\"],[\"Once we have a way of creating our trainloader and testloader, we can take care of the training and ...\"],[\"net = AutoModelForSequenceClassification.from_pretrained(\\n        CHECKPOINT, num_labels=2\\n    ).to(...\"],[\"The `get_parameters` function lets the server get the client's parameters. Inversely, the `set_param...\"],[\"## Putting everything together\\n\\nIf you want to check out everything put together, you should check o...\"],[\"--\\ntitle: \\\"Creating open machine learning datasets? Share them on the Hugging Face Hub!\\\"\\nthumbnail: ...\"],[\"The Hugging Face Hub can help achieve this maximum impact. \\n\\n## What is the Hugging Face Hub?\\n\\nThe [...\"],[\"There are a growing number of tools being created which make it easier to understand datasets hosted...\"],[\"### Community tools \\n\\nAlongside the datasets viewer there are a growing number of community created ...\"],[\"### Support for large datasets\\n\\nThe Hub can host large datasets; it currently hosts datasets with mu...\"],[\"The Hub also has features which allow communities to collaborate more easily. This includes a discus...\"],[\"### How can I share my dataset on the Hugging Face Hub? \\n\\nHere are some resources to help you get st...\"],[\"--\\ntitle: \\\"Assisted Generation: a new direction toward low-latency text generation\\\"\\nthumbnail: \\u002fblog...\"],[\"\\u003c!-- [GIF 1 -- FWD PASS] --\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo\\n        ...\"],[\"From the description above, the latency bottleneck in text generation is clear: running a model forw...\"],[\"```python\\n# Example showcasing the impact of batched generation. Measurement device: RTX3090\\nfrom tr...\"],[\"These three types of improvements can be used in tandem, resulting in [high throughput solutions](ht...\"],[\"This means that you can use a model forward pass for a different purpose: in addition to feeding som...\"],[\"Obviously, there are no latency-free assistant models. Nevertheless, it is relatively easy to find a...\"],[\"Finally, the heuristic. By this point, you have probably noticed the similarities between the movie ...\"],[\"\\u003c!-- [GIF 4 -- ASSISTED GENERATION] --\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvid...\"],[\"\\u003c!-- [SPACE WITH GREEDY DECODING PERFORMANCE NUMBERS] --\\u003e\\n\\u003cscript\\n\\ttype=\\\"module\\\"\\n\\tsrc=\\\"https:\\u002f\\u002fgradi...\"],[\"Drawing samples from a probability distribution for the next token will cause our greedy assistant t...\"],[\"Initially released under our 🤗 Transformers library, to be used with the `.generate()` function, we ...\"],[\"--\\ntitle: \\\"Introducing the Private Hub: A New Way to Build With Machine Learning\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"With this in mind, we launched the [Private Hub](https:\\u002f\\u002fhuggingface.co\\u002fplatform) (PH), a new way to...\"],[\"Each model, dataset or space uploaded to the Hub is a [Git-based repository](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"Each model has a [model card](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fmodels-cards), a simple markdown file ...\"],[\"### Datasets\\n\\nData is a key part of building machine learning models; without the right data, you wo...\"],[\"If you've been generating funny images with [DALL-E mini](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fdalle-mini\\u002fd...\"],[\"- [AutoTrain](https:\\u002f\\u002fhuggingface.co\\u002fautotrain): you can use our AutoML no-code solution to train st...\"],[\"Now that we have covered the basics of what the Private Hub is, let's go over how companies are usin...\"],[\"First, we will search for a pre-trained model relevant to our use case and fine-tune it on a custom ...\"],[\"We [clone the model](https:\\u002f\\u002fhuggingface.co\\u002fFinanceInc\\u002ffinbert-pretrain) in our own Private Hub, so ...\"],[\"Next, we select \\\"manual\\\" as the model choice and choose our [cloned Finbert model](https:\\u002f\\u002fhuggingfa...\"],[\"### Easily demo models to relevant stakeholders\\n\\nNow that we have trained our custom model for analy...\"],[\"51 lines of code are all it took to get this ML demo app up and running! 🤯\\n\\n### Scale inferences whi...\"],[\"```python\\nimport requests\\n\\nAPI_URL = \\\"https:\\u002f\\u002fapi-inference.huggingface.co\\u002fmodels\\u002fFinanceInc\\u002fauditor...\"],[\"--\\ntitle: \\\"Speculative Decoding for 2x Faster Whisper Inference\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fwhisper-sp...\"],[\"In this blog post, we demonstrate how Speculative Decoding can be employed to reduce the \\ninference ...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cvideo\\n        style=\\\"max-width: 90%; margin...\"],[\"The inference process then repeats, the assistant model generating a new set of \\\\\\\\( N \\\\\\\\) candidate ...\"],[\"The only constraint for selecting an assistant model is that it must share the same vocabulary as th...\"],[\"## English Speech Transcription\\n\\n### Baseline Implementation\\n\\nWe start by benchmarking Whisper [larg...\"],[\"processor = AutoProcessor.from_pretrained(model_id)\\n```\\n\\nLet's load the English speech transcription...\"],[\"Alright! We see that transcribing the 73 samples took 73 seconds. Let's check the WER of the predict...\"],[\"assistant_model.to(device);\\n```\\n\\n-------------------------------------------------------------------...\"],[\"```python\\nprint(wer.compute(predictions=predictions, references=references))\\n```\\n**Outputs:**\\n```\\n0....\"],[\"To use speculative decoding for multilingual speech transcription, one could either use one of the [...\"],[\"```python\\nall_time = 0\\npredictions = []\\nreferences = []\\n\\nfor sample in tqdm(dataset):\\n    audio = sa...\"],[\"print(\\\"Time:\\\", all_time)\\nprint(\\\"WER:\\\", wer_result)\\n```\\n**Outputs:**\\n```\\n100%|██████████| 73\\u002f73 [01:0...\"],[\"#### Batch Size\\n\\nIt is worth noting that the largest speed gains with speculative decoding come with...\"],[\"--\\ntitle: \\\"Snorkel AI x Hugging Face: unlock foundation models for enterprises\\\"\\nthumbnail: \\u002fblog\\u002fass...\"],[\"## Foundation models in Snorkel Flow\\n\\nThe Snorkel Flow development platform enables users to [adapt ...\"],[\"## How does Hugging Face help?\\n\\nSnorkel AI’s partnership with Hugging Face supercharges Snorkel Flow...\"],[\"#### \\\"With Snorkel AI and Hugging Face Inference Endpoints, companies will accelerate their data-cen...\"],[\"--\\ntitle: \\\"An Introduction to Q-Learning Part 2\\u002f2\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f73_deep_rl_q_part2\\u002fthumbn...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f73_deep_rl_q_part2\\u002fenvs.gif\\\" ...\"],[\"If we take this maze example:\\n\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"asset...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f73_deep_rl_q_part2\\u002flink-value...\"],[\"Epsilon Greedy Strategy is a policy that handles the exploration\\u002fexploitation trade-off.\\n\\nThe idea i...\"],[\"It means that to update our \\\\\\\\(Q(S_t, A_t)\\\\\\\\):\\n\\n- We need \\\\\\\\(S_t, A_t, R_{t+1}, S_{t+1}\\\\\\\\).\\n- To upd...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f73_deep_rl_q_part2\\u002foff-on-4.j...\"],[\"Let's do it for 2 training timesteps:\\n\\nTraining timestep 1:\\n\\n**Step 2: Choose action using Epsilon G...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f73_deep_rl_q_part2\\u002fq-ex-7.jpg...\"],[\"That’s **normal if you still feel confused** with all these elements. **This was the same for me and...\"],[\"--\\ntitle: \\\"Overview of natively supported quantization schemes in 🤗 Transformers\\\" \\nthumbnail: \\u002fblog\\u002f...\"],[\"## Resources\\n\\n- [GPTQ blogpost](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fgptq-integration) – gives an overview on...\"],[\"### What are the benefits of bitsandbytes?\\n**easy**: bitsandbytes still remains the easiest way to q...\"],[\"**easily-serializable**: GPTQ models support serialization for any number of bits. Loading models fr...\"],[\"## Diving into speed benchmarks \\nWe decided to provide an extensive benchmark for both inference and...\"],[\"with batch size = 16:\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token laten...\"],[\"![Benchmark use_cache=False A100](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fr...\"],[\"![Benchmark A100](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog...\"],[\"with 7b model: \\n\\n| model_id                           | Average | ARC   | Hellaswag | MMLU  | Truthf...\"],[\"We hope that this overview will make it easier for everyone to use LLMs in their applications and us...\"],[\"--\\ntitle: 'Train and Fine-Tune Sentence Transformers Models'\\nthumbnail: \\u002fblog\\u002fassets\\u002f95_training_st_...\"],[\"This is how the Sentence Transformers models work:\\n\\n1. **Layer 1** – The input text is passed throug...\"],[\"Why not use a Transformer model, like BERT or Roberta, out of the box to create embeddings for entir...\"],[\"Unfortunately, there is no single way to prepare your data to train a Sentence Transformers model. I...\"],[\"Most dataset configurations will take one of four forms (below you will see examples of each case):\\n...\"],[\"Note that Sentence Transformers models can be trained with human labeling (cases 1 and 3) or with la...\"],[\"- Case 4: The [Quora Triplets dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fembedding-data\\u002fQQP_triplets) ...\"],[\"Convert the examples into `InputExample`'s. For simplicity, (1) only one of the positives and one of...\"],[\"Case 2: If you only have two similar sentences (two positives) with no labels, then you can use the ...\"],[\"This figure summarizes the different types of datasets formats, example dataets in the Hub, and thei...\"],[\"2. If in a python notebook, you can use `notebook_login`.\\n\\n```py\\nfrom huggingface_hub import noteboo...\"],[\"## Extra Resources\\n\\n- [Getting Started With Embeddings](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fgetting-started-...\"],[\"--\\ntitle: \\\"SDXL in 4 steps with Latent Consistency LoRAs\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002flcm_sdxl\\u002flcm_thumb...\"],[\"## Contents\\n\\n- [Method Overview](#method-overview)\\n- [Why does this matter](#why-does-this-matter)\\n-...\"],[\"1. Select an available teacher model from the Hub. For example, you can use [SDXL (base)](https:\\u002f\\u002fhu...\"],[\"## Fast Inference with SDXL LCM LoRAs\\n\\nThe version of `diffusers` released today makes it very easy ...\"],[\"These are the 8 images displayed in a grid:\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co...\"],[\"Then we can run inference as usual for SDXL. We’ll gather results using varying number of steps:\\n\\n``...\"],[\"prompt = \\\"collage style kid sits looking at the night sky, full of stars\\\"\\n\\ngenerator = torch.Generat...\"],[\"This section is not meant to be exhaustive, but illustrative of the generation speed we achieve on v...\"],[\"## LCM LoRAs and Models Released Today\\n\\n- [Latent Consistency Models LoRAs Collection](https:\\u002f\\u002fhuggi...\"],[\"pipe.load_lora_weights(lcm_lora_id)\\npipe.load_lora_weights(\\\"CiroN2022\\u002ftoy-face\\\", weight_name=\\\"toy_fa...\"],[\"We hope these scripts inspire the community to try their own fine-tunes. Please, do let us know if y...\"],[\"## Credits\\n\\nThe amazing work on Latent Consistency Models was performed by the [LCM Team](https:\\u002f\\u002fla...\"],[\"--\\ntitle: \\\"Introducing Skops\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f94_skops\\u002fintroducing_skops.png\\nauthors:\\n- user...\"],[\"# let's save the model\\nmodel_path = \\\"example.pkl\\\"\\nlocal_repo = \\\"my-awesome-model\\\"\\nwith open(model_pa...\"],[\"You can create the model card by instantiating the `Card` class from `skops`. During model serializa...\"],[\"We can also add any plot of our choice to the card using `add_plot` like below.\\n\\n```python\\nimport ma...\"],[\"The inference widget is enabled to make predictions in the repository.\\n\\n![Hosted Inference Widget](a...\"],[\"--\\ntitle: \\\"Run a Chatgpt-like Chatbot on a Single GPU with ROCm\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fchatbot-am...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002freso...\"],[\"By leveraging this technique, several 4-bit quantized Vicuna models are\\navailable from Hugging Face ...\"],[\"```\\nsudo apt update && sudo apt upgrade -y\\nwget https:\\u002f\\u002frepo.radeon.com\\u002famdgpu-install\\u002f5.4.3\\u002fubuntu\\u002f...\"],[\"python as C extensions. The kernels of this implementation are composed\\nof dequantization + FP32 Mat...\"],[\"nohup python0 -W ignore::UserWarning -m fastchat.serve.model_worker --model-path \\u002fpath\\u002fto\\u002fquantized_...\"],[\"Test environment:\\n\\n\\\\- GPU: Instinct MI210, RX6900XT\\n\\n\\\\- python: 3.10\\n\\n\\\\- pytorch: 2.1.0a0+gitfa08e54...\"],[\"By following this guide, you should now have a better understanding of\\nhow to set up and run the Vic...\"],[\"Quantize Vicuna-13b model with this command. QAT is done based on c4\\ndata-set but you can also use o...\"],[\"--\\ntitle: \\\"Zero-shot image-to-text generation with BLIP-2\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fblip-2\\u002fthumbnail...\"],[\"## Introduction\\n\\nRecent years have seen rapid advancements in computer vision and natural language p...\"],[\"## What's under the hood in BLIP-2?\\n\\nBLIP-2 bridges the modality gap between vision and language mod...\"],[\"In the second pre-training stage, the query embeddings now have the relevant visual information to t...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"```\\n\\\"two cartoon monsters sitting around a campfire\\\"\\n```\\n\\nThis is an impressively accurate descripti...\"],[\"```\\ncontext = [\\n   (\\\"What is a dinosaur holding?\\\", \\\"a torch\\\"),\\n   (\\\"Where are they?\\\", \\\"In the woods....\"],[\"--\\ntitle: \\\"Scaling-up BERT Inference on CPU (Part 1)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f21_bert_cpu_scaling_pa...\"],[\"This blog post is the first part of a series which will cover most of the hardware and software opti...\"],[\"These two metrics will help us understand the benefits and tradeoffs along this blog post.\\n\\nThe benc...\"],[\"## 3. Baselines\\n\\nAll the results below were run on [Amazon Web Services (AWS) c5.metal instance](htt...\"],[\"One possible way to explain such difference between the two frameworks might be the underlying techn...\"],[\"To illustrate this, imagine two tasks **A** and **B**, executing in parallel, each on its own softwa...\"],[\"Back to our model inference workload... If you think about it, in a perfect world with a fully optim...\"],[\"```shell\\nubuntu@some-ec2-machine:~$ lscpu\\nArchitecture:                    x86_64\\nCPU op-mode(s):   ...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image\\\"\\u003e\\n  \\u003cimg class=\\\"centered\\\" alt=\\\"Non-Uniform Memory Access and Uniform Memor...\"],[\"```shell\\npython3 src\\u002fmain.py model=bert-base-cased backend.name=pytorch batch_size=1 sequence_length...\"],[\"In our case, each socket is one NUMA node and there are 2 NUMA nodes. \\nEach socket or each NUMA node...\"],[\"In order to illustrate this, the figure 6. below takes different problem sizes (`batch_size = 1, seq...\"],[\"Instead of throwing more cores to the task as you would do in the core count scaling setup, now we w...\"],[\"The outcomes remain the same, our 4 instances are effectively running in a truly parallel manner.  \\n...\"],[\"This method actually changes both the size of the problem (_batch size_), and the resources involved...\"],[\"Also, it is important to notice the results might look totally different on another system _(i.e. Op...\"],[\"\\u003cfigure class=\\\"image\\\"\\u003e\\n  \\u003cimg alt=\\\"Batch scaling experiment for PyTorch and Tensorflow\\\" src=\\\"assets\\u002f...\"],[\"Last but not least, many of the knobs discussed along this blog post can be automatically tuned thro...\"],[\"## References\\n\\n1. [Benchmarking Transformers: PyTorch and TensorFlow](https:\\u002f\\u002fmedium.com\\u002fhuggingface...\"],[\"--\\ntitle: \\\"Welcome PaddlePaddle to the Hugging Face Hub\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f126_paddlepaddle\\u002ft...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f126_paddlepaddle\\u002fpaddle_tag.png\\\" alt=\\\"PaddlePaddle Tag\\\"\\u002f\\u003e\\n\\u003c\\u002fp\\u003e...\"],[\"![snippet](assets\\u002f126_paddlepaddle\\u002fsnippet.png)\\n\\n## Share Models\\n\\nDepending on the PaddlePaddle libr...\"],[\"--\\ntitle: Image Similarity with Hugging Face Datasets and Transformers\\nthumbnail: \\u002fblog\\u002fassets\\u002fimage...\"],[\"To study the fully working image-similarity system, you can refer to the Colab Notebook linked at th...\"],[\"model_ckpt = \\\"nateraw\\u002fvit-base-beans\\\"\\nprocessor = AutoImageProcessor.from_pretrained(model_ckpt)\\nmod...\"],[\"```py\\nfrom datasets import load_dataset\\n\\n\\ndataset = load_dataset(\\\"beans\\\")\\n```\\n\\nThis is how a single ...\"],[\"We can write a simple utility and `map()` it to our dataset of candidate images to compute the embed...\"],[\"```py\\nall_candidate_embeddings = np.array(candidate_subset_emb[\\\"embeddings\\\"])\\nall_candidate_embeddin...\"],[\"## Perform a query\\n\\nGiven all the utilities, we're equipped to do a similarity search. Let's have a ...\"],[\"🤗 Datasets offers direct integrations with [FAISS](https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002ffaiss) which ...\"],[\"Still looking to learn more? Here are some additional resources that might be useful for you:\\n\\n* [Fa...\"],[\"--\\ntitle: \\\"Japanese Stable Diffusion\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f106_japanese_stable_diffusion\\u002fjsd_thu...\"],[\"- Hugging Face model card: https:\\u002f\\u002fhuggingface.co\\u002frinna\\u002fjapanese-stable-diffusion\\n- Hugging Face Spa...\"],[\"*from [Stable Diffusion with 🧨 Diffusers](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fstable_diffusion)*\\n\\n## Japanes...\"],[\"### Training Details\\n\\nThe biggest challenge in making a Japanese-specific text-to-image model is the...\"],[\"On the other hand, by using our Japanese tokenizer, the prompt is split into interpretable tokens an...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"assets\\u002f106_japanese_stable_diffusion\\u002fjsd-stage2.jpeg\\\" alt=\\\"salary man o...\"],[\"## What’s Next?\\nCompared to Stable Diffusion, Japanese Stable Diffusion is not as versatile and stil...\"],[\"--\\ntitle: \\\"Fine-Tune Wav2Vec2 for English ASR in Hugging Face with 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"In this notebook, we will give an in-detail explanation of how\\nWav2Vec2\\\\'s pretrained checkpoints ca...\"],[\"```python\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()\\n```\\n\\n**Print Output:**\\n```bas...\"],[\"Let\\\\'s start by creating the tokenizer responsible for decoding the\\nmodel\\\\'s predictions.\\n\\n### Creat...\"],[\"Many ASR datasets only provide the target text, `'text'` for each audio\\nfile `'file'`. Timit actuall...\"],[\"Alright! The transcriptions look very clean and the language seems to\\ncorrespond more to written tex...\"],[\"Good! This looks better. We have removed most special characters from\\ntranscriptions and normalized ...\"],[\"-   The model has to learn to predict when a word finished or else the\\n    model prediction would al...\"],[\"```python\\nrepo_name = \\\"wav2vec2-base-timit-demo-colab\\\"\\n```\\n\\nand upload the tokenizer to the [🤗 Hub](...\"],[\"A Wav2Vec2 feature extractor object requires the following parameters to\\nbe instantiated:\\n\\n-   `feat...\"],[\"```python\\nfrom transformers import Wav2Vec2Processor\\n\\nprocessor = Wav2Vec2Processor(feature_extracto...\"],[\"```python\\nimport IPython.display as ipd\\nimport numpy as np\\nimport random\\n\\nrand_int = random.randint(...\"],[\"**Note**: This mapping function is a good example of how the `Wav2Vec2Processor` class should be use...\"],[\"-   Define a data collator. In contrast to most NLP models, Wav2Vec2 has\\n    a much larger input len...\"],[\"```python\\nimport torch\\n\\nfrom dataclasses import dataclass, field\\nfrom typing import Any, Dict, List,...\"],[\"processor: Wav2Vec2Processor\\n    padding: Union[bool, str] = True\\n    max_length: Optional[int] = No...\"],[\"The model will return a sequence of logit vectors:\\n\\n$$ \\\\mathbf{y}_1, \\\\ldots, \\\\mathbf{y}_m $$, \\n\\nwith...\"],[\"model = Wav2Vec2ForCTC.from_pretrained(\\n    \\\"facebook\\u002fwav2vec2-base\\\", \\n    ctc_loss_reduction=\\\"mean\\\"...\"],[\"During training, a checkpoint will be uploaded asynchronously to the hub every 400 training steps. I...\"],[\"### Training\\n\\nTraining will take between 90 and 180 minutes depending on the GPU\\nallocated to the go...\"],[\"**Print Output:**\\n\\n| Step  | Training Loss  | Validation Loss | WER | Runtime | Samples per Second |...\"],[\"**Note**: we evaluate the test data set with `batch_size=1` on purpose\\ndue to this [issue](https:\\u002f\\u002fg...\"],[\"Let's take a look at some predictions to see what errors are made by the model.\\n\\n**Print Output:**\\n\\n...\"],[\"```python\\nmodel.to(\\\"cuda\\\")\\n\\nwith torch.no_grad():\\n  logits = model(torch.tensor(timit[\\\"test\\\"][:1][\\\"i...\"],[\"--\\ntitle: \\\"Hugging Face and AWS partner to make AI more accessible\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f131_aws...\"],[\"There have been significant advances in new Transformer and Diffuser machine learning models that pr...\"],[\"## Collaborating to scale AI in the cloud\\n\\nThis expanded strategic partnership enables Hugging Face ...\"],[\"--\\ntitle: \\\"Deploy Livebook notebooks as apps to Hugging Face Spaces\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f120_eli...\"],[\"## Hugging Face and Elixir\\n\\nThe Elixir community leverages the Hugging Face platform and its open so...\"],[\"--\\ntitle: \\\"Getting Started with Sentiment Analysis using Python\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f50_sentimen...\"],[\"- *\\\"dear @verizonsupport your service is straight 💩 in dallas.. been with y’all over a decade and th...\"],[\"## 2. How to Use Pre-trained Sentiment Analysis Models with Python\\n\\nNow that we have covered what se...\"],[\"```python\\nspecific_model = pipeline(model=\\\"finiteautomata\\u002fbertweet-base-sentiment-analysis\\\")\\nspecifi...\"],[\"Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or Germ...\"],[\"Let's dive in!\\n\\n### a. Fine-tuning model with Python\\n\\nIn this tutorial, you'll use the IMDB dataset ...\"],[\"```python\\nfrom datasets import load_dataset\\nimdb = load_dataset(\\\"imdb\\\")\\n```\\n\\nIMDB is a huge dataset,...\"],[\"For training, you will be using the [Trainer API](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fv4.15.0\\u002fe...\"],[\"You are almost there! Before training our model, you need to define the training arguments and defin...\"],[\"First, let's upload the model to the Hub:\\n\\n```python\\ntrainer.push_to_hub()\\n```\\n\\nNow that you have pu...\"],[\"Training a sentiment analysis model using AutoNLP is super easy and it just takes a few clicks 🤯. Le...\"],[\"Once you add your dataset, go to the \\\"Trainings\\\" tab and accept the pricing to start training your m...\"],[\"You can use [this notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f182UbzmSeAFgOiow7WNMxvnz-yO-SJQ0...\"],[\"```python\\n# Helper function for handling pagination in our search and handle rate limits\\ndef limit_h...\"],[\"First, let's load the results on a dataframe and see examples of tweets that were labeled for each s...\"],[\"Finally, let's see what words stand out for each sentiment by creating a word cloud:\\n\\n```python\\nfrom...\"],[\"## 5. Wrapping up\\nSentiment analysis with Python has never been easier! Tools such as [🤗Transformers...\"],[\"a href=\\\"https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fsanchit-gandhi\\u002fnotebooks\\u002fblob\\u002fmain\\u002ffine_tune_whispe...\"],[\"When scaled to 680,000 hours of labelled pre-training data, Whisper models \\ndemonstrate a strong abi...\"],[\"| Size   | Layers | Width | Heads | Parameters | English-only                                       ...\"],[\"\\\\\\\\({}^1\\\\\\\\) The name Whisper follows from the acronym “WSPSR”, which stands for “Web-scale Supervised...\"],[\"Linking the notebook to the Hub is straightforward - it simply requires entering your \\nHub authentic...\"],[\"print(common_voice)\\n```\\n\\n## Prepare Feature Extractor, Tokenizer and Data\\n\\nThe ASR pipeline can be d...\"],[\"```python\\nfrom transformers import WhisperFeatureExtractor\\n\\nfeature_extractor = WhisperFeatureExtrac...\"],[\"Since \\nour input audio is sampled at 48kHz, we need to _downsample_ it to \\n16kHz prior to passing it...\"],[\"```python\\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[...\"],[\"The `labels` on the other hand are un-padded. We first pad the sequences\\nto the maximum length in th...\"],[\"batch[\\\"labels\\\"] = labels\\n\\n        return batch\\n```\\n\\nLet's initialise the data collator we've just de...\"],[\"```python\\nfrom transformers import WhisperForConditionalGeneration\\n\\nmodel = WhisperForConditionalGen...\"],[\"**Note**: if one does not want to upload the model checkpoints to the Hub, \\nset `push_to_hub=False`....\"],[\"To launch training, simply execute:\\n\\n\\n```python\\ntrainer.train()\\n```\\n\\nOur best WER is 32.0% - not bad...\"],[\"iface.launch()\\n```\\n\\n## Closing Remarks\\n\\nIn this blog, we covered a step-by-step guide on fine-tuning...\"],[\"--\\ntitle: How to train a Language Model with Megatron-LM\\nthumbnail: \\u002fblog\\u002fassets\\u002f100_megatron_traini...\"],[\"## Why Megatron-LM?\\n\\nBefore getting into the training details, let’s first understand what makes thi...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f100_megatron_training\\u002fkernel_fusion.png\\\" width=\\\"600\\\" \\u002f\\u003e\\n\\u003c\\u002fp\\u003e...\"],[\"You also need to add the vocabulary file `vocab.json` and merges table `merges.txt` of your tokenize...\"],[\"train_data = load_dataset('codeparrot\\u002fcodeparrot-clean-train', split='train')\\ntrain_data.to_json(\\\"co...\"],[\"### Training\\nYou can configure the model architecture and training parameters as shown below, or put...\"],[\"This setup uses Data Parallelism, but it is also possible to use Model Parallelism for very large mo...\"],[\"Don't forget to push your model to the hub and share it with the community, it only takes three line...\"],[\"--\\ntitle: \\\"Accelerating Hugging Face Transformers with AWS Inferentia2\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f140...\"],[\"Fitting these models on a single accelerator can be quite difficult, let alone getting the high thro...\"],[\"Inf2 instances are available in multiple sizes, which are equipped with between 1 to 12 Inferentia 2...\"],[\"_Note: that we did not optimize the model for the GPU environment, the models were evaluated in fp32...\"],[\"Let’s highlight a few insights of the benchmark.\\n\\n### BERT-base\\n\\nHere is the latency comparison for ...\"],[\"The initial benchmarking results are promising, and show that Inferentia2 delivers superior latency ...\"],[\"--\\ntitle: \\\"Introducing SafeCoder\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f159_safecoder\\u002fthumbnail.jpg\\nauthors:\\n- us...\"],[\"However, relying on closed-source Code LLMs to create internal code assistants exposes companies to ...\"],[\"The StarCoder models offer unique characteristics ideally suited to enterprise self-hosted solution:...\"],[\"In the deployment phase of SafeCoder, the customer deploys containers provided by Hugging Face on th...\"],[\"All these efforts translate into legal risk minimization for users of the StarCoder models, and cust...\"],[\"SafeCoder inference supports various hardware to give customers a wide range of options: NVIDIA Ampe...\"],[\"“Our collaboration with Hugging Face around SafeCoder fully aligns to VMware’s goal of enabling cust...\"],[\"--\\ntitle: \\\"The Reformer - Pushing the limits of language modeling\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f03_reform...\"],[\"The Reformer pushes the limit of longe sequence modeling by its ability to process up to half a mill...\"],[\"**Note**: *Axial Positional Encodings* are not explained in the official Reformer paper, but are ext...\"],[\"In short, a global self-attention layer projects \\\\\\\\(\\\\mathbf{X}\\\\\\\\) to the query, key and value matric...\"],[\"This is also the reason why `bert-base-cased` has a `config.max_position_embedding_size` of only 512...\"],[\"A simple remedy is to augment each chunk with `config.local_num_chunks_before`, *i.e.* \\\\\\\\(n_{p}\\\\\\\\), ...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"This enhanced local self-attention is better than the vanilla local self-attention architecture but ...\"],[\"The premise of LSH self-attention is to be more or less as efficient as local self-attention while a...\"],[\"First, the authors of Reformer notice that sharing the query and key projections: \\\\\\\\(\\\\mathbf{Q} = \\\\m...\"],[\"Second, the authors make use of the **LSH** algorithm to cluster the query vectors into a predefined...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"The permuted vectors \\\\\\\\(\\\\mathbf{X'}\\\\\\\\) as shown above are chunked and shared query key self-attentio...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"---\\n \\\\\\\\( {}^{1} \\\\\\\\) The authors run some preliminary experiments confirming that shared query key se...\"],[\"from transformers import ReformerConfig, PyTorchBenchmark, PyTorchBenchmarkArguments\\n```\\n\\nFirst, let...\"],[\"The longer the input sequence, the more visible is the quadratic relationship \\\\\\\\( \\\\mathcal{O}(n^2) \\\\...\"],[\"1 \\u002f 1\\n    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total...\"],[\"### Chunked Feed Forward Layer in Reformer\\n\\nIn Reformer, the _LSH_- or _local_ self-attention layer ...\"],[\"Let's illustrate the feed forward layers for \\\\\\\\( \\\\mathbf{\\\\overline{z}}_1, \\\\ldots, \\\\mathbf{\\\\overline{...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"Finally, it is important to remember that *chunked linear layers* yield a mathematically equivalent ...\"],[\"First, let's compare the default `google\\u002freformer-enwik8` model without chunked feed forward layers ...\"],[\"Interesting, chunked feed forward layers do not seem to help here at all. The reason is that `config...\"],[\"1 \\u002f 2\\n    2 \\u002f 2\\n    \\n    ====================      INFERENCE - MEMORY - RESULT       ===============...\"],[\"### Reversible Residual Layers in Reformer\\n\\nLet's start by investigating why training a model requir...\"],[\"Using the same notation as before, the input of a transformer layer *i.e.* \\\\\\\\( \\\\mathbf{X} \\\\\\\\) is fir...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"The reversible transformer layer can be visualized for \\\\\\\\( \\\\mathbf{x}_1, \\\\ldots, \\\\mathbf{x}_{16} \\\\\\\\)...\"],[\"If we assume to know \\\\\\\\( \\\\mathbf{\\\\overline{Y}}^{(1)}, \\\\mathbf{\\\\overline{Y}}^{(2)} \\\\\\\\), it can easily...\"],[\"---\\n \\\\\\\\( ^{1} \\\\\\\\) In the previous two sections, we have omitted the layer norm layers preceding both...\"],[\"Let's measure the required memory for the standard `bert-base-uncased` BERT model by increasing the ...\"],[\"It can be seen that adding a single layer of BERT linearly increases the required memory by more tha...\"],[\"## 4. Axial Positional Encodings\\n\\nReformer makes it possible to process huge input sequences. Howeve...\"],[\"Here, we showcase only the positional encodings \\\\\\\\( \\\\mathbf{e}_{1} \\\\\\\\), \\\\\\\\( \\\\mathbf{e}_{2} \\\\\\\\), and ...\"],[\"![alt text](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002freformer_ben...\"],[\"$$\\\\mathbf{e'}_{i} = \\\\left[ \\\\left[\\\\mathbf{e}_{\\\\text{down, } i \\\\% n_\\\\text{max}^1}\\\\right]^T, \\\\left[\\\\mat...\"],[\"To demonstrate the drastic reduction in size, \\nlet's assume we would have set `config.axial_pos_shap...\"],[\"To begin with, we will compare the shape of axial position encodings with standard positional encodi...\"],[\"Having read the theory, the shape of the axial positional encoding weights should not be a surprise ...\"],[\"--\\ntitle: \\\"Chat Templates: An End to the Silent Performance Killer\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fchat-te...\"],[\"Whether you're fine-tuning a model or using it directly for inference, it's always a good idea to mi...\"],[\"This sequence of messages needs to be converted into a text string before it can be tokenized and us...\"],[\"This is the problem that **chat templates** aim to solve. Chat templates are [Jinja template strings...\"],[\"## Why bother doing this? Why not just pick a standard format?\\n\\nThis is an excellent idea! Unfortuna...\"],[\"## How do templates work?\\n\\nChat templates are part of the **tokenizer**, because they fulfill the sa...\"],[\"## How do I get started with templates?\\n\\nEasy! If a tokenizer has the `chat_template` attribute set,...\"],[\"## Conclusion: Template philosophy\\n\\nWe think templates are a very exciting change. In addition to re...\"],[\"--\\ntitle: \\\"Make your llama generation time fly with AWS Inferentia2\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002finferen...\"],[\"## Export the Llama 2 model to Neuron\\n\\nAs explained in the [optimum-neuron documentation](https:\\u002f\\u002fhu...\"],[\"```\\n\\u003e\\u003e\\u003e model.push_to_hub(\\n        \\\"a_local_path_for_compiled_neuron_model\\\",\\n        repository_id=\\\"...\"],[\"Using them is as simple as:\\n\\n```\\n\\u003e\\u003e\\u003e from optimum.neuron import pipeline\\n\\n\\u003e\\u003e\\u003e p = pipeline('text-gen...\"],[\"All other models are compiled to use the full extent of cores available on the `inf2.48xlarge` insta...\"],[\"![Llama2 inferentia2 encoding-time](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"The \\\"budget\\\" model (`Llama2 7B-B`) is deployed on an `inf2.xlarge` instance while other models are d...\"],[\"Interestingly, the deployed models latency is not too sensitive to the batch size, which opens the w...\"],[\"--\\ntitle: \\\"Introduction to 3D Gaussian Splatting\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games\\u002fthumbnail...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f124_ml-for-ga...\"],[\"This procedure helps the gaussians better fit fine-grained details, while pruning unnecessary gaussi...\"],[\"Finally, there is growing research interest in [Embodied AI](https:\\u002f\\u002fieeexplore.ieee.org\\u002fiel7\\u002f743329...\"],[\"So will we see 3D Gaussian Splatting fully reimplemented in a production environment? The answer is ...\"],[\"--\\ntitle: \\\"Train your ControlNet with diffusers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f136_train-your-controlnet\\u002ft...\"],[\"3. **Training the model**: Once your dataset is ready, it is time to train the model. This is the ea...\"],[\"The `FaceSynthetics` dataset sounded like a great start: it contains ground truth images of faces, a...\"],[\"Now, with the ground truth `image` and the `conditioning_image` on the dataset, we are missing one s...\"],[\"With just 1 epoch (so after the model \\\"saw\\\" 100K images), it already converged to following the pose...\"],[\"And then run the [train_controlnet.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fexamples\\u002fc...\"],[\"Let's break down some of the settings, and also let's go over some optimisation tips for going as lo...\"],[\"- `num_train_epochs`: Each epoch corresponds to how many times the images in the training set will b...\"],[\"### Fitting on a 16GB VRAM GPU\\n```shell \\npip install bitsandbytes\\n\\n--train_batch_size=1 \\\\\\n--gradient...\"],[\"--\\ntitle: \\\"Spread Your Wings: Falcon 180B is here\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f162_falcon_180b\\u002fthumbnai...\"],[\"## What is Falcon-180B?\\n\\nFalcon 180B is a model released by [TII](https:\\u002f\\u002ffalconllm.tii.ae\\u002f) that fo...\"],[\"## How good is Falcon 180B?\\n\\nFalcon 180B is the best openly released LLM today, outperforming Llama ...\"],[\"## How to use Falcon 180B?\\n\\nFalcon 180B is available in the Hugging Face ecosystem, starting with Tr...\"],[\"### Transformers\\n\\nWith the release of Transformers 4.33, you can use Falcon 180B and leverage all th...\"],[\"#### 8-bit and 4-bit with `bitsandbytes`\\n\\nThe 8-bit and 4-bit quantized versions of Falcon 180B show...\"],[\"## Additional Resources\\n\\n- [Models](https:\\u002f\\u002fhuggingface.co\\u002fmodels?other=falcon&sort=trending&search=...\"],[\"--\\ntitle: Swift 🧨Diffusers - Fast Stable Diffusion for Mac\\nthumbnail: \\u002fblog\\u002fassets\\u002ffast-mac-diffuser...\"],[\"Why would you want to run a native Mac app then? There are many reasons:\\n- It uses Core ML models, i...\"],[\"Come check out our benchmarks. All the combinations use the CPU in addition to either the GPU or the...\"],[\"We found that the amount of memory does not seem to play a big factor on performance, but the number...\"],[\"## Other Improvements in Version 1.1\\n\\nIn addition to the performance optimization and fixing a few b...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_ml_director_insights\\u002fth...\"],[\"🚀  Let’s meet some top Machine Learning Directors and hear what they have to say about Machine Learn...\"],[\"_Tightened testing:_ In a capital intensive media venture, there is a need to shorten the time betwe...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f61_ml_director...\"],[\"#### **1. How has ML made a positive impact on Pharmaceuticals?**\\nAI\\u002fML applications have exploded i...\"],[\"As practitioners, we know that currently, neither is true.\\n \\nML models can be incredibly valuable bu...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f61_ml_director...\"],[\"**Machine Learning & Sensing Laboratory:** A University of Florida laboratory that develops machine ...\"],[\"#### **4. What excites you most about the future of ML?**\\nThere are a lot of really exciting directi...\"],[\"**Xpress Technologies:** A digital freight matching technology to connect Shippers, Brokers and Carr...\"],[\"#### **3. What’s a common mistake you see people make trying to integrate ML into Logistics?**\\nI thi...\"],[\"At BEN, Nic innovates intelligent technologies that scale human capabilities to reach people. See hi...\"],[\"#### **1. How has ML made a positive impact on Marketing?**\\nIn so many ways! It’s completely changin...\"],[\"Understanding what makes a creator\\u002finfluencer successful over time is really hard. There is a lot of...\"],[\"#### **4. What excites you most about the future of ML?**\\nThere is so much exciting stuff going on. ...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f61_ml_director...\"],[\"#### **1. How has ML made a positive impact on the Energy\\u002fUtility industry?**\\nAccess to business ins...\"],[\"#### **4. What excites you most about the future of ML?**\\nI’ve been doing this for over a decade, an...\"],[\"---\\n\\n🤗   Thank you for joining us in this first installment of ML Director Insights. Stay tuned for ...\"],[\"--\\ntitle: \\\"My Journey to a serverless transformers pipeline on Google Cloud\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"## The Transformers library\\nI was a bit confused at the beginning when I downloaded the .h5 file. I ...\"],[\"### Step 2 - Test on AI-Platform Prediction\\n\\nAs the model is not a \\\"pure TensorFlow\\\" saved model but...\"],[\"## Implementation of the serverless pipeline\\n\\nThe final solution consists of four different componen...\"],[\"if __name__ == '__main__':\\n    # This is used when running locally only. When deploying to Google Cl...\"],[\"Finally, the `requirement.txt` file\\n```python\\nFlask==1.1.2\\ntorch===1.7.1\\ntransformers~=4.2.0\\ngunicor...\"],[\"We can improve the request handling performance by warming the model, it means loading it on start-u...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Margaret Mitchell\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f57_meg_mitchell_int...\"],[\"**Meg:** I did heavy statistical work as a postdoc at Johns Hopkins and then went to Microsoft Resea...\"],[\"There was also an ah-ha moment when I was feeding my system a sequence of images, getting it to talk...\"],[\"### How can ML teams be more aware of harmful bias?\\n\\n**Meg:** A primary issue is that these concepts...\"],[\"**Meg:** Diversity is when you have a lot of races, ethnicities, genders, abilities, statuses at the...\"],[\"You need to translate this approach to the people sitting at the table.\\n\\nJust how you want to have a...\"],[\"Timnit’s paper was called [‘Data Sheets for Datasets’](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1803.09010). So we call...\"],[\"A static report of how well it works isn’t as informative as you want it to be because you want to k...\"],[\"### What are you working on at Hugging Face?\\n\\n- Working on a few different tools designed for engine...\"],[\"### Rapid Fire Questions:\\n\\n### Best piece of advice for someone looking to get into AI?\\n\\n**Meg:** De...\"],[\"**Meg:** Yes, [the example is that] they were making this claim that there was this angle theta that...\"],[\"Hopefully, we can focus on the things that are most beneficial and continue heading in that directio...\"],[\"We have a lot of people developing technology, which is great, but we don’t have a lot of people in ...\"],[\"\\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fsupport?utm_source=blog&utm_medium=blog&utm_campaign=ml_experts&utm_...\"],[\"--\\ntitle: \\\"Accelerating PyTorch distributed fine-tuning with Intel technologies\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"Running a text classification job, we will fine-tune a [BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-cased...\"],[\"All three major cloud providers offer virtual machines powered by Intel Ice Lake CPUs:\\n\\n- Amazon Web...\"],[\"In fact, PyTorch includes the [```torch.distributed```](https:\\u002f\\u002fpytorch.org\\u002ftutorials\\u002fintermediate\\u002fd...\"],[\"From a networking perspective, we will need the following setup: \\n\\n* Open port 22 for ```ssh``` acce...\"],[\"It looks like a lot, but there's nothing complicated. Here we go!\\n\\n__Installing Intel toolkits__\\n\\nFi...\"],[\"__Compiling and installing oneCCL__\\n\\nThen, we install some native dependencies required to compile o...\"],[\"* List the nodes of the training cluster,\\n* Define environment variables,\\n* Modify the training scri...\"],[\"* Import the ```torch_ccl```package.\\n* Receive the address of the master node and the local rank of ...\"],[\"```\\nmpirun -f hostfile -np 2 -ppn 1 -genv I_MPI_PIN_DOMAIN=[0xfffffff0] \\\\\\n-genv OMP_NUM_THREADS=28 p...\"],[\"--\\ntitle: \\\"Introducing new audio and vision documentation in 🤗 Datasets\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f87_...\"],[\"The 🤗 Datasets team has been building tools and features to make working with these dataset types as...\"],[\"To make all of the modality-specific documentation more discoverable, there are new dedicated sectio...\"],[\"folder\\u002ftrain\\u002fcat\\u002fmaine_coon.png\\nfolder\\u002ftrain\\u002fcat\\u002fbengal.png\\nfolder\\u002ftrain\\u002fcat\\u002fbirman.png\\n```\\n\\n\\u003cfigure...\"],[\"## What’s next?\\n\\nSimilar to how the first iteration of the 🤗 Datasets library standardized text data...\"],[\"--\\ntitle: \\\"Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fmixt...\"],[\"## What is Mixtral 8x7b?\\n\\nMixtral has a similar architecture to Mistral 7B, but comes with a twist: ...\"],[\"| Model                                                                             | License       ...\"],[\"| Model                                                                                             ...\"],[\"Impressively, Mixtral Instruct outperforms all other open-access models on MT-Bench and is the first...\"],[\"Similarly, for the Mixtral instruct model, no details have been shared about the fine-tuning dataset...\"],[\"```python\\nfrom transformers import AutoTokenizer\\nimport transformers\\nimport torch\\n\\nmodel = \\\"mistrala...\"],[\"You can deploy Mixtral on Hugging Face's [Inference Endpoints](https:\\u002f\\u002fui.endpoints.huggingface.co\\u002fn...\"],[\"Training LLMs can be technically and computationally challenging. In this section, we look at the to...\"],[\"As seen above, the challenge for this model is to make it run on consumer-type hardware for anyone t...\"],[\"### Load Mixtral with GPTQ\\n\\nThe GPTQ algorithm is a post-training quantization technique where each ...\"],[\"output = model.generate(**inputs, max_new_tokens=50)\\nprint(tokenizer.decode(output[0], skip_special_...\"],[\"--\\ntitle: Hyperparameter Search with Transformers and Ray Tune\\nthumbnail: \\u002fblog\\u002fassets\\u002f06_ray_tune\\u002fr...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n   \\u003ctd\\u003e\\u003cstrong\\u003eAlgorithm\\u003c\\u002fstrong\\u003e\\n   \\u003c\\u002ftd\\u003e\\n   \\u003ctd\\u003e\\u003cstrong\\u003eBest Val Acc.\\u003c\\u002fstrong\\u003e\\n   \\u003c...\"],[\"To run this example, please first run:\\n\\n**`pip install \\\"ray[tune]\\\" transformers datasets scipy sklea...\"],[\"By default, each trial will utilize 1 CPU, and optionally 1 GPU if available.\\nYou can leverage multi...\"],[\"It also works with [Weights and Biases](https:\\u002f\\u002fwandb.ai\\u002f) out of the box!\\n\\n![alt_text](\\u002fblog\\u002fassets...\"],[\"--\\ntitle: \\\"Accelerate Large Model Training using DeepSpeed\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f83_accelerate_de...\"],[\"e. **Param Offload**: Offloads the model parameters to CPU\\u002fDisk building on top of ZERO Stage 3\\n\\nIn ...\"],[\"To enable DeepSpeed ZeRO Stage-2 without any code changes, please run `accelerate config` and levera...\"],[\"Table 1: Benchmarking DeepSpeed ZeRO Stage-2 on DeBERTa-XL (900M) model\\n\\n---\\nWith this bigger batch ...\"],[\"We will leverage the DeepSpeed Zero Stage-2 config [zero2_config_accelerate.json](https:\\u002f\\u002fgithub.com...\"],[\"**ZeRO Stage-2 DeepSpeed Config File Example**\\n```bash\\ncompute_environment: LOCAL_MACHINE\\ndeepspeed_...\"],[\"- lr_scheduler = get_scheduler(\\n-     name=args.lr_scheduler_type,\\n-     optimizer=optimizer,\\n-     ...\"],[\"![Chatbot](.\\u002fassets\\u002f83_accelerate_deepspeed\\u002fchatbot.png)\\n\\n---\\n## CPU\\u002fDisk Offloading to enable train...\"],[\"We will leverage the DeepSpeed Zero Stage-3 CPU offload config [zero3_offload_config_accelerate.json...\"],[\"**ZeRO Stage-3 CPU Offload DeepSpeed Config File Example**\\n```bash\\ncompute_environment: LOCAL_MACHIN...\"],[\"[2] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f191...\"],[\"--\\ntitle: \\\"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\\\"\\nthumbnail: \\u002f...\"],[\"The main bottleneck is the latency of predictions which can make large deployments expensive to run ...\"],[\"An Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learnin...\"],[\"### Methodologies\\n\\nWhen it comes to benchmarking BERT-like models, two metrics are most adopted:\\n* *...\"],[\"In this blog post, we will highlight a few results of the benchmark including the best latency and t...\"],[\"### Latency \\n\\nBelow, you can find the latency results for an experiment running Hugging Face Infinit...\"],[\"If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co\\u002finfinity-...\"],[\"--\\ntitle: \\\"Introducing ⚔️ AI vs. AI ⚔️ a deep reinforcement learning multi-agents competition system...\"],[\"## How does AI vs. AI works?\\n\\nAI vs. AI is an open-source tool developed at Hugging Face **to rank t...\"],[\"Using this rating, it is possible **to generate matches between models with comparable strengths aut...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e \\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fr...\"],[\"If you’re interested, **you don’t need to participate in the course to be able to participate in the...\"],[\"In the future, we will host multiple multi-agent competitions with this tool and environments we cre...\"],[\"--\\ntitle: \\\"SetFit: Efficient Few-Shot Learning Without Prompts\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f103_setfit\\u002fi...\"],[\"\\u003cp\\u003e🌎 \\u003cstrong\\u003eMultilingual support\\u003c\\u002fstrong\\u003e: SetFit can be used with any Sentence Transformer on the ...\"],[\"And just by switching out the base Sentence Transformer model to a multilingual one, SetFit can func...\"],[\"## Fast training and inference\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f103_setfit\\u002fbars.png\\\" width=4...\"],[\"```python\\ndataset = load_dataset(\\\"SetFit\\u002fSentEval-CR\\\")\\n```\\n\\nTo simulate a real-world scenario with j...\"],[\"## Next steps\\nWe've shown that SetFit is an effective method for few-shot classification tasks. In t...\"],[\"--\\ntitle: \\\"Creating a Coding Assistant with StarCoder\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fstarchat_alpha\\u002fthumbn...\"],[\"- How LLMs can be prompted to act like conversational agents.\\n- OpenAI’s [Chat Markup Language](http...\"],[\"For example, here’s an excerpt from [Anthropic’s HHH prompt](https:\\u002f\\u002fgist.github.com\\u002fjareddk\\u002f2509330...\"],[\"-----\\n\\n...\\n\\n-----\\n\\nHuman: {USER QUERY}\\n\\nAssistant:\\n```\\n\\nAs we can see, the first part of the prompt ...\"],[\"-----\\n\\nHuman: Write a function that takes two lists and returns a list that has alternating elements...\"],[\"## Datasets for chatty language models\\n\\nThe open-source community is rapidly creating diverse and po...\"],[\"```\\n{\\n    \\\"messages\\\": [\\n        {\\n            \\\"content\\\": \\\"Is it possible to imagine a society withou...\"],[\"\\\"role\\\": \\\"user\\\",\\n        },\\n        {\\n            \\\"content\\\": \\\"You are correct that there are other fa...\"],[\"OK, this looks like an interesting dialogue about moral philosophy, with each turn involving a role ...\"],[\"```python\\nsystem_token = \\\"\\u003c|system|\\u003e\\\"\\nuser_token = \\\"\\u003c|user|\\u003e\\\"\\nassistant_token = \\\"\\u003c|assistant|\\u003e\\\"\\nend_...\"],[\"```python\\ntokenizer(\\\"\\u003c|assistant|\\u003e\\\")\\n```\\n\\n```\\n{\\\"input_ids\\\": [49153], \\\"attention_mask\\\": [1]}\\n```\\n\\nGre...\"],[\"## Fine-tuning StarCoder with DeepSpeed ZeRO-3\\n\\nThe StarCoder and StarCoderBase models contain 16B p...\"],[\"```shell\\ntorchrun --nproc_per_node=8 train.py config.yaml --deepspeed=deepspeed_z3_config_bf16.json\\n...\"],[\"# Add padding between plots\\nfig.tight_layout()\\n\\n# Show the final image\\nplt.show()\\n```\\n\\n\\u003cp align=\\\"cen...\"],[\"**Example 3: basketball**\\n\\nPrompt:\\n\\n```\\nThere was a basketball game with the following stats. player...\"],[\"## Evaluating coding assistants\\n\\nEvaluating coding assistants (or chatbots more generally) is tricky...\"],[\"As a simple experiment, we used ChatGPT to test our StarCoder models on several programming language...\"],[\"Instruction-tuned completion (Assistant 2):\\n\\n```\\n\\\"Here is an example implementation of the `reverse_...\"],[\"```\\n\\\"Sure thing! Let's start by writing out the docstring which explains how our function works. We'...\"],[\"Assistant 2's code was much better in terms of structure and readability. The code was accurate, eff...\"],[\"Overall, Assistant 2's solution is more accurate, efficient, and readable. The code structure is cle...\"],[\"## Acknowledgements\\n\\nWe thank Nicolas Patry and Olivier Dehaene for their help with deploying StarCh...\"],[\"--\\ntitle: \\\"AI Policy @🤗: Response to the U.S. NTIA's Request for Comment on AI Accountability\\\"\\nthumb...\"],[\"Hugging Face’s mission is to [“democratize good machine learning”](https:\\u002f\\u002fhuggingface.co\\u002fabout). We...\"],[\"Concretely, we make the following recommendations for accountability mechanisms:\\n\\n* Accountability m...\"],[\"--\\ntitle: \\\"Creating Privacy Preserving AI with Substra\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f139_owkin-substra\\u002ft...\"],[\"As the data never leaves its source, federated learning is naturally a privacy-first approach. Not o...\"],[\"## Substra x HF\\n\\nResearch on the capabilities of federated learning is growing rapidly but the major...\"],[\"Regardless of the methods used, it's important to stay vigilant of the fact that data privacy is a r...\"],[\"--\\ntitle:  Deploy Embedding Models with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f168...\"],[\"Before we start, let's refresh our knowledge about Inference Endpoints.\\n\\n## 1. What is Hugging Face ...\"],[\"Here are some of the most important features:\\n\\n1. [Easy Deployment](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002finfe...\"],[\"You can get started with Inference Endpoints at: https:\\u002f\\u002fui.endpoints.huggingface.co\\u002f\\n\\n## 2. What is...\"],[\"## 3. Deploy Embedding Model as Inference Endpoint\\n\\nTo get started, you need to be logged in with a ...\"],[\"![Test Model](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f168...\"],[\"--\\ntitle: \\\"What Makes a Dialog Agent Useful?\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fdialog-agents\\u002fthumbnail.png\\na...\"],[\"| &nbsp;| LaMDA | BlenderBot 3 |Sparrow | ChatGPT\\u002f InstructGPT | Assistant|\\n| --- | --- | --- | --- ...\"],[\"![ChatGPT instruction example](assets\\u002fdialog-agents\\u002fchatgpt-example.png)\\n\\n### **********************...\"],[\"![IFT spectrum](assets\\u002fdialog-agents\\u002fift-spectrum.png)\\n\\nOn one end is the purely model-generated IFT...\"],[\"SFT and IFT are very closely linked. Instruction tuning can be seen as a subset of supervised fine-t...\"],[\"**Chain-of-thought (CoT)** prompting ([Wei et al., ‘22](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2201.11903)) is a spec...\"],[\"This blog summarizes many of the existing work on what makes a dialog agent useful. But there are st...\"],[\"--\\ntitle: \\\"AI for Game Development: Creating a Farming Game in 5 Days. Part 1\\\"\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"#### Locally \\u003ca name=\\\"locally\\\"\\u003e\\u003c\\u002fa\\u003e\\n\\nWe'll be running Stable Diffusion locally using the [Automatic1...\"],[\"#### Online \\u003ca name=\\\"online\\\"\\u003e\\u003c\\u002fa\\u003e\\n\\nIf you don't meet the requirements to run Stable Diffusion locall...\"],[\"The shared point of emphasis of these is to use a source such as [lexica.art](https:\\u002f\\u002flexica.art\\u002f) t...\"],[\"3. Set up your [Materials](https:\\u002f\\u002fdocs.unity3d.com\\u002fManual\\u002fMaterials.html), using the concept art as...\"],[\"\\u003cfigure class=\\\"image text-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumenta...\"],[\"--\\ntitle: \\\"From PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease\\\"\\nthu...\"],[\"def forward(self, x):\\n        x = self.act(self.conv1(x))\\n        x = self.act(self.conv2(x))\\n      ...\"],[\"Typically from here, one could either throw all of this into a python script or run it on a Jupyter ...\"],[\"def cleanup():\\n    \\\"Cleans up the distributed environment\\\"\\n    dist.destroy_process_group()\\n```\\n\\nThe...\"],[\"The above will run the training script on two GPUs that live on a single machine and this is the bar...\"],[\"# Build model\\n    model = model.to(rank)\\n    ddp_model = DDP(model, device_ids=[rank])\\n\\n    # Build ...\"],[\"train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)\\n    test_dset = ...\"],[\"As a result its now trivialized to perform distributed training with Accelerate and keeping as much ...\"],[\"## Using 🤗 Trainer\\n\\nFinally, we arrive at the highest level of API -- the Hugging Face [Trainer](htt...\"],[\"trainer = MyTrainer(\\n    model,\\n    training_args,\\n    train_dataset=train_dset,\\n    eval_dataset=te...\"],[\"## Resources\\n\\nTo learn more about PyTorch Distributed Data Parallelism, check out the documentation ...\"],[\"--\\ntitle: \\\"Probabilistic Time Series Forecasting with 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f118_ti...\"],[\"So in short, rather than training local point forecasting models, we hope to train **global probabil...\"],[\"To begin with, the use of an Encoder-Decoder architecture is helpful at inference time where typical...\"],[\"A drawback of the Transformer architecture is the limit to the sizes of the context and prediction w...\"],[\"dataset = load_dataset(\\\"monash_tsf\\\", \\\"tourism_monthly\\\")\\n```\\n\\n\\nAs can be seen, the dataset contains 3...\"],[\"```python\\nvalidation_example = dataset['validation'][0]\\nvalidation_example.keys()\\n\\n\\u003e\\u003e\\u003e dict_keys(['s...\"],[\"```python\\nfrom functools import partial\\n\\ntrain_dataset.set_transform(partial(transform_start_field, ...\"],[\"Let's use the default lags provided by GluonTS for the given frequency (\\\"monthly\\\"):\\n\\n\\n```python\\nfrom...\"],[\"model = TimeSeriesTransformerForPrediction(config)\\n```\\n\\nNote that, similar to other models in the 🤗 ...\"],[\"The transformations below are annotated with comments, to explain what they do. At a high level, we ...\"],[\"# a bit like torchvision.transforms.Compose\\n    return Chain(\\n        # step 1: remove static\\u002fdynami...\"],[\"target_field=FieldName.TARGET,\\n                output_field=FieldName.FEAT_TIME,\\n                tim...\"],[\"## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitter` w...\"],[\"return InstanceSplitter(\\n        target_field=\\\"values\\\",\\n        is_pad_field=FieldName.IS_PAD,\\n     ...\"],[\"# we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \\\"train\\\"...\"],[\"We have a test dataloader helper for completion, even though we will not use it here. This is useful...\"],[\"Let's check the first batch:\\n\\n\\n```python\\nbatch = next(iter(train_dataloader))\\nfor k, v in batch.item...\"],[\"```python\\nprint(\\\"Loss:\\\", outputs.loss.item())\\n\\n\\u003e\\u003e\\u003e Loss: 9.069628715515137\\n```\\n\\nNote that the model ...\"],[\"model, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_datalo...\"],[\"```python\\nmodel.eval()\\n\\nforecasts = []\\n\\nfor batch in test_dataloader:\\n    outputs = model.generate(\\n...\"],[\"mase_metric = load(\\\"evaluate-metric\\u002fmase\\\")\\nsmape_metric = load(\\\"evaluate-metric\\u002fsmape\\\")\\n\\nforecast_me...\"],[\"# Major ticks every half year, minor ticks every month,\\n    ax.xaxis.set_major_locator(mdates.MonthL...\"],[\"Of course, we need to be careful with just claiming state-of-the-art results on time series with neu...\"],[\"Another thing on the roadmap is time series classification. This entails adding a time series model ...\"],[\"--\\ntitle: Image Classification with AutoTrain \\nthumbnail: \\u002fblog\\u002fassets\\u002f105_autotrain-image-classific...\"],[\"[Image Classification](https:\\u002f\\u002fhuggingface.co\\u002ftasks\\u002fimage-classification) models learn to *categoriz...\"],[\"Once AutoTrain creates your project, you just need to connect your data. If you have the data locall...\"],[\"\\u003cdiv class=\\\"grid grid-cols-2 gap-4\\\"\\u003e\\n  \\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n    \\u003cmedi...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cmedium-zoom background=\\\"rgba(0,0,0,.7)\\\" alt=\\\"...\"],[\"--\\ntitle: \\\"Fine-Tune XLSR-Wav2Vec2 for low-resource ASR with 🤗 Transformers\\\"\\nthumbnail: \\u002fblog\\u002fassets...\"],[\"-   [**Wav2Vec2-XLS-R-300M**](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fwav2vec2-xls-r-300m)\\n-   [**Wav2Vec2-X...\"],[\"I highly recommend reading the well-written blog post [*Sequence\\nModeling with CTC (2017)*](https:\\u002f\\u002f...\"],[\"In 🤗 Transformers, the XLS-R model is thus accompanied by both a\\ntokenizer, called\\n[Wav2Vec2CTCToken...\"],[\"First, let\\\\'s go to Common Voice [official\\nwebsite](https:\\u002f\\u002fcommonvoice.mozilla.org\\u002fen\\u002fdatasets) and...\"],[\"Let\\\\'s write a short function to display some random samples of the\\ndataset and run it a couple of t...\"],[\"We can see that the transcriptions contain some special characters, such\\nas `,.?!;:`. Without a lang...\"],[\"Good! This looks better. We have removed most special characters from\\ntranscriptions and normalized ...\"],[\"```python\\ndef extract_all_chars(batch):\\n  all_text = \\\" \\\".join(batch[\\\"sentence\\\"])\\n  vocab = list(set(...\"],[\"The model has to learn to predict when a word is finished or else the\\nmodel prediction would always ...\"],[\"In a final step, we use the json file to load the vocabulary into an\\ninstance of the `Wav2Vec2CTCTok...\"],[\"XLS-R was pretrained on audio data of\\n[Babel](http:\\u002f\\u002fwww.reading.ac.uk\\u002fAcaDepts\\u002fll\\u002fspeechlab\\u002fbabel\\u002fr...\"],[\"```python\\nfrom transformers import Wav2Vec2Processor\\n\\nprocessor = Wav2Vec2Processor(feature_extracto...\"],[\"```python\\ncommon_voice_train = common_voice_train.cast_column(\\\"audio\\\", Audio(sampling_rate=16_000))\\n...\"],[\"**Print Output:**\\n\\n```bash\\n    Target text: makedonya bu yıl otuz adet tyetmiş iki tankı aldı\\n    In...\"],[\"Let\\\\'s apply the data preparation function to all examples.\\n\\n```python\\ncommon_voice_train = common_v...\"],[\"-   Define a data collator. In contrast to most NLP models, XLS-R has a\\n    much larger input length...\"],[\"```python\\nimport torch\\n\\nfrom dataclasses import dataclass, field\\nfrom typing import Any, Dict, List,...\"],[\"batch = self.processor.pad(\\n            input_features,\\n            padding=self.padding,\\n          ...\"],[\"pred_str = processor.batch_decode(pred_ids)\\n    # we do not want to group tokens when computing the ...\"],[\"The first component of XLS-R consists of a stack of CNN layers that are\\nused to extract acoustically...\"],[\"**Note**: If one does not want to upload the model checkpoints to the\\nHub, simply set `push_to_hub=F...\"],[\"### Training\\n\\nTraining will take multiple hours depending on the GPU allocated to this\\nnotebook. Whi...\"],[\"For more examples of how XLS-R can be fine-tuned, please take a look at the official \\n[🤗 Transformer...\"],[\"Alright! The transcription can definitely be recognized from our\\nprediction, but it is not perfect y...\"],[\"--\\ntitle: \\\"Advantage Actor Critic (A2C)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f89_deep_rl_a2c\\u002fthumbnail.gif\\nauthor...\"],[\"Remember that the policy gradient estimation is **the direction of the steepest increase in return**...\"],[\"## The Problem of Variance in Reinforce\\nIn Reinforce, we want to **increase the probability of actio...\"],[\"However, increasing the batch size significantly **reduces sample efficiency**. So we need to find a...\"],[\"On the other hand, your friend (Critic) will also update their way to provide feedback so it can be ...\"],[\"Let's see the training process to understand how Actor and Critic are optimized:\\n- At each timestep,...\"],[\"In other words, this function calculates **the extra reward we get if we take this action at that st...\"],[\"It's **normal if you still feel confused** with all these elements. **This was the same for me and f...\"],[\"--\\ntitle: \\\"Faster Training and Inference: Habana Gaudi®2 vs Nvidia A100 80GB\\\"\\nthumbnail: \\u002fblog\\u002fasset...\"],[\"1. Go to the [Intel Developer Cloud landing page](https:\\u002f\\u002fwww.intel.com\\u002fcontent\\u002fwww\\u002fus\\u002fen\\u002fdeveloper\\u002f...\"],[\"Several benchmarks were performed to assess the abilities of first-gen Gaudi, Gaudi2 and A100 80GB f...\"],[\"**Gaudi2 also offers a speedup over A100**: 1580.2 samples\\u002fs versus 981.6 for a batch size of 32 and...\"],[\"The results we got, which are consistent with the numbers published by Habana [here](https:\\u002f\\u002fdevelop...\"],[\"The results we achieved are presented in the table below. **Gaudi2 is x2.44 faster than A100 80GB.**...\"],[\"If you are interested in accelerating your Machine Learning training and inference workflows using t...\"],[\"--\\ntitle: \\\"Parameter-Efficient Fine-Tuning using 🤗 PEFT\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f130_peft\\u002fthumbnail....\"],[\"PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parame...\"],[\"1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2106.09685.pdf)\\n...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"4. When you are ready to save the model for inference, just do the following.\\n```py\\nmodel.save_pretr...\"],[\"## Next steps\\nWe've released PEFT as an efficient way of tuning large LLMs on downstream tasks and d...\"],[\"--\\ntitle: \\\"MTEB: Massive Text Embedding Benchmark\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f110_mteb\\u002fthumbnail.png\\na...\"],[\"## MTEB \\n\\n\\n🐋 **Massive**: MTEB includes 56 datasets across 8 tasks and currently summarizes \\u003e2000 re...\"],[\"**⚖️ Speed and performance** Slightly slower, but significantly stronger, [all-mpnet-base-v2](https:...\"],[\"This should produce a `results\\u002faverage_word_embeddings_komninos\\u002fBanking77Classification.json` file!\\n...\"],[\"--\\ntitle: Deploying 🤗 ViT on Vertex AI\\nthumbnail: \\u002fblog\\u002fassets\\u002f97_vertex_ai\\u002fimage1.png\\nauthors:\\n- us...\"],[\"- Model versioning\\n\\n- Traffic splitting between different versions of a model\\n\\n- Rate limiting\\n\\n- Mo...\"],[\"```bash\\nThe given SavedModel SignatureDef contains the following input(s):\\n  inputs['string_input'] ...\"],[\"These features are important for machine learning in production.\\nBuilding a model registry that guar...\"],[\"![](.\\u002fassets\\u002f97_vertex_ai\\u002fimage3.png)\\n\\n**1.** The first step in the workflow is to upload the `Saved...\"],[\"Here you’re using an `endpoint_service_client` which is an\\n[`EndpointServiceClient`](https:\\u002f\\u002fcloud.g...\"],[\"- `accelerator_type` is the hardware accelerator that will be used\\n    to perform inference.\\n\\n  - `a...\"],[\"**4.** The following utility first prepares a list of instances (only\\none instance in this case) and...\"],[\"If everything goes well with the deployment, when you call\\n`predict_image()`, you should get an outp...\"],[\"This type of monitoring helps you quickly flag the currently deployed\\nEndpoint and make adjustments ...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n\\n| **Machine Type**            | **Hourly Pricing (USD)** |\\n|:-----------------...\"],[\"The series first introduced you to TensorFlow Serving for locally deploying \\na vision model from 🤗 T...\"],[\"--\\ntitle: 'Few-shot learning in practice: GPT-Neo and the 🤗 Accelerated Inference API'\\n# thumbnail: ...\"],[\"Few-Shot NLP examples consist of three main components: \\n\\n- **Task Description**: A short descriptio...\"],[\"All of the currently available GPT-Neo checkpoints are trained with the Pile dataset, a large text c...\"],[\"prompt=\\\"....\\\"             # few-shot prompt\\n\\ndata = query(prompt,parameters,options)\\n```\\n\\n---\\n## Pra...\"],[\"\\u003e ###  \\n\\u003e Tweet: \\\"I'm a disabled happy person\\\"  \\n\\u003e Sentiment: Negative  \\n\\nWhat could go wrong? Imagi...\"],[\"--\\ntitle: \\\"Director of Machine Learning Insights [Part 3: Finance Edition]\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"\\u003cimg class=\\\"mx-auto\\\" style=\\\"float: left;\\\" padding=\\\"5px\\\" width=\\\"200\\\" src=\\\"\\u002fblog\\u002fassets\\u002f78_ml_director...\"],[\"**Fun Fact:** Ioannis was a juniors Greek national tennis champion.🏆\\n\\n**RBC:** The world’s leading o...\"],[\"#### **2. What are the biggest ML challenges within finance?**\\nI can’t speak for companies but estab...\"],[\"It is hard to optimize for the right Business and ML KPIs and define the right objective function or...\"],[\"### [Debanjan Mahata](https:\\u002f\\u002fwww.linkedin.com\\u002fin\\u002fdebanjanmahata\\u002f) - Director of AI & ML at [Moody's...\"],[\"One of the key useful traits of ML is that it can learn from and find hidden patterns in large volum...\"],[\"3. Legacy infrastructure and databases - Many financial institutions still carry legacy infrastructu...\"],[\"#### **3. What’s a common mistake you see people make trying to integrate ML into financial applicat...\"],[\"**Fun Fact:** Soumitri is a prolific inventor with 100+ issued U.S. patents in varied fields includi...\"],[\"#### **2. What are the biggest ML challenges within finance?**\\nThe finance and banking industry brin...\"],[\"#### **4. What excites you most about the future of ML?**\\nNow is a great time to be in applied ML an...\"],[\"--\\ntitle: \\\"Instruction-tuning Stable Diffusion with InstructPix2Pix\\\" \\nthumbnail: assets\\u002finstruction_...\"],[\"Our code, pre-trained models, and datasets can be found [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002finstru...\"],[\"Using a similar philosophy, the authors of FLAN V2 conduct instruction-tuning on a mixture of thousa...\"],[\"But we can still leverage the findings from InstructPix2Pix to suit our customizations. \\n\\nOn the oth...\"],[\"In particular, we:\\n\\n1. Ask [ChatGPT](https:\\u002f\\u002fopenai.com\\u002fblog\\u002fchatgpt) to generate 50 synonymous sent...\"],[\"We took different number of samples from the following datasets for each task and constructed a sing...\"],[\"## Training experiments and results\\n\\nWe based our training experiments on [this script](https:\\u002f\\u002fgith...\"],[\"### Cartoonization results\\n\\nFor testing the [instruction-tuned cartoonization model](https:\\u002f\\u002fhugging...\"],[\"Our model, however, [fails to produce](https:\\u002f\\u002fwandb.ai\\u002fsayakpaul\\u002finstruction-tuning-sd\\u002fruns\\u002fg6cvggw...\"],[\"This failure, perhaps, can be attributed to our model not seeing enough exemplars for the task and p...\"],[\"However, challenges still remain:\\n\\n- These systems need to work for large high-resolution original i...\"],[\"- ***What happens we scale up the datasets?*** How does that impact the quality of the generated sam...\"],[\"## Conclusion\\n\\nIn this post, we presented our exploration of “instruction-tuning” of Stable Diffusio...\"],[\"--\\ntitle: \\\"Deploying the AI Comic Factory using the Inference API\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f165_ai_co...\"],[\"## Duplicating the Space\\n\\nTo duplicate the AI Comic Factory, go to the Space and [click on \\\"Duplicat...\"],[\"## Configuring the models\\n\\nThe AI Comic Factory comes with the following models pre-configured:\\n- `L...\"],[\"--\\ntitle: What's new in Diffusers? 🎨\\nthumbnail: \\u002fblog\\u002fassets\\u002f102_diffusers_2nd_month\\u002finpainting.png\\n...\"],[\"```python\\nfrom diffusers import StableDiffusionImg2ImgPipeline\\n\\npipe = StableDiffusionImg2ImgPipelin...\"],[\"## Experimental inpainting pipeline\\n\\nInpainting allows to provide an image, then select an area in t...\"],[\"This is super exciting as this will reduce even more the barrier to use these models!\\n\\n\\n## Diffusers...\"],[\"💅 Because of this, we did a Docs sprint and we're very excited to do a first release of our [documen...\"],[\"* See all the images in the diffusion process\\n* Analyze how each token in the prompt influences the ...\"],[\"* Replace a target in the prompt (e.g. replace cat by dog)\\n* Reduce or increase the importance of wo...\"],[\"--\\ntitle: \\\"Announcing the Hugging Face Fellowship Program\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f62_fellowship\\u002ffel...\"],[\"How? In the ways that they prefer. Here are some examples of the first Fellows:\\n\\n- **María Grandury*...\"],[\"Additionally, there are strategic areas where Hugging Face is looking for open-source contributions....\"],[\"* **Where and how can I contribute?**\\n  \\nIt depends on your interests. Here are some ideas of areas ...\"],[\"* **Will I receive benefits during the Fellowship?**\\n  \\nYes, the benefits will depend on the particu...\"],[\"--\\ntitle: \\\"How 🤗 Accelerate runs very large models thanks to PyTorch\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f104_ac...\"],[\"Then step 2 will load in memory a second copy of the model (so another 26.8GB in RAM in default prec...\"],[\"```\\ntensor(..., device='meta', size=(100000, 100000))\\n```\\n\\nAs we said before, there is no data assoc...\"],[\"## Computing a device map\\n\\nBefore we start loading the pretrained weights, we will need to know wher...\"],[\"device_map = infer_auto_device_map(model)\\n```\\n\\nThis will return a dictionary mapping modules or weig...\"],[\"```python\\ndevice_map = infer_auto_device_map(model, no_split_module_classes=[\\\"OPTDecoderLayer\\\"])\\n```...\"],[\"In this precision, we can fit the model up to layer 21 on the GPU:\\n\\n```python out\\n\\n\\n{'model.decoder....\"],[\"This is why large models on the Hugging Face Hub are not saved and shared with one big file containi...\"],[\"# Will error\\ncheckpoint = \\\"facebook\\u002fopt-13b\\\"\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint...\"],[\"```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM\\n\\ncheckpoint = \\\"facebook\\u002fopt-13b...\"],[\"This way, your model can be loaded and run even if you don't have enough GPU RAM and CPU RAM. The on...\"],[\"--\\ntitle: 'The Partnership: Amazon SageMaker and Hugging Face'\\nthumbnail: \\u002fblog\\u002fassets\\u002f17_the_partne...\"],[\"---\\n\\n## **Features & Benefits 🔥**\\n\\n## One Command is All you Need\\n\\nWith the new Hugging Face Deep Le...\"],[\"---\\n\\n## **Resources, Documentation & Samples 📄**\\n\\nBelow you can find all the important resources to ...\"],[\"## Documentation\\n\\n- [Hugging Face documentation for Amazon SageMaker](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fsa...\"],[\"## Sample Notebook\\n\\n- [all Notebooks](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002ftree\\u002fmaster\\u002fsagemaker...\"],[\"---\\n\\n## **Getting started: End-to-End Text Classification 🧭**\\n\\nIn this getting started guide, we wil...\"],[\"```python\\nimport sagemaker\\n\\nsess = sagemaker.Session()\\n# sagemaker session bucket -\\u003e used for upload...\"],[\"# Data, model, and output directories\\n    parser.add_argument(\\\"--output-data-dir\\\", type=str, default...\"],[\"# download model from model hub\\n    model = AutoModelForSequenceClassification.from_pretrained(args....\"],[\"# tokenizer used in preprocessing\\ntokenizer_name = 'distilbert-base-uncased'\\n\\n# filesystem client fo...\"],[\"## Create a HuggingFace Estimator and train our model\\n\\nIn order to create a SageMaker `Trainingjob` ...\"],[\"```python\\n# starting the train job with our uploaded datasets as input\\nhuggingface_estimator.fit({'t...\"],[\"The \\\"Getting started: End-to-End Text Classification 🧭\\\" example can be used for distributed training...\"],[\"## Spot instances\\n\\nWith the creation of HuggingFace Framework extension for the SageMaker Python SDK...\"],[\"# Training seconds: 874\\n# Billable seconds: 105\\n# Managed Spot Training savings: 88.0%\\n```\\n\\n## Git R...\"],[\"## SageMaker Metrics\\n\\n[SageMaker Metrics](https:\\u002f\\u002fdocs.aws.amazon.com\\u002fsagemaker\\u002flatest\\u002fdg\\u002ftraining-m...\"],[\"_Q: Do I have to use the SageMaker Python SDK to use the Hugging Face Deep Learning Containers?_\\n\\nA:...\"],[\"A: Yes, you can download your trained model from S3 and directly use it with transformers or upload ...\"],[\"A: No - the Hugging Face DLCs are open source and licensed under Apache 2.0.\\n\\n_Q: How can I run infe...\"],[\"_Q: I use Hugging Face with Azure Machine Learning or Google Cloud Platform, what does this partners...\"],[\"--\\ntitle: \\\"Introducing Optimum: The Optimization Toolkit for Transformers at Scale\\\"\\nauthors:\\n- user:...\"],[\"### 🏭 Optimum puts Transformers to work\\n\\nTo get optimal performance training and serving models, the...\"],[\"## 🤗 Optimum in practice: how to quantize a model for Intel Xeon CPU\\n### 🤔 Why quantization is impor...\"],[\"3. Balance the trade-off between quantization and an acceptable accuracy loss.\\n4. Export the quantiz...\"],[\"### 🔥 How to easily quantize Transformers for Intel Xeon CPUs with Optimum\\n\\n![Automatic quantization...\"]],\"hovertemplate\":\"source=blog\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"blog, circle\",\"marker\":{\"color\":\"#B6E880\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"blog, circle\",\"showlegend\":true,\"x\":[-4.9181485,0.75906855,12.839362,11.264464,11.652306,11.288557,-13.301718,-12.662307,-12.807499,-12.933401,-12.6806555,3.651578,-1.4478163,1.2153251,-0.1932041,-14.471626,-0.9329243,-21.734528,-17.416859,-17.897896,-6.003185,6.6751328,14.356687,2.8987834,-0.6215449,3.5127492,0.62442964,0.619343,-10.069925,-10.151164,-9.04185,-10.104363,-9.480796,3.0848918,3.2680361,12.835897,13.389695,12.689587,-0.7898953,-0.9049361,-0.8198716,0.046661466,-0.80969644,12.971558,13.231016,12.95638,13.38416,7.468543,7.907764,7.8101025,-7.1182504,-9.706512,7.588613,0.08583059,-4.6949654,-2.4272199,-5.60347,-5.815722,6.187835,-5.298827,-5.5186563,-5.177237,-5.3487186,-4.733731,-4.5829864,-3.2792606,-6.6283407,-6.308739,-6.323313,-11.144405,7.6037364,9.13796,-9.554179,-9.848303,-6.8404393,-10.033843,9.650443,-6.683691,-6.985676,1.0906414,-6.487645,-6.5259104,-1.2352033,-9.3957615,-3.6925354,-6.7002974,-6.460161,-6.6030583,-6.632623,-10.666808,7.7510533,-9.9079275,-21.730772,-18.137905,-5.050901,-4.168411,-2.8732927,6.8821697,-3.6126966,-3.3989534,3.9456878,3.3586795,3.04849,3.4345484,3.1684916,3.6566768,3.3927312,3.503026,3.7164319,3.4181805,8.814671,8.815363,8.715514,8.6093645,-6.0185227,-1.9440757,-7.353001,-7.3154154,-7.346945,-7.349582,-7.3005486,-7.3598404,-6.7491307,5.488098,5.120878,5.153039,4.535232,8.741322,8.784843,8.712167,-4.3186316,-7.2437263,-3.8134897,-7.7804947,-4.3177934,-4.468009,-4.0281854,-14.731487,-13.605798,-17.415592,-18.374905,-18.845942,-18.700277,-18.844894,-18.738567,-18.902412,-0.7598112,-0.7349501,-0.6760924,-0.68780255,-0.6958614,9.549873,9.450847,9.493052,9.80772,9.893638,10.511844,-8.464192,-8.623896,-7.154007,-2.3622708,4.6448708,4.644144,3.9963048,4.5844045,-4.294718,-4.118705,-4.562139,-4.0956984,-3.2170446,-7.860028,-5.0250077,-3.9351032,-3.5395706,-3.5358558,-4.261046,-4.05022,-3.7797234,-3.829602,-3.793353,-3.8757942,16.038784,16.351107,16.232742,16.41288,16.264387,14.855215,16.189585,8.878289,16.141222,16.02768,16.09404,16.262276,15.707556,8.971536,16.35085,16.288918,16.047138,16.259901,15.562141,18.058575,16.146301,16.416172,16.478476,14.854449,16.950258,-0.6986545,16.694227,16.022371,16.24297,16.128706,16.55112,16.231045,16.42899,16.161795,18.004648,16.859205,16.839874,16.804695,16.686268,12.965948,13.019595,13.53316,13.074108,11.995975,12.824052,16.428892,16.009422,16.629711,18.055449,16.302553,15.766384,16.223545,16.288725,16.298119,16.288511,15.166454,8.894765,15.995959,16.248028,16.321316,16.130949,16.394524,16.039942,16.390722,16.064789,15.918345,15.984669,16.19135,15.634088,16.26287,15.120163,16.540157,16.487896,16.439487,16.598661,16.43103,16.67743,16.539328,16.349255,16.020296,16.903517,16.576914,16.494957,16.825153,16.338467,16.518267,16.611427,16.70687,12.921187,13.018014,8.828478,16.241348,16.489777,16.260794,16.34374,16.331192,13.469571,13.408525,9.121745,16.178295,16.240776,15.304663,16.894506,16.460192,-4.208948,-3.0193026,-2.578919,-2.6998677,-3.4343302,-2.463175,-2.9019103,1.7663982,4.1144485,8.003158,-3.5403483,-3.6813788,-3.350019,7.8491044,-4.850123,-4.666785,12.458618,-5.126753,-4.8643007,-3.7841296,0.37640855,-3.9079108,-4.176597,-4.9965777,-1.9228688,-0.81008446,-9.791735,-4.4434037,-10.360837,-10.050484,-18.695131,-3.2086818,-2.0325,-2.4403884,-0.6216866,-2.1996608,-1.9174627,-2.3860672,-2.2348409,-5.399782,-3.9150496,-4.477637,-3.946073,-4.8593645,14.840873,-4.5968485,-3.9354029,-4.1808267,3.884709,4.123856,9.985942,9.743964,-4.1736355,9.509079,9.131432,9.72123,4.7329545,14.462754,5.302691,4.1005106,3.6139991,3.2489994,4.4220176,2.0155056,1.2657831,-3.692403,-4.496632,-3.275453,13.413882,-5.331065,-2.9405947,-1.3659247,-3.455376,-3.9975069,-5.605365,-4.283645,13.203504,12.988228,12.915761,12.898552,12.844964,11.852843,8.519217,10.774772,3.5334814,3.6944246,3.5694532,-10.516254,-10.377269,-8.028082,-7.589667,-7.6565404,-7.465458,-7.808509,-5.939177,6.714725,0.22527559,0.84841055,0.7483334,-0.23476326,3.164109,3.3364985,3.6169384,3.633197,3.4344726,3.7676413,3.1010237,4.0562997,7.9067106,-2.421711,-2.4096022,-2.3896813,-2.3803303,4.351912,3.7078855,3.4494858,-2.3769953,-6.695394,-5.960186,15.985809,16.765774,16.970812,17.820055,18.224535,18.262447,17.953909,15.878165,15.177172,17.28595,17.326265,16.98914,18.187637,18.157452,17.638262,-4.9259276,-3.1980019,-5.1628957,-2.749477,-1.5965707,-2.6047888,-1.9724025,-3.51781,-3.6727455,-3.6862407,-3.6797576,-3.718146,-3.6792188,-6.155411,-5.853369,-6.129719,-5.8931375,-5.771262,4.338575,5.5453925,11.59277,-3.6337917,8.956247,7.091509,-1.3566737,-1.9859122,-0.89544886,-1.2875227,-1.5729262,6.938998,13.096414,-7.1260023,-3.4662814,-3.7841423,-3.301195,-3.1160245,-3.3096864,-4.610404,-3.7651353,-6.001447,-6.565396,-4.3130093,-2.3449798,-4.0793896,8.256214,-5.090109,-2.6229694,-0.5972704,-0.13100038,7.597097,7.729896,-5.960082,-1.516716,-1.850951,-2.295955,-0.7876836,-1.9243883,-1.146866,-3.0098937,-2.654345,-3.0488043,-2.5523186,-1.5299597,7.559981,-2.9220016,1.8898537,-0.5108735,-0.8360572,-1.0324316,-1.2115263,-1.4853042,-1.2243513,-0.39053982,-2.8324237,-1.3682381,-0.13853835,-0.8690269,-5.483527,-3.5669012,4.3809686,3.3145018,-4.965817,-4.0891733,-1.1933442,-2.2735033,-5.64527,13.307814,-7.4036117,-7.5075808,-2.4444041,12.991844,12.644264,17.69743,-9.867046,-9.882313,-9.929994,-9.723188,-9.623246,-9.780596,7.6855373,-9.214022,-9.841515,-1.4354514,-1.4534957,-10.21173,-10.000199,-9.805324,-9.740527,-9.437855,-8.077236,2.5954592,1.0978565,3.7440014,-3.7930448,-3.6518822,-4.1098866,-13.063209,-10.375073,-9.529912,-7.0378494,-5.9588094,0.7246944,-6.557915,-6.331497,1.7889341,0.42064044,-1.9020826,-1.9086894,-1.4083401,-2.1938262,-2.8086734,-1.7743479,-1.509606,-1.6010398,0.77476394,-1.411221,-0.83418924,-1.5550569,-2.3593423,-0.64415145,-5.610139,-1.4722496,-5.1201053,-2.9517498,-3.9845338,-4.0015564,2.4340234,3.411245,1.0084732,3.217,11.840583,11.273461,-4.7134085,-14.984649,-21.73327,0.14306377,-0.9738123,0.1999984,-0.7504048,0.54848874,-0.26594114,2.607778,-3.0335553,0.24874263,-0.98147357,-1.9124691,-1.0163424,0.7791554,12.764656,-10.652902,0.10953016,-0.09694289,0.058961228,-6.9524183,-5.8366823,-2.3087254,-2.9520097,0.6255747,-0.92877394,-5.3347173,-7.7220125,18.939688,17.127697,18.109024,16.784922,16.784935,19.599968,19.327013,16.855871,16.505268,4.324814,4.107909,1.8952092,4.281912,4.3866687,4.3579097,4.5004377,-9.83913,0.17185558,-9.428339,9.551015,9.937091,9.420227,9.410641,9.988746,-7.2933526,-7.2927794,-6.3339357,-6.343779,-1.6599596,17.575274,4.6096377,13.866594,13.740347,13.866113,13.707355,13.773829,-14.869998,-13.605599,-17.416674,-4.2556047,12.930636,3.6172645,4.1075892,4.0458655,1.9234209,9.284409,3.8911395,3.9038923,-4.840171,-3.354037,14.477313,-6.5284085,6.111373,-2.0430727,-2.4009058,13.681837,-3.4460707,-3.2443302,8.410092,1.748201,1.9488144,2.1217656,2.1731725,-18.486994,-4.895207,-3.2193599,-5.489925,-4.8513455,-2.5262153,-4.92324,-10.490884,-10.287352,-11.083252,-10.206507,-9.678221,-9.547369,-8.552493,-2.360451,-2.2621849,-2.4703965,-4.598327,-4.100663,-4.0599403,-2.4900668,-3.2168908,4.7822323,2.445339,0.7846368,12.838768,13.183877,-10.40974,-10.556775,-9.71297,6.1307325,5.633622,-3.4077716,7.843173,5.9175425,5.4776416,5.7788877,5.692345,4.310255,6.650162,-9.439644,-9.581316,-12.823566,-13.041522,-12.787921,-12.509542,-13.613346,-12.624818,-12.738457,-13.063198,-12.646588,-12.728031,-12.498495,-12.407103,-12.354132,-12.624179,-12.386524,-12.971561,-12.998822,-12.850118,-12.781203,-12.654717,-12.8000345,-12.791206,-12.484102,-12.258052,-12.760429,-12.762414,-12.801797,-12.224721,-12.818769,-12.739681,-12.8062,-12.829326,-12.604472,-12.507194,-12.460196,-12.835634,-12.845421,-12.820308,-12.873419,-12.949085,-12.806666,7.923764,8.404579,8.304189,-5.148103,-11.1175,4.270867,6.4418373,-3.0271327,-12.882519,-13.152161,-12.684062,-12.334377,-12.374006,-13.0743685,-12.891412,-12.759748,-12.812768,-12.772766,-12.791227,-12.583339,-12.616281,-13.270309,-12.574645,-12.684294,-12.909472,-12.770829,-13.024291,-12.778396,-12.664983,-13.363083,-12.509572,-12.761095,-12.852436,-12.18635,-12.5270815,-12.616872,-12.574148,-12.605058,-7.9950614,-7.946314,-7.5853267,-8.037897,-8.652209,-7.7395115,-7.6009727,1.6542927,8.205757,-18.32831,-18.65473,-3.1306534,-2.1847117,-2.306007,-6.3015623,-0.965079,-3.3219883,-3.6821094,-7.4682584,-7.4711385,-1.3362991,-3.0474675,14.151846,12.133719,13.119579,12.941804,13.137685,13.874246,13.149163,0.77585953,-1.2782239,0.15077665,-1.3500974,-0.74047273,-0.58415186,-3.3784378,-1.493424,-8.979507,-2.4259105,3.0225377,12.341985,-1.6652806,-2.3931234,-2.6278913,-2.2680092,-2.0627303,-2.576098,19.127266,19.6971,-3.9689112,-9.505638,-0.64837605,-1.4492348,-2.0082679,0.3996749,-14.520593,2.0266519,-6.1596613,-6.466883,-6.328029,-1.4646066,-2.8682501,-2.1544163,-1.1064838,-0.7495089,0.15547332,-1.327153,-0.11008928,-6.076442,-6.6321254,-6.3008533,7.6220827,9.108414,9.186069,-2.8938897,-1.4753685,-2.5082135,-1.9251006,-2.7151134,-1.6321414,-10.660333,-10.534017,-9.3224535,-2.4025774,-9.561851,-9.396935,1.5525442,-1.4371988,1.5625744,-1.1157225,-9.955656,-9.811243,-2.448824,-0.1441347,0.48809233,0.46776664,0.5353475,0.28049394,-7.053072,-8.177932,-8.644913,-7.8968744,-7.780969,-5.2545843,-4.0664244,-3.177392,-4.0854516,1.0617707,-6.447814,-7.489152,-9.769235,-9.678562,-0.04051993,-0.42152837,-1.380545,2.6755388,8.572127,-3.7003744,-14.844834,-21.730648,1.3006989,0.40098995,-8.338202,-7.385985,-8.431433,-4.233614,-3.5474927,1.9733694,2.978827,3.088185,3.4722638,8.787839,3.2229369,3.8470972,-6.8998704,-3.846824,-3.8790479,-2.4236982,-10.073956,8.737517,8.685408,8.640217,8.359752,8.67562,8.666066,8.688096,8.678088,8.670701,8.680348,8.683259,8.239804,8.653354,-6.9296265,-4.052399,-6.508319,-2.1770518,-3.377759,13.15147,13.1745615,13.014871,13.380405,0.77589405,6.6984715,6.936807,6.90122,5.0623984,-3.3064651,-2.8437033,-3.452178,-3.2117596,-2.9914117,-0.72695935,-1.6816742,-2.6132514,-2.887,-1.3381027,-2.0480728,9.313947,9.314469,9.322388,9.323244,9.312821,9.299202,9.297696,9.295964,9.296632,9.297888,9.297021,9.298923,9.297904,9.295491,9.2932005,6.3643265,-4.172103,1.7146896,1.2273436,0.8406016,1.4035218,4.8585978,7.291555,7.3261867,-6.0229015,-6.93361,-7.245858,-6.3150764,-1.2605072,-1.8542211,-3.8187835,-3.4167826,-1.2153642,-1.9958117,-2.323824,-2.2863438,-2.1788628,-2.5290573,-1.5285939,-0.68897045,1.9026824,-1.7868832,-2.1348066,0.560878,0.19946922,0.46958822,-0.23494011,-7.3671193,-6.9532886,-7.81226,-6.3129992,12.929152,-6.250724,-6.5958076,-6.9609113,-5.7867494,-5.682192,-5.723826,-4.8518305,-7.3972187,-6.939889,-10.02844,-3.519609,-4.0538793,-1.030751,-3.207667,-1.1945869,0.055301763,-0.85452086,0.7640585,-2.2228122,-0.99256665,2.6608987,0.6742592,-1.0423974,-0.3130691,-3.5234134,-6.028487,-3.687811,-2.1731026,3.6761332,3.6813421,2.9446926,3.0068972,3.8634167,4.524534,5.0680714,-0.93360436,-0.16507792,0.4483946,-0.2906682,0.32774147,-0.1492403,-3.928952,-3.415052,3.0952675,-0.56855166,1.942898,-0.12568529,-2.6947343,-2.1602464,-2.4395742,-2.0063431,0.47604835,-2.3589869,-0.80143845,-2.995733,-5.1964602,-6.3107114,-5.528665,-0.6195606,-3.6432576,-2.619732,7.8394866,-2.3730698,-1.4950802,-2.2475083,-1.5604094,-2.160922,7.8484616,-2.4358878,-1.9437084,-0.7030274,-1.8617314,-0.61167437,0.6049248,-2.3439116,-1.9798739,-1.7662802,-1.5515989,-0.96835595,-0.9876629,0.25017378,0.14675623,0.01819171,-0.35476384,-0.5019239,-1.1679921,-1.5319277,-6.007425,-9.692091,-2.5699692,-2.6051278,5.9076858,5.909417,5.668812,4.802761,3.5928156,0.5164009,8.412362,8.375185,8.526513,-7.4004183,1.5017362,1.869774,1.6930244,1.3593856,1.4292719,-6.2636037,1.6070403,3.5392206,13.411112,3.1880932,4.0955353,3.862648,3.844827,4.497104,3.8393564,4.320268,3.1708214,1.9599469,3.4719255,4.1081996,3.7025847,3.8343956,3.5177755,0.10141644,13.357027,-7.167256,-7.198235,-7.298699,-6.9208527,-7.1255608,-6.922795,-7.499683,-7.569281,-8.214751,-7.532625,-6.7919517,-7.379905,-7.1978254,-7.656529,-7.984005,-8.543292,-5.7724695,-1.6534294,-7.009453,-6.980172,-2.6519983,-2.5593214,-2.6806304,-3.2874525,-2.906723,-3.1165085,-4.641394,-3.035734,0.07223379,-3.6291194,-3.4955537,-0.7193881,-3.916079,-5.9318757,-5.8703704,-5.87084,-5.804922,-7.487159,11.86337,11.719643,11.735319,11.944016,12.047916,4.0223556,3.9806764,4.1159663,8.595891,5.264324,-4.165912,-4.599502,-7.450668,-7.39193,-2.4503543,12.728815,16.563164,18.527664,3.6578274,4.6519427,-6.2853904,-6.5045433,3.717802,9.4382925,3.658471,-5.5935197,-3.0362387,-2.0080814,-4.4892297,-2.308629,10.431141,8.980947,3.6349077,-6.661477,-6.752203,-3.8811638,12.998496,11.857886,-12.293183,-12.382569,-12.317245,-9.687785,-12.013335,-12.367984,-12.25572,8.289684,8.317551,-12.176738,8.321455,8.356576,8.330799,8.312803,8.326913,-12.3129,8.336543,8.292935,2.1965764,2.3532043,8.269142,8.114664,-1.246413,3.1516228,2.7716737,-7.9718494,-7.598268,-7.513823,-14.938307,-21.732525,-18.286135,-18.86161,13.423602,8.479905,7.7330046,-2.9806488,-8.038136,-8.175463,-8.203291,-7.9204426,-6.5764737,-8.018315,-7.959882,1.5438132,7.9922676,7.812219,7.59463,0.34299782,0.4353457,0.8233309,2.153573,-5.6700215,-1.6775185,0.250423,0.43891782,-0.08745073,-1.3118173,-0.089238726,-1.5313133,-5.542905,-1.9356905,-3.532683,-3.1571646,-3.0773706,-3.1224508,-2.5251389,-3.2145338,-3.1151972,-3.047356,-2.4809508,-2.9182055,-3.5325296,-3.5392342,-3.2916346,-3.3196201,-3.3973384,-3.6372297,-3.5329385,-2.9429286,-1.7860283,-2.0375438,13.371633,17.657808,18.679976,16.384005,17.017153,17.235504,16.284418,16.230394,18.036932,17.067694,-14.894924,-21.731688,-17.415012,-18.339094,-18.634344,-4.462967,4.2070413,8.425551,8.603626,-7.5486493,-7.1602583,-7.117019,-0.622086,-0.8348159,-1.8073499,-1.9642451,0.21921693,-2.8565428,-1.4324546,-3.0181327,-0.7025758,-0.41086152,-1.0710752,12.580333,-6.385116,-6.339562,7.1679745,-6.432919,-2.8469636,-1.9307309,3.2807002,12.500299,-0.5029621,-2.18445,8.543011,-1.3769038,13.031393,-3.1315525,12.956803,-8.041836,-6.2416196,-7.673738,3.9232855,5.319563,4.9074154,7.378669,7.62652,8.098501,8.071892,-12.448504,-12.43712,-12.521219,-12.383072,-12.346268,-12.394282,-12.596783,-12.37997,8.036932,-12.702583,-12.363923,-12.406732,-12.380232,-12.399808,-12.365885,-12.578335,-12.534472,-12.6074295,-12.84363,-12.857696,-12.66599,-12.47511,-12.64409,-12.545035,-12.486949,-12.586634,-12.346357,-12.488933,-12.770082,-12.376141,-12.499007,-13.138778,-12.553637,-12.168486,-12.473762,-12.444492,-12.322391,-12.361706,-3.947278,-4.0308185,-4.14106,-4.118168,-3.9829562,-3.0628092,-2.6094172,1.694493,-6.6691604,-1.9119867,-1.7969046,-1.9717208,-2.4283571,-1.896399,-2.6236446,-1.8125921,-1.702627,-1.514564,-0.94549674,-1.3152093,-1.5083128,-1.5829775,-1.4080017,-1.8726858,-2.3056157,-2.9537518,-3.2645319,-2.8493426,-2.5858386,-2.454718,-2.0372128,-2.7071471,14.44175,-13.253976,-21.73363,-4.0338187,7.7331376,-7.1788774,8.443386,8.425692,-3.4872975,-2.5210352,-2.5493395,-10.248364,-3.5200346,-3.3017852,-3.3303924,-3.7517614,-2.7980285,4.8704705,4.9619107,4.781155,5.043652,8.8305025,4.421868,8.736619,8.880006,8.752864,-2.4148421,8.951794,8.892554,3.6128242,-4.856726,-5.3097982,-9.142067,-0.2586724,-1.5008249,-1.9981091,-3.738021,-3.495848,-4.6725245,-3.327852,-3.4175549,-3.1156797,0.61134285,-3.0310187,-2.4108138,-2.5963526,-2.2737384,-2.5445988,-2.624193,-3.1859818,-3.714743,-3.911845,-1.4143757,-4.385191,8.512467,-7.9101276,-7.0043774,-7.0097775,-7.4448814,-6.5720673,-6.5189843,-7.1976004,-2.3933573,8.43364,8.324892,8.266616,1.9242078,2.0418353,-0.8167952,-1.9601493,13.270264,13.029512,11.268099,12.148822,10.898323,-3.5974534,-3.83595,-4.1249394,-4.255085,-4.2806287,-0.13156465,0.29391897,-0.15920693,-0.11993343,-12.419348,8.273644,4.5029626,-3.855496,-3.6258614,2.553626,-2.2526932,-0.86100125,-4.8232713,-3.9836137,-3.534822,-7.80884,-6.3080487,-6.9998817,-5.328163,-6.043569,-4.9645286,-5.9050264,-3.4496293,-0.6103145,-0.65140635,-5.6894507,-4.6702576,-4.9754853,8.985682,-5.7574587,13.622236,-0.26580086,-0.097120196,8.393701,8.383387,1.9971458,2.0999458,8.927193,9.3562765,9.229716,-5.0655,-5.2515893,-5.066646,-10.168706,-10.157719,-10.073209,5.2651253,14.5198345,-14.503151,-13.604296,-17.839327,-12.949014,-6.5778,-6.632546,-7.2634797,12.217144,-10.535386,-10.783045,-6.0733,-9.449751,-4.69385,-4.6793957,-3.9665625,4.445692,4.795184,9.088275,7.7777824,7.530138,7.6221857,2.954653,-1.2703671,-2.4492502,19.686852,19.741587,19.549814,19.546997,17.640387,16.2165,17.891071,17.969784,-0.96502805,-3.1028678,-3.619161,-4.4892735,-1.8372943,-0.9306404,11.905138,11.054704,-14.595126,-21.72789,-18.39358,-18.87137,-2.4362016,4.1447897,17.036871,17.597067,18.345749,17.397352,19.55527,19.196873,18.2806,17.295454,-0.3647631,-0.85889906,5.9459305,-0.31214303,15.778473,15.774019,15.760199,15.769649,15.756248,15.775276,15.7792225,15.76866,15.772227,15.7807865,15.779481,15.772307,15.773326,15.713638,18.488197,17.975979,19.164347,18.883934,18.190254,-7.152283,-7.304846,-2.419109,6.0234857,6.0203905,8.151263,14.572145,-7.358367,-7.297423,-6.531484,-6.428567,1.7526364,-6.4029436,-2.7741027,-6.6089272,-6.9319563,3.3075104,-14.868685,-21.733332,-7.103154,-21.73153,-18.367617,-18.89821,-18.502893,14.310135,13.380538,-1.4807649,-1.9570222,-2.327719,13.354599,12.687021,11.300893,10.46006,11.386181,6.145262,2.6708367,0.030768238,0.19430302,-1.8048748,-4.1972003,-6.922192,-7.1699495,-7.0088553,-7.037263,-6.8790264,8.312191,-6.8682075,2.2747743,0.8135628,-6.833659,-6.989741,-10.508591,-10.645566,-1.3411655,-9.2820215,13.14262,8.673377,0.6915614,-6.6498313,-6.8920865,-8.903496,-5.921417,-6.9123974,-6.8262987,-6.914443,-7.075653,-6.789905,-6.6738143,-6.7048674,-6.7222896,-6.720563,-5.945561,-8.134336,-4.1928167,2.7515967,4.2343044,4.1071196,3.9535618,3.7886817,4.2082744,5.985297,7.701592,7.8319745,7.846044,-2.9271564,-2.7521224,7.8158875,-0.29573,1.1501625,-10.073983,-9.477237,2.7662466,-0.18089773,0.6802107,-3.4989076,13.650804,3.6278622,3.6110618,8.250671,8.2336235,-3.3423991,8.152813,3.2265384,2.7028608,2.6594248,8.59474,9.060476,-6.617322,2.4262168,-2.9608092,-0.5776453,-1.917657,-1.984995,0.26610586,-0.5626176,-1.1271193,-2.9947517,-3.62406,-3.585753,-14.512814,-21.734455,-2.295149,-4.75787,-3.4594984,-5.9271913,1.8962977,2.0165815,-1.2109872,0.2780724,-1.0018638,-5.046171,-5.416185,-6.463548,-4.7377453,-4.7064753,-4.0827947,-2.1513515,-1.8807333,-3.651519,-2.8671904,-0.36689648,2.70254,-8.335424,-8.255786,-7.943115,-8.407881,-8.293636,-8.012393,-8.31913,-8.346792,-8.083689,-8.371509,-8.378308,-7.8102818,14.206995,3.9795313,4.330472,12.229509,12.591305,11.060127,12.451196,8.421721,8.489565,8.711617,8.651499,8.710735,7.763647,1.9261185,2.576385,8.232929,7.7365494,7.892462,8.032223,8.34017,8.069566,8.08365,8.322172,2.5911934,2.3596284,1.8228817,-1.4661655,-0.89161146,-0.40989655,0.18659778,1.296107,2.363411,-2.5849495,-2.3731902,-2.1191964,-2.0910451,-2.0024984,-0.7119779,7.5345054,-6.894782,8.256669,0.68300843,2.315657,-2.123158,-14.907241,-21.736023,-18.497934,-18.688704,-18.68274,-18.694065,-18.691837,-18.693249,-18.752483,-18.691483,-18.695345,-3.8119397,9.284578,9.300434,1.9595877,3.3167233,-3.1054597,-4.535736,-3.1653304,-3.4318213,-1.7304621,-1.379832,-4.531236,-4.05219,-3.8591287,-3.8537219,-4.0133524,-3.6920042,-2.9303143,-5.973941,-5.1770487,-3.3523552,-3.3866453,-3.6393342,-9.578022,-9.365505,2.220384,2.3921256,1.9640533,2.7970455,-1.321922,-6.851872,-6.7316084,-7.189909,-3.9671776,-6.512058,-6.633612,-6.5914154,0.61548483,-1.5035431,-1.646453,-5.1470757,-0.685404,-6.473553,-6.799099,-7.4347415,-7.517985,-2.4263895,-5.117565,-5.275262,-5.199931,-5.084731,-6.48816,-4.8662815,-5.322899,-1.5116003,-1.7735832,-2.5569267,-2.2896621,-3.6386254,-0.33514035,-0.25236928,-3.949422,-0.35953856,-4.1938124,5.5340085,-5.4906263,9.022177,8.958658,2.5146983,-1.716254,-1.90051,-1.2005328,-1.2180836,0.754822,0.19048722,-2.7318718,-0.56543523,-0.13080797,-1.3634031,-1.2727845,-4.94962,-6.422652,8.674375,-10.0032215,-10.187517,-9.570619,2.3888907,-4.271467,-2.4534836,6.1979,6.2087355,-2.1122158,-2.133425,-1.1691265,-2.4856505,-2.236035,-2.2085865,-2.1330383,-0.77401245,-1.1269399,-1.4295565,-1.753269,-3.491373,-5.8494124,-4.7575326,-3.6120903,-3.5748048,-4.0655937,-8.770966,-8.554582,4.6285186,4.57723,-4.5561614,-3.9360297,-3.4841006,-0.18199107,0.5977331,1.2210482,0.5206021,0.79992306,0.75860894,0.5222158,3.5978067,4.5286145,4.660546,-14.530831,-21.734549,-17.416325,-17.819239,-14.99306,-21.732018,-18.15682,-18.655651,-18.689842,3.0721068,2.758899,9.053462,-3.2753646,1.2200478,1.6380899,-2.7330387,-2.6768048,-3.0624626,-2.6787653,-10.358584,-10.638661,0.19143145,-10.5883465,-10.612378,0.26653,-10.379615,1.4641354,0.95480233,1.4112533,1.7215465,3.228751,0.0648721,0.29165173,0.037438434,0.3008625,0.8593615,0.61175376,0.5634518,-1.09909,-0.12613891,-0.45900318,-1.6043435,-0.27418506,-0.2437848,0.1287945,0.08340537,-6.9315267,-6.374568,-6.290312,-1.9480782,-1.5315231,2.8507726,0.2651429,2.5285752,12.784394,12.710527,7.461166,-3.6715498,-3.3930259,-3.4200087,-3.443089,-3.3847756,-3.3361328,-3.3676677,-9.668306,-9.940781,-8.957254,-1.2859454,-2.4545817,-3.2378037,-1.8438547,-1.8699428,-0.9176192,-2.7717366,-1.9465039,-4.317891,-5.837606,-5.8817334,-2.2763891,0.21469556,-2.266318,-5.977294,-0.15094085,-0.09659588,-4.3807096,-3.7666285,-3.6396956,-4.0978866,-3.82728,-2.345975,-2.9674916,-1.638351,-2.1570923,11.811273,11.169989,-9.67538,-9.742496,-1.403107,-0.16581827,3.804061,14.152298,13.97087,14.115344,14.135271,14.1477785,3.3753188,14.681063,-7.1974854,-13.258903,-21.73303,-18.256935,-18.64269,-18.685328,-18.618721,4.714181,-1.5924798,2.5114987,0.86885625,1.6678926,10.826484,-0.11952329,0.17864527,0.08559283,1.2087238,0.23294027,-6.7214675,-6.9781914,-6.939842,-6.9124103,-6.6702156,1.1242206,1.3229427,-6.746987,1.3071865,1.71835,-6.4645243,-1.6836991,-0.50151354,1.2834945,4.231936,-7.1531487,-0.045457523,-6.8502626,-6.1087775,-6.506536,-5.8698444,-6.7720256,-0.0740222,-18.587164,-18.635746,-18.576477,-18.612715,7.1678934,7.22628,7.305208,0.40924162,-2.490477,3.6909225,2.9561045,3.3365736,2.3000183,6.114732,5.379302,6.672346,3.6953344,11.798612,-3.223154,3.1534903,2.0559323,3.0238147,0.75775814,0.58760667,-10.227759,0.9530917,0.19284569,0.13247193,0.23277329,-14.847223,-21.731335,13.2595415,13.454434,12.727674,13.824872,6.4773746,-7.4788065,5.846968,-5.1043835,12.751931,11.857674,11.140241,-4.0749707,-4.231331,7.812551,0.6697096,0.63662845,-4.0621037,-3.0347872,-3.6366389,-3.3973591,-3.295455,-2.633633,-3.3589897,-3.985814,-5.9506683,-5.927936,-1.3908612,-1.8805318,-4.8966722,-1.9246159,-1.046981,-5.105079,-6.371563,7.703216,12.808428,12.603531,11.620795,4.3196287,12.136453,11.84088,13.692967,-14.795983,-13.60654,-17.413673,0.499639,2.1335988,-4.7934623,-7.012026,-1.5802238,-1.5496024,-3.5590973,-2.3451922,-4.914021,-5.1641626,-5.310119,1.948162,2.1834204,1.2649614,1.9591845,0.6972714,4.095352,3.8768716,4.1637197,5.262368,5.7455153,6.639546,-6.045369,-2.4549575,-6.3084407,0.18375929,-2.305451,3.981591,6.305419,6.1647677,6.898611,4.310551,5.271416,5.3300095,4.806574,4.5869427,7.1619463,18.00262,18.494331,-15.147583,-21.73475,-18.150349,-18.671734,-18.49282,4.0515575,3.8285985,4.040043,4.1300817,6.524692,12.547011,6.0494614,6.2151275,6.2870817,-8.499526,-8.550549,8.48843,8.3340225,-4.0316515,-1.4550956,-0.80551076,0.2309487,-1.1094635,-6.2943487,-3.3634098,-2.9670286,-3.6960924,-3.5894856,-3.2729628,-2.8009362,-4.4033527,0.75573856,0.6058615,-5.002587,-0.54507613,5.235898,-8.1426325,-7.515065,-15.086967,-21.73171,-18.239088,-18.819977,-2.1489882,-2.2996223,-3.1056955,-1.9917624,-2.9438477,-2.6166842,-1.9928939,-2.9614441,-3.2244246,-3.0993938,-0.7867252,-2.6228213,-2.2080247,-2.2509751,-3.1500294,-3.0285146,-2.8782866,-3.0988684,-2.6092134,-5.1204777,-5.0210633,-5.0297837,-4.2742176,-2.446062,13.410073,-10.056389,-6.630017,1.9213104,0.90195304,10.929187,0.05686329,0.17973647,0.2954898,0.3054888,0.6340949,-1.1534392,-1.2261431,-1.0974891,-0.8261844,-2.7681494,-2.297371,-2.2290504,-10.855572,-10.759854,11.851705,11.748101,7.9481864,7.700751,7.9551926,8.151934,8.168133,7.863257,17.560854,17.56273,7.139255,8.135643,8.04775,7.9192653,8.024623,8.739535,8.08032,7.21376,7.9802055,-12.498886,-12.956018,-12.329197,-12.867885,-12.762672,-12.270756,-12.568431,-12.625013,-12.615669,-12.855984,-12.562133,-12.397863,8.423203,-11.165199,0.164731,1.8722299,0.21692623,10.883485,0.077632576,18.282722,17.475998,17.370266,-4.6059213,-2.858945,-3.349061,-2.9731393,-3.5517569,-0.85761166,0.12026403,-4.144619,-3.0912447,-0.58216643,-3.8230793,-1.5580418,-2.6280181,-5.5815883,-3.1126118,-2.3081546,-3.265428,-2.426422,-2.665052,-2.5266745,-14.854407,-13.606381,-17.417131,-6.623332,-2.6250637,7.263193,7.815191,8.413282,-2.8196566,8.513383,8.608663,1.1818337,-12.37559,-12.556259,-12.351642,-12.248551,-6.223032,-3.670848,-3.8479345,-3.9486969,-3.7290888,-3.439076,-13.605484,-18.15088,-18.54522,-3.3725798,-3.483339,-2.974761,-2.5581427,-2.4495862,-2.2370117,-2.1227462,-4.0574565,-3.1055183,13.655447,-6.5488772,-4.463047,2.5383408,-3.6997244,-2.1761992,2.3998435,-1.5909076,2.4775643,2.5388095,-4.449432,13.364008,0.024541812,-0.4983379,11.168801,11.789907,11.850405,-14.459994,-13.604975,-17.414665,-6.535864,-2.4369404,-6.8356304,-5.0830846,-1.2266847,-6.6818404,13.049479,-6.5089865,6.1835737,13.137065,-9.188306,0.5250924,-1.1743703,-1.5349135,-1.124111,1.8981103,0.47597146,-8.126188,-5.6847014,3.408742,-5.8070035,-5.932089,3.7189934,-15.12595,-21.734573,-7.3139954,-6.5344505,-0.63749456,-1.5993506,-6.5604916,-6.8172007,-6.7623034,-6.765723,-7.1798224,3.6082225,-0.4265127,-7.336358,-7.1723285,-2.3049471,-2.7328513,-2.579887,9.113345,-6.4983826,1.6934259,-6.92396,-6.644581,-3.2882805,-6.5605516,1.1745924,1.1319873,1.9127647,-6.875934,3.469434,3.2372901,-6.6454625,-0.5724914,-1.514269,3.3898065,-0.99584645,-4.676321,1.1752945,-6.953232,-7.006742,0.94639254,-1.2630446,-4.264475,-6.531925,-6.75098,-6.7291594,-7.099079,-7.6501007,-7.6008983,-7.351322,-7.2191772,-7.3972454,-3.7988155,-2.5955086,-2.8065329,-3.5699236,-3.1296153,-1.6740822,-2.4914324,-3.2661903,-1.5109946,-2.6906965,-2.4562416,-14.425065,-13.6057415,-17.41521,-5.263906,-4.236964,-4.1871924,-3.467663,-3.0270956,-2.9367094,-3.9940765,-3.4326556,-4.203516,-3.1842902,-3.4151366,-3.7633047,-1.0637951,-3.8161964,-3.1535795,-3.0485654,-7.0798244,-9.617857,6.80957,6.637235,7.0956945,6.551048,6.149402,5.8841166,5.198799,4.442989,-4.193036,-6.5624847,8.909038,4.8936696,5.097059,4.952967,5.1063256,5.199784,4.3036494,8.740703,8.381109,8.348817,8.371256,8.905133,2.2979922,1.9885346,3.558131,-5.7840476,0.46503392,-1.3796723,-1.2949355,-1.1254247,-1.6981144,-3.736329,-2.647744,2.8801405,-1.4829303,-2.9288063,0.33892322,-1.9863764,-1.1458468,8.4892,-4.4274216,-2.8135362,-2.3534796,-4.6360226,-1.7485534,-2.7358003,-1.6311319,-3.9496872,4.5989842,3.904964,5.642252,-4.6650605,-4.8428206,-1.251253,12.893184,12.895228,5.2153716,-6.7804966,-6.7438865,-6.5687757,-5.9541297,-6.1502233,-4.1306763,-5.415138,-5.4182386,-5.419178,-5.417235,-5.4184084,-5.4191256,5.959427,5.980887,3.3770251,-3.5617661,-1.4439515,-2.2060342,-1.7727562,-2.4000807,-15.07821,-21.73351,13.270433,0.8449975,0.56673056,1.0231853,0.64694124,2.449275,5.18228,5.3598056,4.978748,5.668618,-4.170366,-4.0311003,-3.7001483,-2.8864615,-2.3091722,-2.479355,-3.1438684,-2.9981449,-2.5140574,-2.215326,8.647733,-8.336859,-5.2334886,-5.892292,-1.6155355,-0.6295619,-15.102375,-21.734339,3.3374994,1.3398827,-4.1325154,-4.6725006,-1.4717281,-3.5102987,-1.6917728,-0.31134802,-0.34429058,13.401555,14.022057,13.461258,12.402469,7.1017423,7.9546866,6.5624003,7.9203873,7.772821,8.240344,7.9647923,7.846588,17.562311,17.562878,7.4330196,7.7745404,7.7332244,-12.89571,-12.894187,-12.945537,-12.914046,-12.681484,-12.815139,-12.356254,-12.419852,-12.344394,-12.570117,-12.514344,-12.971145,-13.18679,-12.868285,-3.3022807,-4.095578,-4.3879824,-4.042583,-2.8034506,-7.1736073,-6.7184935,-3.9869826,-6.0128965,-5.169269,-5.8968043,-6.2357216,15.73534,-4.206599,-3.7314055,8.293626,11.592852,13.976176,14.244321,-0.23401152,0.10687059,-0.1151323,13.252491,-7.29889,-6.7054296,-3.570007,-1.3541375,-1.5286564,8.389296,-2.2607872,-3.0314038,-3.0048883,-3.0268025,-2.4827824,-2.720385,-3.7174187,-4.949905,-2.516274,-2.4235113,12.429569,11.507291,11.27333,11.292574,-3.4448986,-4.708289,-1.5925049,-2.3727744,-1.7704568,-1.7516415,-1.4898021,-1.5355101,-2.1669056,-2.2603426,-1.6375022,-2.0572793,-1.7470151,-1.9934646,-2.6103873,-2.1006236,-1.2026234,-2.1850162,-1.5347425,-1.8188244,-2.5825481,12.263363,-6.1070576,2.783082,-4.1906857,-1.1716061,-0.35166806,-7.8338056,-7.806055,-1.8417583,-1.7934086,2.5445745,2.8542366,2.51062,1.5941263,-3.1918905,1.6616708,4.963186,3.8587687,4.1437864,-0.92856795,4.5476804,-3.2096117,-1.4217485,-1.1565871,-3.1290817,1.2787225,-1.6853249,13.297434,13.036041,12.621999,-4.0072436,-2.8273127,-2.8505778,13.225388,-3.5080173,-2.315989,-3.1642115,-8.02102,-7.6014185,-10.532445,-10.736099,-10.32132,8.677862,8.672672,8.680956,12.18044,12.095181,11.134234,11.438383,-6.974015,-6.2507205,-6.700426,-6.529016,-6.226887,-6.554316,-6.0036535,0.29093617,0.46154073,-6.180382,3.7047567,7.2352796,8.914726,8.834467,-0.24128418,0.23955618,0.35807243,0.45363975,0.45082992,5.608172,5.100627,3.5516098,8.439234,8.446804,-4.219782,-3.365106,1.2915529,8.610937,8.631511,19.305954,19.38884,19.356472,19.54386,16.94642,18.30535,9.038291,17.884037,19.074825,19.576435,19.396017,18.396208,17.362453,19.028128,0.22922646,0.3548136,7.2666035,0.58435434,0.4102283,0.33573556,0.3221476,0.41706553,0.103788175,0.29108742,7.301266,4.23557,4.3045783,4.391242,4.4609838,2.6636534,2.9155705,-0.42277423,-0.2909593,19.294449,19.400555,17.876963,5.9256124,10.115023,9.813184,9.957266,8.502815,0.60401726,-0.5584629,3.9143844,2.6829937,2.3048086,-1.4967271,-2.948136,-1.6044695,-2.752026,-0.91159976,-2.2891152,-1.5162636,-2.3445923,-1.5254213,7.089731,-14.910134,-21.733194,8.685957,8.684699,8.677807,8.66408,3.4693127,3.7230077,3.2725239,2.7123375,-0.17912756,8.193456,1.7720909,1.970893,1.5251305,7.844972,0.20652339,8.427951,-0.9941374,-0.77073497,8.681014,-1.9807774,-0.8731546,-1.7876827,-1.4234287,-1.3579481,8.019015,8.073466,10.25308,2.244698,8.195605,8.598516,8.664018,8.714131,-2.6018054,-3.1184182,-14.77445,13.517949,2.6511533,2.260488,2.2791722,-10.123832,-10.01158,-9.258003,-4.2403193,-5.9660664,-5.936048,-5.866113,-5.7417307,3.084999,-5.718845,-3.5111885,-2.0837507,0.5447528,2.794059,-3.5774276,-3.4169528,-3.6041262,-7.0620184,3.6091335,3.5206087,-6.7566314,-6.5333767,-7.000378,5.504327,-1.1194049,-3.5007823,13.338173,-3.8219547,6.197073,6.2089243,-3.0308092,0.034407176,-1.585935,-1.6806309,-2.5840507,-1.0705031,-2.020497,2.5752392,2.5842667,2.3182244,2.5453475,2.2222085,-1.3949217,-1.351399,-1.7486136,-2.269509,-2.6318834,-2.101023,-1.947597,-1.9020432,11.593886,3.2533324,3.290166,-1.5576338,-3.7258985,-4.708421,-0.56313103,-1.7409475,-4.0573325,-1.5320767,-6.619256,-6.9602585,-7.0499797,-7.137133,-7.119733,-6.9031606,12.33564,-0.0515801,0.22505295,0.475703,0.48397404,-14.868991,-4.473548,-4.7038307,-4.4630165,-4.2575345,-4.448051,-4.78469,8.545887,-4.553136,-4.746802,-5.6840696,-3.6109447,-5.260348,-1.7670965,-1.0910863,-1.4099591,-4.2623987,-3.5450058,-3.6706417,-1.2073225,-0.3694393,8.669532,-0.5187155,-2.7233047,-0.6042121,-1.7390883,-1.9647787,-6.8847613,-1.091033,-2.9559438,-10.245836,-6.3584723,-6.4881964,-6.327131,8.947162,-2.7053022,8.769127,8.765103,8.952217,8.745008,8.695081,13.20402,13.10159,-10.000363,-6.535718,-6.176485,-7.080102,-6.721158,-6.5239472,-5.2890306,-5.599452,-5.3737164,-6.0039997,-5.6373415,-5.601025,-5.4866657,-5.530565,-5.2948775,-5.609273,-5.790806,-5.6270833,-4.873495,-4.290927,-4.8095455,-3.914223,-5.8771224,-5.979423,6.7730975,-7.967528,8.873105,9.00205,8.731608,8.822059,8.8391695,8.823762,8.664372,8.659867,8.641081,8.714874,8.64076,8.634296,8.7871275,8.759921,8.88858,-3.45176,-0.1508709,0.27141404,-0.20420867,7.1839085,-5.798005,-2.5178192,-2.4291005,2.7087288,5.2764106,5.293416,5.2647567,11.946847,11.5484,11.109471,12.42968,12.784188,-0.21520662,0.33738312,0.44900605,0.47131857,0.4616948,12.34984,-4.720747,-4.966084,-4.5991516,-1.6607057,-1.6810739,-0.833595,-2.6995087,-9.469554,2.0484703,2.337487,2.7545238,-2.3087618,12.158077,11.967011,11.400803,2.4252193,2.4209552,0.5984288,-2.7691615,-0.01566572,11.852573,11.794091,-6.3375745,-14.646567,-21.73144,4.2611194,8.580951,8.580452,-3.9073253,4.6360593,5.078011,1.2648287,1.4789649,1.9642525,0.9647101,-7.6148133,1.118037,-0.23010696,-0.96012205,-1.1993328,1.3880473,-2.7257147,-5.1691017,-3.4542074,-3.5302281,-3.610981,-9.652704,1.6155776,1.5783125,-6.669312,-4.1549997,5.8815007,-5.842604,-4.9583535,-5.997563,-5.8323693,-2.3792174,-5.160073,-4.543378,-0.70980394,-2.0817904,13.426984,-4.1168504,-3.88577,3.711638,3.8567336,3.8683286,2.8231566,3.7190955,0.50318027,0.5212225,0.4715898,12.866406,8.580752,-5.597488,-5.510926,-5.8067193,-1.5285141,-2.4185536,-2.4929013,-2.467456,-2.4379652,8.945003,9.023385,7.742665,7.8458714,7.702945,-3.0338628,-2.177491,-2.1315744,-4.580838,1.2552879,-5.748958,-3.8849947,-4.36079,-2.1219938,0.41289875,0.5956172,-2.3324733,-3.027655,-9.268861,-5.9393206,-4.099127,-1.2385849,-2.1872919,0.11115579,-1.4494334,-1.7584915,-2.2805412,0.08636378,-0.9701784,-1.4553491,-5.3600984,8.976152,9.064519,0.85660905,2.7023654,-3.1288564,-2.15445,-1.6431627,-0.76088923,-0.20052771,-2.8794892,-1.3223885,-1.1472125],\"xaxis\":\"x\",\"y\":[1.7992861,0.18216032,3.8562248,2.3221164,4.341427,2.5593083,-18.229391,-18.705856,-18.49212,-18.0449,-17.291603,0.73240167,1.1243412,-0.351942,6.928961,-2.1721008,3.3142023,8.553989,14.81118,-4.0341287,1.952742,2.5096765,4.685263,3.5075078,4.6297193,-2.0637228,7.965701,7.8722153,0.4091743,-0.23765992,0.32616603,0.19947034,-0.7857268,3.7168264,4.0202503,3.6829088,3.998052,3.9194214,-5.491407,-6.2840867,-6.1029305,0.7245662,-6.4314632,3.8585215,3.8017967,3.6801052,5.636684,3.4613016,3.5594907,3.417187,6.5813727,-0.5639795,3.549384,4.9244013,-6.826605,-11.936018,-5.697566,-5.924848,1.7490857,-6.1449103,-6.096514,-6.0069895,-6.0606604,-6.175049,-6.654408,-7.3731713,4.001016,3.7277646,3.829582,-1.094382,2.7748637,1.7552336,-0.81546944,-0.68156284,3.932973,-0.83516794,3.6405966,4.707481,5.5441256,2.2192566,4.055764,3.8870175,0.63242805,2.7693803,-0.57381475,4.2083135,3.9643137,3.9512427,3.949447,-0.8244853,2.84864,-0.7742591,8.5534315,-4.152784,-6.62737,-6.8455796,-8.798427,-1.145176,5.045012,5.153928,1.075444,2.4007022,1.5336912,1.7178086,1.5269653,1.2935445,1.8372464,2.1148384,1.238589,1.67209,-11.741112,-11.671951,-11.423426,-11.374673,1.7485673,7.1603985,4.7018504,4.681245,4.8791337,4.7243123,5.0807385,5.013163,2.8397832,2.7188294,2.4856884,2.3123972,2.1644788,-11.666481,-11.704329,-11.711046,4.4355416,-0.7701861,4.3079295,-1.9488456,4.452331,4.339877,4.0595727,-2.0062532,19.449142,14.812984,-4.209578,-4.3508463,-3.7361887,-4.3544044,-4.2828875,-4.3504434,2.1825378,2.152798,2.2240896,2.248767,2.2672324,2.8680072,2.6794677,2.7732,3.0219178,2.790837,2.8289251,2.734898,2.681844,4.5834928,-12.555398,0.42936757,0.5651242,0.29522756,1.040888,-0.66410047,-0.6925797,-0.81500584,-0.5355314,-0.12501101,1.2413081,-1.4313533,-0.60915565,-0.7388344,-0.55913466,-0.5045951,-0.83918285,-0.61637926,-0.5927159,-0.5737581,-0.99054646,3.3086765,3.0622075,3.2047763,3.4228592,3.3460786,3.4707682,3.392629,2.134596,3.6307638,3.2628624,3.4524624,3.3674462,3.482195,2.0403564,3.2975533,3.5033047,3.1955533,3.364449,3.6371894,3.140383,3.4032283,3.3144617,3.3561904,3.426856,2.8522897,-6.6200857,3.133324,3.4489133,3.2054446,3.1721113,3.2573297,3.2419512,3.4296553,3.4536228,3.315132,2.811671,2.8500707,2.794382,2.7819262,3.993135,3.930125,3.3133163,3.8455827,3.756915,4.0948734,3.581606,3.6092389,3.092517,3.4911354,3.500243,3.6041052,3.9084678,3.4263122,3.5332844,3.5274763,3.2878635,2.1389923,3.5249429,3.536207,3.3079088,3.3078294,3.4383001,3.4847677,3.0095217,3.0084682,3.2387557,3.4040813,3.2856307,3.2687154,3.3541608,3.5933385,2.9120018,2.7685125,2.870091,2.8950481,2.9468644,2.5724654,2.9462082,3.032749,3.063792,1.8521717,3.4951942,3.156657,2.264295,3.1156,3.419183,2.9634426,2.2392616,3.7716458,4.088601,2.2513206,3.1187372,2.7382824,3.0615888,3.001066,2.9897938,3.663712,3.807651,2.088556,3.1145694,3.0650635,3.6522336,2.7616549,2.9302964,2.4086523,1.3068798,1.2257663,1.6128771,0.021910803,1.4426309,0.5372455,0.06678041,1.5537757,3.8856692,4.90201,4.476693,4.851901,3.7647505,4.057104,3.4320996,4.6999483,-1.1809895,-1.5681343,-0.4444795,-6.240914,-0.40530807,-0.43053547,-1.7171808,-1.9241796,-1.1721811,0.15039125,-5.3198066,-0.01966883,-0.6169227,-4.2909794,-7.2929587,-6.793985,-7.121326,-6.9969707,-7.51812,-6.453376,-7.2917657,-7.494484,-1.4669012,-0.612361,-0.9276237,-0.78403294,-1.2611554,3.4772236,4.087576,4.8941097,4.862353,-0.862303,-0.54419756,3.2045085,3.1947315,-6.78569,3.1708944,3.1293664,3.2237582,1.2760073,5.1112943,1.0838879,0.75677073,0.9928489,1.1693344,0.87585187,0.58582777,1.4069301,-7.6342716,-6.961577,-5.838381,5.577629,-2.0474186,-1.9447739,-0.8094096,-2.2709463,-2.3831847,-1.8589269,-2.147566,3.722738,3.9303231,4.064297,4.110369,3.914968,2.1360781,0.4476562,2.7010565,1.8023757,1.4212732,1.7596934,-0.41408318,-0.43763596,-1.9780384,-2.394447,-2.290258,-2.2135963,-2.4037964,2.39781,1.69517,7.5874324,7.5725803,7.913135,7.201963,3.2529583,1.7531472,1.2751516,1.3132179,1.3036833,1.591963,1.3535922,0.4126585,-0.15366964,-12.027976,-12.11807,-12.230701,-12.377558,-1.0606894,0.780868,0.875361,3.166765,3.136703,3.6398652,3.338173,3.1459134,3.2250602,3.9418013,3.6720927,3.7064402,4.2447033,3.3374207,3.4908109,3.1802897,3.238739,3.261929,3.169886,2.0303555,3.8302722,-6.2615395,-8.457907,-6.4777613,-6.842391,-6.7974973,-6.776098,-7.1025863,-6.921963,-6.535271,-6.6155834,-6.723465,-6.7876406,-6.5428467,0.46053746,0.80813146,0.7076109,0.6894498,1.348099,4.1175337,3.2583046,17.957302,6.150709,-11.152768,-9.438872,-6.9489417,-7.200927,-7.0952353,-7.2870417,-7.2267127,-9.436599,5.3832436,3.6381786,2.3411067,-0.27750054,0.123828486,-0.091131575,0.23949677,-1.1848484,1.4948993,2.1083648,0.8584227,1.9902582,1.4828815,2.2739692,3.9200602,2.7087011,3.597485,5.038582,5.293737,3.331573,3.3093219,2.5446155,5.0737743,4.905091,4.8295097,5.863025,5.0861,4.9864597,4.5423923,4.8826485,4.185347,4.443438,4.1781173,3.3031504,4.510103,1.718246,0.07220399,0.02574419,0.48147857,0.0019687794,6.1991215,5.8215528,0.6371227,4.1264777,3.6312387,-0.46726283,-0.068442754,2.0687318,-1.4370325,1.451906,3.3661149,-1.4778074,-0.63687485,-1.3911694,-1.4405558,-1.2541934,5.58331,6.4057937,6.427161,-11.903019,5.3480434,4.888004,2.6908274,0.5219817,0.5706676,0.46010685,0.44726554,0.5044386,0.44754902,0.23307125,0.22118631,0.3760996,0.2815353,2.642081,-0.450653,-0.27584824,0.30914494,0.32886532,0.5236223,-0.65449005,0.33527622,-0.00114343,-1.5063491,-2.1733158,-2.329432,-2.0236218,-1.4742087,-0.3159547,-0.74744415,0.24504773,0.0044703595,0.65704924,2.634781,0.68526334,3.1071465,4.9985337,4.651755,4.7263665,4.3145456,3.9352124,3.2351794,-1.4975578,0.44813594,-1.16404,6.414728,0.54032874,0.12722774,-0.43865716,-1.0812106,4.8458104,-0.008935755,0.11628474,-6.3886914,-7.1112766,-2.0335357,-2.0479918,-0.17308883,0.4489883,0.5909673,0.09500032,2.3299608,1.9561764,-5.7587285,-2.171209,8.552613,-0.060257666,1.3292164,-6.2665343,2.8250751,0.28458384,1.2874092,-0.79193604,-2.3295846,-6.2550497,-0.312059,-1.3185685,-1.4951475,0.50000197,5.184899,-0.469544,-6.1210318,-6.247018,-6.226208,0.7904967,2.4047065,-1.6810957,-1.9400579,0.7403353,-0.21528345,-0.41152087,-1.0397362,2.488795,2.9589105,2.6522431,3.1677334,3.0150223,2.3713236,2.4252286,4.066071,3.6828341,2.542623,2.5553775,3.6319532,2.5634491,2.3280227,2.532127,2.0379686,-0.72296154,-6.288264,-0.8050221,3.198554,3.269394,3.1322658,3.3262157,3.3702295,6.5202117,6.633282,7.3988905,7.679666,0.3008768,3.7728512,-0.58358353,3.6975884,3.445764,3.6768785,3.3985786,3.4546993,-1.9874918,19.45037,14.810439,-7.828755,5.0960965,-1.9071358,-1.0406201,-0.913004,2.9440727,3.3175492,-0.77321464,-0.98299444,-6.8466554,-8.265377,5.067021,0.197436,1.7185459,-3.8472452,-3.6820061,5.2372165,6.673887,7.0004425,-10.246197,3.6322224,3.6147077,3.7507272,3.8085425,-3.574482,-6.866651,-8.347032,2.169813,1.6697556,2.2156916,1.7983526,0.10726291,-0.13879645,-0.9055022,-0.20024222,0.5856714,-0.6479938,-0.60026854,-12.5631485,1.6148947,1.5502198,1.6727937,2.0133932,1.0539255,-0.85672027,-0.80845034,0.48447633,1.1771926,2.0623357,4.6534867,5.3189635,0.04221977,0.066440254,-0.5652588,1.7483351,1.5384837,-2.288424,2.561742,1.5640148,1.5083659,1.5797615,1.4359089,1.8698704,2.2303708,0.5783709,0.45388776,-18.448362,-18.306751,-18.236057,-18.29602,-18.30654,-18.630442,-18.729748,-18.0715,-17.467161,-17.820827,-18.31603,-18.080456,-17.850637,-17.253597,-18.339174,-18.17691,-18.25308,-18.169102,-18.261763,-17.100393,-18.19693,-19.304005,-18.527056,-18.682436,-18.277431,-17.762224,-18.73536,-18.46176,-18.293667,-18.439823,-19.09612,-19.210093,-18.627728,-18.338303,-18.441021,-19.160927,-19.25387,-18.360777,-18.630411,-18.799934,-19.165232,2.2678447,-0.27640492,-0.24400854,2.215679,4.509377,1.8842006,1.9628351,0.7747059,-18.084658,-18.23514,-18.552263,-17.981031,-17.926203,-18.213793,-18.199392,-18.240467,-18.339985,-18.211859,-18.235891,-18.486568,-18.467196,-18.27132,-18.457508,-18.250517,-18.173336,-17.869724,-18.260876,-18.492903,-18.500881,-18.203003,-18.694378,-19.171753,-19.243464,-18.456505,-18.268015,-18.233912,-18.374372,-18.593958,1.420601,1.3048387,0.9615317,1.5435477,1.6784507,1.4796058,0.5645558,5.056105,2.5368023,-4.204144,-4.1532035,-1.7774602,-1.291216,-0.45264357,0.28633162,1.6055392,-1.5191752,-2.292526,-2.3302405,-1.725017,-6.1307693,4.198014,3.7766197,3.7148373,3.9566708,4.0031896,3.8749666,4.089824,5.2061396,6.5209665,0.8257955,-1.1334685,0.8120122,1.2935967,0.4733348,3.111291,3.11978,-0.83583724,-12.047391,4.2580757,4.2325172,-7.2645087,-7.150998,3.88295,-7.0258904,-7.0021257,-6.96594,2.2891953,2.3497665,-2.4757967,-0.726355,-1.8672729,0.4428227,0.98710364,6.0158916,-2.2343152,0.99010956,7.1315107,7.8206453,7.7865357,4.2039714,-0.0045681,-1.5615023,-0.15899618,-0.23039475,7.4396386,3.4691348,-1.0325698,7.0597997,0.60967886,0.2584063,2.7077515,-11.4064455,-11.4212055,0.6783384,0.24400896,-0.58659494,0.45898038,-0.32808524,0.6623266,0.31039235,0.0066166623,-0.6977014,-12.215789,0.13491742,0.5197004,0.617109,-4.1433992,3.8638413,-2.902743,0.29870057,-0.31178272,-11.895958,8.910246,8.023014,7.871317,7.9512353,6.213538,0.51772,1.8029613,2.5615315,1.2631072,0.9028677,-0.5928154,1.1098433,3.9197848,3.1960716,0.57568556,1.6498452,-1.0682529,-0.57785577,-0.8594406,-5.939663,-6.2166886,2.7286491,4.244311,-10.080736,6.723192,-1.9162076,8.552445,4.399715,4.4271536,3.2645276,3.4884999,3.249816,-6.8866796,-6.642337,1.0158708,1.3259178,1.1367474,0.9634979,-11.514178,-2.247308,-1.5585072,-1.5099277,-0.3776599,-1.2338753,-12.047058,0.21380883,-11.573573,-11.752847,-11.768484,-10.483713,-12.067655,-12.110793,-12.081503,-12.030308,-12.005444,-12.053242,-12.0051,-10.32053,-10.520062,1.736024,3.4645836,3.7768297,5.6024084,6.214722,3.7375736,3.718072,3.7235782,3.6917677,5.206854,1.4440728,1.4511194,1.5719119,1.7420012,-2.1277788,-1.6550738,-1.998034,-2.027557,-1.8736925,-0.38192853,-0.8090294,-1.2745621,-0.9305742,-0.9363175,-1.8167431,-19.999487,-19.999634,-19.991667,-19.992767,-19.99939,-20.00816,-20.006304,-20.00842,-20.00746,-20.005625,-20.00601,-20.005816,-20.004454,-20.005924,-20.00946,-0.5413449,2.6890292,1.4548684,2.0192034,1.9911414,0.71059525,2.1866553,-9.526836,-9.566112,2.6739483,2.1262581,2.894857,2.0206442,-3.7818086,-2.667772,-6.398097,-5.839658,-3.7983494,-3.1040573,-2.6496,-2.840183,-5.9283056,-3.376954,-4.2875953,-2.4359698,-0.24323565,-0.43345037,-0.10237545,1.0607457,7.6416845,7.1335588,4.190173,-1.0067296,-0.52362895,-2.4242306,-0.39684382,4.1509542,1.9799145,2.167457,2.691326,0.51046157,0.9397808,1.1420984,-0.15186843,3.251438,6.4715176,-0.6037436,3.7927532,4.3086667,4.3182025,3.8516703,0.6504917,-0.076978706,0.8858473,-0.11531695,0.9676507,-0.5175751,0.18583934,-0.09723422,0.78787684,-0.22492601,2.63872,2.5501,-5.722133,-2.6467962,3.1760035,2.6984842,1.8733088,2.6550782,2.3757024,1.8530068,1.6775264,1.8040944,1.0866802,0.5762766,1.2496848,0.60397273,0.97230613,-6.5589113,-8.506823,0.15714985,-1.850689,3.594904,-6.220583,-6.504872,-6.8863206,-5.432104,-6.6545277,-0.16000487,-6.663268,-0.24088845,-5.2336726,3.0598667,1.1163939,2.7803903,6.311977,4.1616178,4.754054,3.1808088,4.9865184,4.8598914,4.8731713,4.952278,5.258666,3.1839774,4.8426576,4.978877,4.3503504,4.1910152,4.552531,4.4284263,2.687521,4.299411,3.3284154,3.1897204,5.493926,5.4536,7.8029747,6.2440886,7.2658596,0.7886718,0.4374573,1.006093,0.8169448,2.5915313,0.9945598,3.39536,3.5482554,1.7271847,1.6210713,1.5053014,1.1194147,1.0285107,5.0553713,2.602122,2.6127746,2.3482952,-1.2028493,6.0900197,5.705082,6.001873,5.9862976,6.051135,1.3248247,6.0718455,0.89663124,5.010339,4.5577345,2.413361,2.570291,2.3131936,1.9783193,2.356738,1.9847267,2.4613385,3.8685668,2.6767206,2.2280517,2.8024719,2.4529831,2.607039,8.561245,5.41829,0.1804964,3.7097,3.6550589,2.6216834,3.263451,3.0199823,3.2796597,3.4306588,3.6349862,3.3758392,3.541011,3.680672,3.7758152,3.6630104,3.5061178,3.6753123,3.3860645,6.7678814,3.636694,3.6331813,1.630129,3.579925,3.184023,3.410282,2.069759,3.6324012,2.881129,1.6066043,6.782765,1.0198681,1.8682657,6.4617486,4.8557177,2.2992506,2.6660562,1.7659767,1.6664451,-1.996564,2.1611586,2.0600564,2.0432649,2.0825427,2.2408347,0.81290376,0.80187243,0.73022157,-11.123813,0.8334375,-0.26735282,-1.1434991,6.8168116,6.7324467,-11.861344,4.350019,3.0958207,3.8124764,-1.8222511,2.4575243,0.42169017,0.008823956,-1.3407954,3.3425915,0.39355865,1.6233685,4.601221,-6.5837555,3.5993907,-6.09791,3.47336,3.1686323,-1.7272168,0.5071482,-0.03262065,4.664847,5.291568,2.1813478,-1.5637419,-1.6279359,-1.5902331,-0.5735342,-1.5003014,-1.5819718,-1.5513333,5.3182764,5.3874145,-1.5462732,5.406247,5.249659,5.205465,5.1504087,5.0812454,-1.5904056,-10.176848,-10.062836,1.1556389,2.0679042,-9.995702,-9.924366,-2.4457655,-0.4476045,0.03465631,-2.0353541,-2.3312407,-2.2415695,-2.0661607,8.552351,-4.2556214,-4.3509088,5.6431427,-11.762403,3.0513494,-9.296955,-1.8273741,-2.5701385,-2.5683734,-2.2812645,-0.06295048,-2.364464,-2.2749631,0.96744424,0.5322101,0.09869396,-0.0627157,4.273525,4.000564,4.3785024,2.6341915,0.5223826,-0.32553715,1.8827076,3.895175,1.1931728,4.607359,3.983554,-0.15122163,2.1867023,-1.1984069,-2.233203,-1.9057372,-1.7284607,-2.0562785,-2.0835595,-1.9135919,-2.6222222,-2.4248521,-1.4210898,-1.3232483,-1.8957052,-1.9225237,-1.8644842,-2.2004142,-2.244442,-2.1061845,-1.9078534,-1.203305,-1.9790274,-1.558006,5.428139,3.9125776,3.4125311,2.784638,3.0980022,2.7411857,3.0254765,3.244787,2.1022668,2.9665093,-1.973337,8.554198,14.810666,-4.214427,-4.2570534,2.064653,2.5939515,-9.921417,-10.822064,6.3943872,6.426846,6.246835,3.0208359,2.72035,-6.915211,-6.893101,-6.3427463,-7.214571,-6.652336,-6.32346,-0.44248113,2.9616675,-6.4910827,4.2443075,7.4046154,7.4567933,0.1956423,2.9352608,-3.976013,-2.7214613,0.49971423,4.806883,1.5779234,-0.29006988,2.6902542,0.22057444,3.9782934,4.347551,3.8292415,-1.9930615,1.3231217,-2.1027591,2.292691,1.7554126,1.8058506,0.33911017,0.21660772,-0.36171588,0.3581939,-17.634418,-17.574379,-18.23184,-17.457298,-17.457493,-18.31877,-17.869455,-17.470728,1.1174419,-17.757349,-17.431034,-17.90421,-17.503263,-17.523762,-17.787466,-17.096134,-17.642673,-17.899603,-18.167603,-18.217777,-18.027798,-17.807108,-17.229317,-17.732403,-18.31573,-17.652172,-18.269405,-17.889334,-18.072853,-17.45699,-17.726307,-18.085176,-18.072056,-18.492077,-17.839289,-17.705975,-17.967365,-18.011808,-7.3157287,-7.2983894,-7.169761,-7.228014,-7.2043767,-8.163687,-7.5087643,2.0308795,2.626609,-2.3529968,-1.9676692,-2.125659,-1.2411612,-2.3086746,-1.9251271,-2.2342997,-1.849765,-1.2204056,-0.22898254,-0.4469395,-1.7799389,-0.6157048,0.5449561,-2.0174844,-2.1616907,-2.3553987,-1.6024494,-2.466875,-2.1882842,-2.2428565,-1.4823344,-2.4831905,5.100093,-1.0191575,8.554433,2.119776,1.8478442,4.5404058,-10.525856,-10.672292,1.2626164,0.6249556,0.9759931,0.22150384,6.3055577,5.8200197,6.0844865,-8.12952,-6.645445,3.9101472,3.9199083,3.8984616,3.9783459,3.016357,-0.49959648,2.8165507,3.0985794,2.8951678,6.4774275,3.0937607,2.98921,-0.83635604,2.6011484,1.7995762,0.73354316,-0.46858805,4.3840714,3.418738,-0.69158936,-0.14886063,-0.85200375,-0.045620374,-0.6329978,-2.1694424,0.74119896,-0.016144706,0.7698458,0.2800543,1.8408959,2.2985594,1.3529971,-0.50076157,-0.31868184,-0.6013093,3.1751418,-1.0392241,-11.607288,-1.8053523,-1.8887484,-1.5383605,-2.4060223,2.4485166,2.6582496,4.526646,-12.254028,-10.441872,-10.371261,-10.208139,0.3735383,0.3286815,-0.8297832,3.3602057,5.601772,4.1818233,3.924194,2.4358313,3.4186409,-6.981234,-7.289233,-7.2920446,-6.692857,-7.3894916,8.318575,8.105225,8.508646,8.4621,-1.5296624,4.9212375,3.7154102,-4.751555,-4.989108,-0.27601948,-5.4847593,-7.0113535,-5.4370437,-2.4305437,-2.277639,2.0562656,1.3012881,1.0040648,1.8008809,1.6164472,1.7667253,2.540834,4.332852,5.717255,5.758753,2.3674467,2.1478624,1.5731972,2.306336,2.1954992,5.35113,8.169708,7.9818144,-10.648919,-10.474929,0.4165488,0.40746722,2.9956784,2.939688,2.7839637,3.420964,3.2973835,3.3551013,-0.048993308,-0.048373383,-0.17179312,1.8448472,5.134875,-2.2461994,19.452984,-4.005616,-18.028103,1.628985,1.5402193,4.5039935,5.124586,0.061062925,-0.08902672,-0.025618376,-0.33336547,-6.5772066,-6.4637227,-6.467059,1.0086073,1.3196694,3.1931994,0.07128248,-0.16929449,-0.045987505,1.0431517,0.10032822,1.8517894,2.3323462,2.3105302,2.4130094,2.326751,2.661302,3.8652115,3.968386,4.2940817,0.11958196,2.4041102,6.486226,-0.7866519,3.5497978,-0.7105852,4.8843307,3.2157288,-1.9686241,8.552804,-4.220301,-4.374874,-11.9925375,3.1004317,3.6403997,3.2228057,2.2686377,2.348288,3.2230802,2.388237,2.1738524,3.627786,0.76034546,-6.8846736,1.6911347,0.42685127,-16.035076,-16.040567,-16.04229,-16.04119,-16.05413,-16.037498,-16.035694,-16.041338,-16.041872,-16.036877,-16.03324,-16.038553,-16.040133,-16.079252,3.8574898,4.247794,2.5120058,2.4434757,2.0817697,2.4378076,2.2576427,-12.082683,1.6673907,1.6924574,-10.5117035,5.128825,7.071279,7.1159115,7.4713273,7.597515,1.958289,7.660822,3.0610168,7.309421,7.1248045,3.4488232,-1.8841286,8.554628,-0.4119795,8.553566,-4.2350698,-4.3608885,-3.7973998,3.3521442,3.685861,4.852662,4.9012437,4.3617673,5.5549183,3.7825544,3.5335486,2.8798475,3.882112,-0.41162404,1.1227232,3.3074055,4.762549,4.6922383,-6.472352,5.544793,6.281931,6.109913,6.0376925,5.8186736,5.1931806,6.1449547,-0.25652832,-0.47084033,6.141828,5.775169,0.0032925142,-0.11514487,1.780729,-0.5811499,5.02704,-11.494758,3.0590086,0.73191345,0.25470227,0.08929648,1.5954239,0.63975126,0.47821215,0.5649159,0.40765637,0.64341825,0.51365066,1.031055,0.71310246,0.44171554,0.045759376,-2.1879208,-6.333561,-2.372259,3.1090562,2.4222298,2.5630262,2.5266414,1.8655174,3.0245538,0.06618361,0.04542425,0.0153143415,3.0964434,3.1981432,0.14538544,0.9897443,0.73672837,-0.16728191,-0.75225854,3.4185703,10.012337,7.9371533,6.037532,5.400293,2.9462903,3.2286477,-10.425097,-10.252713,-0.4262218,-9.89057,-0.5379079,0.021860447,0.7870298,-10.153725,0.33320728,6.4208,1.0687882,4.1562424,4.884111,4.7858496,4.3987927,-1.0099012,0.24111456,1.1036882,2.328438,3.7653043,4.090422,-2.250679,8.553195,-4.6688895,-6.7830563,-6.434896,2.3547287,1.2337084,0.79293483,-1.3368605,0.6013842,0.9500807,1.8208349,1.8635163,2.8953662,2.174184,1.9884858,2.9099038,4.4733396,4.7336283,0.823129,-0.18256113,-1.2271378,1.1309367,-2.7802162,-2.8190393,-2.524484,-2.852521,-2.7796576,-2.6057045,-2.8496978,-2.8072848,-2.814835,-2.8353112,-2.8438215,-2.3014677,3.6313152,-1.2357152,-0.48789307,3.2630389,3.5760133,2.8191469,4.154955,-10.268449,-10.332017,-10.473551,-10.687527,-11.123262,-9.61532,0.33573595,0.38299394,-10.573047,-10.300536,-10.317046,-10.433758,-10.343969,-10.180652,-10.326322,-10.042289,-0.14907579,-0.016885087,0.0489823,3.1137743,4.530929,4.787993,4.6381073,4.1324544,3.2205186,-7.499393,-5.4114966,-2.6467178,-2.6383204,-2.9799666,0.9293645,-9.420558,3.0821867,1.7915316,0.84743583,0.9852609,4.5563655,-2.1036332,8.553054,-4.2003627,-4.1836405,-3.6359456,-4.182335,-3.7038562,-4.180929,-4.18675,-3.7224038,-4.150142,2.6146843,3.574782,3.6479685,0.5613054,-0.801265,3.0288467,-2.3335805,0.85888284,-0.5033707,1.6236503,2.6473713,-1.4941908,-7.402922,-7.1608696,-7.000668,-7.8685665,-7.6807494,-8.441097,1.9662158,1.053273,5.18542,5.1781287,3.3452685,-0.71872395,-0.82034117,2.662527,3.0723612,4.156892,2.4022677,-2.0219488,6.5115294,6.769268,6.8502564,6.1831064,7.1348777,6.996473,7.172567,3.92442,4.180903,4.2477016,6.107905,0.07990024,7.037317,6.9205694,6.3483024,6.486051,-11.987827,3.1812603,3.1141245,3.3848562,3.228316,3.0581872,3.396855,2.9326615,1.7288845,0.99835145,1.2472403,1.1970308,3.4392042,4.6791744,5.356289,3.4080708,4.697133,3.571242,3.22776,2.2851324,0.37714618,0.5378605,1.1617095,4.6497045,4.3767195,4.003784,4.2034235,6.4778194,-1.1027222,1.1025087,1.4076997,0.38811767,1.1286869,3.1073515,2.0795178,3.1553936,-11.319747,0.07094725,0.030176628,-0.7096656,4.0852613,-5.948672,-5.7064304,20.72238,20.754091,-4.743092,-5.5493493,-5.098203,-6.0358043,-5.5934696,-5.279962,-5.7180367,-2.8791866,-3.5587516,-4.424317,-4.9051495,-5.7853017,-1.0273706,-1.1890514,6.3004956,6.188213,-5.6294303,2.4216993,2.463984,3.6917114,3.727261,-6.787576,-6.2702355,-6.5882688,10.013854,7.9828362,6.0381274,6.338136,6.290975,6.540024,6.4932504,1.7638317,1.6287323,2.042424,-2.2939496,8.554477,14.810285,-3.9858224,-2.1193392,8.551975,-4.1761765,-4.252331,-3.702876,4.143871,4.045937,3.2931993,-8.052989,2.275573,2.646405,-1.8780987,-1.5623138,-2.294778,-1.9487991,-0.69257766,-0.94098216,-5.9502835,-0.77369505,-0.804833,2.9622855,-0.88463503,3.3215904,4.1408634,2.8314922,2.8126907,2.011452,-6.3159766,-6.2253766,-6.340475,-6.27562,6.4293184,6.6903167,6.7064404,0.88825274,-1.0735524,0.30797464,2.0576174,0.70771897,-6.237071,-6.2961817,-6.252366,0.3519323,1.848288,2.2995815,4.202902,4.723549,3.3057847,4.8502436,3.7344358,4.2174454,3.992775,1.1715486,6.15545,5.7401547,5.8746185,6.0872593,5.7864494,5.746502,5.7493978,0.41845044,0.258928,0.4535457,2.5756373,-11.808879,3.0537941,3.364244,1.0382472,4.2333694,2.9681816,3.7089944,0.53110856,-0.5102318,-0.076740205,2.470599,0.33178625,0.36040425,-0.39635155,9.630326,7.393823,2.070848,4.9257517,4.456582,4.7989416,5.060221,0.13401891,-5.722692,-2.8032184,-2.3082674,4.597879,4.4811687,2.8894672,2.941659,0.22563794,3.8386953,-0.67654467,3.879756,3.6225045,3.9310637,3.5878413,3.79324,1.5503352,3.8206513,-0.5882388,-0.97577184,8.553354,-4.1840234,-4.0994487,-3.6505861,-3.9943287,3.8113537,1.3572967,1.1601782,4.108971,3.1571822,3.9163392,-5.776711,-6.045783,-6.190945,3.4046881,-6.276855,5.512456,5.8997083,5.676923,5.715041,5.314926,1.811645,2.198562,6.03041,0.8448348,0.95717597,6.8280854,-1.5211433,-1.0592124,1.0505159,2.6099784,6.716224,7.9593086,6.5231156,7.344316,6.7819867,7.265158,6.7743835,5.01417,-3.99115,-4.1524143,-4.093217,-4.0752664,2.0819337,2.2153506,2.2014794,2.112211,4.133516,1.6097616,1.411263,-1.4816698,0.2844315,1.8614572,1.9901994,1.9361734,3.046633,2.1293786,-0.2884215,4.181056,4.694337,4.650967,4.7933517,4.822313,-0.6312141,3.0908537,-6.233859,-6.2804027,-6.3130054,-2.0567124,8.551929,3.666435,3.742235,3.621532,3.777216,1.7423761,-1.2952082,2.0643616,-6.07216,4.57977,2.1243482,2.476404,-1.6284652,0.7131279,0.21470307,0.30741364,0.3047573,-1.6428176,-0.5660924,-0.6203523,6.6659374,6.9657845,6.8206863,6.9594393,5.6049924,0.37888452,0.8392075,4.3976326,0.7509995,-0.49541736,-1.664922,-0.5141264,-0.591181,0.15070695,0.34434336,4.2091064,3.5742455,2.1017315,0.89084965,2.2605305,2.1622508,3.857047,-2.0096467,19.451113,14.813631,0.086579345,0.05187152,1.9830538,3.8130987,-4.472753,1.0743903,2.381666,0.6337935,2.1186106,1.9997437,1.7359185,4.1590247,4.08434,4.421144,4.2350416,4.488019,0.8938722,0.77407485,1.1872703,0.4404407,0.4416645,0.43306273,1.6191142,-11.760154,2.1608894,0.93450487,4.194198,0.58249474,0.17019816,0.2019495,0.20448743,0.8265416,0.12886567,0.26850373,0.56299186,0.79832095,0.6213413,3.9650688,3.8607514,-2.1877067,8.551737,-4.178686,-4.1751094,-3.7989013,0.9452462,1.2478013,1.2308537,1.1543586,1.9012024,3.3150053,1.743415,1.8753351,2.0309136,2.7613726,2.7428865,-11.473813,-11.877548,0.36415362,0.43428704,1.0870517,-1.0780946,0.2944855,7.120026,0.22100379,0.73624367,-0.46439543,-0.24239634,0.9536437,1.1114109,1.0425351,5.4919047,7.914384,1.8843261,3.2229304,0.5202578,-1.7335464,-2.2475176,-2.1485438,8.552663,-4.1953487,-4.339401,-7.2136927,-7.410677,-6.802941,-7.18093,-6.8816814,-7.158041,-7.1460705,-7.063413,-6.248524,-6.536898,-7.1554384,-6.9928684,-7.2065916,-5.903876,-4.3544145,-4.502412,-4.3142476,-4.8812523,-5.0350285,-2.7442477,-2.615008,-2.6703532,-0.47423026,-11.831484,5.5616145,-0.31346136,0.8172757,3.3474355,3.3891094,4.078928,-6.2001653,-6.267004,-6.277886,-6.2108803,6.5964828,-0.78001535,2.6085753,2.3222048,1.8416295,1.5667415,1.7407922,2.5198896,-0.84247494,-0.70135367,2.594226,2.4362848,-0.2919392,0.32506973,3.606899,3.6573684,3.1777039,3.450907,-5.3938313,-5.3961596,3.1150465,0.5833117,0.40504295,0.25226775,0.04075939,-0.13702913,0.17334455,-0.16176732,0.09485012,-18.37867,-18.145123,-18.592669,-19.29773,-18.82823,-18.306911,-18.222637,-18.143208,-18.390368,-19.323082,-18.518045,-18.410019,-0.25597152,4.529881,-6.2359247,3.686957,-6.1576204,4.1060486,-6.1819096,3.3269413,2.8694186,2.3787735,4.056516,5.558427,4.4366345,5.591459,5.6423473,4.440836,3.17185,4.771561,3.5515893,5.645675,4.1382127,5.0795794,3.7244792,3.2724905,4.1379914,4.141351,3.730459,4.390646,4.5575504,-11.246347,-2.026784,19.451113,14.810036,2.9393291,3.336493,3.1868188,0.2617121,0.10131484,3.1975784,-0.26500368,0.42810348,0.74713415,-17.929974,-17.43914,-18.578995,-18.430725,0.8709997,3.627368,4.101893,4.0913324,4.2301326,2.2440398,19.451742,-4.1689563,-4.2037883,-5.05515,-2.9258432,-4.2323594,-4.383884,-4.7561097,-4.6007857,-4.535664,-2.7332566,-4.942896,5.0784993,0.24446885,-1.1134275,-2.3713365,2.405186,1.875444,-2.0557802,1.0818626,-1.7215593,-2.0169835,0.17696837,3.7335415,7.5852976,7.4533877,2.4782424,2.5078053,2.1900854,-2.1661665,19.452467,14.811576,0.2624636,-11.950687,0.41689068,0.5390687,1.9352571,0.5570082,3.8746364,0.51556504,-0.36935547,3.9094048,-0.6201243,0.853537,1.7628485,1.6446266,1.9546964,-0.12513457,6.3529444,-2.3377032,1.391268,-2.1937933,-0.04723257,-0.6634337,-1.7367998,-2.1807146,8.55171,6.4144893,6.287995,-1.1538868,2.6391559,6.4080725,6.183122,6.301644,6.3121376,5.928744,1.3713785,-0.77425003,6.4663973,6.265972,-1.5167297,-1.4515164,-1.2391018,1.7452662,7.0454926,4.4324775,6.7729764,6.762995,6.1894846,6.607225,0.78612226,1.0414543,1.7471074,5.9158163,0.54937315,0.47584108,6.60962,-0.66831654,-1.707962,0.8845547,-1.0342354,-0.65948576,-0.18661444,6.130827,6.741272,4.8245807,2.2542417,5.9411182,6.800121,6.724605,6.644997,6.7362933,-1.1936578,-1.4977821,6.5561514,0.60236895,6.2396026,-7.3059025,-6.6328316,-7.0416713,-7.5292606,-5.54886,-6.7130847,-6.8161106,-5.825865,-6.1700344,-6.2194533,-6.064463,-1.8859774,19.449394,14.812303,0.47842512,-2.1229663,-2.3869753,-2.3247244,-1.7937578,-3.1507657,-2.334575,-1.5427034,-1.9811589,-2.2416112,-1.2050064,-2.2575912,2.071803,2.610611,-1.5314122,-1.1649928,-1.511773,0.16360037,0.17149445,0.1940361,0.23653103,0.16722503,0.27613777,0.13053276,0.44444942,1.0154653,-7.489239,0.9681446,2.631924,3.9539597,3.7865984,3.871929,3.619006,3.7319283,-0.43789864,3.7466998,4.260139,4.250198,4.108282,3.11625,0.5566976,0.6194714,0.69284564,2.2228422,1.3101851,1.5106564,1.573173,1.2905805,1.2765377,4.720452,4.0124955,1.4874988,1.5084392,2.8516822,2.4106832,1.7753553,-0.21605675,-11.716682,-2.0316844,4.7846947,4.586381,-6.5751104,-7.101098,-6.3462434,-6.8150005,-7.1127076,2.0887923,5.559007,3.0120459,-7.4957304,-7.329418,-1.3966495,4.6292505,4.3337345,3.2873237,1.4001902,2.5096748,-0.28226054,0.13961326,0.0024672414,-1.4011533,13.719691,13.734635,13.736963,13.731753,13.735335,13.738223,-1.0446097,-1.0970893,4.6978464,-5.8036866,-4.108345,-2.9746892,-3.7724228,0.75665444,-2.1526697,8.551198,5.6295123,6.7283278,7.5304437,6.571944,7.1679873,3.1235204,0.33601782,0.33360863,0.40203995,0.3738506,-6.890779,-6.9903183,-7.6885104,-8.446697,-6.6636147,-5.6668863,-8.145811,-8.401284,-7.026199,-7.0469956,-11.6390705,0.92988366,1.6321548,2.8302398,0.55364823,2.515324,-2.1643085,8.551542,-2.100281,-0.09908844,0.50525564,1.9628541,-1.4471275,2.6532133,4.2539983,-0.40090755,-0.90036637,3.4149208,3.4410017,3.8351092,3.7701516,2.7114756,-0.33946076,0.6212123,0.3763797,3.6525445,3.72039,2.921044,3.4581742,-5.3981237,-5.397912,3.1824348,0.8090159,0.18059927,-18.062387,-18.121964,-18.356518,-18.347723,-18.656418,-18.079836,-18.452438,-17.917719,-17.853268,-17.163738,-18.114534,-18.1928,-18.27302,-18.247936,-8.410146,-7.0890303,-7.0349073,-6.954273,-9.39772,6.316386,6.788684,2.9168634,1.131381,2.120243,1.5231695,-0.21680899,3.3887653,-7.5117908,-7.240297,2.5231636,17.956415,3.6915915,3.8777318,9.931595,8.068277,8.22728,5.3763947,-0.09129197,-0.26085383,-5.6590033,1.2664452,2.1029396,3.9456239,1.6840504,2.485378,4.6679187,2.0351403,2.0578327,1.52762,1.60073,-6.653873,-7.897553,-12.009366,5.0973854,4.517653,4.4190917,4.3900847,0.9445456,-6.1255937,-7.089115,-7.268391,-7.0272307,-7.198875,-7.298438,-7.1996703,-7.274027,-7.503231,-7.4891505,-7.0225167,-7.441701,-7.023833,-7.22798,-7.249445,-7.0410805,-7.1230164,-7.0345154,-6.9725823,-5.5426145,5.1228957,2.1902666,-2.3392093,-3.2355943,-1.8967335,4.203548,1.8124512,1.6194668,1.9331009,1.8532445,-1.9954131,0.913143,-1.8833975,-0.8436101,3.8090467,-1.3026361,0.32801282,0.50413036,0.5436121,3.326618,0.8529878,-5.592665,-3.8276193,-3.0343394,-5.694035,3.1342402,-4.7451096,3.6000483,3.4444487,3.313052,-7.3323126,-7.735436,-8.254761,5.254185,-7.0624204,4.5707974,4.5036225,-1.9092317,-2.2845666,-0.6495676,-0.5754579,-0.5515608,-11.958591,-12.079652,-12.084394,5.112968,4.580319,4.443609,4.3098445,5.554339,6.762544,7.1473117,7.8589907,7.490465,7.5184774,7.4657545,7.468455,7.121776,1.6693124,4.455755,2.7420726,-11.350867,-11.84545,9.701125,6.9467926,8.089648,8.246296,8.440807,1.4380019,1.5643274,3.019176,-11.777295,-11.95721,4.1947684,5.488436,6.101513,2.7766392,2.7851155,2.4164712,2.4540887,2.4570606,2.3670938,2.9546201,3.774996,1.9766163,4.2664795,3.340877,2.37553,2.4339056,2.2218535,2.3984191,2.5972445,7.631904,7.3015084,3.1252716,7.235077,6.934912,7.611476,7.474228,7.2147593,7.240079,7.4994364,-9.389487,1.6711837,2.5792446,2.4343383,2.386699,3.210265,3.4010308,7.6839666,7.7361603,2.3514006,2.6633391,3.3108926,1.5008279,3.0146844,2.98477,2.9072518,2.1530101,0.7928379,0.9855549,0.8505911,-2.315384,-1.5585738,-7.9917755,-7.140594,-7.6929746,-7.302313,-7.204933,-7.3248,-7.8790126,-7.3714314,-7.981639,3.1863005,-2.1173644,8.551562,-12.067708,-12.136841,-12.051311,-11.953779,2.1728892,3.29509,2.1726446,3.619107,10.02911,-10.153174,0.5467154,0.38446972,0.54459184,-10.348742,3.3144712,-10.979694,1.9368075,2.343833,-12.02799,1.2080361,2.0221016,1.5107085,1.9675386,2.1574883,-10.157708,-10.258827,3.4646766,-0.10345838,-10.022137,-10.349417,-10.193775,-10.268953,-7.2337947,-7.0714593,-1.9956771,5.22486,3.5579412,3.6863852,3.6660552,-0.062144343,0.3704591,-0.09840035,-8.17635,3.4297137,3.5628073,3.6778667,3.6743786,2.3958156,3.388323,-6.8362756,-7.253481,0.79450846,1.0571367,6.0419683,5.9669566,5.92032,-1.1631094,5.942358,5.9126925,6.850897,6.9140215,6.4521513,-0.37325504,-1.3633469,-1.8790802,5.3927584,-5.670174,20.718512,20.756218,-5.148517,-6.3245807,-5.949184,-1.2898686,-5.7284718,2.4784317,-4.8164334,-2.1004007,-2.160861,-1.2743783,-2.1901774,-1.6769286,-3.5225968,-3.9548447,-3.517495,-4.9728227,-4.7497025,-5.396571,-5.424597,-4.442875,17.955261,3.2589955,2.8363268,-0.62330014,-1.6240064,-1.2395697,-0.43280217,-1.85311,-1.1998612,-0.81450266,0.24616247,-0.567895,-0.5963761,-1.0840788,-1.0387179,-0.68995446,4.535091,7.7239256,8.230093,8.498073,8.490096,-2.1070125,1.7498778,1.8659525,1.9338462,1.9095042,1.8365592,1.6694742,-11.299868,1.7278937,1.8254199,0.99653786,3.6715593,1.4705606,-0.09897836,0.7999299,1.3567123,1.8255216,1.596189,1.7759924,2.5485852,3.2111526,-12.02938,1.7014556,-8.966735,3.5495336,3.991512,-1.8958913,6.4835615,-1.866399,-1.8200468,-0.4716949,3.6653588,1.6591872,0.22342984,2.1467326,0.7265208,0.8556329,0.71386105,0.6045883,1.1858345,1.7617594,3.9136333,3.8128002,-0.11781487,2.8644047,3.129995,3.3978605,3.1940942,3.0861144,-0.9101581,-0.56565416,-0.7899803,-0.16419484,-0.44159946,-0.5323219,-0.860842,-0.63639444,-0.767534,-0.17353234,-0.37918612,-0.60294574,-0.8488143,-1.5907997,-1.3093717,-1.8096933,-0.32877302,0.23917922,1.6929702,-2.320652,-10.883575,-11.19759,-11.501783,-11.704171,-11.802366,-11.710313,-11.463737,-11.474867,-11.535388,-11.477978,-11.370605,-11.446397,-11.483093,-11.437296,-11.445155,4.2361803,8.308188,8.132514,8.402183,3.095995,2.6805186,2.1847157,4.1304607,0.58798385,0.010480963,0.008647197,0.09231328,3.6808202,3.9118383,3.8685932,3.858246,3.391546,9.764479,8.11975,8.342251,8.3002825,8.46487,4.589502,-6.4380665,-6.5605216,-6.262916,-6.845849,-6.607595,-6.8765674,-3.9799736,-0.8025109,2.7137642,2.962782,2.300118,-0.10702666,5.105558,4.6449795,4.4738107,0.9479778,0.6008669,0.2969242,-1.0250529,0.042906485,2.2599657,2.3076086,-0.3518566,-1.9498411,8.55284,1.0874177,-11.521618,-11.545829,-5.4879847,1.1929113,1.0355294,1.0571023,0.96870464,0.90723646,1.0691787,0.25083333,3.1984215,3.984971,-0.11412667,1.1588533,2.6041255,0.20172378,2.3168821,5.0244527,3.975728,6.1294856,1.0128906,0.9293295,1.0502551,3.322049,3.5628054,-0.0056790356,3.1269968,1.9950123,3.3665187,3.4502654,-12.427631,2.4819448,2.3756163,0.10669661,-1.2691631,5.603869,4.130659,4.3819356,0.89405656,0.8790633,0.3445262,0.6411384,0.90572214,6.4428177,7.363111,8.417245,3.723305,-10.545815,2.0129273,1.2134461,1.0434687,3.2110882,-12.033931,-11.491152,-11.728618,-11.910236,-11.135369,-11.363654,3.2871537,3.3598638,3.608425,5.466742,5.099034,4.0525494,2.7601595,-0.22646748,2.5556917,-0.44491678,-0.82478434,-0.2733619,0.30714092,0.432838,0.017680502,-0.14342782,1.6899098,1.2170174,-3.1825657,-1.1862879,5.0978246,6.6455483,4.8788586,4.192751,-1.5959392,-0.7016071,-0.75155973,2.6132128,3.0971322,0.40312952,0.3400492,0.7940495,1.1765554,5.545368,5.165911,4.8724356,1.2135127,0.36436212,3.0266607,3.1126177,3.3696856],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"hat is dynamic padding? In the \\\"Batching Inputs together\\\" video, we have seen that to be able to gro...\"],[\". To apply dynamic padding, we must defer the padding to the batch preparation, so we remove that pa...\"],[\"ow to slice and dice a dataset. Most of the time, the data you work with won’t be perfectly prepared...\"],[\". In this example, we've created a sample of 5 elements from the SQuAD dataset. The last way to pick...\"],[\"he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library t...\"],[\". Here we indicate a maximum length of 128 and pad inputs shorter than this length, truncate inputs ...\"],[\"efore diving in character-based tokenization, understanding why this kind of tokenization is interes...\"],[\". This leads to another issue with character-based tokenizers: their sequences are translated into v...\"],[\"Normalization and pre-tokenization[[normalization-and-pre-tokenization]]\\n\\n\\u003cCourseFloatingBanner chap...\"],[\"```py\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-u...\"],[\"```python out\\n[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 1...\"],[\"Now that we've seen a little of how some different tokenizers process text, we can start to explore ...\"],[\"Model | BPE | WordPiece | Unigram\\n:----:|:---:|:---------:|:------:\\nTraining | Starts from a small v...\"],[\"ow to preprocess pairs of sentences? We have seen how to tokenize single sentences and batch them to...\"],[\". Zooming in a little bit, here are the input IDs, aligned with the tokens they correspond to, their...\"],[\"The Hugging Face Course\\n\\nThis repo contains the content that's used to create the **[Hugging Face co...\"],[\"| Language                                                                      | Source            ...\"],[\"| [Bengali](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fbn\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002fbn`](ht...\"],[\"| [Persian](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002ffa\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002ffa`](ht...\"],[\"| [Hebrew](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fhe\\u002fchapter1\\u002f1) (WIP)                   | [`chapters\\u002fhe`](ht...\"],[\"| [Italian](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fit\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002fit`](ht...\"],[\"| [Portuguese](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fpt\\u002fchapter1\\u002f1) (WIP)               | [`chapters\\u002fpt`](ht...\"],[\"| [Turkish](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002ftr\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002ftr`](ht...\"],[\"| [Chinese (traditional)](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fzh-TW\\u002fchapter1\\u002f1) (WIP) | [`chapters\\u002fzh-TW`]...\"],[\"### Translating the course into your language\\n\\nAs part of our mission to democratise machine learnin...\"],[\"```bash\\ncd ~\\u002fpath\\u002fto\\u002fcourse\\ncp -r chapters\\u002fen\\u002fCHAPTER-NUMBER chapters\\u002fLANG-ID\\u002fCHAPTER-NUMBER\\n```\\n\\nHe...\"],[\"```\\npip install hf-doc-builder\\n```\\n\\n```\\ndoc-builder preview course ..\\u002fcourse\\u002fchapters\\u002fLANG-ID --not_...\"],[\"\\u003e Note: we are not currently accepting community contributions for new chapters. These instructions ...\"],[\"What to do when you get an error[[what-to-do-when-you-get-an-error]]\\n\\n\\u003cCourseFloatingBanner chapter=...\"],[\"```python\\nfrom distutils.dir_util import copy_tree\\nfrom huggingface_hub import Repository, snapshot_...\"],[\"and the first thing you think of is to load the model using the `pipeline` from 🤗 Transformers:\\n\\n```...\"],[\"\\u003cTip\\u003e\\n\\n🚨 See that blue box around \\\"6 frames\\\" in the traceback from Google Colab? That's a special fe...\"],[\"Hmm, it indeed looks like our colleague's model is not on the Hub... aha, but there's a typo in the ...\"],[\"```python out\\n['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'token...\"],[\"Now we can test if this worked by loading the model from the latest commit on the `main` branch:\\n\\n``...\"],[\"## Debugging the forward pass of your model[[debugging-the-forward-pass-of-your-model]]\\n\\nAlthough th...\"],[\"~\\u002fminiconda3\\u002fenvs\\u002fhuggingface\\u002flib\\u002fpython3.8\\u002fsite-packages\\u002ftorch\\u002fnn\\u002fmodules\\u002fmodule.py in _call_impl(s...\"],[\"~\\u002fminiconda3\\u002fenvs\\u002fhuggingface\\u002flib\\u002fpython3.8\\u002fsite-packages\\u002ftransformers\\u002fmodels\\u002fdistilbert\\u002fmodeling_di...\"],[\"```\\n~\\u002fminiconda3\\u002fenvs\\u002fhuggingface\\u002flib\\u002fpython3.8\\u002fsite-packages\\u002ftransformers\\u002fmodels\\u002fdistilbert\\u002fmodelin...\"],[\"The answer recommends that we add `return_tensors='pt'` to the tokenizer, so let's see if that works...\"],[\"hat is transfer learning? The idea of Transfer Learning is to leverage the knowledge acquired by a m...\"],[\". GPT-2 for instance, was pretrained this way using the content of 45 millions links posted by users...\"],[\"ow to batch inputs together? In this video, we will see how to batch input sequences together. In ge...\"],[\". To get the same results with or without padding, we need to indicate to the attention layers that ...\"],[\"upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on whi...\"],[\". You can still use a classic evaluation loop such as the one we saw in the \\\"Raw training loop\\\" vide...\"],[\"Building your first demo[[building-your-first-demo]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  classNames...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-hello-world.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"250\\\" title=\\\"Gradio a...\"],[\"Here, we've created an input textbox with a label, a placeholder, and a set number of lines.\\nYou cou...\"],[\"```py\\nimport gradio as gr\\n\\ngr.Interface(fn=predict, inputs=\\\"text\\\", outputs=\\\"text\\\").launch()\\n```\\n\\n\\nTh...\"],[\"Natural Language Processing[[natural-language-processing]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n  ...\"],[\"## Why is it challenging?[[why-is-it-challenging]]\\n\\nComputers don't process information in the same ...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={8}\\n    classNames=\\\"absolute z-10 ri...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Sharing pretrained models[[sharing-pretrained-models]]\\n\\n{#if fw ===...\"],[\"## Using the `push_to_hub` API[[using-the-pushtohub-api]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cYoutube id=\\\"Zh0FfmVrK...\"],[\"To upload your model to an organization you are a member of, just pass it with `hub_model_id = \\\"my_o...\"],[\"To get an idea of how it works, let's first initialize a model and a tokenizer:\\n\\n{#if fw === 'pt'}\\n`...\"],[\"Click on the \\\"Files and versions\\\" tab, and you should see the files visible in the following screens...\"],[\"Jump to the last section to see how to upload files to your newly created repository!\\n\\n## Using the ...\"],[\"The `create_repo` method can be used to create a new repository on the hub:\\n\\n```py\\nfrom huggingface_...\"],[\"Next, enter your model's name. This will also be the name of the repository. Finally, you can specif...\"],[\"The system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (w...\"],[\"```py\\nfrom huggingface_hub import Repository\\n\\nrepo = Repository(\\\"\\u003cpath_to_dummy_folder\\u003e\\\", clone_from...\"],[\"Using this class requires having git and git-lfs installed, so make sure you have [git-lfs](https:\\u002f\\u002f...\"],[\"checkpoint = \\\"camembert-base\\\"\\n\\nmodel = AutoModelForMaskedLM.from_pretrained(checkpoint)\\ntokenizer = ...\"],[\"We can now go ahead and proceed like we would usually do with traditional Git repositories. We can a...\"],[\"Objects not staged for commit:\\n\\n\\n```\\n\\nWe can see that all files have `Git` as a handler, except *t5_...\"],[\"The UI allows you to explore the model files and commits and to see the diff introduced by each comm...\"],[\"n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a masked language model[[fine-tuning-a-masked-language-...\"],[\"This process of fine-tuning a pretrained language model on in-domain data is usually called _domain ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Picking a pretrained model for masked language modeling[[picking-a-pretrained-model-for-m...\"],[\"We can see how many parameters this model has by calling the `num_parameters()` method:\\n\\n```python\\nd...\"],[\"{\\u002fif}\\n\\nWith around 67 million parameters, DistilBERT is approximately two times smaller than the BER...\"],[\"for token in top_5_tokens:\\n    print(f\\\"'\\u003e\\u003e\\u003e {text.replace(tokenizer.mask_token, tokenizer.decode([to...\"],[\"```python\\nfrom datasets import load_dataset\\n\\nimdb_dataset = load_dataset(\\\"imdb\\\")\\nimdb_dataset\\n```\\n\\n`...\"],[\"'\\u003e\\u003e\\u003e Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogu...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nNow that we've had a quick look at the data, let's dive into preparing it for masked languag...\"],[\"# Use batched=True to activate fast multithreading!\\ntokenized_datasets = imdb_dataset.map(\\n    token...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nSo, in order to run our experiments on GPUs like those found on Google Colab, we'll pick som...\"],[\"for chunk in chunks[\\\"input_ids\\\"]:\\n    print(f\\\"'\\u003e\\u003e\\u003e Chunk length: {len(chunk)}'\\\")\\n```\\n\\n```python out\\n...\"],[\"Let's now apply `group_texts()` to our tokenized datasets using our trusty `Dataset.map()` function:...\"],[\"```python out\\ntokenizer.decode(lm_datasets[\\\"train\\\"][1][\\\"labels\\\"])\\n```\\n\\n```python out\\n\\\".... at..........\"],[\"```python\\nfrom transformers import DataCollatorForLanguageModeling\\n\\ndata_collator = DataCollatorForL...\"],[\"Nice, it worked! We can see that the `[MASK]` token has been randomly inserted at various locations ...\"],[\"from transformers import default_data_collator\\n\\nwwm_probability = 0.2\\n\\n\\ndef whole_word_masking_data_...\"],[\"# Randomly mask words\\n        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\\n       ...\"],[\"\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Run the code snippet above several times to see the random masking happen ...\"],[\"which will display a widget where you can enter your credentials. Alternatively, you can run: \\n\\n```\\n...\"],[\"In addition, we set up a `PushToHubCallback` that will save the model to the Hub after each epoch. Y...\"],[\"batch_size = 64\\n# Show the training loss with every epoch\\nlogging_steps = len(downsampled_dataset[\\\"t...\"],[\"```python\\nfrom transformers import Trainer\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=training_ar...\"],[\"```python\\nimport math\\n\\neval_results = trainer.evaluate()\\nprint(f\\\"\\u003e\\u003e\\u003e Perplexity: {math.exp(eval_resu...\"],[\"{\\u002fif}\\n\\n```python out\\n\\u003e\\u003e\\u003e Perplexity: 11.32\\n```\\n\\nNice -- this is quite a reduction in perplexity, whi...\"],[\"Next, we'll apply this function to our test set and drop the unmasked columns so we can replace them...\"],[\"Now that our model, optimizer, and dataloaders are configured, we can specify the learning rate sche...\"],[\"loss = outputs.loss\\n        losses.append(accelerator.gather(loss.repeat(batch_size)))\\n\\n    losses =...\"],[\"Neat -- our model has clearly adapted its weights to predict words that are more strongly associated...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Using pretrained models[[using-pretrained-models]]\\n\\n{#if fw === 'pt...\"],[\"```py\\nfrom transformers import pipeline\\n\\ncamembert_fill_mask = pipeline(\\\"fill-mask\\\", model=\\\"camember...\"],[\"tokenizer = CamembertTokenizer.from_pretrained(\\\"camembert-base\\\")\\nmodel = CamembertForMaskedLM.from_p...\"],[\"Understanding the Interface class[[understanding-the-interface-class]]\\n\\n\\u003cCourseFloatingBanner chapte...\"],[\"In this example, we'll build an audio-to-audio function that takes an\\naudio file and simply reverses...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-audio-reverse.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"250\\\" title=\\\"Gradio...\"],[\"gr.Interface(\\n    generate_tone,\\n    [\\n        gr.Dropdown(notes, type=\\\"index\\\"),\\n        gr.Slider(m...\"],[\"Let's build an interface that allows you to demo a **speech-recognition** model.\\nTo make it interest...\"],[\"If your browser doesn't ask you for microphone permissions, \\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fc...\"],[\"ou are at the right place if you want to understand what the Byte pair Encoding subword tokenization...\"],[\". Once we know all the pairs and their frequency of appearance, we will choose the one that appears ...\"],[\"ome bugs in your code are very straightforward. You try running it, you get a syntax error somewhere...\"],[\".data.Dataset that your model is training on, right at the end of the training pipeline. And we can ...\"],[\". So nan destroyed our model. But where did it creep in first? To find out, we need to re-initialize...\"],[\". Let's split the difference and pick 5e-5. And if you recompile with that, you'll find that trainin...\"],[\"How to write a good issue[[how-to-write-a-good-issue]]\\n\\n\\u003cCourseFloatingBanner chapter={8}\\n  classNam...\"],[\"\\u003cTip\\u003e\\n\\n🚨 Many issues in the 🤗 Transformers repository are unsolved because the data used to reproduc...\"],[\"```\\ntransformers-cli env\\n```\\n\\nand you should get something like this:\\n\\n```out\\nCopy-and-paste the tex...\"],[\"```\\n```python\\n```\\n\\nthen paste in your minimal reproducible example and type a new line with three ba...\"],[\"oading a custom dataset. Although the Hugging Face Hub hosts over a thousand public datasets, you'll...\"],[\". As you can see, these files are processed line-by-line, so empty lines in the raw text are also re...\"],[\"Summary[[summary]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-10 right-0 top-...\"],[\"What if my dataset isn't on the Hub?[[what-if-my-dataset-isnt-on-the-hub]]\\n\\n\\u003cCourseFloatingBanner ch...\"],[\"## Loading a local dataset[[loading-a-local-dataset]]\\n\\nFor this example we'll use the [SQuAD-it data...\"],[\"By default, loading local files creates a `DatasetDict` object with a `train` split. We can see this...\"],[\"This is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the d...\"],[\"```py\\nurl = \\\"https:\\u002f\\u002fgithub.com\\u002fcrux82\\u002fsquad-it\\u002fraw\\u002fmaster\\u002f\\\"\\ndata_files = {\\n    \\\"train\\\": url + \\\"SQuA...\"],[\"rite your own training loop in PyTorch. In this video, we will look at how we can do the same fine-t...\"],[\". As seen in the model API video, we use the from_pretrained method and adjust the number of labels ...\"],[\". Then we go through all the steps we have seen already: send the data to the GPU, compute the model...\"],[\"atasets and DataFrames equals love. Although the processing functions of Datasets will cover most th...\"],[\". In this case, the __getitem__() method of the raw dataset starts off by returning Python dictionar...\"],[\"n this video, we'll study the decoder architecture. An example of a popular decoder-only architectur...\"],[\". So when should one use a decoder? Decoders, like encoders, can be used as standalone models. As th...\"],[\". It is what we call, auto-regressive: it outputs values that are then used as its input values. We ...\"],[\"n our other videos, and as always, there'll be links below if you want to check those out, we showed...\"],[\". If you're following along with our datasets and fine-tuning videos, we got our data from the MRPC ...\"],[\"Unigram tokenization[[unigram-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6}\\n  classNames=\\\"absolu...\"],[\"This is all a very costly operation, so we don't just remove the single symbol associated with the l...\"],[\"Here are the frequencies of all the possible subwords in the vocabulary:\\n\\n```\\n(\\\"h\\\", 15) (\\\"u\\\", 36) (\\\"...\"],[\"In this case, it was easy to find all the possible segmentations and compute their probabilities, bu...\"],[\"Each word in the corpus has a score, and the loss is the negative log likelihood of those scores -- ...\"],[\"We will use the same corpus as before as an example:\\n\\n```python\\ncorpus = [\\n    \\\"This is the Hugging ...\"],[\"We group the characters with the best subwords to arrive at an initial vocabulary of size 300:\\n\\n```p...\"],[\"Once the main loop is finished, we just start from the end and hop from one start position to the ne...\"],[\"```python out\\n(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\\n(['This'], 6.28826703069...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nWith all of this in place, the last thing we need to do is add the special tokens used by th...\"],[\"n this video, we're going to go over the HuggingFace Model Hub navigation. This is the huggingface.c...\"],[\". When clicking on a model, you should be facing its model card. The model card contains information...\"],[\"n this video, we're going to see how to load and fine-tune a pre-trained model. It's very quick, and...\"],[\". It basically encourages the network to output large values for the right class, and low values for...\"],[\". Fit() is pretty much the central method for Keras models - it tells the model to break the data in...\"],[\"Part 2 Release Event[[part-2-release-event]]\\n\\nFor the release of part 2 of the course, we organized ...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"VzvG23gmcYU\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"h...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"8j9HRMjh_s8\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"h...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"gZIP-_2XYMM\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"h...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"C6jweXYFHSA\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"h...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"RBw1TmdEZp0\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"h...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cYoutube id=\\\"c7mle2yYpwQ\\\"\\u002f\\u003e\\n\\u003c\\u002fdiv\\u003e\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n\\u003cimg src=\\\"h...\"],[\"How do Transformers work?[[how-do-transformers-work]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    cla...\"],[\"- **October 2019**: [DistilBERT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1910.01108), a distilled version of BERT that...\"],[\"An example of a task is predicting the next word in a sentence having read the *n* previous words. T...\"],[\"Unfortunately, training a model, especially a large one, requires a large amount of data. This becom...\"],[\"## Transfer Learning[[transfer-learning]]\\n\\n\\u003cYoutube id=\\\"BqqfQnyjmgg\\\" \\u002f\\u003e\\n\\n*Pretraining* is the act of...\"],[\"For example, one could leverage a pretrained model trained on the English language and then fine-tun...\"],[\"\\u003cYoutube id=\\\"H39Z_720T5s\\\" \\u002f\\u003e\\n\\n## Introduction[[introduction]]\\n\\nThe model is primarily composed of tw...\"],[\"We will dive into those architectures independently in later sections.\\n\\n## Attention layers[[attenti...\"],[\"Now that you have an idea of what attention layers are all about, let's take a closer look at the Tr...\"],[\"Note that the first attention layer in a decoder block pays attention to all (past) inputs to the de...\"],[\"n this video we'll take a look at how you upload your very own dataset to the Hub. The first you'll ...\"],[\"n this video we take a look at the mysterious sounding metric called Perplexity. You might have enco...\"],[\"Decoder models[[decoder-models]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-1...\"],[\"n this video, we'll study the encoder architecture. An example of a popular encoder-only architectur...\"],[\"Let's dive in this representation. It contains one vector per word that was passed through the encod...\"],[\". This vector can then be handled down the road by additional layers of neurons to make sense of the...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Translation[[translation]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingBanne...\"],[\"\\u003cYoutube id=\\\"1JvfrvZgi6c\\\"\\u002f\\u003e\\n\\nIf you have a big enough corpus of texts in two (or more) languages, yo...\"],[\"\\u003ca class=\\\"flex justify-center\\\" href=\\\"\\u002fhuggingface-course\\u002fmarian-finetuned-kde4-en-to-fr\\\"\\u003e\\n\\u003cimg class...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-course\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fen\\u002fch...\"],[\"```py\\nfrom transformers import pipeline\\n\\nmodel_checkpoint = \\\"Helsinki-NLP\\u002fopus-mt-en-fr\\\"\\ntranslator ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Processing the data[[processing-the-data]]\\n\\n\\u003cYoutube id=\\\"XAR8jnZZuUs\\\"\\u002f\\u003e\\n\\nYou should know...\"],[\"inputs = tokenizer(en_sentence, text_target=fr_sentence)\\ninputs\\n```\\n\\n```python out\\n{'input_ids': [47...\"],[\"Note that we set the same maximum length for our inputs and outputs. Since the texts we're dealing w...\"],[\"```py\\nfrom transformers import AutoModelForSeq2SeqLM\\n\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(...\"],[\"This is all done by a [`DataCollatorForSeq2Seq`](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002fmain_classes\\u002fda...\"],[\"And we can also have a look at the decoder input IDs, to see that they are shifted versions of the l...\"],[\"As we saw in [Chapter 1](\\u002fcourse\\u002fchapter1\\u002f6), the decoder performs inference by predicting tokens on...\"],[\"```py\\n!pip install sacrebleu\\n```\\n\\nWe can then load it via `evaluate.load()` like we did in [Chapter ...\"],[\"```py\\npredictions = [\\\"This This This This\\\"]\\nreferences = [\\n    [\\n        \\\"This plugin allows you to ...\"],[\"```py\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tqdm import tqdm\\n\\ngeneration_data_collator = D...\"],[\"decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\\n\\n    # Replace -100s in the ...\"],[\"```python\\nfrom transformers import create_optimizer\\nfrom transformers.keras_callbacks import PushToH...\"],[\"\\u003cTip\\u003e\\n\\n💡 If the output directory you are using already exists, it needs to be a local clone of the r...\"],[\"Note that you can specify the full name of the repository you want to push to with the `hub_model_id...\"],[\"Next is the training, which will also take a bit of time:\\n\\n```python\\ntrainer.train()\\n```\\n\\nNote that ...\"],[\"{\\u002fif}\\n\\n{#if fw === 'pt'}\\n\\n## A custom training loop[[a-custom-training-loop]]\\n\\nLet's now take a look...\"],[\"accelerator = Accelerator()\\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepar...\"],[\"### Training loop[[training-loop]]\\n\\nWe are now ready to write the full training loop. To simplify it...\"],[\"optimizer.step()\\n        lr_scheduler.step()\\n        optimizer.zero_grad()\\n        progress_bar.upda...\"],[\"```python out\\nepoch 0, BLEU score: 53.47\\nepoch 1, BLEU score: 54.24\\nepoch 2, BLEU score: 54.44\\n```\\n\\n...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={6}\\n    classNames=\\\"absolute z-10 ri...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={5}\\n    classNames=\\\"absolute z-10 ri...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Question answering[[question-answering]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCours...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-bert-finetuned-squad.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"450\\\" title=...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Preparing the data[[preparing-the-data]]\\n\\nThe dataset that is used the most as an academi...\"],[\"```py\\nprint(\\\"Context: \\\", raw_datasets[\\\"train\\\"][0][\\\"context\\\"])\\nprint(\\\"Question: \\\", raw_datasets[\\\"trai...\"],[\"For evaluation, however, there are several possible answers for each sample, which may be the same o...\"],[\"we can see that the answer can indeed be one of the three possibilities we saw before.\\n\\n### Processi...\"],[\"```py\\ncontext = raw_datasets[\\\"train\\\"][0][\\\"context\\\"]\\nquestion = raw_datasets[\\\"train\\\"][0][\\\"question\\\"]\\n...\"],[\"In this case the context is not too long, but some of the examples in the dataset have very long con...\"],[\"for ids in inputs[\\\"input_ids\\\"]:\\n    print(tokenizer.decode(ids))\\n```\\n\\n```python out\\n'[CLS] To whom d...\"],[\"As we can see, our example has been in split into four inputs, each of them containing the question ...\"],[\"```python out\\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflo...\"],[\"This information will be useful to map each feature we get to its corresponding label. As mentioned ...\"],[\"# If the answer is not fully inside the context, label is (0, 0)\\n    if offset[context_start][0] \\u003e s...\"],[\"decoded_example = tokenizer.decode(inputs[\\\"input_ids\\\"][idx])\\nprint(f\\\"Theoretical answer: {answer}, d...\"],[\"offset_mapping = inputs.pop(\\\"offset_mapping\\\")\\n    sample_map = inputs.pop(\\\"overflow_to_sample_mappin...\"],[\"To apply this function to the whole training set, we use the `Dataset.map()` method with the `batche...\"],[\"```py\\ndef preprocess_validation_examples(examples):\\n    questions = [q.strip() for q in examples[\\\"qu...\"],[\"Now that we have preprocessed all the data, we can get to the training. \\n\\n{#if fw === 'pt'}\\n\\n## Fine...\"],[\"- We masked the start and end logits corresponding to tokens outside of the context.\\n- We then conve...\"],[\"Now that the preprocessing is done, we change the tokenizer back to the one we originally picked:\\n\\n`...\"],[\"For ease of experimentation, let's convert these outputs to NumPy arrays:\\n\\n```python\\nstart_logits = ...\"],[\"start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\\n        end_indexes = np.arg...\"],[\"We can now check that we get sensible results by looking at the first element of both lists:\\n\\n```pyt...\"],[\"{\\u002fif}\\n\\n```python\\nfrom tqdm.auto import tqdm\\n\\n\\ndef compute_metrics(start_logits, end_logits, features...\"],[\"# Select the answer with the best score\\n        if len(answers) \\u003e 0:\\n            best_answer = max(a...\"],[\"```python\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()\\n```\\n\\nIf you aren't working in...\"],[\"{:else}\\n\\nNow that's done, we can create our TF Datasets. We can use the simple default data collator...\"],[\"{#if fw === 'pt'}\\n\\n\\u003cTip\\u003e\\n\\n💡 If the output directory you are using exists, it needs to be a local clo...\"],[\"{:else}\\n\\nOnce the training is complete, we can finally evaluate our model (and pray we didn't spend ...\"],[\"If you want to dive a bit more deeply into the training loop, we will now show you how to do the sam...\"],[\"```py\\nfrom torch.optim import AdamW\\n\\noptimizer = AdamW(model.parameters(), lr=2e-5)\\n```\\n\\nOnce we hav...\"],[\"```py\\nfrom huggingface_hub import Repository, get_full_repo_name\\n\\nmodel_name = \\\"bert-finetuned-squad...\"],[\"Here's the complete code for the training loop:\\n\\n```py\\nfrom tqdm.auto import tqdm\\nimport torch\\n\\nprog...\"],[\"```py\\naccelerator.wait_for_everyone()\\nunwrapped_model = accelerator.unwrap_model(model)\\nunwrapped_mo...\"],[\"# Replace this with your own checkpoint\\nmodel_checkpoint = \\\"huggingface-course\\u002fbert-finetuned-squad\\\"...\"],[\"ow to write a good issue on GitHub? GitHub is the main place for the Hugging Face open source librar...\"],[\". Click on the button next to Bug Report and you will discover there is a template to fill. It will ...\"],[\"The Hugging Face Hub[[the-hugging-face-hub]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={4}\\n    classNames=\\\"...\"],[\"n this video, we will study together \\\"the Unigram Language Model subword tokenization algorithm\\\".\\n\\nT...\"],[\"Before going further in the explanation of the training algorithm, I need to explain what is an Unig...\"],[\". The training of the Unigram tokenizer is based on the Expectation-Maximization method: at each ite...\"],[\". Let's remove for example the token 'ug'. We notice that the tokenization for \\\"hug\\\" with the letter...\"],[\". The second token that could be removed at this iteration is the \\\"du\\\" token. And that's it, we just...\"],[\"Creating your own dataset[[creating-your-own-dataset]]\\n\\n\\u003cCourseFloatingBanner chapter={5}\\n  classNam...\"],[\"If you click on one of these issues you'll find it contains a title, a description, and a set of lab...\"],[\"```python out\\n[{'url': 'https:\\u002f\\u002fapi.github.com\\u002frepos\\u002fhuggingface\\u002fdatasets\\u002fissues\\u002f2792',\\n  'repositor...\"],[\"'comments': 1,\\n  'created_at': '2021-08-12T11:40:18Z',\\n  'updated_at': '2021-08-12T12:31:17Z',\\n  'cl...\"],[\"Whoa, that's a lot of information! We can see useful fields like `title`, `body`, and `number` that ...\"],[\"```py\\nimport time\\nimport math\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom tqdm.notebook import...\"],[\"Once the issues are downloaded we can load them locally using our newfound skills from [section 2](\\u002f...\"],[\"Since the contents of issues and pull requests are quite different, let's do some minor preprocessin...\"],[\"```py\\nissues_dataset = issues_dataset.map(\\n    lambda x: {\\\"is_pull_request\\\": False if x[\\\"pull_reques...\"],[\"```python out\\n[{'url': 'https:\\u002f\\u002fapi.github.com\\u002frepos\\u002fhuggingface\\u002fdatasets\\u002fissues\\u002fcomments\\u002f897594128'...\"],[\"'type': 'User',\\n   'site_admin': False},\\n  'created_at': '2021-08-12T12:21:52Z',\\n  'updated_at': '20...\"],[\"We can see that the comment is stored in the `body` field, so let's write a simple function that ret...\"],[\"## Uploading the dataset to the Hugging Face Hub[[uploading-the-dataset-to-the-hugging-face-hub]]\\n\\n\\u003c...\"],[\"\\u003cTip\\u003e\\n\\n💡 You can also upload a dataset to the Hugging Face Hub directly from the terminal by using `...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-course\\u002fdocum...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 3. Which of the following is an example of subword tokenization?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\tt...\"],[\"{#if fw === 'pt'}\\n### 5. What is an AutoModel?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A model that aut...\"],[\"{\\u002fif}\\n\\n### 6. What are the techniques to be aware of when batching sequences of different lengths to...\"],[\"### 8. What method is most of the tokenizer API centered around?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext:...\"],[\"tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-cased\\\")\\nmodel = AutoModel.from_pretrained(\\\"gpt2...\"],[\"tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-cased\\\")\\nmodel = TFAutoModel.from_pretrained(\\\"gp...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Processing the data[[processing-the-data]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCou...\"],[\"```python\\nimport tensorflow as tf\\nimport numpy as np\\nfrom transformers import AutoTokenizer, TFAutoM...\"],[\"### Loading a dataset from the Hub[[loading-a-dataset-from-the-hub]]\\n\\n{#if fw === 'pt'}\\n\\u003cYoutube id=...\"],[\"This command downloads and caches the dataset, by default in *~\\u002f.cache\\u002fhuggingface\\u002fdatasets*. Recall...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Preprocessing a dataset[[preprocessing-a-dataset]]\\n\\n{#if fw === 'pt'}\\n\\u003cYoutube id=\\\"0u3io...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIf we decode the IDs inside `input_ids` back to words:\\n\\n```py\\ntokenizer.convert_ids_to_token...\"],[\"In general, you don't need to worry about whether or not there are `token_type_ids` in your tokenize...\"],[\"```py\\ndef tokenize_function(example):\\n    return tokenizer(example[\\\"sentence1\\\"], example[\\\"sentence2\\\"...\"],[\"The way the 🤗 Datasets library applies this processing is by adding new fields to the datasets, one ...\"],[\"### Dynamic padding[[dynamic-padding]]\\n\\n\\u003cYoutube id=\\\"7q5NyFT8REg\\\"\\u002f\\u003e\\n\\n{#if fw === 'pt'}\\nThe function ...\"],[\"{\\u002fif}\\n\\nTo do this in practice, we have to define a collate function that will apply the correct amou...\"],[\"```py\\nbatch = data_collator(samples)\\n{k: v.shape for k, v in batch.items()}\\n```\\n\\n{#if fw === 'tf'}\\n\\n...\"],[\"And that's it! We can take those datasets forward into the next lecture, where training will be plea...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Models[[models]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingBanner chapter=...\"],[\"{\\u002fif}\\n\\nHowever, if you know the type of model you want to use, you can use the class that defines it...\"],[\"# Model is randomly initialized!\\n```\\n{:else}\\n```py\\nfrom transformers import BertConfig, TFBertModel\\n...\"],[\"{\\u002fif}\\n\\nIn the code sample above we didn't use `BertConfig`, and instead loaded a pretrained model vi...\"],[\"{#if fw === 'pt'}\\nThe *pytorch_model.bin* file is known as the *state dictionary*; it contains all y...\"],[\"model_inputs = tf.constant(encoded_sequences)\\n```\\n{\\u002fif}\\n\\n### Using the tensors as inputs to the mode...\"],[\"ow to instantiate a Transformers model? In this video we will look at how we can create and use a mo...\"],[\". We can also use the specific class corresponding to a checkpoint, but we will need to change the c...\"],[\"he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library t...\"],[\". Here we indicate a maximum length of 128 and pad inputs shorter than this length, truncate inputs ...\"],[\"Introduction to Gradio[[introduction-to-gradio]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={9}\\n    classNam...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-question-answering-simple.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"640\\\" t...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Handling multiple sequences[[handling-multiple-sequences]]\\n\\n{#if fw...\"],[\"{#if fw === 'pt'}\\n```py\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForSequenceCla...\"],[\"{#if fw === 'pt'}\\n```py\\ntokenized_inputs = tokenizer(sequence, return_tensors=\\\"pt\\\")\\nprint(tokenized_...\"],[\"output = model(input_ids)\\nprint(\\\"Logits:\\\", output.logits)\\n```\\n{\\u002fif}\\n\\nWe print the input IDs as well ...\"],[\"## Padding the inputs[[padding-the-inputs]]\\n\\nThe following list of lists cannot be converted to a te...\"],[\"print(model(tf.constant(sequence1_ids)).logits)\\nprint(model(tf.constant(sequence2_ids)).logits)\\nprin...\"],[\"attention_mask = [\\n    [1, 1, 1],\\n    [1, 1, 0],\\n]\\n\\noutputs = model(tf.constant(batched_ids), attent...\"],[\"Advanced Interface features[[advanced-interface-features]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  clas...\"],[\"iface = gr.Interface(\\n    chat,\\n    [\\\"text\\\", \\\"state\\\"],\\n    [\\\"chatbot\\\", \\\"state\\\"],\\n    allow_screensho...\"],[\"# Download human-readable labels for ImageNet.\\nresponse = requests.get(\\\"https:\\u002f\\u002fgit.io\\u002fJJkYN\\\")\\nlabel...\"],[\"This wraps up our deep dive into the `Interface` class of Gradio. As we've seen, this class makes it...\"],[\"n this video we will see together what is the purpose of training a tokenizer, what are the key step...\"],[\". Finally, if we use again this tokenizer to tokenize medical vocabulary we see again that a single ...\"],[\". It's good timing, this dataset is known by the datasets library and we can load it in two lines of...\"],[\"he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune ...\"],[\". This is because we didn't specify anything metric for the evaluation. To get those metrics, we wil...\"],[\"he tokenizer pipeline. In this video, we'll look at how a tokenizer converts raw text to numbers tha...\"],[\". You may have noticed that we don't have the exact same result as in our first slide — or not, as t...\"],[\"Sharing demos with others[[sharing-demos-with-others]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  classNam...\"],[\"To add additional content to your demo, the `Interface` class supports some optional parameters:\\n   ...\"],[\"article = \\\"Check out [the original Rick and Morty Bot](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fkingabzpro\\u002fRick...\"],[\"```python\\ngr.Interface(classify_image, \\\"image\\\", \\\"label\\\").launch(share=True)\\n```\\n\\nThis generates a pu...\"],[\"\\u003cYoutube id=\\\"LS9Y2wDVI0k\\\" \\u002f\\u003e\\n\\n## ✏️ Let's apply it![[lets-apply-it]]\\n\\nUsing what we just learned in ...\"],[\"Now that we have a `predict()` function. The next step is to define and launch our gradio interface:...\"],[\"ow to preprocess pairs of sentences? We have seen how to tokenize single sentences and batch them to...\"],[\". Zooming in a little bit, here are the input IDs, aligned with the tokens they correspond to, their...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fast tokenizers in the QA pipeline[[fast-tokenizers-in-the-qa-pipel...\"],[\"As we saw in [Chapter 1](\\u002fcourse\\u002fchapter1), we can use the `question-answering` pipeline like this t...\"],[\"Why should I use transformers?\\n\\n1. Easy-to-use state-of-the-art models:\\n  - High performance on NLU ...\"],[\"Let's see how it does all of this!\\n\\n## Using a model for question answering[[using-a-model-for-quest...\"],[\"Models for question answering work a little differently from the models we've seen up to now. Using ...\"],[\"start_logits = tf.where(mask, -10000, start_logits)\\nend_logits = tf.where(mask, -10000, end_logits)\\n...\"],[\"```py\\nscores = torch.triu(scores)\\n```\\n\\n{:else}\\n\\nThen we'll mask the values where `start_index \\u003e end_...\"],[\"```python out\\n{'answer': 'Jax, PyTorch and TensorFlow',\\n 'start': 78,\\n 'end': 105,\\n 'score': 0.97773...\"],[\"[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text...\"],[\"```py\\nsentence = \\\"This sentence is not too long but we are going to split it anyway.\\\"\\ninputs = token...\"],[\"gets us:\\n\\n```python out\\n[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\\n```\\n\\nwhich means the first sentence is spl...\"],[\"```py\\noutputs = model(**inputs)\\n\\nstart_logits = outputs.start_logits\\nend_logits = outputs.end_logits...\"],[\"{#if fw === 'pt'}\\n\\n```py\\ncandidates = []\\nfor start_probs, end_probs in zip(start_probabilities, end_...\"],[\"```python out\\n{'answer': '\\\\n🤗 Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0...\"],[\"he tokenization pipeline involves several steps that convert raw text into numbers. In this video, w...\"],[\"et's see how to preprocess a dataset for summarization. This is the task of well summarizing a long ...\"],[\"he post-processing step in a question answering task. When doing question answering, the processing ...\"],[\". We will need a map from examples to features, which we can create like this. Now, for the main par...\"],[\"A full training[[a-full-training]]\\n\\n\\u003cCourseFloatingBanner chapter={3}\\n  classNames=\\\"absolute z-10 ri...\"],[\"Our `tokenized_datasets` has one method for each of those steps:\\n\\n```py\\ntokenized_datasets = tokeniz...\"],[\"```py\\noutputs = model(**batch)\\nprint(outputs.loss, outputs.logits.shape)\\n```\\n\\n```python out\\ntensor(0...\"],[\"```py\\nimport torch\\n\\ndevice = torch.device(\\\"cuda\\\") if torch.cuda.is_available() else torch.device(\\\"cp...\"],[\"metric.compute()\\n```\\n\\n```python out\\n{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}\\n```\\n\\n...\"],[\"And here are the changes:\\n\\n```diff\\n+ from accelerate import Accelerator\\n  from transformers import A...\"],[\"Then the main bulk of the work is done in the line that sends the dataloaders, the model, and the op...\"],[\"Putting this in a `train.py` script will make that script runnable on any kind of distributed setup....\"],[\"Transformers, what can they do?[[transformers-what-can-they-do]]\\n\\n\\u003cCourseFloatingBanner chapter={1}\\n...\"],[\"\\u003cTip\\u003e\\n⚠️ The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of mod...\"],[\"Some of the currently [available pipelines](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002fmain_classes\\u002fpipelin...\"],[\"\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Play around with your own sequences and labels and see how the model behav...\"],[\"Let's try the [`distilgpt2`](https:\\u002f\\u002fhuggingface.co\\u002fdistilgpt2) model! Here's how to load it in the ...\"],[\"## Mask filling[[mask-filling]]\\n\\nThe next pipeline you'll try is `fill-mask`. The idea of this task ...\"],[\"Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (OR...\"],[\"```python\\nfrom transformers import pipeline\\n\\nsummarizer = pipeline(\\\"summarization\\\")\\nsummarizer(\\n    ...\"],[\"Like with text generation, you can specify a `max_length` or a `min_length` for the result.\\n\\n\\n## Tra...\"],[\"sing the Python debugger in a notebook. In this video, we'll learn how to use the Python debugger in...\"],[\". Those labels are definitely weird: they are of various size, which we can actually confirm by prin...\"],[\"ow to instantiate a Transformers model? In this video we will look at how we can create and use a mo...\"],[\". We can also use the specific class corresponding to a checkpoint, but we will need to change the c...\"],[\"n this video, we're going to understand how to manage a model repository on the HuggingFace model hu...\"],[\". The final tab is the \\\"Settings\\\" tab, which allow you to manage your model's visibility and availab...\"],[\". Let's add a README markdown file to complete it a little bit. This README is known as the modelcar...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Behind the pipeline[[behind-the-pipeline]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCou...\"],[\"As we saw in [Chapter 1](\\u002fcourse\\u002fchapter1), this pipeline groups together three steps: preprocessing...\"],[\"Since the default checkpoint of the `sentiment-analysis` pipeline is `distilbert-base-uncased-finetu...\"],[\"Don't worry about padding and truncation just yet; we'll explain those later. The main things to rem...\"],[\"## Going through the model[[going-through-the-model]]\\n\\n{#if fw === 'pt'}\\nWe can download our pretrai...\"],[\"### A high-dimensional vector?[[a-high-dimensional-vector]]\\n\\nThe vector output by the Transformer mo...\"],[\"The output of the Transformer model is sent directly to the model head to be processed.\\n\\nIn this dia...\"],[\"```python\\nprint(outputs.logits.shape)\\n```\\n\\n{#if fw === 'pt'}\\n```python out\\ntorch.Size([2, 2])\\n```\\n{:...\"],[\"Now we can see that the model predicted `[0.0402, 0.9598]` for the first sentence and `[0.9995,  0.0...\"],[\"🤗 Datasets, check![[datasets-check]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={5}\\n    classNames=\\\"absolute...\"],[\"n this video we will see together what is the normalizer component that we find at the beginning of ...\"],[\". These transformations can also be undetectable with a simple \\\"print\\\". Indeed, keep in mind that fo...\"],[\"Building a tokenizer, block by block[[building-a-tokenizer-block-by-block]]\\n\\n\\u003cCourseFloatingBanner c...\"],[\"\\u003cYoutube id=\\\"MR8tZm5ViWU\\\"\\u002f\\u003e\\n\\nMore precisely, the library is built around a central `Tokenizer` class...\"],[\"```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"wikitext\\\", name=\\\"wikitext-2-raw...\"],[\"The first step of tokenization is normalization, so let's begin with that. Since BERT is widely used...\"],[\"```python\\nprint(tokenizer.normalizer.normalize_str(\\\"Héllò hôw are ü?\\\"))\\n```\\n\\n```python out\\nhello how...\"],[\"```python out\\n[(\\\"Let's\\\", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]...\"],[\"In both cases, we can then test the tokenizer on a text by calling the `encode()` method:\\n\\n```python...\"],[\"Once this is added, going back to our previous example will give:\\n\\n```python\\nencoding = tokenizer.en...\"],[\"```python\\nfrom transformers import PreTrainedTokenizerFast\\n\\nwrapped_tokenizer = PreTrainedTokenizerF...\"],[\"```python\\ntokenizer.pre_tokenizer.pre_tokenize_str(\\\"Let's test pre-tokenization!\\\")\\n```\\n\\n```python ou...\"],[\"```python out\\n' test'\\n```\\n\\nFinally, we add a byte-level decoder:\\n\\n```python\\ntokenizer.decoder = deco...\"],[\"```python\\ntokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\\n```\\n\\nWe can have a look at the pre-to...\"],[\"```python\\ncls_token_id = tokenizer.token_to_id(\\\"\\u003ccls\\u003e\\\")\\nsep_token_id = tokenizer.token_to_id(\\\"\\u003csep\\u003e\\\"...\"],[\"Or alternatively:\\n\\n```python\\nfrom transformers import XLNetTokenizerFast\\n\\nwrapped_tokenizer = XLNetT...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 2. What part of the preprocessing for token classification differs from the other preprocessing ...\"],[\"### 4. What does \\\"domain adaptation\\\" mean?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It's when we run a m...\"],[\"### 6. Which of these tasks can be seen as a sequence-to-sequence problem?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"{#if fw === 'pt'}\\n\\n### 8. Why is there a specific subclass of `Trainer` for sequence-to-sequence pro...\"],[\"{\\u002fif}\\n\\n### 10. When should you pretrain a new model?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"When there...\"],[\"### 12. What are the main challenges when preprocessing data for a question answering task?\\n\\n\\u003cQuesti...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 3. What can you do using the Hugging Face Hub web interface? \\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext...\"],[\"{#if fw === 'pt'}\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A tokenizer\\\",\\n\\t\\t\\texplain: \\\"Correct! All tokeni...\"],[\"correct: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \\\"All of the above with a dedicated callback\\\",\\n\\t\\t\\texplain: \\\"That's ri...\"],[\"### 6. What is the first step when using the `push_to_hub()` method or the CLI tools?\\n\\n\\u003cQuestion\\n\\tch...\"],[\"ext embeddings and semantic search. In this video we’ll explore how Transformer models represent tex...\"],[\". With mean pooling only thing we need to make sure is that we don't include the padding tokens in t...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 3. Suppose you try to run the following code, which throws an error:\\n\\n```py\\nfrom transformers im...\"],[\"### 4. Suppose you've tried to run `trainer.train()` and are faced with a cryptic error that doesn't...\"],[\"### 5. What is the best way to debug a CUDA error?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"Post the err...\"],[\"### 7. Why is overfitting to one batch usually a good debugging technique?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"### 8. Why is it a good idea to include details on your compute environment with `transformers-cli e...\"],[\"et's study the transformer architecture. This video is the introductory video to the encoders, decod...\"],[\". Here too, we recommend you check out the video on decoders especially to understand how all of thi...\"],[\"Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  ...\"],[\"We will explore all of these concepts below.\\n\\n### Creating a simple demo using Blocks[[creating-a-si...\"],[\"2. You can define regular Python functions anywhere in your code and run them with user input using ...\"],[\"Here's what you should keep in mind: any components created under a `Column` (this is also the defau...\"],[\"text_button.click(flip_text, inputs=text_input, outputs=text_output)\\n    image_button.click(flip_ima...\"],[\"- `fn`: the function to run\\n- `inputs`: a (list of) component(s) whose values should supplied as the...\"],[\"### Creating multi-step demos[[creating-multi-step-demos]]\\n\\nIn some cases, you might want a _multi-s...\"],[\"### Updating Component Properties[[updating-component-properties]]\\n\\nSo far, we have seen how to crea...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 3. Where can you launch a Gradio demo from?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \\\"Standard ...\"],[\"### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?\\n\\n\\u003cQ...\"],[\"### 8. Which of the following are components included in the Gradio library?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n...\"],[\"### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spac...\"],[\"i, this is going to be a video about the push_to_hub API for Tensorflow and Keras. So, to get starte...\"],[\". So the first argument is the temporary directory that files are going to be saved to before they'r...\"],[\". Okay, we're back and our model was uploaded, both by the PushToHubCallback and also by our call to...\"],[\". So that's just been done, so now if we go back here, I'm going to use a slightly different sentenc...\"],[\". So if I want to use this model again, if I want to load it from the hub, I just run this one line ...\"],[\"et's take a look at word-based tokenization. Word-based tokenization is the idea of splitting the ra...\"],[\". For example, when training our tokenizer on a text, we might want to take the 10,000 most frequent...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Putting it all together[[putting-it-all-together]]\\n\\n{#if fw === 'pt...\"],[\"As we'll see in some examples below, this method is very powerful. First, it can tokenize a single s...\"],[\"# Returns PyTorch tensors\\nmodel_inputs = tokenizer(sequences, padding=True, return_tensors=\\\"pt\\\")\\n\\n# ...\"],[\"## Wrapping up: From tokenizer to model[[wrapping-up-from-tokenizer-to-model]]\\n\\nNow that we've seen ...\"],[\"n this video, I'm going to give you a very quick introduction to how our transformers models work to...\"],[\". And whether you write your own model from scratch or load a pre-trained one, you interact with the...\"],[\"Gradio, check![[gradio-check]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={9}\\n    classNames=\\\"absolute z-10 ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fast tokenizers' special powers[[fast-tokenizers-special-powers]]\\n\\n...\"],[\"|               | Fast tokenizer | Slow tokenizer\\n:--------------:|:--------------:|:-------------:\\n...\"],[\"```python\\ntokenizer.is_fast\\n```\\n\\n```python out\\nTrue\\n```\\n\\nor check the same attribute of our `encodin...\"],[\"✏️ **Try it out!** Create a tokenizer from the `bert-base-cased` and `roberta-base` checkpoints and ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Inside the `token-classification` pipeline[[inside-the-token-classification-pipeline]]\\n\\nI...\"],[\"```py\\nfrom transformers import pipeline\\n\\ntoken_classifier = pipeline(\\\"token-classification\\\")\\ntoken_c...\"],[\"The `aggregation_strategy` picked will change the scores computed for each grouped entity. With `\\\"si...\"],[\"Since we're using `AutoModelForTokenClassification` here, we get one set of logits for each token in...\"],[\"probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\\npredictions = output...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```py\\ninputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\\ninputs_with_offsets[\\\"off...\"],[\"print(results)\\n```\\n\\n```python out\\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', ...\"],[\"```py\\nexample[33:45]\\n```\\n\\n```python out\\nHugging Face\\n```\\n\\nTo write the code that post-processes the ...\"],[\"Another example of a task where these offsets are extremely useful is question answering. Diving int...\"],[\"et's take a look at subword-based tokenization. Understanding why subword-based tokenization is inte...\"],[\". It will also understand that tokenization, modernization, and immunization, which all have the sam...\"],[\"et's see together what is the training strategy of the WordPiece algorithm and how it performs the t...\"],[\". We can now add to the vocabulary the pair with the highest score, after merging it of course! And ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Summarization[[summarization]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingB...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-mt5-small-finetuned-amazon-en-es.hf.space\\\" frameBorder=\\\"0\\\" height=...\"],[\"```python\\nfrom datasets import load_dataset\\n\\nspanish_dataset = load_dataset(\\\"amazon_reviews_multi\\\", ...\"],[\"'\\u003e\\u003e Title: meh'\\n'\\u003e\\u003e Review: Does it’s job and it’s gorgeous but mine is falling apart, I had to basi...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThis sample shows the diversity of reviews one typically finds online, ranging from positive...\"],[\"The most popular products in the English dataset are about household items, clothing, and wireless e...\"],[\"'\\u003e\\u003e Title: Good art, good price, poor design'\\n'\\u003e\\u003e Review: I had gotten the DC Vintage calendar the p...\"],[\"'\\u003e\\u003e Title: PARCIALMENTE DAÑADO'\\n'\\u003e\\u003e Review: Me llegó el día que tocaba, junto a otros libros que ped...\"],[\"Now that we've prepared our corpus, let's take a look at a few possible Transformer models that one ...\"],[\"| Transformer model | Description                                                                   ...\"],[\"|  [mBART-50](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fmbart-large-50)   | A multilingual version of BART, pr...\"],[\"As you can see from this table, the majority of Transformer models for summarization (and indeed mos...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Preprocessing the data[[preprocessing-the-data]]\\n\\n\\u003cYoutube id=\\\"1m7BerpSq8A\\\"\\u002f\\u003e\\n\\nOur next t...\"],[\"To tokenize our corpus, we have to deal with a subtlety associated with summarization: because our l...\"],[\"\\u003cTip\\u003e\\n\\n💡 You may have noticed that we used `batched=True` in our `Dataset.map()` function above. Thi...\"],[\"\\u003cTip\\u003e\\n\\n🙋 Don't worry if this is the first time you've heard of precision and recall -- we'll go thro...\"],[\"```py\\n!pip install rouge_score\\n```\\n\\nand then loading the ROUGE metric as follows:\\n\\n```python\\nimport ...\"],[\"```python\\nscores[\\\"rouge1\\\"].mid\\n```\\n\\n```python out\\nScore(precision=0.86, recall=1.0, fmeasure=0.92)\\n`...\"],[\"```python\\nfrom nltk.tokenize import sent_tokenize\\n\\n\\ndef three_sentence_summary(text):\\n    return \\\"\\\\n...\"],[\"{#if fw === 'pt'}\\n\\n## Fine-tuning mT5 with the `Trainer` API[[fine-tuning-mt5-with-the-trainer-api]]...\"],[\"which will display a widget where you can enter your credentials. Alternatively, you can run this co...\"],[\"The `push_to_hub=True` argument will allow us to push the model to the Hub after training; you'll fi...\"],[\"```python\\nimport numpy as np\\n\\n\\ndef compute_metrics(eval_pred):\\n    predictions, labels = eval_pred\\n ...\"],[\"```python\\nfrom transformers import DataCollatorForSeq2Seq\\n\\ndata_collator = DataCollatorForSeq2Seq(to...\"],[\"```python\\nfeatures = [tokenized_datasets[\\\"train\\\"][i] for i in range(2)]\\ndata_collator(features)\\n```\\n...\"],[\"We finally have all the ingredients we need to train with! We now simply need to instantiate the tra...\"],[\"```python out\\n'https:\\u002f\\u002fhuggingface.co\\u002fhuggingface-course\\u002fmt5-finetuned-amazon-en-es\\u002fcommit\\u002faa0536b82...\"],[\"optimizer, schedule = create_optimizer(\\n    init_lr=5.6e-5,\\n    num_warmup_steps=0,\\n    num_train_st...\"],[\"```python\\nfrom tqdm import tqdm\\nimport numpy as np\\n\\ngeneration_data_collator = DataCollatorForSeq2Se...\"],[\"```\\n{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}\\n```\\n\\n\\n{\\u002fif}\\n\\n{#i...\"],[\"```python\\nfrom torch.optim import AdamW\\n\\noptimizer = AdamW(model.parameters(), lr=2e-5)\\n```\\n\\nFinally...\"],[\"return preds, labels\\n```\\n\\nThis should look familiar to you if you recall how we defined the `compute...\"],[\"These steps can be seen in the following block of code:\\n\\n```python\\nfrom tqdm.auto import tqdm\\nimport...\"],[\"decoded_preds, decoded_labels = postprocess_text(\\n                decoded_preds, decoded_labels\\n    ...\"],[\"{\\u002fif}\\n\\n## Using your fine-tuned model[[using-your-fine-tuned-model]]\\n\\nOnce you've pushed the model t...\"],[\"```python\\nprint_summary(0)\\n```\\n\\n```python out\\n'\\u003e\\u003e\\u003e Review: Es una trilogia que se hace muy facil de ...\"],[\"et's study how to preprocess a dataset for token classification! Token classification regroups any t...\"],[\". We just have to make sure we change the B- labels to their I- counterparts for tokens that are ins...\"],[\"he fast tokenizers of the Transformers library are fast, but they also implement features that will ...\"],[\". To enable this, the fast tokenizers store additional information at each step of their internal pi...\"],[\"et's have a look inside the question answering pipeline. The question answering pipeline can extract...\"],[\". Of course, a start index greater than an end index corresponds to an impossible answer. Here is th...\"],[\"Basic usage completed![[basic-usage-completed]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={2}\\n    className...\"],[\"et's study how to preprocess a dataset for question answering! Question answering is the task of fin...\"],[\". We want those two tokens, because there will be the labels we pass to our model. In a one-hot enco...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={7}...\"],[\"Each section can be read independently.\\n\\n{\\u002fif}\\n\\n\\n\\u003cTip\\u003e\\n\\nIf you read the sections in sequence, you wi...\"],[\"hat happens inside the pipeline function? In this video, we will look at what actually happens when ...\"],[\". This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that a...\"],[\". To make sense of those logits, we need to dig into the third and last step of the pipeline: post-p...\"],[\"et's see how to preprocess a dataset for translation. This is the task of well translating a sentenc...\"],[\". Then we pad the inputs with the pad token and the targets with the -100 index, to make sure they a...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning, Check![[fine-tuning-check]]\\n\\n\\u003cCourseFloatingBanner\\n   ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Debugging the training pipeline[[debugging-the-training-pipeline]]\\n...\"],[\"To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model o...\"],[\"```python out\\nValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_clas...\"],[\"`break` ends the loop after one iteration, so this grabs the first batch that comes out of `train_da...\"],[\"The problem has now become clearer: we passed a `loss` argument, which means we're asking Keras to c...\"],[\"```python out\\n  246\\u002f24543 [..............................] - ETA: 15:52 - loss: nan\\n```\\n\\nOh no. \\n\\n`n...\"],[\"Well, this is tricky. Everything is `nan`! But that's strange, isn't it? How would all our logits be...\"],[\"```py\\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\nmodel(batch)\\n``...\"],[\"```python out\\narray([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,\\n        23212, ...\"],[\"0,     0,     0,     0,     0,     0,     0,     0,     0,\\n            0,     0,     0,     0,     0...\"],[\"2105,  1012,   102,     0,     0,     0,     0,     0,     0,\\n            0,     0,     0,     0,   ...\"],[\"0,     0,     0,     0],\\n       [  101, 13543,  1999,  2049,  6143,  2933,  2443,   102,  2025,\\n    ...\"],[\"Well, there's a lot in here, but nothing stands out as unusual. Let's look at the labels:\\n\\n```python...\"],[\"### Check your hyperparameters[[check-your-hyperparameters]]\\n\\nIf you look back at the code above, yo...\"],[\"```python\\nfrom tensorflow.keras.optimizers import Adam\\n\\nmodel = TFAutoModelForSequenceClassification...\"],[\"### Dealing with out-of-memory errors[[dealing-with-out-of-memory-errors]]\\n\\nThe telltale sign of run...\"],[\"If you're running on Colab you don't need to worry about this, but if you're running locally this is...\"],[\"```py\\nlabels = batch[\\\"labels\\\"].numpy()\\nlabel = labels[0]\\n```\\n\\nOnce you can view your data like this,...\"],[\"Doing this once you have defined your `model` is really easy; just grab a batch of training data, th...\"],[\"Once you have a good enough model, you can start tweaking a bit. Don't try launching a thousand runs...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Debugging the training pipeline[[debugging-the-training-pipeline]]\\n...\"],[\"The best way to debug an error that arises in `trainer.train()` is to manually go through this whole...\"],[\"To avoid countless hours spent trying to fix something that is not the source of the bug, we recomme...\"],[\"tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\\nmodel = AutoModelForSequenc...\"],[\"```py\\ntokenizer.decode(trainer.train_dataset[0][\\\"input_ids\\\"])\\n```\\n\\n```python out\\n'[CLS] conceptually...\"],[\"```python out\\nTrue\\n```\\n\\nThat's good! Lastly, let's check our label:\\n\\n```py\\ntrainer.train_dataset[0][...\"],[\"```python out\\n~\\u002fgit\\u002ftransformers\\u002fsrc\\u002ftransformers\\u002fdata\\u002fdata_collator.py in torch_default_data_collat...\"],[\"raw_datasets = load_dataset(\\\"glue\\\", \\\"mnli\\\")\\n\\nmodel_checkpoint = \\\"distilbert-base-uncased\\\"\\ntokenizer ...\"],[\"```py\\ndata_collator = trainer.get_train_dataloader().collate_fn\\nbatch = data_collator([trainer.train...\"],[\"Why is that? It has to do with the way GPUs work. They are extremely efficient at executing a lot of...\"],[\"```python\\ntrainer.model.config.num_labels\\n```\\n\\n```python out\\n2\\n```\\n\\nWith two labels, only 0s and 1s ...\"],[\"We aren't including the `trainer.train()` line yet, to take the time to check that everything looks ...\"],[\"### Dealing with CUDA out-of-memory errors[[dealing-with-cuda-out-of-memory-errors]]\\n\\nWhenever you g...\"],[\"You can run the evaluation loop of the `Trainer` independently form the training like this:\\n\\n```py\\nt...\"],[\"compute_metrics((predictions, labels))\\n```\\n\\n```python out\\nTypeError: only size-1 arrays can be conve...\"],[\"tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\\nmodel = AutoModelForSequenc...\"],[\"### Check your data (again!)[[check-your-data-again]]\\n\\nYour model will only learn something if it's ...\"],[\"When you are sure your data is perfect, you can see if the model is capable of training on it with o...\"],[\"```python out\\n{'accuracy': 1.0}\\n```\\n\\n100% accuracy, now this is a nice example of overfitting (meani...\"],[\"Here are some additional resources that may prove helpful:\\n\\n- [\\\"Reproducibility as a vehicle for eng...\"],[\"Part 1 completed![[part-1-completed]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={4}\\n    classNames=\\\"absolut...\"],[\"Bias and limitations[[bias-and-limitations]]\\n\\n\\u003cCourseFloatingBanner chapter={1}\\n  classNames=\\\"absolu...\"],[\"When you use these tools, you therefore need to keep in the back of your mind that the original mode...\"],[\"n this video, we'll study the encoder-decoder architecture. An example of a popular encoder-decoder ...\"],[\". As we have seen before with the decoder, it can act in an auto-regressive manner; the word it has ...\"],[\". We've translated the sentence! Where the encoder-decoder really shines, is that we have an encoder...\"],[\". There are a lot of sequence to sequence models. This contains a few examples of popular encoder-de...\"],[\"et's have a look inside the token classification pipeline. In the pipeline video, we looked at the d...\"],[\". We will need to use the offset mapping of the tokenizer to get those (look at the video linked bel...\"],[\"hat is the ROUGE metric? For many NLP tasks we can use common metrics like accuracy or F1 score, but...\"],[\". Perfect recall sounds great, but imagine if our generated summary had been “I really really really...\"],[\". The main advantage of ROUGE-L over ROUGE-1 or ROUGE-2 is that is doesn't depend on consecutive n-g...\"],[\"WordPiece tokenization[[wordpiece-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6}\\n  classNames=\\\"ab...\"],[\"```\\nw ##o ##r ##d\\n```\\n\\nThus, the initial alphabet contains all the characters present at the beginni...\"],[\"The splits here will be:\\n\\n```\\n(\\\"h\\\" \\\"##u\\\" \\\"##g\\\", 10), (\\\"p\\\" \\\"##u\\\" \\\"##g\\\", 5), (\\\"p\\\" \\\"##u\\\" \\\"##n\\\", 12), (\\\"...\"],[\"and we continue like this until we reach the desired vocabulary size.\\n\\n\\u003cTip\\u003e\\n\\n✏️ **Now your turn!** ...\"],[\"\\u003cTip\\u003e\\n\\n✏️ **Now your turn!** How will the word `\\\"pugs\\\"` be tokenized?\\n\\n\\u003c\\u002fTip\\u003e\\n\\n## Implementing WordP...\"],[\"As we saw before, the alphabet is the unique set composed of all the first letters of words, and all...\"],[\"scores = {\\n        pair: freq \\u002f (letter_freqs[pair[0]] * letter_freqs[pair[1]])\\n        for pair, fr...\"],[\"```python out\\n['ab', '##o', '##u', '##t']\\n```\\n\\nNow we have everything we need to loop until we have ...\"],[\"```python\\ndef encode_word(word):\\n    tokens = []\\n    while len(word) \\u003e 0:\\n        i = len(word)\\n    ...\"],[\"emory mapping and streaming. In this video we'll take a look at two core features of the Datasets li...\"],[\"To handle these large datasets, the Datasets library is built on two core features: the Apache Arrow...\"],[\". This object is an iterable, which means we can't index it to access elements, but instead iterate ...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={2}\\n    classNames=\\\"absolute z-10 ri...\"],[\"Then we'll look at the tokenizer API, which is the other main component of the `pipeline()` function...\"],[\"Introduction[[introduction]]\\n\\nWelcome to the Hugging Face course! This introduction will guide you t...\"],[\"The next step is to install the libraries that we'll be using in this course. We'll use `pip` for th...\"],[\"When running a Python command in your terminal, such as `python --version`, you should think of the ...\"],[\"```\\nwhich python\\n```\\n\\n```out\\n\\u002fhome\\u002f\\u003cuser\\u003e\\u002ftransformers-course\\u002f.env\\u002fbin\\u002fpython\\n```\\n\\n### Installing de...\"],[\"n this video we take a look at the data processing necessary to train causal language models. Causal...\"],[\". First we tokenize the dataset with the flags I just mentioned. Then we go through each chunk and i...\"],[\".If we want to calculate the loss on a batch we can just pass the input_ids as labels and all the sh...\"],[\"hat is the BLEU metric? For many NLP tasks we can use common metrics like accuracy or F1 score, but ...\"],[\". One problem with unigram precision is that translation models sometimes get stuck in repetitive pa...\"],[\". By default, the mean of all four n-gram precisions is reported, a metric that is sometimes also ca...\"],[\"elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging ...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-10 ri...\"],[\"- Chapters 1 to 4 provide an introduction to the main concepts of the 🤗 Transformers library. By the...\"],[\"## Who are we?[[who-are-we]]\\n\\nAbout the authors:\\n\\n[**Abubakar Abid**](https:\\u002f\\u002fhuggingface.co\\u002fabidlab...\"],[\"[**Merve Noyan**](https:\\u002f\\u002fhuggingface.co\\u002fmerve) is a developer advocate at Hugging Face, working on ...\"],[\"- **Where can I ask a question if I have one?**\\nIf you have a question about any section of the cour...\"],[\"- ** What were the choices made for each translation?**\\nEach translation has a glossary and `TRANSLA...\"],[\"ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks pl...\"],[\"Question Answering video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a lo...\"],[\"Causal Language Modeling video\\n\\nWelcome to the Hugging Face tasks series! In this video we’ll take a...\"],[\"Masked Language Modeling video\\n\\nWelcome to the Hugging Face tasks series! In this video we’ll take a...\"],[\"Summarization video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a look at...\"],[\"Translation video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a look at t...\"],[\"as recorded adlib - need to generate transcript with Whisper :)...\"],[\"Big data? 🤗 Datasets to the rescue![[big-data-datasets-to-the-rescue]]\\n\\n\\u003cCourseFloatingBanner chapte...\"],[\"In this section we'll explore these features of 🤗 Datasets with a huge 825 GB corpus known as [the P...\"],[\"```python out\\nDataset({\\n    features: ['meta', 'text'],\\n    num_rows: 15518009\\n})\\n```\\n\\nWe can see th...\"],[\"# Process.memory_info is expressed in bytes, so convert to megabytes\\nprint(f\\\"RAM used: {psutil.Proce...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIf you're familiar with Pandas, this result might come as a surprise because of Wes Kinney's...\"],[\"```python out\\n'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB\\u002fs'\\n```\\n\\nHere ...\"],[\"```py\\nnext(iter(pubmed_dataset_streamed))\\n```\\n\\n```python out\\n{'meta': {'pmid': 11409574, 'language':...\"],[\"In this example, we selected a random example from the first 10,000 examples in the buffer. Once an ...\"],[\"Let's round out our exploration of dataset streaming with a common application: combining multiple d...\"],[\"combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])\\nlist(islice(...\"],[\"```python out\\n{'meta': {'pile_set_name': 'Pile-CC'},\\n 'text': 'It is done, and submitted. You can pl...\"],[\"et's see how we can preprocess our data for masked language modeling. As a reminder, masked language...\"],[\". Here is how it can be done with code, with one loop to concatenate all the texts and another one t...\"],[\"Subtitles for the course videos\\n\\nThis folder contains all the subtitles for the course videos on You...\"],[\"```\\n1\\n00:00:05,850 --\\u003e 00:00:07,713\\n欢迎来到 Hugging Face 课程。\\n```\\n\\nTo handle this, we provide a script t...\"],[\"he pipeline function. The pipeline function is the most high-level API of the Transformers library. ...\"],[\". Going on the model hub (huggingface.co\\u002fmodels), you can filter the available models by task. The d...\"],[\". Finally, the last task supported by the pipeline API is translation. Here we use a French\\u002fEnglish ...\"],[\"Sequence-to-sequence models[sequence-to-sequence-models]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    ...\"],[\"aving and reloading a dataset. In this video we'll take a look saving a dataset in various formats, ...\"],[\". We simply pass the path of our dataset directory and voila the original dataset is recovered! If w...\"],[\"n our other videos we talked about the basics of fine-tuning a language model with Tensorflow (and a...\"],[\". Once we know how many training steps we're taking, we just pass all that information to the schedu...\"],[\"Tokenizers, check![[tokenizers-check]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={6}\\n    classNames=\\\"absolu...\"],[\"et's have a look inside the question answering pipeline. The question answering pipeline can extract...\"],[\". Of course, a start index greater than an end index corresponds to an impossible answer. Here is th...\"],[\"hat happens inside the pipeline function? In this video, we will look at what actually happens when ...\"],[\". This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that a...\"],[\". This is because each model of the Transformers library returns logits. To make sense of those logi...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 3. What should replace ... in this code sample?\\n\\n```py\\nfrom transformers import pipeline\\n\\nfiller...\"],[\"### 5. What does \\\"transfer learning\\\" mean?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"Transferring the kno...\"],[\"### 7. Select the sentence that best describes the terms \\\"model\\\", \\\"architecture\\\", and \\\"weights\\\".\\n\\n\\u003cQ...\"],[\"### 9. Which of those types of models would you use for summarizing texts?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"### 11. What possible source can the bias observed in a model have?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\tte...\"],[\"n this video we will see how you can create your own tokenizer from scratch! To create your own toke...\"],[\". We will define it as a succession of 2 normalizations used to clean up characters not visible in t...\"],[\"Gradio Blocks Party[[gradio-blocks-party]]\\n\\nAlong with the release of the Gradio chapter of the cour...\"],[\"Training a new tokenizer from an old one[[training-a-new-tokenizer-from-an-old-one]]\\n\\n\\u003cCourseFloatin...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Assembling a corpus[[assembling-a-corpus]]\\n\\nThere's a very simple API in 🤗 Transformers t...\"],[\"```py\\nprint(raw_datasets[\\\"train\\\"][123456][\\\"whole_func_string\\\"])\\n```\\n\\nwhich should print the followin...\"],[\"```py\\ntraining_corpus = (\\n    raw_datasets[\\\"train\\\"][i : i + 1000][\\\"whole_func_string\\\"]\\n    for i in ...\"],[\"```py\\nfrom transformers import AutoTokenizer\\n\\nold_tokenizer = AutoTokenizer.from_pretrained(\\\"gpt2\\\")\\n...\"],[\"This command might take a bit of time if your corpus is very large, but for this dataset of 1.6 GB o...\"],[\"Most of the Transformer models have a fast tokenizer available (there are some exceptions that you c...\"],[\"def __call__(self, x):\\n        return x @ self.weights + self.bias\\n    \\\"\\\"\\\"\\ntokenizer.tokenize(exampl...\"],[\"```bash\\nhuggingface-cli login\\n```\\n\\nOnce you've logged in, you can push your tokenizer by executing t...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n\\u003c!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-...\"],[\"### 3. How does the BERT model expect a pair of sentences to be processed?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t...\"],[\"### 5. What does dynamic padding mean?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It's when you pad the in...\"],[\"### 6. What is the purpose of a collate function?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It ensures al...\"],[\"\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"Nothing, but you get a warning.\\\",\\n\\t\\t\\texplain: \\\"You do get a warn...\"],[\"### 9. Why should you use the 🤗 Accelerate library?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It provides...\"],[\"### 5. The TensorFlow models from `transformers` are already Keras models. What benefit does this of...\"],[\"et's have a look inside the token classification pipeline. In the pipeline video, we looked at the d...\"],[\". We will need to use the offset mapping of the tokenizer to get those (look at the video linked bel...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a model with Keras[[fine-tuning-a-model-with-keras]]\\n\\n\\u003c...\"],[\"### Training[[training]]\\n\\nTensorFlow models imported from 🤗 Transformers are already Keras models. H...\"],[\"\\u003cTip\\u003e\\n\\nNote that 🤗 Transformers models have a special ability that most Keras models don't - they ca...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\n### Improving training performance[[improving-training-performance]]\\n\\n\\u003cYoutube id=\\\"cpzq6ESS...\"],[\"```py\\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\\n\\nbatch_size = 8\\nnum_epochs =...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Model predictions[[model-predictions]]\\n\\n\\u003cYoutube id=\\\"nx10eh4CoOs\\\"\\u002f\\u003e\\n\\n\\nTraining and watch...\"],[\"This concludes the introduction to fine-tuning using the Keras API. An example of doing this for mos...\"],[\"he post-processing step in a question answering task. When doing question answering, the processing ...\"],[\". We will need a map from examples to features, which we can create like this. Now, for the main par...\"],[\"Asking for help on the forums[[asking-for-help-on-the-forums]]\\n\\n\\u003cCourseFloatingBanner chapter={8}\\n  ...\"],[\"And naturally, we should also mention the [Course](https:\\u002f\\u002fdiscuss.huggingface.co\\u002fc\\u002fcourse\\u002f20) categ...\"],[\"The Transformers mecha were largely designed by Shōji Kawamori, the creator of\\nthe Japanese mecha an...\"],[\"The Transformers TV series began around the same time. Produced by Sunbow\\nProductions and Marvel Pro...\"],[\"This brings up a writing interface where we can input the title of our topic, select a category, and...\"],[\"\\u003e Source of IndexError in the AutoModel forward pass?\\n\\nThis title tells the reader _where_ you think...\"],[\"### Including the full traceback[[including-the-full-traceback]]\\n\\nSince the last line of the traceba...\"],[\"However, we can make things even easier for them by providing the actual code that triggered the err...\"],[\"ow to batch inputs together? In this video, we will see how to batch input sequences together. In ge...\"],[\". To get the same results with or without padding, we need to indicate to the attention layers that ...\"],[\"n this video, we will see how to debug an error you encounter when running trainer.train(). As an ex...\"],[\". So let's fix the issue and run again. This time we get a nasty CUDA error. They are very difficult...\"],[\". It looks like our model has the wrong number of labels! We can indeed confirm that, and now that w...\"],[\"Encoder models[[encoder-models]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-1...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Token classification[[token-classification]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cC...\"],[\"\\u003cYoutube id=\\\"wVHdVlPScxA\\\"\\u002f\\u003e\\n\\nOf course, there are many other types of token classification problem; ...\"],[\"\\u003cTip\\u003e\\n\\n💡 As long as your dataset consists of texts split into words with their corresponding labels,...\"],[\"Since we want to perform named entity recognition, we will look at the NER tags:\\n\\n```py\\nraw_datasets...\"],[\"Now decoding the labels we saw earlier gives us this:\\n\\n```python\\nwords = raw_datasets[\\\"train\\\"][0][\\\"t...\"],[\"To begin, let's create our `tokenizer` object. As we said before, we will be using a BERT pretrained...\"],[\"Fortunately, because we're using a fast tokenizer we have access to the 🤗 Tokenizers superpowers, wh...\"],[\"As we can see, our function added the `-100` for the two special tokens at the beginning and the end...\"],[\"```py\\ntokenized_datasets = raw_datasets.map(\\n    tokenize_and_align_labels,\\n    batched=True,\\n    re...\"],[\"{\\u002fif}\\n\\nTo test this on a few samples, we can just call it on a list of examples from our tokenized t...\"],[\"Next stop: the model itself.\\n\\n{\\u002fif}\\n\\n{#if fw === 'tf'}\\n\\n### Defining the model[[defining-the-model]]...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Fine-tuning the model[[fine-tuning-the-model]]\\n\\nWe are now ready to train our model! We ...\"],[\"optimizer, schedule = create_optimizer(\\n    init_lr=2e-5,\\n    num_warmup_steps=0,\\n    num_train_step...\"],[\"At this stage, you can use the inference widget on the Model Hub to test your model and share it wit...\"],[\"```python out\\n['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\\n```\\n\\nWe can then create fa...\"],[\"```py\\nimport numpy as np\\n\\n\\ndef compute_metrics(eval_preds):\\n    logits, labels = eval_preds\\n    pred...\"],[\"```py\\nimport numpy as np\\n\\nall_predictions = []\\nall_labels = []\\nfor batch in tf_eval_dataset:\\n    log...\"],[\"```py\\nid2label = {i: label for i, label in enumerate(label_names)}\\nlabel2id = {v: k for k, v in id2l...\"],[\"If you aren't working in a notebook, just type the following line in your terminal:\\n\\n```bash\\nhugging...\"],[\"Note that while the training happens, each time the model is saved (here, every epoch) it is uploade...\"],[\"Next we reinstantiate our model, to make sure we're not continuing the fine-tuning from before but s...\"],[\"lr_scheduler = get_scheduler(\\n    \\\"linear\\\",\\n    optimizer=optimizer,\\n    num_warmup_steps=0,\\n    num...\"],[\"```py\\ndef postprocess(predictions, labels):\\n    predictions = predictions.detach().cpu().clone().num...\"],[\"optimizer.step()\\n        lr_scheduler.step()\\n        optimizer.zero_grad()\\n        progress_bar.upda...\"],[\"```py\\naccelerator.wait_for_everyone()\\nunwrapped_model = accelerator.unwrap_model(model)\\nunwrapped_mo...\"],[\"```python out\\n[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18...\"],[\"n this video we take a look at setting up a custom loss function for training. In the default loss f...\"],[\". This tensor has an additional dimension as the loss tensor we just saw because we get the informat...\"],[\"Time to slice and dice[[time-to-slice-and-dice]]\\n\\n\\u003cCourseFloatingBanner chapter={5}\\n  classNames=\\\"ab...\"],[\"```py\\nfrom datasets import load_dataset\\n\\ndata_files = {\\\"train\\\": \\\"drugsComTrain_raw.tsv\\\", \\\"test\\\": \\\"dr...\"],[\"Note that we've fixed the seed in `Dataset.shuffle()` for reproducibility purposes. `Dataset.select(...\"],[\"\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Use the `Dataset.unique()` function to find the number of unique drugs and...\"],[\"```\\nlambda x : x * x\\n```\\n\\nTo apply this function to an input, we need to wrap it and the input in pa...\"],[\"Let's define a simple function that counts the number of words in each review:\\n\\n```py\\ndef compute_re...\"],[\"As we suspected, some reviews contain just a single word, which, although it may be okay for sentime...\"],[\"As you can see, the `Dataset.map()` method is quite useful for processing data -- and we haven't eve...\"],[\"Using `Dataset.map()` with `batched=True` will be essential to unlock the speed of the \\\"fast\\\" tokeni...\"],[\"Options         | Fast tokenizer | Slow tokenizer\\n:--------------:|:--------------:|:-------------:\\n...\"],[\"Options         | Fast tokenizer | Slow tokenizer\\n:--------------:|:--------------:|:-------------:\\n...\"],[\"```py\\ndef tokenize_and_split(examples):\\n    return tokenizer(\\n        examples[\\\"review\\\"],\\n        tr...\"],[\"```py\\ntokenized_dataset = drug_dataset.map(\\n    tokenize_and_split, batched=True, remove_columns=dru...\"],[\"We can see it works with `Dataset.map()` without us needing to remove the old columns:\\n\\n```py\\ntokeni...\"],[\"```py\\ndrug_dataset.set_format(\\\"pandas\\\")\\n```\\n\\nNow when we access elements of the dataset we get a `pa...\"],[\"Let's create a `pandas.DataFrame` for the whole training set by selecting all the elements of `drug_...\"],[\"freq_dataset = Dataset.from_pandas(frequencies)\\nfreq_dataset\\n```\\n\\n```python out\\nDataset({\\n    featur...\"],[\"```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['patient_id', 'drugName', 'condi...\"],[\"where we can see that each split is associated with its own *dataset.arrow* table, and some metadata...\"],[\"```py\\n!head -n 1 drug-reviews-train.jsonl\\n```\\n\\n```python out\\n{\\\"patient_id\\\":141780,\\\"drugName\\\":\\\"Escita...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={3}...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 3. Suppose you have a dataset about household pets called `pets_dataset`, which has a `name` col...\"],[\"### 5. Which of the following are the main benefits of memory mapping?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t...\"],[\"### 7. Which of the following are the main benefits of creating a dataset card?\\n\\n\\u003cQuestion\\n\\tchoices=...\"],[\"### 8. What is semantic search?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"A way to search for exact match...\"],[\"### 10. Can I use 🤗 Datasets to load data for use in other domains, like speech processing?\\n\\n\\u003cQuesti...\"],[\"hy are fast tokenizers called fast? In this video we will see exactly how much faster the so-called ...\"],[\"Part 2 completed![[part-2-completed]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={8}\\n    classNames=\\\"absolut...\"],[\"n a lot of our examples, you're going to see DataCollators popping up over and over. They're used in...\"],[\". These are DefaultDataCollator and DataCollatorWithPadding. These are the ones you should use if yo...\"],[\". You choose which mode you want with the mlm argument - set it to True for masked language modeling...\"],[\"Integrations with the Hugging Face Hub[[integrations-with-the-hugging-face-hub]]\\n\\n\\u003cCourseFloatingBan...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-gpt-j-6B.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"750\\\" title=\\\"Gradio app\\\"...\"],[\"One of the cool things about loading demos from the Hub or Spaces is that you customize them \\nby ove...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a model with the Trainer API[[fine-tuning-a-model-with-...\"],[\"tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\\ndata_collator = DataCollatorW...\"],[\"Once we have our model, we can define a `Trainer` by passing it all the objects constructed up to no...\"],[\"### Evaluation[[evaluation]]\\n\\nLet's see how we can build a useful `compute_metrics()` function and u...\"],[\"```py\\nimport numpy as np\\n\\npreds = np.argmax(predictions.predictions, axis=-1)\\n```\\n\\nWe can now compar...\"],[\"```py\\ntraining_args = TrainingArguments(\\\"test-trainer\\\", evaluation_strategy=\\\"epoch\\\")\\nmodel = AutoMod...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Tokenizers[[tokenizers]]\\n\\n{#if fw === 'pt'}\\n\\n\\u003cCourseFloatingBanner ...\"],[\"Let's take a look at some examples of tokenization algorithms, and try to answer some of the questio...\"],[\"Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model...\"],[\"But here too some questions arise concerning spaces and punctuation:\\n\\n\\u003cdiv class=\\\"flex justify-cente...\"],[\"Here is an example showing how a subword tokenization algorithm would tokenize the sequence \\\"Let's d...\"],[\"Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading ...\"],[\"\\u003cYoutube id=\\\"Yffk5aydLzg\\\"\\u002f\\u003e\\n\\nTranslating text to numbers is known as _encoding_. Encoding is done in...\"],[\"### From tokens to input IDs[[from-tokens-to-input-ids]]\\n\\nThe conversion to input IDs is handled by ...\"],[\"Building a model card[[building-a-model-card]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={4}\\n    classNames...\"],[\"Let's take a look at what each of these sections should contain.\\n\\n### Model description[[model-descr...\"],[\"### Variable and metrics[[variable-and-metrics]]\\n\\nHere you should describe the metrics you use for e...\"],[\"For example, if you take a look at the [`camembert-base` model card](https:\\u002f\\u002fhuggingface.co\\u002fcamember...\"],[\"Byte-Pair Encoding tokenization[[byte-pair-encoding-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6...\"],[\"\\u003cTip\\u003e\\n\\nThe GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with th...\"],[\"Thus, the first merge rule learned by the tokenizer is `(\\\"u\\\", \\\"g\\\") -\\u003e \\\"ug\\\"`, which means that `\\\"ug\\\"`...\"],[\"Let's take the example we used during training, with the three merge rules learned:\\n\\n```\\n(\\\"u\\\", \\\"g\\\") ...\"],[\"print(word_freqs)\\n```\\n\\n```python out\\ndefaultdict(int, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1...\"],[\"Let's have a look at a part of this dictionary after the initial splits:\\n\\n```python\\npair_freqs = com...\"],[\"Now we have everything we need to loop until we have learned all the merges we want. Let's aim for a...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nTo tokenize a new text, we pre-tokenize it, split it, then apply all the merge rules learned...\"],[\"Live sessions and workshops[[live-sessions-and-workshops]]\\n\\nFor the release of parts 1 and 2 of the ...\"],[\"sing the Python debugger in a terminal. In this video, we'll learn how to use the Python debugger in...\"],[\". Those labels are definitely weird: they are of various size, which we can actually confirm by prin...\"],[\"n this video, we will learn the first things to do when you get an error. Let's say we want to use t...\"],[\". It's telling us that we should check our model is a correct model identifier, so let's hop on to h...\"],[\"Mastering NLP[[mastering-nlp]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={7}\\n    classNames=\\\"absolute z-10 ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Semantic search with FAISS[[semantic-search-with-faiss]]\\n\\n{#if fw =...\"],[\"In this section we'll use embeddings to develop a semantic search engine. These search engines offer...\"],[\"```py\\nissues_dataset = issues_dataset.filter(\\n    lambda x: (x[\\\"is_pull_request\\\"] == False and len(x...\"],[\"```py\\nissues_dataset.set_format(\\\"pandas\\\")\\ndf = issues_dataset[:]\\n```\\n\\nIf we inspect the first row in...\"],[\"```py\\ncomments_df = df.explode(\\\"comments\\\", ignore_index=True)\\ncomments_df.head(4)\\n```\\n\\n\\u003ctable border...\"],[\"Great, we can see the rows have been replicated, with the `comments` column containing the individua...\"],[\"```py\\ndef concatenate_text(examples):\\n    return {\\n        \\\"text\\\": examples[\\\"title\\\"]\\n        + \\\" \\\\n ...\"],[\"model_ckpt = \\\"sentence-transformers\\u002fmulti-qa-mpnet-base-dot-v1\\\"\\ntokenizer = AutoTokenizer.from_pretr...\"],[\"```py\\nembeddings_dataset = comments_dataset.map(\\n    lambda x: {\\\"embeddings\\\": get_embeddings(x[\\\"text...\"],[\"```py\\nembeddings_dataset.add_faiss_index(column=\\\"embeddings\\\")\\n```\\n\\nWe can now perform queries on thi...\"],[\"@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline...\"],[\"----------\\n\\nAbout I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already i...\"],[\"he Push to Hub API. Let's have a look at the push_to_hub API. You will need to be logged in with you...\"],[\". Two, this will draft a model card that will be the landing page of your model repo. Going back to ...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n\\u003cCourseFloatingB...\"],[\"### 2. What is the advantage of using a generator of lists of texts compared to a list of lists of t...\"],[\"### 4. How does the `token-classification` pipeline handle entities that span over several tokens?\\n\\n...\"],[\"### 5. How does the `question-answering` pipeline handle long contexts?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t...\"],[\"### 7. What is pre-tokenization for a subword tokenizer?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \\\"It's t...\"],[\"### 8. Select the sentences that apply to the BPE model of tokenization.\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n...\"],[\"### 9. Select the sentences that apply to the WordPiece model of tokenization.\\n\\n\\u003cQuestion\\n\\tchoices={...\"],[\"### 10. Select the sentences that apply to the Unigram model of tokenization.\\n\\n\\u003cQuestion\\n\\tchoices={[...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Training a causal language model from scratch[[training-a-causal-la...\"],[\"{\\u002fif}\\n\\nUp until now, we've mostly been using pretrained models and fine-tuning them for new use case...\"],[\"\\u003cYoutube id=\\\"Vpjb1lu0MDk\\\"\\u002f\\u003e\\n\\nIn [Chapter 6](\\u002fcourse\\u002fchapter6) we created an efficient tokenizer to p...\"],[\"However, training on the full corpus is time- and compute-consuming, and we only need the subset of ...\"],[\"split = \\\"train\\\"  # \\\"valid\\\"\\nfilters = [\\\"pandas\\\", \\\"sklearn\\\", \\\"matplotlib\\\", \\\"seaborn\\\"]\\n\\ndata = load_dat...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nLet's look at an example from the dataset. We'll just show the first 200 characters of each ...\"],[\"Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum len...\"],[\"We can see that we get 34 segments in total from those two examples. Looking at the chunk lengths, w...\"],[\"We now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion token...\"],[\"config = AutoConfig.from_pretrained(\\n    \\\"gpt2\\\",\\n    vocab_size=len(tokenizer),\\n    n_ctx=context_le...\"],[\"{\\u002fif}\\n\\nOur model has 124M parameters that we'll have to tune. Before we can start training, we need ...\"],[\"{#if fw === 'tf'}\\n\\nNow we can use the `prepare_tf_dataset()` method to convert our datasets to Tenso...\"],[\"```py\\nfrom transformers import Trainer, TrainingArguments\\n\\nargs = TrainingArguments(\\n    output_dir=...\"],[\"```py\\nfrom transformers.keras_callbacks import PushToHubCallback\\n\\ncallback = PushToHubCallback(outpu...\"],[\"{:else}\\n\\n```py\\nfrom transformers import pipeline\\n\\ncourse_model = TFGPT2LMHeadModel.from_pretrained(\\\"...\"],[\"# calculate the mean income per profession\\nprofession = df.groupby(['profession']).mean()\\n\\n# compute...\"],[\"{\\u002fif}\\n\\n{#if fw === 'pt'}\\n\\n## Training with 🤗 Accelerate[[training-with-accelerate]]\\n\\nWe've seen how ...\"],[\"```python out\\n'Keyword has not single token: testtest'\\n```\\n\\nGreat, that seems to work nicely! We can...\"],[\"Before we can start training with this awesome new loss function, we need to prepare a few things:\\n\\n...\"],[\"losses.append(accelerator.gather(outputs.loss))\\n    loss = torch.mean(torch.cat(losses))\\n    try:\\n  ...\"],[\"lr_scheduler = get_scheduler(\\n    name=\\\"linear\\\",\\n    optimizer=optimizer,\\n    num_warmup_steps=1_000...\"],[\"Before we train, let's run a quick test to see if the evaluation function works properly:\\n\\n```py\\neva...\"],[\"```py\\nfrom tqdm.notebook import tqdm\\n\\ngradient_accumulation_steps = 8\\neval_steps = 5_000\\n\\nmodel.trai...\"],[\"\\u003cTip\\u003e\\n\\n✏️ **Try it out!** Either create your own custom loss function tailored to your use case, or ...\"],[\"ow to ask a question on the Hugging Face forums?\\n\\nIf you have a general question or are looking to d...\"],[\"For this example, we will use the following code,\\n\\nthat produces an error, as we saw in the \\\"What to...\"],[\"hat is domain adaptation? When fine-tuning a pretrained model on a new dataset, the fine-tuned model...\"]],\"hovertemplate\":\"source=course\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"course, circle\",\"marker\":{\"color\":\"#FF97FF\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"course, circle\",\"showlegend\":true,\"x\":[-15.097838,-0.9849392,9.075507,8.992492,-12.636361,-12.550053,-13.118389,-12.877588,-2.3805401,-0.99996156,6.9003067,6.7448864,7.0364084,-3.4665396,-3.617194,-17.415022,-18.170042,-18.813824,-18.869944,-18.732054,-18.858376,2.6031291,-5.0450478,0.48255727,-8.958383,-9.267946,-9.337229,-9.515898,0.07400865,0.6078721,0.17805932,0.420248,-9.620839,-5.723694,-15.14402,-13.605887,-17.415297,-4.7348747,-3.1624534,-2.584107,11.322113,11.630185,-4.326178,-4.5981135,15.938336,16.034746,16.10388,16.208174,16.337234,16.660105,16.451347,14.708741,14.0427885,12.167252,16.531204,16.580307,17.182974,16.988497,16.82391,16.51763,16.377466,16.542213,16.297672,16.470762,8.827691,15.874909,16.362932,16.718204,12.697471,-6.6698866,-7.9673586,-5.6917048,-6.037606,-4.793819,-4.272175,-2.3805923,-2.3684459,-2.8281815,-3.5862646,3.9325273,4.126536,3.4841528,3.284651,3.506784,3.1245878,3.3903098,3.3628585,3.0976295,3.5438237,3.5175471,3.2793329,3.1803746,3.376578,2.5916948,3.1298544,2.835683,3.3542595,3.6631782,-2.7168334,-3.4709578,7.952605,-4.1400642,-3.9274437,7.961117,-3.5653703,7.876151,-3.8636448,9.666486,10.026466,-4.8046746,19.384226,20.062487,17.959019,-1.1879563,-0.9970422,-10.473557,-10.384955,-3.5295134,-2.2244966,17.344751,13.232804,12.966373,12.593615,12.651549,12.8248625,16.862389,16.936714,18.818499,-2.2203345,-1.3802203,13.253438,12.385136,10.555702,-3.229544,-3.8167431,0.15407242,0.51458794,-0.009237005,0.29467368,0.6292522,0.49228597,-2.7262082,-2.9773552,-2.830445,-2.6524322,-3.3283753,1.4311627,-13.128049,-12.481665,-12.475652,0.55704015,-1.1264567,-1.6536185,0.17713901,0.41001698,-6.208406,-3.551209,-5.886404,8.21792,7.7348456,7.0572095,-10.274919,-6.4095607,-3.4527798,-3.3934145,0.23565988,0.01977027,-2.2965808,-12.718211,-4.6872945,-8.985347,-8.594078,-8.440855,-2.4307582,-8.4066305,-7.433327,-8.519996,-8.122202,-8.5425415,-8.510572,-8.572999,-8.81567,-9.159695,-8.708103,-8.696956,-8.6396885,-8.863358,-8.848903,-8.489101,-8.653028,-7.9171877,-2.1053762,-2.5411406,-1.7890632,-8.346046,-7.1928425,-2.207455,-2.4218092,7.064268,-2.7966132,-9.495795,-9.19646,-1.0190167,-9.224895,-9.034562,-1.1734521,-1.1167563,-9.5271015,-9.436582,-9.550911,0.5815355,-0.05888089,0.17364524,-0.03787143,-0.11113576,-0.050258394,0.08126752,0.32121986,0.25600561,0.25673816,-0.8385478,0.018131305,-9.536985,-9.655757,7.6707897,5.1714764,4.502846,4.353961,-9.118548,-1.3003696,1.3367003,8.859339,8.958228,9.040571,9.017258,2.993108,-0.44511008,-0.46268874,19.758873,17.55612,16.95623,9.090323,15.955234,-9.178626,-6.994896,-2.7874732,-10.279396,-6.641567,-6.1390457,3.4768615,-6.272057,-5.4878097,3.6641786,-8.843587,-6.2881,-6.1229053,-6.486975,4.595008,-2.8794508,-0.9659681,-2.8114367,-2.6323643,-1.608845,-4.397538,-4.547674,-3.7303545,1.014866,2.9957254,-3.929828,-2.9469092,-2.5904872,-1.8172412,3.0732923,2.8949504,2.8395646,-2.1134262,2.706038,2.4757297,0.8892473,0.7675919,1.5188699,0.5113682,0.6059073,0.19505456,-4.0575285,-3.2624655,9.293789,9.300771,16.616993,17.774124,3.0295026,-3.992484,-3.7846687,-3.8483565,-4.3463755,-1.2257166,-4.1635118,-6.930584,-6.4796524,-2.5323915,-9.1883545,2.5471327,3.1971827,4.182738,-1.9508694,-0.08942319,-2.6678576,-4.3417606,-3.9559834,-4.015133,-2.8176231,-3.4751656,-3.398701,-3.4774368,11.715239,11.514656,8.915404,9.037237,9.040977,8.868598,8.797112,8.787835,-7.2988634,-6.776783,-1.9669094,-4.034733,-6.134058,-7.129471,-3.2632973,-3.898282,1.1551456,-0.14620006,-0.782581,-3.238924,-12.375837,-12.334795,7.8229175,8.068664,8.133062,-11.170737,-4.3827443,-3.7490392,13.6113405,13.189537,13.603178,11.390903,-4.4941216,-3.7276695,-7.018444,-5.847881,-6.2259345,-3.9291732,13.613012,-5.5744824,-3.6204164,8.378848,7.818445,8.840023,-1.4000194,-2.7814057,-1.991915,-3.0135453,12.474282,11.708008,0.3986988,12.269089,11.455751,1.4225322,-4.1772375,3.5655653,19.18008,19.702566,16.728104,16.927042,19.649458,18.433243,17.363415,16.491302,8.096691,2.43257,12.191965,12.745136,12.607761,6.56626,-18.449537,-18.8175,-18.872831,-18.419231,19.452662,19.997467,18.059843,-10.73801,-9.982456,-2.4168987,19.423927,19.76552,5.1290417,4.183748,15.780132,15.78014,15.545143,15.777324,15.769205,15.776958,15.775996,15.748221,15.780204,15.779481,15.7773905,15.778655,15.807107,15.7725315,15.760657,15.763213,8.727538,8.750284,-14.996797,-21.732155,-14.9770155,-0.87298673,-1.705817,0.50302875,0.23620306,0.96579725,6.3565574,-0.016308779,-2.080016,-2.1807816,-3.253838,-3.7613158,-3.7159567,-2.6119895,-3.3624809,-3.2478337,-2.430139,-3.0291712,-7.3661613,-7.256974,-2.4197156,-6.2851763,-6.7680435,-3.5303876,-2.3597677,-5.5906777,13.215697,11.537478,10.517646,8.6226,-1.3469579,-3.4191947,-1.3091038,-6.3685546,-7.0067654,-6.396027,-6.348732,-7.1159363,-7.27567,-6.0660486,-7.2937336,-6.5550923,-6.538321,-6.3108788,-4.269325,-1.2783751,-0.22891264,-3.5198808,-3.4691427,-2.6530728,-2.482883,-3.9075024,-5.6060066,-6.393431,-5.7333317,15.755441,-6.075665,8.720553,-4.774951,-3.8307197,6.194655,6.2104487,-2.1712503,-2.1820335,-2.3538506,-1.7928239,-1.84525,-6.858147,-4.885585,2.7724617,8.769881,8.730283,8.663031,8.707608,8.710186,-3.5437157,-2.467375,-2.7873595,-4.2673087,-3.7160118,-2.7219396,-2.3043766,-8.083251,14.462508,4.402003,-6.701307,13.126292,5.176994,18.455212,19.384634,19.75671,18.681145,19.0778,17.895504,17.888945,17.31736,17.21518,16.954704,17.776655,17.681936,19.749945,18.270971,17.217434,15.095247,17.22734,-2.8177326,-1.3204838,-2.0827265,-4.3534365,0.96052897,0.3432989,-1.8241981,-9.798736,-9.875267,-4.231108,-0.0102138575,-9.6528425,-9.897753,2.5445256,-0.34451288,-1.808565,-4.3809004,5.8934865,3.4225752,8.694061,8.746382,-3.6599696,-2.16669,-7.1969514,4.0193024,4.1186914,8.480349,8.348462,1.9286413,-1.4520516,-1.8213552,-0.9448813,-2.8972752,1.9243057,8.44186,7.6269693,7.982734,8.039533,8.373481,8.680528,8.508566,8.403002,8.565233,8.123524,8.3409605,11.506395,-14.88731,12.975293,12.958767,5.462323,6.0196595,-1.8327795,-3.0571024,-1.0268482,11.668356,11.026167,11.777808,-7.0860047,-12.481377,-12.784151,-12.571422,-12.60072,-12.834094,-12.589693,-12.578311,8.443659,-11.169462,-14.475318,-13.606106,-17.414953,3.838476,-3.5142457,-3.3927999,3.0177586,2.737807,-6.5440655,-6.433429,-1.8272194,-6.70774,-6.478157,13.437905,-3.4176245,8.024542,8.048854,-1.976146,-0.80424076,-3.000056,18.191225,18.56058,1.297359,-5.165152,1.3461736,-2.7454588,-3.934253,-1.7948284,-1.7402856,-5.463122,-3.2028446,-2.443011,-1.6894118,-3.8897564,-2.8568466,-1.0596753,-1.977648,-2.2879257,-2.3903685,-2.5283945,-2.2155113,3.9338272,1.078292,1.1408386,5.071238,5.273272,4.8424397,-1.3065431,1.7838706,0.11946239,-0.39574996,4.665096,-6.266966,-0.14212117,1.3014861,-4.3602543,-5.058795,-2.9371257,6.089988,8.664371,-1.0790223,-0.40938368,8.456774,8.472505,8.513969,-1.6996735,-2.3267093,-2.9181185,-2.0343013,2.7198908,3.7179022,7.6309357,7.7045164,7.7124543,7.6455083,7.7576733,7.708508,8.039825,-5.5478888,-4.430817,-0.238033,-1.3768873,-12.529653,-12.564471,-13.287882,-12.454078,-12.821569,-12.833303,-12.241654,-12.573399,-12.645386,-12.601508,-12.864503,-12.583745,-12.667237,-12.483711,-12.748034,-12.826702,-12.792507,-12.834639,-12.89697,-12.856495,-12.453945,8.234159,8.323435,-11.161209,6.487902,3.4005764,3.8000593,4.9888625,-3.6906943,-3.753625,-6.7682123,-6.6417856,-2.5024831,8.484082,-5.938851,2.5232189,2.458096,2.1943097,2.2048378,1.1359233,2.2307153,2.2354505,2.4690118,1.3843046,2.5219579,1.6891501,1.5588727,-6.3301387,-6.2145133,-1.581088,1.4987953,1.8457892,-0.64975685,1.6505054,8.206847,7.9015403,0.3909626,-2.102158,18.322088,17.516647,4.882267,1.4737853,0.725835,2.763496,2.8866136,2.437181,2.348247,2.6452844,2.6565614,0.9726176,8.391598,8.70101,8.891396,8.556011,8.571772,8.983232,8.6663265,8.838124,-3.616782,-2.416903,-3.16962,-3.155502,-2.8059406,-1.5313785,6.180836,-7.0263467,-7.474321,-7.3958306,-7.3986144,-10.079782,3.2781045,-3.8426638,-3.5064268,-3.6245337,-2.180123,2.2041574,-0.14371741,-5.819855,3.1244893,2.688396,3.5164983,8.548688,0.78515196,-7.8332105,4.1327887,4.281836,3.7027931,4.240639,-6.755757,2.5513175,2.7358775,3.3029797,-7.2852473,-4.186444,5.184117,4.183394,-0.23088425,0.10054145,0.02632805,-1.1790822,-6.7663627,-6.6080437,-4.8773437,-6.384166,-6.5763235,-6.566157,-2.059553,-6.4887757,-0.40346786,-6.166756,-5.946401,-6.058398,-6.4069667,-6.590434,-6.3597455,-5.8935738,-6.538768,-6.3984795,6.841649,-6.8432918,-6.7088733,-6.6213193,-2.1861002,-1.098248,8.52533],\"xaxis\":\"x\",\"y\":[-2.1731572,3.2408588,-11.485399,-11.187448,-18.366228,-18.42417,-18.305946,-18.242395,-7.4468513,-7.171006,0.25629306,0.23090227,0.23432219,6.3752184,6.0851207,14.812017,-4.1579223,-4.338401,-4.369183,-4.251645,-4.378899,3.7415679,2.1868699,4.167052,-0.40474528,-0.25687483,-0.007877224,0.04951652,4.513535,6.2829275,6.160922,5.1511974,2.8381484,0.27338782,-2.1727397,19.449453,14.811178,-6.090621,-7.8511753,-10.59225,2.3415685,2.1293774,-2.6943092,-6.0183697,3.6778536,3.1119869,3.1955926,3.2870295,3.5541716,2.6955783,3.1123421,3.2746727,3.5465102,3.8858864,2.8199663,2.7770278,2.869423,2.1008391,2.072005,2.9681728,3.1706855,2.959932,3.3201137,2.902091,2.1304827,3.511545,3.0773177,2.4702234,3.6656184,3.0486403,0.1830129,3.5478873,3.4000542,3.573391,2.2874355,-12.39667,-12.487127,4.383387,4.916399,1.4047512,1.1462924,1.4561691,1.3982737,1.3269341,1.5293238,1.6098017,1.4946802,1.6357974,1.6870819,1.652998,2.646992,1.5218755,1.6612734,1.3835315,1.5503398,1.7740031,1.5906243,1.3975654,5.771652,4.9406424,3.7535722,4.5581717,4.1579742,3.7106981,4.562608,3.8155818,4.281321,2.9629543,3.262565,-7.4243813,2.3339543,2.317746,3.7606485,-1.1453723,-2.1378183,0.43434075,0.68478286,1.0389515,1.7462376,2.5625374,3.8300796,4.4431915,3.9957669,4.445902,3.8726392,3.7488267,3.1790922,2.7925384,3.715988,0.30999193,5.5821366,4.2553782,3.7326136,-3.6707814,-4.084434,7.805495,7.1278234,3.8806052,7.409368,8.018962,7.8614874,4.423964,4.563594,4.599031,4.6588483,3.791939,1.8386574,-18.35358,-18.494064,-18.549303,6.282,-0.0071699517,4.3113923,6.0478387,5.976767,2.3640366,-1.6653185,3.320284,3.6888132,3.5600681,3.1283352,-0.46051165,0.6523022,3.8012643,3.58119,-5.953919,-6.0392127,3.2753427,-18.900597,4.416689,2.666177,3.2249022,3.3645346,3.0689526,3.1080823,3.3530543,3.643051,3.1143377,3.655873,3.4358292,3.6948261,3.235526,2.879847,3.257484,3.29709,3.4529567,3.2636821,3.263025,3.4023788,3.316617,2.5855327,3.4591446,3.6249437,2.8781626,3.1636386,3.350368,3.4779081,3.6702075,1.2994139,3.0153806,2.5127919,2.7811794,1.7959926,2.6866002,2.7214396,2.0197024,2.3506348,2.907919,2.9517415,3.0281477,4.6592393,4.158665,3.6934078,3.4492314,3.5199702,3.6075842,4.3517275,4.909489,5.000231,4.9689217,1.7765119,4.676822,2.8672636,2.897917,2.692281,2.2184618,1.7221801,1.6185441,-0.007912033,4.0771675,3.5881596,2.7260551,2.6468208,2.7203,2.7879522,2.6365929,4.03848,4.495925,2.351474,2.7557144,3.8773103,1.9264193,3.5899806,-0.14911017,0.0989774,0.894998,-0.4252906,1.9682155,0.9183182,2.5687852,0.6494989,-1.3102367,-2.1307487,-0.8600375,0.5542472,0.1971147,0.41840985,3.7705464,-6.7694683,-6.6662536,-6.405966,-6.6353383,-6.8593087,-6.849877,-7.0549135,-6.867464,0.3291157,1.0030639,-6.5439053,-5.829489,-6.696412,-5.956536,-2.2912903,-2.3837948,-2.3260362,3.9068048,-2.2763875,-2.044469,6.3539877,6.6332626,3.4859943,5.7649426,6.3316526,5.129447,-4.8215475,-4.804692,-20.008036,-20.004192,3.1171331,3.1851988,2.3724039,-1.6312325,-1.7563897,-2.0580978,-0.25393546,-2.8783824,-1.538853,3.0444472,2.908188,3.86584,0.7844629,1.013999,1.1072792,0.8590078,4.254235,4.4340196,3.2857153,2.654337,5.194759,4.309367,4.6960306,4.7792234,4.3680806,4.3844137,2.46746,2.0286796,-11.404086,-11.403064,-11.365043,-11.501214,-11.561615,-11.533991,2.80411,3.2182267,2.0568097,4.409538,3.9884717,-0.43144318,0.009626932,-0.04441943,3.5667408,-6.229927,2.3775926,-7.223664,-17.409948,-17.430164,0.94077355,0.056271296,-0.003743242,4.5421467,-6.8984776,-6.7480645,4.136011,5.6245437,4.612233,3.7469249,0.9832204,6.7726707,0.9085076,1.7680645,2.107011,4.4463787,3.6705158,-0.14489985,6.505865,4.238792,3.1037536,2.9384224,1.0979798,-3.9967983,-6.918962,-4.6686807,4.815399,4.653932,4.783847,4.4125686,4.4699683,0.97903484,-8.648528,1.1255009,2.5635633,2.3914208,2.9762378,2.860108,2.3142478,2.206388,2.365635,2.984209,-10.009796,0.11299246,2.2922304,2.6362514,2.5426867,-0.8948525,-4.237343,-4.3379183,-4.358016,-3.5877755,2.3420804,2.3027754,3.7198656,0.008483462,-0.52887243,-12.109244,2.355485,2.3176925,1.7322681,1.6122465,-16.031158,-16.03641,-16.224333,-16.034767,-16.045008,-16.035742,-16.03779,-16.062252,-16.038666,-16.037878,-16.037273,-16.035563,-16.015747,-16.038155,-16.041153,-16.038174,-11.913063,-11.878349,-2.1799922,8.552175,-2.1229324,-0.8527387,-1.3438008,0.8145564,0.094548434,1.2179878,2.747785,5.9563036,-5.6632104,-6.431036,-5.3883796,-6.286074,3.6083984,3.871588,4.0357914,2.065979,-11.91444,2.1726098,2.6490777,2.7943423,-12.032011,3.1518738,3.3035574,-0.332213,3.1213953,3.5321734,3.2966497,3.6629567,3.499974,-10.349148,2.3136046,-5.4708014,-2.3142326,7.3298917,6.5690126,6.5691357,7.091541,6.524877,6.4119015,6.952314,6.4026256,-0.20170918,-0.07144751,-0.075609654,-0.20877199,4.564182,4.1339736,4.2344275,5.758974,3.687718,4.012912,-0.7593464,-0.13383886,-0.010328336,0.17798202,-16.042765,0.2965455,-10.2601795,1.2537173,-6.0806923,20.71309,20.75944,-5.3412786,-5.768859,-5.532626,-4.3569894,-5.9132814,-1.0570667,-1.4047496,0.6540061,-11.858844,-12.0053,-12.025159,-12.03603,-12.1049,5.6022544,3.619223,3.3738823,4.4969206,5.1175284,0.26636356,-0.52831,1.3020985,5.1671634,-0.073503,2.6703527,5.2486887,0.8847153,2.3812313,2.459472,2.3040245,2.470958,3.1198611,4.2087026,3.3495517,2.7083151,2.6922255,2.6176944,2.558002,2.5776782,2.342983,2.1073315,3.260647,3.299562,2.70473,1.0422986,-6.633348,-5.801313,-6.607643,4.1897364,3.5410404,-6.8055778,-0.14084914,-0.2999587,3.3529131,6.1686983,-0.3128778,0.05439845,2.9211893,-6.320447,-6.973052,-6.8094096,1.914786,3.100698,-11.674222,-11.575048,-6.835933,0.3635121,4.5011253,0.8542961,0.95527416,-10.862092,-10.270814,-0.40098852,-3.5108552,-7.3290763,-6.904011,-4.458233,0.4463651,-10.707811,-10.18784,-10.429231,-10.422787,-11.039227,-11.573477,-11.073755,-10.990341,-11.0815,-10.444197,-10.308221,4.0495906,-2.0568917,3.1180937,3.1629395,1.7824818,1.6269344,5.362377,6.000295,5.1323595,2.3269224,2.59285,2.1517384,1.8905959,-18.715418,-18.872856,-18.22113,-18.377424,-19.135847,-18.514874,-18.571613,-0.27594367,4.542377,-1.9261639,19.449787,14.811982,-0.82651716,6.457375,6.432822,-0.8687857,0.35850996,7.5682473,7.64202,3.3653674,7.4041977,7.7463546,5.105296,6.686842,-9.627596,-9.719533,-1.4249936,-1.1257654,4.9616237,3.850136,3.6819603,1.2482619,-1.2810732,3.2908387,-2.2516854,-2.372194,1.6240351,-1.8296831,2.499839,2.4404867,0.5253572,0.30804804,-2.86297,-7.092838,1.0114131,-4.8716936,-4.1563635,-5.156765,-3.2687373,-5.8733006,1.1488869,1.9258336,2.173605,0.21110193,2.4397533,1.8204067,1.0232412,1.5773668,2.3153546,3.6149054,1.6543727,7.3182125,2.1528554,2.1128924,-6.951712,-6.6414475,-8.928615,-1.0879413,-11.922444,3.0946157,5.0169806,3.1684513,3.1667652,3.2691064,2.5439231,1.1445513,-0.3972115,-1.0639699,4.4663873,4.512091,3.2592082,3.29952,3.34639,3.3114328,3.5974166,3.5681283,2.8882806,-1.315559,4.0197616,-0.4064451,4.315755,-18.444807,-18.36345,-18.214012,-18.657394,-19.053364,-19.164799,-18.370934,-18.220787,-18.280777,-18.302895,-19.108065,-18.56737,-18.397264,-18.39781,-19.04606,-19.133884,-18.951387,-18.583792,-18.632118,-18.918205,-18.407507,-0.008381101,-0.2246558,4.5353584,-0.48310825,-0.98222893,-1.3225583,0.60488635,-7.4784455,-3.0082593,6.5266085,6.7214904,-11.17924,-11.401295,0.9529832,3.9506829,3.8782551,3.9990306,3.9754632,4.2233357,3.9172697,3.8958628,3.675325,4.5254683,3.916129,4.351695,4.268116,-0.28458107,0.05869019,-2.7505343,0.5955259,0.88428146,0.9587995,0.9821637,-10.180047,-9.770763,1.0819465,-7.3165393,2.1861272,3.7175636,1.0194925,-0.06931494,0.056471705,0.33600193,0.86331815,-1.8591576,-1.8316449,-0.2280058,-2.2260523,3.485845,1.3156359,0.96890163,0.7647235,0.68352413,1.8212368,0.5147158,1.1357715,0.69333416,-7.0428224,-6.208819,0.45918167,0.48480377,0.6041176,1.5319545,1.7840248,1.7555785,6.3988566,6.4522667,6.450403,-0.35413867,-2.1317089,-1.5612427,-1.6673883,-1.8518013,0.5484413,3.925949,-0.46626675,2.4528458,4.3392844,4.5728106,4.5855813,3.0566852,4.546765,-1.8207601,1.0919123,1.011377,1.4000005,2.8455634,-1.4561931,0.6209184,-1.1062886,-0.93937963,-1.9239609,4.2827463,1.5073719,0.84033614,9.805318,8.414323,8.464863,7.5776286,5.6679263,5.3788805,3.397393,5.304703,5.164922,6.583901,1.2845634,7.291527,5.668963,6.079416,5.5619235,6.1422005,7.036766,5.83771,7.0165467,6.230598,5.540635,5.626541,-0.95878935,2.8409545,3.0202565,2.4669082,1.4755105,3.6637766,-11.122692],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The original codebase can be found at [ai-forever\\u002fKandinsky-3](https:\\u002f\\u002fgithub.com\\u002fai-forever\\u002fKandins...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThis guide will explore the [train_text_to_image_lora.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdif...\"],[\"\\u003cTip\\u003e\\n\\nThe following sections highlight parts of the training script that are important for understa...\"],[\"The script begins by adding the [new LoRA weights](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fdd9...\"],[\"## Launch the script\\n\\nOnce you've made all your changes or you're okay with the default configuratio...\"],[\"Once training has been completed, you can use your model for inference:\\n\\n```py\\nfrom diffusers import...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Text-to-audio (TTA) system has recently gained attention for its a...\"],[\"During inference:\\n\\n* The _quality_ of the predicted audio sample can be controlled by the `num_infer...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"LCM-LoRAs are available for [stable-diffusion-v1-5](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion...\"],[\"pipe = DiffusionPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\",\\n    varian...\"],[\"```python\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretr...\"],[\"# pass prompt and image to pipeline\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt,\\n    i...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm\\u002flcm_...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm\\u002flcm_...\"],[\"prompt = \\\"Mystical fairy in real, magic, 4k picture, high quality\\\"\\nnegative_prompt = \\\"extra digit, f...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm\\u002flcm_...\"],[\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2...\"],[\"```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsave...\"],[\"```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsave...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](http...\"],[\"## Available Pipelines:\\n\\n| Pipeline | Tasks | Colab\\n|---|---|:---:|\\n| [pipeline_stable_diffusion.py]...\"],[\"```\\ngit lfs install\\ngit clone https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-v1-5\\n```\\n\\nand simply ...\"],[\"```python\\nimport requests\\nimport torch\\nfrom PIL import Image\\nfrom io import BytesIO\\n\\nfrom diffusers ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\u003cTip\\u003e\\n\\nMake sure to check out the schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to le...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about MultiDiffusion on the [project page](https:\\u002f\\u002fmultidiffusio...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The original codebase can be found at [openai\\u002fshap-e](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002fshap-e).\\n\\n\\u003cTip\\u003e\\n\\nSee...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"Our examples aspire to be **self-contained**, **easy-to-tweak**, **beginner-friendly** and for **one...\"],[\"We provide **official** examples that cover the most popular tasks of diffusion models.\\n*Official* e...\"],[\"## Community\\n\\nIn addition, we provide **community** examples, which are examples added and maintaine...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Creating noise from data is easy; creating data from noise is gene...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"As reported in the [paper](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2303.17604), ToMe can greatly preserve the ...\"],[\"| **GPU**  | **Resolution** | **Batch size** | **Vanilla** | **ToMe**       | **ToMe + xFormers** |\\n...\"],[\"|          |                |              1 |        1.88 | 1.57 (+16.49%) |      1.57 (+16.49%) |\\n...\"],[\"As seen in the tables above, the speed-up from `tomesd` becomes more pronounced for larger image res...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to lo...\"],[\"```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\nimport requests\\nfrom PIL import ...\"],[\"prompt = \\\"A majestic tiger sitting on a bench\\\"\\nimage = pipeline(prompt, image=init_image, mask_image...\"],[\"Then [`~AutoPipelineForImage2Image.from_pipe`] maps the original `\\\"stable-diffusion\\\"` pipeline class...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Free-form inpainting is the task of adding new content to an image...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Don't forget to save the exported model\\npipeline.save_pretrained(\\\"openvino-sd-v1-5\\\")\\n```\\n\\nTo furth...\"],[\"--\\n{{ card_data }}\\n---\\n\\n\\u003c!-- This model card has been generated automatically according to the infor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Recent text-to-image generative models have demonstrated an unpara...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"You can find lucidrains' DALL-E 2 recreation at [lucidrains\\u002fDALLE2-pytorch](https:\\u002f\\u002fgithub.com\\u002flucid...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*The incredible generative ability of large-scale text-...\"],[\"## Usage example with the base model of StableDiffusion-1.4\\u002f1.5\\n\\nIn the following we give a simple e...\"],[\"Finally, pass the prompt and control image to the pipeline\\n\\n```py\\n# fix the random seed, so you will...\"],[\"pipe.to(\\\"cuda\\\")\\n```\\n\\nFinally, pass the prompt and control image to the pipeline\\n\\n```py\\n# fix the ran...\"],[\"| Model Name | Control Image Overview| Control Image Example | Generated Image Example |\\n|---|---|--...\"],[\"|[TencentARC\\u002ft2iadapter_sketch_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_sketch_sd14v1)\\u003cb...\"],[\"|[TencentARC\\u002ft2iadapter_openpose_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_openpose_sd14v...\"],[\"|[TencentARC\\u002ft2iadapter_seg_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_seg_sd14v1)\\u003cbr\\u002f\\u003e*Tr...\"],[\"## Combining multiple adapters\\n\\n[`MultiAdapter`] can be used for applying multiple conditionings at ...\"],[\"![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fkeypose_depth_...\"],[\"Adapt a model to a new task\\n\\nMany diffusion systems share the same components, allowing you to adapt...\"],[\"```py\\nfrom diffusers import UNet2DConditionModel\\n\\nmodel_id = \\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nunet =...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nimport torch\\nfrom diffusers import ShapEPipeline\\n\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.i...\"],[\"prompt = \\\"A cheeseburger, white background\\\"\\n\\nimage_embeds, negative_image_embeds = prior_pipeline(pr...\"],[\"```py\\nimport torch\\nfrom diffusers import ShapEPipeline\\n\\ndevice = torch.device(\\\"cuda\\\" if torch.cuda.i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*With the advance of text-to-image models (e.g., Stable...\"],[\"Motion Adapter checkpoints can be found under [guoyww](https:\\u002f\\u002fhuggingface.co\\u002fguoyww\\u002f). These checkp...\"],[\"Here are some sample outputs:\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestqualit...\"],[\"# enable memory savings\\npipe.enable_vae_slicing()\\npipe.enable_model_cpu_offload()\\n\\noutput = pipe(\\n  ...\"],[\"scheduler = DDIMScheduler.from_pretrained(\\n    model_id, subfolder=\\\"scheduler\\\", clip_sample=False, t...\"],[\"!--Copyright 2023 The GLIGEN Authors and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the [paper](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2301.07093) is:\\n\\n*Large-scale text-to-im...\"],[\"## StableDiffusionGLIGENTextImagePipeline\\n\\n[[autodoc]] StableDiffusionGLIGENTextImagePipeline\\n\\t- all...\"],[\"Würstchen text-to-image fine-tuning\\n\\n## Running locally with PyTorch\\n\\nBefore running the scripts, ma...\"],[\"## Training with LoRA\\n\\nLow-Rank Adaption of Large Language Models (or LoRA) was first introduced by ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[Improving Sample Quality of Diffusion Models Using Self-Attention Guidance](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"# [Deprecated] Multi Token Textual Inversion\\n\\n**IMPORTART: This research project is deprecated. Mult...\"],[\"## Running locally with PyTorch\\n### Installing the dependencies\\n\\nBefore running the scripts, make su...\"],[\"And launch the training using\\n\\n**___Note: Change the `resolution` to 768 if you are using the [stabl...\"],[\"```bash\\npip install -U -r requirements_flax.txt\\n```\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*By decomposing the image formation process into a sequential appli...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## LDMTextToImagePipeline\\n[[autodoc]] LDMTextToImagePipeline\\n\\t- all\\n\\t- __call__\\n\\n## LDMSuper...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Next, load a LoRA checkpoint with the [`~diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lor...\"],[\"![pixel-art](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers...\"],[\"Impressive! As you can see, the model was able to generate an image that mixes the characteristics o...\"],[\"## Fusing adapters into the model\\n\\nYou can use PEFT to easily fuse\\u002funfuse multiple adapters directly...\"],[\"Consistency Decoder\\n\\nConsistency decoder can be used to decode the latents from the denoising UNet i...\"],[\"# Amused training\\n\\nAmused can be finetuned on simple datasets relatively cheaply and quickly. Using ...\"],[\"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir \\u003coutput path\\u003e \\\\\\n    --train_batch_size \\u003cb...\"],[\"Batch size: 16, Learning rate: 2e-5, Gives decent results in ~750 steps\\n\\n| Batch Size | Gradient Acc...\"],[\"#### Full finetuning + lora\\n\\nBatch size: 16, Learning rate: 8e-4, Gives decent results in 1000-1250 ...\"],[\"### Finetuning the 512 checkpoint\\n\\nThese examples finetune on this [minecraft](https:\\u002f\\u002fhuggingface.c...\"],[\"#### Full finetuning + 8 bit adam\\n\\nBatch size: 8, Learning rate: 5e-6, Gives decent results in 500-1...\"],[\"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir \\u003coutput path\\u003e \\\\\\n    --train_batch_size \\u003cb...\"],[\"Learning rate: 4e-4, Gives decent results in 1500-2000 steps\\n\\nMemory used: 6.5 GB \\n\\n```sh\\naccelerate...\"],[\"# Diffusers examples with Intel optimizations\\n\\n**This research project is not actively maintained by...\"],[\"🧨 Diffusers Experimental\\n\\nWe are adding experimental code to support novel applications and usages o...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Navigate to the example folder with the training script and install the required dependencies for th...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Script parameters\\n\\n\\u003cTip warning={true}\\u003e\\n\\nDreamBooth is very sensitive to training hyperpa...\"],[\"### Min-SNR weighting\\n\\nThe [Min-SNR](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2303.09556) weighting strategy ca...\"],[\"```bash\\naccelerate launch train_dreambooth.py \\\\\\n  --train_text_encoder\\n```\\n\\n## Training script\\n\\nDrea...\"],[\"```py\\n# Load the tokenizer\\nif args.tokenizer_name:\\n    tokenizer = AutoTokenizer.from_pretrained(arg...\"],[\"train_dataloader = torch.utils.data.DataLoader(\\n    train_dataset,\\n    batch_size=args.train_batch_s...\"],[\"```bash\\n--validation_prompt=\\\"a photo of a sks dog\\\"\\n--num_validation_images=4\\n--validation_steps=100\\n...\"],[\"You should also change the default Adam optimizer to DeepSpeed’s optimized version of Adam [`deepspe...\"],[\"```py\\nfrom diffusers import DiffusionPipeline, UNet2DConditionModel\\nfrom transformers import CLIPTex...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## LoRA\\n\\nLoRA is a training technique for significantly reducing the numbe...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"That was super easy, but how did the pipeline do that? Let's breakdown the pipeline and take a look ...\"],[\"```py\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e sample_size = model.config.sample_size\\n\\u003e\\u003e\\u003e noise = torch.randn((1, 3, sa...\"],[\"Let's try it out!\\n\\n## Deconstruct the Stable Diffusion pipeline\\n\\nStable Diffusion is a text-to-image...\"],[\"```py\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import CLIPTextModel, CLIPTok...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nFeel free to choose any prompt you like if you want to generate something else!\\n\\n```py\\n\\u003e\\u003e\\u003e p...\"],[\"\\u003cTip\\u003e\\n\\n💡 The height and width are divided by 8 because the `vae` model has 3 down-sampling layers. Y...\"],[\"...     # compute the previous noisy sample x_t -\\u003e x_t-1\\n...     latents = scheduler.step(noise_pred...\"],[\"Overview\\n\\nThese examples show how to run [Diffuser](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2205.09991) in Diffusers. ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"There are two options for converting a `.ckpt` file: use a Space to convert the checkpoint or conver...\"],[\"```bash\\ncd TemporalNet && git fetch origin refs\\u002fpr\\u002f13:pr\\u002f13\\ngit checkout pr\\u002f13\\n```\\n\\n3. There are sev...\"],[\"## Keras .pb or .h5\\n\\n\\u003cTip warning={true}\\u003e\\n\\n🧪 This is an experimental feature. Only Stable Diffusion ...\"],[\"Click the **Submit** button to automatically convert the KerasCV checkpoint! Once the checkpoint is ...\"],[\"pipeline = StableDiffusionXLPipeline.from_pretrained(\\n    \\\"Lykon\\u002fdreamshaper-xl-1-0\\\", torch_dtype=to...\"],[\"InstructPix2Pix SDXL training example\\n\\n***This is based on the original InstructPix2Pix training exa...\"],[\"Configure environment variables such as the dataset identifier and the Stable Diffusion\\ncheckpoint:\\n...\"],[\"We recommend this type of validation as it can be useful for model debugging. Note that you need `wa...\"],[\"## Inference\\n\\n Once training is complete, we can perform inference:\\n\\n ```python\\nimport PIL\\nimport re...\"],[\"## Compare between SD and SDXL\\n\\nWe aim to understand the differences resulting from the use of SD-1....\"],[\"The following two GIFs provide intuitive visual results. We observed, for each step, what kind of re...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Here's the overview from the [project page](https:\\u002f\\u002fvislearn.github.io\\u002fControlNet-XS\\u002f):\\n\\n*With incre...\"],[\"ControlNet training example\\n\\n[Adding Conditional Control to Text-to-Image Diffusion Models](https:\\u002f\\u002f...\"],[\"Our training examples use [Stable Diffusion 1.5](https:\\u002f\\u002fhuggingface.co\\u002frunwayml\\u002fstable-diffusion-v1...\"],[\"```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path to save model\\\"\\n\\nac...\"],[\"## Example results\\n\\n#### After 300 steps with batch size 8\\n\\n| |  | \\n|-------------------|:----------...\"],[\"## Training on a 16 GB GPU\\n\\nOptimizations:\\n- Gradient checkpointing\\n- bitsandbyte's 8-bit optimizer\\n...\"],[\"Optimizations:\\n- Gradient checkpointing\\n- xformers\\n- set grads to none\\n- DeepSpeed stage 2 with para...\"],[\"```py\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepSchedu...\"],[\"When connected install JAX `0.4.5`:\\n\\n```\\npip install \\\"jax[tpu]==0.4.5\\\" -f https:\\u002f\\u002fstorage.googleapis...\"],[\"```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"runs\\u002ffill-circle-{times...\"],[\"```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"runs\\u002funcanny-faces-{tim...\"],[\"We also support gradient accumulation - it is a technique that lets you use a bigger batch size than...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"2. Pass a prompt to the pipeline to generate an image:\\n\\n```py\\nimage = pipeline(\\n\\t\\\"stained glass of d...\"],[\"### Stable Diffusion XL\\n\\nSDXL is a much larger version of the previous Stable Diffusion models, and ...\"],[\"### ControlNet\\n\\nControlNet models are auxiliary models or adapters that are finetuned on top of text...\"],[\"\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhu...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2I...\"],[\"\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhu...\"],[\"\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhu...\"],[\"There are several ways to exert more control over how an image is generated outside of configuring a...\"],[\"There are many types of conditioning inputs you can use, and 🤗 Diffusers supports ControlNet for Sta...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The architecture of Stable Diffusion 2 is more or less identical to the original [Stable Diffusion m...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Text-to-image\\n\\n```py\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler...\"],[\"# let's download an  image\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhf-internal-testing\\u002fdiffusers-imag...\"],[\"# Textual Inversion fine-tuning example\\n\\n[Textual inversion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618) is a ...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport DATA_DIR=\\\"path-to-dir-containing-di...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\n# uncomment to install the necessary libraries in Colab\\n#!pip install diffusers[training]\\n```\\n...\"],[\"\\u003e\\u003e\\u003e config = TrainingConfig()\\n```\\n\\n## Load the dataset\\n\\nYou can easily load the [Smithsonian Butterf...\"],[\"```py\\n\\u003e\\u003e\\u003e from torchvision import transforms\\n\\n\\u003e\\u003e\\u003e preprocess = transforms.Compose(\\n...     [\\n...    ...\"],[\"Pretrained models in 🧨 Diffusers are easily created from their model class with the parameters you w...\"],[\"Great! Next, you'll need a scheduler to add some noise to the image.\\n\\n## Create a scheduler\\n\\nThe sch...\"],[\"First, you'll need an optimizer and a learning rate scheduler:\\n\\n```py\\n\\u003e\\u003e\\u003e from diffusers.optimizatio...\"],[\"\\u003cTip\\u003e\\n\\n💡 The training loop below may look intimidating and long, but it'll be worth it later when yo...\"],[\"...     global_step = 0\\n\\n...     # Now you train the model\\n...     for epoch in range(config.num_epo...\"],[\"...         # After each epoch you optionally sample some demo images with evaluate() and save the m...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"ConsistencyDecoderScheduler\\n\\nThis scheduler is a part of the [`ConsistencyDecoderPipeline`] and was ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The callback function should have the following arguments:\\n\\n* `pipe` (or the pipeline instance) prov...\"],[\"```py\\nimport torch\\nfrom diffusers import StableDiffusionPipeline\\n\\npipe = StableDiffusionPipeline.fro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Ethical guidelines\\n\\nThe following ethical guidelines apply generally, but we will primarily imple...\"],[\"- [**Community tab**](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002frepositories-pull-requests-discussions): it en...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| Input                                                                           | Output          ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion text-to-image fine-tuning\\n\\nThe `train_text_to_image.py` script shows how to fine-tu...\"],[\"Run the following command to authenticate your token\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nIf you have...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport TRAIN_DIR=\\\"path_to_your_dataset\\\"\\n\\na...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-bl...\"],[\"## Training with LoRA\\n\\nLow-Rank Adaption of Large Language Models was first introduced by Microsoft ...\"],[\"**___Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https:\\u002f\\u002fhuggingf...\"],[\"The final LoRA embedding weights have been uploaded to [sayakpaul\\u002fsd-model-finetuned-lora-t4](https:...\"],[\"## Training with Flax\\u002fJAX\\n\\nFor faster training on TPUs and GPUs you can leverage the flax training e...\"],[\"### Training with xFormers:\\n\\nYou can enable memory efficient attention by [installing xFormers](http...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Diffusers examples with ONNXRuntime optimizations\\n\\n**This research project is not actively maintai...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The Intel Labs Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicensed u...\"],[\"The abstract from the paper is:\\n\\n*This research paper proposes a Latent Diffusion Model for 3D (LDM3...\"],[\"# Upscaler\\n\\n[LDM3D-VR](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f2311.03226.pdf) is an extended version of LDM3D. \\n\\nThe ...\"],[\"# Training examples\\n\\nCreating a training image set is [described in a different document](https:\\u002f\\u002fhu...\"],[\"Models\\n\\nFor more detail on the models, please refer to the [docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffus...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Diffusion models have significantly advanced the fields of image, ...\"],[\"!--Copyright 2023 Custom Diffusion authors The HuggingFace Team. All rights reserved.\\n\\nLicensed unde...\"],[\"Before running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https:\\u002f...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Script parameters\\n\\nThe training script contains all the parameters to help you customize ...\"],[\"Many of the parameters for prior preservation loss are described in the [DreamBooth](dreambooth#prio...\"],[\"Next, the `modifier_token` is [added to the tokenizer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob...\"],[\"```py\\nst = unet.state_dict()\\nfor name, _ in unet.attn_processors.items():\\n    cross_attention_dim = ...\"],[\"The [optimizer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f84cd9e8d01adb47f046b1ee449fc76a0c32dc4...\"],[\"Set the environment variable `MODEL_NAME` to a model id on the Hub or a path to a local model, `INST...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"multiple concepts\\\"\\u003e\\n\\nCustom Diffusion can also learn multiple concepts if ...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"multiple concepts\\\"\\u003e\\n\\n```py\\nimport torch\\nfrom huggingface_hub.repocard impo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to le...\"],[\"Stable Diffusion text-to-image fine-tuning\\n\\nThe `train_text_to_image.py` script shows how to fine-tu...\"],[\"Run the following command to authenticate your token\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nIf you have...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Prompt weighting works by increasing or decreasing the scale of the text embedding vector that corre...\"],[\"prompt = \\\"a red cat playing with a ball\\\"\\n\\ngenerator = torch.Generator(device=\\\"cpu\\\").manual_seed(33)\\n...\"],[\"generator = torch.manual_seed(33)\\n\\nimage = pipe(prompt_embeds=prompt_embeds, generator=generator, nu...\"],[\"image = pipe(prompt_embeds=prompt_embeds, generator=generator, num_inference_steps=20).images[0]\\nima...\"],[\"```py\\nprompt_embeds = compel_proc('(\\\"A red cat++ playing with a ball \\u003cmidjourney-style\\u003e\\\")')\\n\\nimage =...\"],[\"## Stable Diffusion XL\\n\\nStable Diffusion XL (SDXL) has two tokenizers and text encoders so it's usag...\"],[\"\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhf...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\npretrained_model_name_or_path = \\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nrepo_id_embeds = \\\"sd-concepts...\"],[\"file = hf_hub_download(\\\"dn118\\u002funaestheticXL\\\", filename=\\\"unaestheticXLv31.safetensors\\\")\\nstate_dict = ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find the original codebase for Stable Diffusion v1.0 at [CompVis\\u002fstable-diffusion](https:\\u002f\\u002fg...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cdiv class=\\\"rounded-xl border border-gray-200\\\"\\u003e\\n    \\u003ctable cla...\"],[\"\\u003c\\u002ftd\\u003e\\n        \\u003c\\u002ftr\\u003e\\n        \\u003ctr\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003e\\n            \\u003ca hre...\"],[\"\\u003c\\u002ftd\\u003e\\n        \\u003c\\u002ftr\\u003e\\n        \\u003ctr\\u003e\\n            \\u003ctd class=\\\"px-4 py-2 text-gray-700\\\"\\u003e\\n            \\u003ca hre...\"],[\"## Tips\\n\\nTo help you get the most out of the Stable Diffusion pipelines, here are a few tips for imp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"If a community pipeline doesn't work as expected, please open a GitHub issue and mention the author....\"],[\"diffuser_pipeline.enable_attention_slicing()\\ndiffuser_pipeline = diffuser_pipeline.to(device)\\n\\npromp...\"],[\"\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercont...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Speed\\n\\n\\u003cTip\\u003e\\n\\n💡 If you don't have access to a GPU, you can use one for free from a GPU provider l...\"],[\"\\u003cTip\\u003e\\n\\n💡 We strongly suggest always running your pipelines in `float16`, and so far, we've rarely se...\"],[\"```python\\nfrom diffusers import DPMSolverMultistepScheduler\\n\\npipeline.scheduler = DPMSolverMultistep...\"],[\"```python\\npipeline.enable_attention_slicing()\\n```\\n\\nNow try increasing the `batch_size` to 8!\\n\\n```pyt...\"],[\"### Better pipeline components\\n\\nYou can also try replacing the current pipeline components with a ne...\"],[\"Pretty impressive! Let's tweak the second image - corresponding to the `Generator` with a seed of `1...\"],[\"- Learn how [PyTorch 2.0](.\\u002foptimization\\u002ftorch2.0) and [`torch.compile`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fst...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Text-to-image diffusion models have recently received a lot of int...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## VQModel\\n\\n[[autodoc]] VQModel\\n\\n## VQEncoderOutput\\n\\n[[autodoc]] models.vq_model.VQEncoderOutput...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We present ControlNet, a neural network architecture to add spatia...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIf you don't see a checkpoint you're interested in, you can train your own SDXL ControlNet w...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Inspired by [Stable Diffusion](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fapi\\u002fpipelines\\u002fstable_diffusion\\u002f...\"],[\"The abstract of the paper is the following:\\n\\n*Although audio generation shares commonalities across ...\"],[\"All checkpoints share the same model size for the text encoders and VAE. They differ in the size and...\"],[\"### Evaluating generated waveforms:\\n\\n* The quality of the generated waveforms can vary significantly...\"],[\"!---\\nCopyright 2022 - The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License,...\"],[\"🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating ima...\"],[\"```sh\\nconda install -c conda-forge diffusers\\n```\\n\\n### Flax\\n\\nWith `pip` (official package):\\n\\n```bash\\n...\"],[\"| **Documentation**                                                   | **What can I learn?**       ...\"],[\"| [Training](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002ftraining\\u002foverview) | Guides for how to train a di...\"],[\"We ❤️  contributions from the open-source community!\\nIf you want to contribute to this library, plea...\"],[\"\\u003ctable\\u003e\\n  \\u003ctr\\u003e\\n    \\u003cth\\u003eTask\\u003c\\u002fth\\u003e\\n    \\u003cth\\u003ePipeline\\u003c\\u002fth\\u003e\\n    \\u003cth\\u003e🤗 Hub\\u003c\\u002fth\\u003e\\n  \\u003c\\u002ftr\\u003e\\n  \\u003ctr style=\\\"borde...\"],[\"\\u003c\\u002ftr\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003eText-guided Image-to-Image\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e\\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdi...\"],[\"## Popular libraries using 🧨 Diffusers\\n\\n- https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fTaskMatrix\\n- https:\\u002f\\u002fgithub.c...\"],[\"## Citation\\n\\n```bibtex\\n@misc{von-platen-etal-2022-diffusers,\\n  author = {Patrick von Platen and Sura...\"],[\"Stable Diffusion text-to-image fine-tuning\\nThis extended LoRA training script was authored by [haofa...\"],[\"With LoRA, it's possible to fine-tune Stable Diffusion on a custom image-caption pair dataset\\non con...\"],[\"The above command will also run inference as fine-tuning progresses and log the results to Weights a...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"We provide a high level explanation of how the generation can be controlled as well as a snippet of ...\"],[\"|                     **Method**                      | **Inference only** | **Requires training \\u002f\\u003cb...\"],[\"|              [DreamBooth](#dreambooth)              |         ❌         |                   ✅     ...\"],[\"[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2211.09800)\\n\\n[InstructPix2Pix](..\\u002fapi\\u002fpipelines\\u002fpix2pix) is fine-tuned...\"],[\"\\u003cTip\\u003e\\n\\nPix2Pix Zero is the first model that allows \\\"zero-shot\\\" image editing. This means that the mo...\"],[\"## Semantic Guidance (SEGA)\\n\\n[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2301.12247)\\n\\n[SEGA](..\\u002fapi\\u002fpipelines\\u002fsema...\"],[\"It conditions on a monocular depth estimate of the original image.\\n\\n## MultiDiffusion Panorama\\n\\n[Pap...\"],[\"## Custom Diffusion\\n\\n[Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2212.04488)\\n\\n[Custom Diffusion](..\\u002ftraining\\u002fcusto...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Sliced VAE\\n\\nSliced VAE enables decoding large batches of images with limited VRAM or batches with...\"],[\"```python\\nimport torch\\nfrom diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\\n\\npipe ...\"],[\"\\u003cTip\\u003e\\n\\nConsider using [model offloading](#model-offloading) if you want to optimize for speed becaus...\"],[\"prompt = \\\"a photo of an astronaut riding a horse on mars\\\"\\npipe.enable_model_cpu_offload()\\nimage = pi...\"],[\"To trace a UNet:\\n\\n```python\\nimport time\\nimport torch\\nfrom diffusers import StableDiffusionPipeline\\ni...\"],[\"# save the model\\nunet_traced.save(\\\"unet_traced.pt\\\")\\n```\\n\\nReplace the `unet` attribute of the pipelin...\"],[\"pipe = DiffusionPipeline.from_pretrained(\\n    \\\"runwayml\\u002fstable-diffusion-v1-5\\\",\\n    torch_dtype=torc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\n- SDXL Turbo uses the exact same architecture as [SDXL](.\\u002fstable_diffusion_xl), which means...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Loading from the original format\\n\\nBy default the [`AutoencoderKL`] should be loaded with [`~Model...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"We enormously value feedback from the community, so please do not be afraid to speak up if you belie...\"],[\"In the following, we give an overview of different ways to contribute, ranked by difficulty in ascen...\"],[\"As said before, **all contributions are valuable to the community**.\\nIn the following, we will expla...\"],[\"**Please** keep in mind that the more effort you put into asking or answering a question, the higher...\"],[\"The 🧨 Diffusers library is robust and reliable thanks to the users who notify us of\\nthe problems the...\"],[\"New issues usually include the following.\\n\\n#### 2.1. Reproducible, minimal bug reports\\n\\nA bug report...\"],[\"#### 2.2. Feature requests\\n\\nA world-class feature request addresses the following points:\\n\\n1. Motiva...\"],[\"You can open an issue about feedback [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fissues\\u002fnew?assi...\"],[\"### 3. Answering issues on the GitHub issues tab\\n\\nAnswering issues on GitHub might require some tech...\"],[\"### 4. Fixing a \\\"Good first issue\\\"\\n\\n*Good first issues* are marked by the [Good first issue](https:\\u002f...\"],[\"Contributing to the library can have many forms:\\n\\n- Correcting spelling or grammatical errors.\\n- Cor...\"],[\"- Official Pipelines\\n- Community Pipelines\\n\\nBoth official and community pipelines follow the same de...\"],[\"Community pipeline PRs are only checked at a superficial level and ideally they should be maintained...\"],[\"```bash\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\n```\\n\\nas well as to install all additional...\"],[\"To contribute an example, it is highly recommended to look at already existing examples such as [dre...\"],[\"### 8. Fixing a \\\"Good second issue\\\"\\n\\n*Good second issues* are marked by the [Good second issue](http...\"],[\"Diffusers has a couple of open feature requests for all three components - feel free to gloss over t...\"],[\"If you are unsure or stuck in the PR, don't hesitate to leave a message to ask for a first review or...\"],[\"1. Make sure that you've used the correct template for your issue. You can pick between *Bug Report*...\"],[\"4. **Minimalistic**: Try to help the reader as much as you can to understand the issue as quickly as...\"],[\"## How to write a good PR...\"],[\"1. Be a chameleon. Understand existing design patterns and syntax and make sure your code additions ...\"],[\"[`hf-internal-testing`](https:\\u002f\\u002fhuggingface.co\\u002fhf-internal-testing) or [huggingface\\u002fdocumentation-im...\"],[\"## How to open a PR\\n\\nBefore writing code, we strongly advise you to search through the existing PRs ...\"],[\"Before you run the tests, please make sure you install the dependencies required for testing. You ca...\"],[\"An extensive test suite is included to test the library behavior and several examples. Library tests...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Our current collection of training scripts include:\\n\\n| Training | SDXL-support | LoRA-support | Flax...\"],[\"These examples are **actively** maintained, so please feel free to open an issue if they aren't work...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"![teaser-img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffuser...\"],[\"model_id = \\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\npipe = TextToVideoZeroPipeline.from_pretrained(model_id,...\"],[\"# Concatenate chunks and save\\nresult = np.concatenate(result)\\nresult = [(r * 255).astype(\\\"uint8\\\") fo...\"],[\"# Set the attention processor\\n    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2)...\"],[\"### Text-To-Video with Edge Control\\n\\nTo generate a video from prompt with additional Canny edge cont...\"],[\"### DreamBooth specialization\\n\\nMethods **Text-To-Video**, **Text-To-Video with Pose Control** and **...\"],[\"# fix latents for all frames\\n    latents = torch.randn((1, 4, 64, 64), device=\\\"cuda\\\", dtype=torch.fl...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nA ControlNet model has two sets of weights (or blocks) connected by a zero-convolution layer...\"],[\"image = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concate...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentat...\"],[\"```py\\nfrom diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultiste...\"],[\"Load an initial image and a mask image:\\n\\n```py\\nfrom diffusers.utils import load_image, make_image_gr...\"],[\"Load a ControlNet model conditioned on inpainting and pass it to the [`StableDiffusionControlNetInpa...\"],[\"\\u003cTip\\u003e\\n\\nGuess mode does not have any impact on prompt conditioning and you can still provide a prompt...\"],[\"## ControlNet with Stable Diffusion XL\\n\\nThere aren't too many ControlNet models compatible with Stab...\"],[\"Load a SDXL ControlNet model conditioned on canny edge detection and pass it to the [`StableDiffusio...\"],[\"You can use [`StableDiffusionXLControlNetPipeline`] in guess mode as well by setting the parameter t...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nYou can compose multiple ControlNet conditionings from different image inputs to create a *M...\"],[\"image = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image....\"],[\"Load a list of ControlNet models that correspond to each conditioning, and pass them to the [`Stable...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\t\\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhugg...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"We enormously value feedback from the community, so please do not be afraid to speak up if you belie...\"],[\"In the following, we give an overview of different ways to contribute, ranked by difficulty in ascen...\"],[\"As said before, **all contributions are valuable to the community**.\\nIn the following, we will expla...\"],[\"**Please** keep in mind that the more effort you put into asking or answering a question, the higher...\"],[\"The 🧨 Diffusers library is robust and reliable thanks to the users who notify us of\\nthe problems the...\"],[\"New issues usually include the following.\\n\\n#### 2.1. Reproducible, minimal bug reports\\n\\nA bug report...\"],[\"#### 2.2. Feature requests\\n\\nA world-class feature request addresses the following points:\\n\\n1. Motiva...\"],[\"You can open an issue about feedback [here](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fissues\\u002fnew?assi...\"],[\"### 3. Answering issues on the GitHub issues tab\\n\\nAnswering issues on GitHub might require some tech...\"],[\"### 4. Fixing a \\\"Good first issue\\\"\\n\\n*Good first issues* are marked by the [Good first issue](https:\\u002f...\"],[\"Contributing to the library can have many forms:\\n\\n- Correcting spelling or grammatical errors.\\n- Cor...\"],[\"- Official Pipelines\\n- Community Pipelines\\n\\nBoth official and community pipelines follow the same de...\"],[\"Community pipeline PRs are only checked at a superficial level and ideally they should be maintained...\"],[\"```bash\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\n```\\n\\nas well as to install all additional...\"],[\"To contribute an example, it is highly recommended to look at already existing examples such as [dre...\"],[\"### 8. Fixing a \\\"Good second issue\\\"\\n\\n*Good second issues* are marked by the [Good second issue](http...\"],[\"Diffusers has a couple of open feature requests for all three components - feel free to gloss over t...\"],[\"1. Make sure that you've used the correct template for your issue. You can pick between *Bug Report*...\"],[\"4. **Minimalistic**: Try to help the reader as much as you can to understand the issue as quickly as...\"],[\"## How to write a good PR...\"],[\"1. Be a chameleon. Understand existing design patterns and syntax and make sure your code additions ...\"],[\"[`hf-internal-testing`](https:\\u002f\\u002fhuggingface.co\\u002fhf-internal-testing) or [huggingface\\u002fdocumentation-im...\"],[\"## How to open a PR\\n\\nBefore writing code, we strongly advise you to search through the existing PRs ...\"],[\"Before you run the tests, please make sure you install the dependencies required for testing. You ca...\"],[\"An extensive test suite is included to test the library behavior and several examples. Library tests...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## DPMSolverSinglestepScheduler\\n[[autodoc]] DPMSolverSinglestepScheduler\\n\\n## SchedulerOutput\\n[[autod...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PriorTransformer\\n\\n[[autodoc]] PriorTransformer\\n\\n## PriorTransformerOutput\\n\\n[[autodoc]] models.pri...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"To load any community pipeline on the Hub, pass the repository id of the community pipeline to the `...\"],[\"For more information about community pipelines, take a look at the [Community pipelines](custom_pipe...\"],[\"```python\\nfrom transformers import CLIPFeatureExtractor\\n\\nfeature_extractor = CLIPFeatureExtractor.fr...\"],[\"Once everything is in place, you can initialize the `TextToVideoIFPipeline` with the `ShowOneUNet3DC...\"],[\"As an additional reference example, you can refer to the repository structure of [stabilityai\\u002fjapane...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We present the vector quantized diffusion (VQ-Diffusion) model for...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom optimum.habana import GaudiConfig\\nfrom optimum.habana.diffusers import GaudiDDIMSched...\"],[\"For [Stable Diffusion v2.1](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fstable-diffusion-2-1) on 768x768 imag...\"],[\"DreamBooth training example for Stable Diffusion XL (SDXL)\\n\\n[DreamBooth](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208....\"],[\"Or if your environment doesn't support an interactive shell (e.g., a notebook)\\n\\n```python\\nfrom accel...\"],[\"To better track our training experiments, we're using the following flags in the command above:\\n\\n* `...\"],[\"We can further refine the outputs with the [Refiner](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fstable-diffu...\"],[\"* SDXL has two text encoders. So, we fine-tune both using LoRA.\\n* When not fine-tuning the text enco...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nfrom diffusers import DiffusionPipeline\\n\\npipeline = DiffusionPipeline.from_pretrained(\\\"runwaym...\"],[\"## Why use safetensors?\\n\\nThere are several reasons for using safetensors:\\n\\n- Safety is the number on...\"],[\"Deprecated Pipelines\\n\\nThis folder contains pipelines that have very low usage as measured by model d...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is as follows:\\n\\n*Latent Diffusion models (LDMs) have achieved remarkable r...\"],[\"## LatentConsistencyModelImg2ImgPipeline\\n\\n[[autodoc]] LatentConsistencyModelImg2ImgPipeline\\n    - al...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https:\\u002f...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe training script provides many parameters to help you customize your training run. All of...\"],[\"### Min-SNR weighting\\n\\nThe [Min-SNR](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2303.09556) weighting strategy ca...\"],[\"```py\\ntokenizer_one = AutoTokenizer.from_pretrained(\\n    args.pretrained_model_name_or_path, subfold...\"],[\"After calculating the embeddings, the text encoder, VAE, and tokenizer are deleted to free up some m...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport VAE_NAME=\\\"madeby...\"],[\"```py\\nfrom diffusers import DiffusionPipeline\\nimport torch\\nimport torch_xla.core.xla_model as xm\\n\\nde...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Schedulers\\n\\nFor more information on the schedulers, please refer to the [docs](https:\\u002f\\u002fhuggingface.c...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage\\n\\nBefore you can use IF, you need to accept its usage conditions. To do so:\\n1. Make sure to ...\"],[\"- *Stage-3*\\n  - [stabilityai\\u002fstable-diffusion-x4-upscaler](https:\\u002f\\u002fhuggingface.co\\u002fstabilityai\\u002fstable...\"],[\"# stage 2\\nstage_2_output = stage_2(\\n    image=stage_1_output,\\n    prompt_embeds=prompt_embeds,\\n    n...\"],[\"# stage 2\\nstage_2 = IFImg2ImgSuperResolutionPipeline.from_pretrained(\\n    \\\"DeepFloyd\\u002fIF-II-L-v1.0\\\", ...\"],[\"**Note**: You can also directly move the weights of the text-to-image pipelines to the image-to-imag...\"],[\"# stage 2\\nstage_2_output = stage_2(\\n    image=stage_1_output,\\n    original_image=original_image,\\n   ...\"],[\"Or with the `timesteps` argument:\\n\\n```py\\nfrom diffusers.pipelines.deepfloyd_if import fast27_timeste...\"],[\"text_encoder = T5EncoderModel.from_pretrained(\\n    \\\"DeepFloyd\\u002fIF-I-XL-v1.0\\\", subfolder=\\\"text_encoder...\"],[\"#pt_to_pil(stage_1_output)[0].save(\\\".\\u002fif_stage_I.png\\\")\\n\\n# Remove the pipeline so we can load the sup...\"],[\"## IFPipeline\\n[[autodoc]] IFPipeline\\n\\t- all\\n\\t- __call__\\n\\n## IFSuperResolutionPipeline\\n[[autodoc]] IF...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"pipeline = DiffusionPipeline.from_pretrained(\\n    \\\"runwayml\\u002fstable-diffusion-v1-5\\\", torch_dtype=torc...\"],[\"```py\\nfrom diffusers import DiffusionPipeline\\nimport torch\\n\\npipeline = DiffusionPipeline.from_pretra...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Then navigate to the example folder containing the training script and install the required dependen...\"],[\"\\u003cTip\\u003e\\n\\nThe following sections highlight parts of the training script that are important for understa...\"],[\"### Min-SNR weighting\\n\\nThe [Min-SNR](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2303.09556) weighting strategy ca...\"],[\"Within the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f64603389da01082055a901f2883c4810...\"],[\"## Launch the script\\n\\nNow you're ready to launch the training script! 🚀\\n\\nThis guide uses the [fusing...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"8GB\\\"\\u003e\\n\\nOn a 8GB GPU, you'll need to use [DeepSpeed](https:\\u002f\\u002fwww.deepspeed....\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n\\u003chfoptions id=\\\"training-inference\\\"\\u003e\\n\\u003chfoption id=\\\"PyTorch\\\"\\u003e\\n\\n```bash\\nexpor...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```bash\\npython3 train_controlnet_flax.py \\\\\\n --pretrained_model_name_or_path=$MODEL_DIR \\\\\\n --...\"],[\"## Next steps\\n\\nCongratulations on training your own ControlNet! To learn more about how to use your ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usability over Performance\\n\\n- While Diffusers has many built-in performance-enhancing features (s...\"],[\"## Simple over easy\\n\\nAs PyTorch states, **explicit is better than implicit** and **simple is better ...\"],[\"## Tweakable, contributor-friendly over abstraction\\n\\nFor large parts of the library, Diffusers adopt...\"],[\"At Hugging Face, we call this design the **single-file policy** which means that almost all of the c...\"],[\"### Pipelines\\n\\nPipelines are designed to be easy to use (therefore do not follow [*Simple over easy*...\"],[\"The following design principles are followed:\\n- Pipelines follow the single-file policy. All pipelin...\"],[\"- Pipelines should be very readable, self-explanatory, and easy to tweak.\\n- Pipelines should be desi...\"],[\"### Models\\n\\nModels are designed as configurable toolboxes that are natural extensions of [PyTorch's ...\"],[\"The following design principles are followed:\\n- Models correspond to **a type of model architecture*...\"],[\"- Models should be designed to be easily extendable to future changes. This can be achieved by limit...\"],[\"### Schedulers\\n\\nSchedulers are responsible to guide the denoising process for inference as well as t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion weights (or checkpoints) are stored in the PyTorch format, so you need to convert t...\"],[\"## Selecting the Core ML Variant to Use\\n\\nStable Diffusion models can be converted to different Core ...\"],[\"- The supported inference framework.\\n    * `packages` are suitable for Python inference. This can be...\"],[\"```Python\\nfrom huggingface_hub import snapshot_download\\nfrom pathlib import Path\\n\\nrepo_id = \\\"apple\\u002fc...\"],[\"## Core ML inference in Swift\\n\\nRunning inference in Swift is slightly faster than in Python because ...\"],[\"For more details, please refer to the [instructions in Apple's repo](https:\\u002f\\u002fgithub.com\\u002fapple\\u002fml-sta...\"],[\"If you feel strongly about any missing features, please feel free to open a feature request or, bett...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```bash\\ncd examples\\u002fwuerstchen\\u002ftext_to_image\\npip install -r requirements.txt\\n```\\n\\n\\u003cTip\\u003e\\n\\n🤗 Accelerat...\"],[\"For example, to speedup training with mixed precision using the fp16 format, add the `--mixed_precis...\"],[\"optimizer = optimizer_cls(\\n    prior.parameters(),\\n    lr=args.learning_rate,\\n    betas=(args.adam_b...\"],[\"## Launch the script\\n\\nOnce you’ve made all your changes or you’re okay with the default configuratio...\"],[\"## Next steps\\n\\nCongratulations on training a Wuerstchen model! To learn more about how to use your n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide explores the [train_text_to_image_prior.py](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob...\"],[\"\\u003cTip\\u003e\\n\\nThe following sections highlight parts of the training scripts that are important for underst...\"],[\"Add the `--snr_gamma` parameter and set it to the recommended value of 5.0:\\n\\n```bash\\naccelerate laun...\"],[\"Kandinsky uses a [`PriorTransformer`] to generate the image embeddings, so you'll want to setup the ...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"decoder model\\\"\\u003e\\n\\nThe [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fb...\"],[\"If you want to learn more about how the training loop works, check out the [Understanding pipelines,...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\u003chfoptions id=\\\"training-inference\\\"\\u003e\\n\\u003chfoption id=\\\"prior model\\\"\\u003e\\n\\n```bash\\nexport DATASET_NAME...\"],[\"pipe.enable_model_cpu_offload()\\nprompt=\\\"A robot pokemon, 4k photo\\\"\\nimage = pipeline(prompt=prompt, n...\"],[\"InstructPix2Pix training example\\n\\n[InstructPix2Pix](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2211.09800) is a method to...\"],[\"## Running locally with PyTorch\\n\\n### Installing the dependencies\\n\\nBefore running the scripts, make s...\"],[\"```bash\\nexport MODEL_NAME=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport DATASET_ID=\\\"fusing\\u002finstructpix2pix...\"],[\"We recommend this type of validation as it can be useful for model debugging. Note that you need `wa...\"],[\"url = \\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fsayakpaul\\u002fsample-datasets\\u002fresolve\\u002fmain\\u002ftest_pix2pix_4.png\\\"\\n\\n\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"1. `rescale_betas_zero_snr=True`, rescales the noise schedule to zero terminal signal-to-noise ratio...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## DPMSolverMultistepScheduler\\n[[autodoc]] DPMSolverMultistepScheduler\\n\\n## SchedulerOutput\\n[[autodoc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\n- Using SDXL with a DPM++ scheduler for less than 50 steps is known to produce [visual arti...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In order from the least verbose to the most verbose:\\n\\n|                                             ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Training an unconditional diffusion model\\n\\nCreating a training image set is [described in a differ...\"],[\"### Unconditional Pokemon\\n\\nThe command to train a DDPM UNet model on the Pokemon dataset:\\n\\n```bash\\na...\"],[\"Below, we explain both in more detail.\\n\\n#### Provide the dataset as a folder\\n\\nIf you provide your ow...\"],[\"Next, push it to the hub!\\n\\n```python\\n# assuming you have ran the huggingface-cli login command in a ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*The most advanced text-to-image (T2I) models require significant t...\"],[\"You can find the original codebase at [PixArt-alpha\\u002fPixArt-alpha](https:\\u002f\\u002fgithub.com\\u002fPixArt-alpha\\u002fPi...\"],[\"text_encoder = T5EncoderModel.from_pretrained(\\n    \\\"PixArt-alpha\\u002fPixArt-XL-2-1024-MS\\\",\\n    subfolder...\"],[\"By deleting components you aren't using and flushing the GPU VRAM, you should be able to run [`PixAr...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Scenarios\\n\\nWe cover Diffusion models with the following pipelines:\\n\\n- Text-guided image generatio...\"],[\"These benchmarks allow for side-by-side human evaluation of different image generation models.\\n\\nFor ...\"],[\"```python\\nimport torch\\n\\nseed = 0\\ngenerator = torch.manual_seed(seed)\\n\\nimages = sd_pipeline(sample_pr...\"],[\"Let's first load a [`StableDiffusionPipeline`]:\\n\\n```python\\nfrom diffusers import StableDiffusionPipe...\"],[\"```python\\nseed = 0\\ngenerator = torch.manual_seed(seed)\\n\\nimages = sd_pipeline(prompts, num_images_per...\"],[\"Here is one example:\\n\\n![edit-instruction](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002freso...\"],[\"```python\\nidx = 0\\nprint(f\\\"Original caption: {dataset[idx]['input']}\\\")\\nprint(f\\\"Edit instruction: {dat...\"],[\"input_images = []\\noriginal_captions = []\\nmodified_captions = []\\nedited_images = []\\n\\nfor idx in range...\"],[\"def preprocess_image(self, image):\\n        image = self.image_processor(image, return_tensors=\\\"pt\\\")[...\"],[\"Let's put `DirectionalSimilarity` to use now.\\n\\n```python\\ndir_similarity = DirectionalSimilarity(toke...\"],[\"\\u003cTip\\u003e\\n\\nBoth CLIP score and CLIP direction similarity rely on the CLIP model, which can make the eval...\"],[\"FID aims to measure how similar are two datasets of images. As per [this resource](https:\\u002f\\u002fmmgenerat...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002f...\"],[\"fid = FrechetInceptionDistance(normalize=True)\\nfid.update(real_images, real=True)\\nfid.update(fake_im...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*This paper proposes a unified diffusion framework (dubbed UniDiffu...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThis pipeline was contributed by [dg845](https:\\u002f\\u002fgithub.com\\u002fdg845). ❤️\\n\\n## Usage Examples\\n\\nB...\"],[\"You can also generate only an image or only text (which the UniDiffuser paper calls \\\"marginal\\\" gener...\"],[\"device = \\\"cuda\\\"\\nmodel_id_or_path = \\\"thu-ml\\u002funidiffuser-v1\\\"\\npipe = UniDiffuserPipeline.from_pretraine...\"],[\"### Text Variation\\n\\nSimilarly, text variation can be performed on an input prompt with a text-to-ima...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nInitialize an 🤗 Accelerate environment:\\n\\n```bash\\naccelerate config\\n```\\n\\nTo setup a default 🤗...\"],[\"- `--original_image_column`: the original image before the edits are made\\n- `--edited_image_column`:...\"],[\"```py\\noptimizer = optimizer_cls(\\n    unet.parameters(),\\n    lr=args.learning_rate,\\n    betas=(args.a...\"],[\"```py\\nencoder_hidden_states = text_encoder(batch[\\\"input_ids\\\"])[0]\\noriginal_image_embeds = vae.encode...\"],[\"Set the `MODEL_NAME` environment variable to the name of the model (can be a model id on the Hub or ...\"],[\"pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\\\"your_cool_model\\\", torch_dtype=tor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nimage.save(\\\"generated_image.png\\\")\\n```\\n\\nYou can also try experimenting with the `num_inference_...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nfrom diffusers import StableDiffusionXLPipeline\\nimport torch\\n\\npipeline = StableDiffusionXLPipe...\"],[\"# use from_pipe to avoid consuming additional memory when loading a checkpoint\\npipeline = AutoPipeli...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Image generation has recently seen tremendous advances, with diffu...\"],[\"* The pipeline can generate masks that can be fed into other inpainting pipelines.\\n* In order to gen...\"],[\"* Swap the `source_prompt` and `target_prompt` in the arguments to `generate_mask`.\\n    * Change the...\"],[\"## StableDiffusionDiffEditPipeline\\n[[autodoc]] StableDiffusionDiffEditPipeline\\n    - all\\n    - gener...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nimport torch\\nfrom accelerate import PartialState\\nfrom diffusers import DiffusionPipeline\\n\\npipe...\"],[\"from diffusers import DiffusionPipeline\\n\\nsd = DiffusionPipeline.from_pretrained(\\n    \\\"runwayml\\u002fstabl...\"],[\"T2I-Adapter training example for Stable Diffusion XL (SDXL)\\n\\nThe `train_t2i_adapter_sdxl.py` script ...\"],[\"## Training\\n\\nOur training examples use two test conditioning images. They can be downloaded by runni...\"],[\"```python\\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscrete...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```diff\\n  import torch\\n  from diffusers import DiffusionPipeline\\n+ from diffusers.models.attention_p...\"],[\"Depending on GPU type, `torch.compile` can provide an *additional speed-up* of **5-300x** on top of ...\"],[\"### Stable Diffusion image-to-image\\n\\n```python\\nfrom diffusers import StableDiffusionImg2ImgPipeline\\n...\"],[\"prompt = \\\"ghibli style, a fantasy landscape with castles\\\"\\n\\nfor _ in range(3):\\n    image = pipe(promp...\"],[\"### DeepFloyd IF text-to-image + upscaling\\n\\n```python\\nfrom diffusers import DiffusionPipeline\\nimport...\"],[\"![t2i_speedup](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002fpt2_benchmarks\\u002ft2i...\"],[\"### V100 (batch size: 1)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno...\"],[\"### T4 (batch size: 4)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003cbr\\u003eno c...\"],[\"### RTX 3090 (batch size: 16)\\n\\n| **Pipeline** | **torch 2.0 - \\u003cbr\\u003eno compile** | **torch nightly - \\u003c...\"],[\"## Notes\\n\\n* Follow this [PR](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fpull\\u002f3313) for more details on...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```bash\\ngit clone https:\\u002f\\u002fgithub.com\\u002f\\u003cYOUR-USERNAME\\u003e\\u002fdiffusers.git\\n```\\n\\n**📋 Copy-paste the English v...\"],[\"Once you have translated the `_toctree.yml` file, you can start translating the [MDX](https:\\u002f\\u002fmdxjs....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## StableDiffusionPipelineOutput\\n\\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\n💡 Skip to the [DiffusionPipeline explained](#diffusionpipeline-explained) section if you are ...\"],[\"```python\\nfrom diffusers import StableDiffusionImg2ImgPipeline\\n\\nrepo_id = \\\"runwayml\\u002fstable-diffusion...\"],[\"repo_id = \\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nstable_diffusion = DiffusionPipeline.from_pretrained(repo...\"],[\"```python\\nfrom diffusers import DiffusionPipeline\\n\\nrepo_id = \\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nstable...\"],[\"```py\\nfrom diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\\n\\nmodel_id = \\\"run...\"],[\"| **checkpoint type** | **weight name**                     | **argument for loading weights** |\\n|--...\"],[\"To save a checkpoint stored in a different floating-point type or as a non-EMA variant, use the [`Di...\"],[\"The above example is therefore deprecated and won't be supported anymore for `diffusers \\u003e= 1.0.0`.\\n\\n...\"],[\"Schedulers are loaded from the [`SchedulerMixin.from_pretrained`] method, and unlike models, schedul...\"],[\"## DiffusionPipeline explained\\n\\nAs a class method, [`DiffusionPipeline.from_pretrained`] is responsi...\"],[\"```json\\nStableDiffusionPipeline {\\n  \\\"feature_extractor\\\": [\\n    \\\"transformers\\\",\\n    \\\"CLIPImageProcess...\"],[\"You can access each of the components of the pipeline as an attribute to view its configuration:\\n\\n``...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Creating noise from data is easy; creating data from noise is gene...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## DiTPipeline\\n[[autodoc]] DiTPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ImagePipelineOutput\\n[[autodoc]...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\npipeline = StableDiffusionXLPipeline.from_pretrained(..., add_watermarker=False)\\n```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline_text2image = AutoPipeli...\"],[\"```py\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load_image, make_i...\"],[\"### Base + refiner model\\n\\nWhen you use the base and refiner model together to generate an image, thi...\"],[\"\\u003cTip\\u003e\\n\\nThe `denoising_end` and `denoising_start` parameters should be a float between 0 and 1. These...\"],[\"```py\\nfrom diffusers import StableDiffusionXLInpaintPipeline\\nfrom diffusers.utils import load_image,...\"],[\"Load the base and refiner models:\\n\\n```py\\nfrom diffusers import DiffusionPipeline\\nimport torch\\n\\nbase ...\"],[\"## Micro-conditioning\\n\\nSDXL training involves several additional conditioning techniques, which are ...\"],[\"- [`target_size`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002fmain\\u002fen\\u002fapi\\u002fpipelines\\u002fstable_diffusion\\u002fstabl...\"],[\"```py\\nfrom diffusers import StableDiffusionXLPipeline\\nimport torch\\n\\npipeline = StableDiffusionXLPipe...\"],[\"pipeline = StableDiffusionXLPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"...\"],[\"```diff\\n+ base.enable_xformers_memory_efficient_attention()\\n+ refiner.enable_xformers_memory_efficie...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Text-conditioned image generation models have recently achieved as...\"],[\"There are 4 configurations (`SafetyConfig.WEAK`, `SafetyConfig.MEDIUM`, `SafetyConfig.STRONG`, and `...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Great, now you can import the rest of the dependencies you'll need:\\n\\n```python\\nimport jax.numpy as j...\"],[\"Model parameters and inputs have to be replicated across the 8 parallel devices. The parameters dict...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nYou need to ensure all your inputs have the same shape in subsequent calls, ot...\"],[\"prompt_ids = pipeline.prepare_inputs(prompts)\\nprompt_ids = shard(prompt_ids)\\n\\nimages = pipeline(prom...\"],[\"```python\\np_generate = pmap(pipeline._generate)\\n```\\n\\nAfter calling `pmap`, the prepared function `p_...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"Community leaders have the right and responsibility to remove, edit, or reject\\ncomments, commits, co...\"],[\"### 2. Warning\\n\\n**Community Impact**: A violation through a single incident or series\\nof actions.\\n\\n*...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```bash\\ncd examples\\u002ft2i_adapter\\npip install -r requirements.txt\\n```\\n\\n\\u003cTip\\u003e\\n\\n🤗 Accelerate is a librar...\"],[\"For example, to activate gradient accumulation, add the `--gradient_accumulation_steps` parameter to...\"],[\"Within the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002faab6de22c33cc01fb7bc81c0807d6109...\"],[\"If you want to learn more about how the training loop works, check out the [Understanding pipelines,...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```bash\\nexport MODEL_DIR=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport OUTPUT_DIR=\\\"path ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion XL for JAX + TPUv5e\\n\\n[TPU v5e](https:\\u002f\\u002fcloud.google.com\\u002fblog\\u002fproducts\\u002fcompute\\u002fhow-c...\"],[\"👉 Try it out for yourself:\\n\\n[![Hugging Face Spaces](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002f%F0%9F%A4%97%20Hugg...\"],[\"**2. Casting Parameter Types**\\n\\n```python\\nscheduler_state = params.pop(\\\"scheduler\\\")\\nparams = jax.tre...\"],[\"**7. Compilation Step**\\n\\n```python\\nstart = time.time()\\nprint(f\\\"Compiling ...\\\")\\ngenerate(default_prom...\"],[\"Once the function is compiled, these parameters are omitted from future calls and\\ncannot be changed ...\"],[\"# convert the images to PIL\\n    images = images.reshape((images.shape[0] * images.shape[1], ) + imag...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This guide shows how to perform inference with LCMs for \\n- text-to-image\\n- image-to-image\\n- combined...\"],[\"## Image-to-image\\n\\nLCMs can be applied to image-to-image tasks too. For this example, we'll use the ...\"],[\"```python\\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\\nimport...\"],[\"image = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concate...\"],[\"image = np.array(image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_thre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Diffusion probabilistic models (DPMs) have demonstrated a very pro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Take a look at the tensor values in the [`DDIMPipeline`] after two inference steps:\\n\\n```python\\nfrom ...\"],[\"If you run this code example on your specific hardware and PyTorch version, you should get a similar...\"],[\"You'll see the results are much closer now!\\n\\n```python\\nimport torch\\nfrom diffusers import DDIMPipeli...\"],[\"PyTorch typically benchmarks multiple algorithms to select the fastest one, but if you want reproduc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Guided image synthesis enables everyday users to create and edit p...\"],[\"## StableDiffusionPipelineOutput\\n\\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutp...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"login()\\n\\npipeline = DiffusionPipeline.from_pretrained(\\n    \\\"runwayml\\u002fstable-diffusion-v1-5\\\", torch_d...\"],[\"```python\\npipeline.scheduler.compatibles\\n```\\n\\n**Output**:\\n```\\n[diffusers.utils.dummy_torch_and_torch...\"],[\"```python\\npipeline.scheduler.config\\n```\\n\\nreturns a dictionary of the configuration of the scheduler:...\"],[\"[`LMSDiscreteScheduler`] usually leads to better results:\\n\\n```python\\nfrom diffusers import LMSDiscre...\"],[\"```python\\nfrom diffusers import DPMSolverMultistepScheduler\\n\\npipeline.scheduler = DPMSolverMultistep...\"],[\"# shard inputs and rng\\nparams = replicate(params)\\nprng_seed = jax.random.split(prng_seed, jax.device...\"],[\"# Textual Inversion fine-tuning example\\n\\n[Textual inversion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618) is a ...\"],[\"Let's first download it locally:\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nlocal_dir = \\\"...\"],[\"```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\n\\nmodel_id = \\\"path-to-your-train...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nThe quicktour is a simplified version of the introductory 🧨 Diffusers [notebook](https:\\u002f\\u002fcola...\"],[\"| **Task**                     | **Description**                                                    ...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nFor [Stable Diffusion](https:\\u002f\\u002fhuggingface.co\\u002fCompVis\\u002fstable-diffusion) models...\"],[\"```python\\n\\u003e\\u003e\\u003e image = pipeline(\\\"An image of a squirrel in Picasso style\\\").images[0]\\n\\u003e\\u003e\\u003e image\\n```\\n\\n\\u003c...\"],[\"## Models\\n\\nMost models take a noisy sample, and at each timestep it predicts the *noise residual* (o...\"],[\"To use the model for inference, create the image shape with random Gaussian noise. It should have a ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nFor the quicktour, you'll instantiate the [`DDPMScheduler`] with its [`~diffusers.ConfigMixi...\"],[\"First, create a function that postprocesses and displays the denoised image as a `PIL.Image`:\\n\\n```py...\"],[\"## Next steps\\n\\nHopefully, you generated some cool images with 🧨 Diffusers in this quicktour! For you...\"],[\"🧨 Diffusers Pipelines\\n\\nPipelines provide a simple way to run state-of-the-art diffusion models in in...\"],[\"To that end, we strive to offer all open-sourced, state-of-the-art diffusion system under a unified ...\"],[\"| Pipeline                                                                                          ...\"],[\"| [latent_diffusion_uncond](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fsrc\\u002fdiffusers\\u002fpipelin...\"],[\"| [stable_diffusion](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmain\\u002fsrc\\u002fdiffusers\\u002fpipelines\\u002fstab...\"],[\"**Note**: Pipelines are simple examples of how to play around with the diffusion systems as describe...\"],[\"- [`from_pretrained` method](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f5cbed8e0d157f65d3ddc2420d...\"],[\"- [`__call__`] method to use the pipeline in inference. `__call__` defines inference logic of the pi...\"],[\"**Note**: All pipelines have PyTorch's autograd disabled by decorating the `__call__` method with a ...\"],[\"- **Self-contained**: A pipeline shall be as self-contained as possible. More specifically, this mea...\"],[\"## Examples\\n\\n### Text-to-Image generation with Stable Diffusion\\n\\n```python\\n# make sure you're logged...\"],[\"### Tweak prompts reusing seeds and latents\\n\\nYou can generate your own latents to reproduce results,...\"],[\"Distillation for quantization on Textual Inversion models to personalize text2image\\n\\n[Textual invers...\"],[\"## Do distillation for quantization\\n\\nDistillation for quantization is a method that combines [interm...\"],[\"```bash\\nexport INT8_MODEL_NAME=\\\".\\u002fint8_model\\\"\\n\\npython text2images.py \\\\\\n  --pretrained_model_name_or_...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cbr\\u003e\\n\\nPipelines do not offer any training functionality. You'll notice PyTorch's autograd is disable...\"],[\"| Pipeline | Tasks |\\n|---|---|\\n| [AltDiffusion](alt_diffusion) | image2image |\\n| [AnimateDiff](anima...\"],[\"| [Pix2Pix Zero](pix2pix_zero) | image editing |\\n| [PixArt-α](pixart) | text2image |\\n| [PNDM](pndm) ...\"],[\"## DiffusionPipeline\\n\\n[[autodoc]] DiffusionPipeline\\n\\t- all\\n\\t- __call__\\n\\t- device\\n\\t- to\\n\\t- components...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" ...\"],[\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2...\"],[\"```bash\\nexport MODEL_NAME=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsaved\\u002fmodel\\\"\\n\\n...\"],[\"```bash\\nexport MODEL_NAME=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsaved\\u002fmodel\\\"\\n\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Loading from the original format\\n\\nBy default the [`ControlNetModel`] should be loaded with [`~Mod...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[Kandinsky 3](..\\u002fapi\\u002fpipelines\\u002fkandinsky3) simplifies the architecture and shifts away from the two-...\"],[\"\\u003chfoptions id=\\\"text-to-image\\\"\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.1\\\"\\u003e\\n\\n```py\\nfrom diffusers import KandinskyP...\"],[\"Pass the `image_embeds` and `negative_image_embeds` to the [`KandinskyV22Pipeline`] to generate an i...\"],[\"prompt = \\\"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\\\"\\nnegati...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 3\\\"\\u003e\\n\\nKandinsky 3 doesn't require a prior model so you can direct...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nfrom diffusers.utils import make_image_grid\\n\\nimage ...\"],[\"original_image.thumbnail((768, 768))\\n\\nimage = pipeline(prompt=prompt, negative_prompt=negative_promp...\"],[\"```py\\nfrom diffusers import KandinskyInpaintPipeline, KandinskyPriorPipeline\\nfrom diffusers.utils im...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fh...\"],[\"```py\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nfrom diffusers import AutoPipelineForInp...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Kandinsky 2.2\\\"\\u003e\\n\\n```py\\nfrom diffusers import KandinskyV22PriorPipeline, Ka...\"],[\"pipeline = KandinskyPipeline.from_pretrained(\\\"kandinsky-community\\u002fkandinsky-2-1\\\", torch_dtype=torch....\"],[\"Then you can use the `depth-estimation` [`~transformers.Pipeline`] from 🤗 Transformers to process th...\"],[\"generator = torch.Generator(device=\\\"cuda\\\").manual_seed(43)\\n\\nimage_emb, zero_image_emb = prior_pipeli...\"],[\"depth_estimator = pipeline(\\\"depth-estimation\\\")\\nhint = make_hint(img, depth_estimator).unsqueeze(0).h...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fh...\"],[\"```py\\nfrom diffusers import DDPMScheduler\\nfrom diffusers import DiffusionPipeline\\n\\nscheduler = DDPMS...\"],[\"# Diffusers examples with ONNXRuntime optimizations\\n\\n**This research project is not actively maintai...\"],[\"Multi Subject DreamBooth training\\n\\n[DreamBooth](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.12242) is a method to per...\"],[\"See an example with 2 subjects below, which learns a model for one dog subject and one human subject...\"],[\"**Important**: New parameters are added to the script, making possible to validate the progress of t...\"],[\"You can use the helper from the script to get a better sense of each parameter.\\n\\n### Inference\\n\\nOnce...\"],[\"And launch the training using\\n\\n**___Note: Change the `resolution` to 768 if you are using the [stabl...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport INSTANCE_DIR=\\\"path-to-instance-imag...\"],[\"### Training on a 8 GB GPU:\\n\\nBy using [DeepSpeed](https:\\u002f\\u002fwww.deepspeed.ai\\u002f) it's possible to offloa...\"],[\"### Fine-tune text encoder with the UNet.\\n\\nThe script also allows to fine-tune the `text_encoder` al...\"],[\"```\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\" --\\u003e export MODEL_NAME=\\\"BAAI\\u002fAltDiffusion-m9\\\"\\no...\"],[\"Inference Examples\\n\\n**The inference examples folder is deprecated and will be removed in a future ve...\"],[\"ControlNet training example for Stable Diffusion XL (SDXL)\\n\\nThe `train_controlnet_sdxl.py` script sh...\"],[\"## Training\\n\\nOur training examples use two test conditioning images. They can be downloaded by runni...\"],[\"```python\\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, UniPCMultistep...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nYou'll notice throughout the guide, we use [`~DiffusionPipeline.enable_model_cpu_offload`] an...\"],[\"## Popular models\\n\\nThe most popular image-to-image models are [Stable Diffusion v1.5](https:\\u002f\\u002fhuggin...\"],[\"# pass prompt and image to pipeline\\nimage = pipeline(prompt, image=init_image).images[0]\\nmake_image_...\"],[\"# pass prompt and image to pipeline\\nimage = pipeline(prompt, image=init_image, strength=0.5).images[...\"],[\"# pass prompt and image to pipeline\\nimage = pipeline(prompt, image=init_image).images[0]\\nmake_image_...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nfrom diffusers.utils import make...\"],[\"### Guidance scale\\n\\nThe `guidance_scale` parameter is used to control how closely aligned the genera...\"],[\"# pass prompt and image to pipeline\\nimage = pipeline(prompt, image=init_image, guidance_scale=8.0).i...\"],[\"# prepare image\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain...\"],[\"```py\\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\\nimport torch\\nfrom ...\"],[\"Start by generating an image:\\n\\n```py\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image\\nf...\"],[\"Repeat one more time to generate the final image in a [pixel art style](https:\\u002f\\u002fhuggingface.co\\u002fkohba...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nChain it to an upscaler pipeline to increase the image resolution:\\n\\n```py\\nfrom diffusers imp...\"],[\"[`AutoPipelineForImage2Image`] has a `prompt_embeds` (and `negative_prompt_embeds` if you're using a...\"],[\"Load a ControlNet model conditioned on depth maps and the [`AutoPipelineForImage2Image`]:\\n\\n```py\\nfro...\"],[\"Let's apply a new [style](https:\\u002f\\u002fhuggingface.co\\u002fnitrosocke\\u002felden-ring-diffusion) to the image gener...\"],[\"```py\\npipeline.unet = torch.compile(pipeline.unet, mode=\\\"reduce-overhead\\\", fullgraph=True)\\n```\\n\\nTo l...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```bash\\noptimum-cli export onnx --model runwayml\\u002fstable-diffusion-v1-5 sd_v15_onnx\\u002f\\n```\\n\\nThen to per...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion XL text-to-image fine-tuning\\n\\nThe `train_text_to_image_sdxl.py` script shows how to...\"],[\"### Training\\n\\n```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport VAE_NAME=\\\"...\"],[\"**Notes**:\\n\\n*  The `train_text_to_image_sdxl.py` script pre-computes text embeddings and the VAE enc...\"],[\"model_id = \\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\npipe = DiffusionPipeline.from_pretrained(model...\"],[\"[cloneofsimo](https:\\u002f\\u002fgithub.com\\u002fcloneofsimo) was the first to try out LoRA training for Stable Diff...\"],[\"```bash\\nhuggingface-cli login\\n```\\n\\nNow we can start training!\\n\\n```bash\\naccelerate launch train_text_...\"],[\"### Inference\\n\\nOnce you have trained a model using above command, the inference can be done simply u...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nIf you're using **PyTorch 1.13**, you need to \\\"prime\\\" the pipeline with an additional one-ti...\"],[\"\\u003c!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ve...\"],[\"Before running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https:\\u002f...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Script parameters\\n\\nThe training script has many parameters to help you tailor the trainin...\"],[\"## Training script\\n\\nUnlike some of the other training scripts, textual_inversion.py has a custom dat...\"],[\"The special [placeholder token](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fb81c69e489aad3a0ba7379...\"],[\"```py\\nfrom huggingface_hub import snapshot_download\\n\\nlocal_dir = \\\".\\u002fcat\\\"\\nsnapshot_download(\\n    \\\"dif...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax\\\"\\nexp...\"],[\"prompt = \\\"A \\u003ccat-toy\\u003e train\\\"\\nprng_seed = jax.random.PRNGKey(0)\\nnum_inference_steps = 50\\n\\nnum_samples...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Consistency Models were proposed in [Consistency Models](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2303.01469) b...\"],[\"## Tips\\n\\nFor an additional speed-up, use `torch.compile` to generate multiple images in \\u003c1 second:\\n\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## AutoPipelineForText2Image\\n\\n[[autodoc]] AutoPipelineForText2Image\\n\\t- all\\n\\t- from_pretrained\\n\\t- fro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## AutoencoderTiny\\n\\n[[autodoc]] AutoencoderTiny\\n\\n## AutoencoderTinyOutput\\n\\n[[autodoc]] models.autoen...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https:\\u002f...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe training script provides many parameters to help you customize your training run. All of...\"],[\"## Training script\\n\\nThe training script starts by creating a dataset class - [`Text2ImageDataset`](h...\"],[\"```py\\nteacher_unet = UNet2DConditionModel.from_pretrained(\\n    args.pretrained_teacher_model, subfol...\"],[\"```py\\npred_x_0 = predicted_origin(\\n    noise_pred,\\n    start_timesteps,\\n    noisy_model_input,\\n    n...\"],[\"```bash\\nexport MODEL_DIR=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsaved\\u002fmodel\\\"\\n\\na...\"],[\"prompt = \\\"sushi rolls in the form of panda heads, sushi platter\\\"\\n\\nimage = pipeline(prompt, num_infer...\"],[\"Custom Diffusion training example \\n\\n[Custom Diffusion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2212.04488) is a method...\"],[\"Now let's get our dataset. Download dataset from [here](https:\\u002f\\u002fwww.cs.cmu.edu\\u002f~custom-diffusion\\u002fass...\"],[\"To track your experiments using Weights and Biases (`wandb`) and to save intermediate results (which...\"],[\"```bash\\npip install clip-retrieval\\npython retrieve.py --class_prompt {} --class_data_dir {} --num_cl...\"],[\"Then start training!\\n\\n```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport OUTPUT_DIR=\\\"p...\"],[\"model_id = \\\"sayakpaul\\u002fcustom-diffusion-cat\\\"\\ncard = RepoCard.load(model_id)\\nbase_model_id = card.data...\"],[\"More info: https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fgenerated\\u002ftorch.optim.Optimizer.zero_grad.html\\n\\n## Experi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"!---\\nCopyright 2023- The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, ...\"],[\"---\\n\\n## Adding a new element to the navigation bar\\n\\nAccepted files are Markdown (.md).\\n\\nCreate a fil...\"],[\"### Adding a new tutorial\\n\\nAdding a new tutorial or section is done in two steps:\\n\\n- Add a new Markd...\"],[\"You can follow the same process to create a new scheduler under the `docs\\u002fsource\\u002f\\u003clanguageCode\\u003e\\u002fapi\\u002f...\"],[\"```\\n    Args:\\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\\n     ...\"],[\"```\\n    Returns:\\n        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special tok...\"],[\"This script may have some weird failures if you made a syntax mistake or if you uncover a bug. There...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase of this paper can be found at [ermongroup\\u002fddim](https:\\u002f\\u002fgithub.com\\u002fermongroup\\u002f...\"],[\"## DDIMScheduler\\n[[autodoc]] DDIMScheduler\\n\\n## DDIMSchedulerOutput\\n[[autodoc]] schedulers.scheduling...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https:\\u002f...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe training script provides many parameters to help you customize your training run. All of...\"],[\"Add the `--snr_gamma` parameter and set it to the recommended value of 5.0:\\n\\n```bash\\naccelerate laun...\"],[\"```py\\nload_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder=\\\"unet\\\")\\nmodel.register_...\"],[\"Once you've made all your changes or you're okay with the default configuration, you're ready to lau...\"],[\"\\u003cTip\\u003e\\n\\nTo train on a local dataset, set the `TRAIN_DIR` and `OUTPUT_DIR` environment variables to th...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## Next steps\\n\\nCongratulations on training your own text-to-image model! T...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*StableDiffusion is a revolutionary text-to-image generator that is...\"],[\"## Example Usage\\n\\n```python\\nfrom diffusers import AsymmetricAutoencoderKL, StableDiffusionInpaintPip...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We introduce Würstchen, a novel architecture for text-to-image syn...\"],[\"## Würstchen Overview\\nWürstchen is a diffusion model, whose text-conditional model works in a highly...\"],[\"- v2-base\\n- v2-aesthetic\\n- **(default)** v2-interpolated (50% interpolation between v2-base and v2-a...\"],[\"```python\\nimport torch\\nfrom diffusers import WuerstchenDecoderPipeline, WuerstchenPriorPipeline\\nfrom...\"],[\"The original codebase, as well as experimental ideas, can be found at [dome272\\u002fWuerstchen](https:\\u002f\\u002fg...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*By decomposing the image formation process into a sequential appli...\"],[\"Make sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to explore th...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usability over Performance\\n\\n- While Diffusers has many built-in performance-enhancing features (s...\"],[\"## Simple over easy\\n\\nAs PyTorch states, **explicit is better than implicit** and **simple is better ...\"],[\"## Tweakable, contributor-friendly over abstraction\\n\\nFor large parts of the library, Diffusers adopt...\"],[\"At Hugging Face, we call this design the **single-file policy** which means that almost all of the c...\"],[\"### Pipelines\\n\\nPipelines are designed to be easy to use (therefore do not follow [*Simple over easy*...\"],[\"The following design principles are followed:\\n- Pipelines follow the single-file policy. All pipelin...\"],[\"- Pipelines should be very readable, self-explanatory, and easy to tweak.\\n- Pipelines should be desi...\"],[\"### Models\\n\\nModels are designed as configurable toolboxes that are natural extensions of [PyTorch's ...\"],[\"The following design principles are followed:\\n- Models correspond to **a type of model architecture*...\"],[\"- Models should be designed to be easily extendable to future changes. This can be achieved by limit...\"],[\"### Schedulers\\n\\nSchedulers are responsible to guide the denoising process for inference as well as t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Install with conda\\n\\nAfter activating your virtual environment, with `conda` (maintained by the co...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nYou must keep the `diffusers` folder if you want to keep using the library.\\n\\n\\u003c...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| A1111\\u002fk-diffusion    | 🤗 Diffusers                         | Usage                                ...\"],[\"| DPM2                | [`KDPM2DiscreteScheduler`]          |                                       ...\"],[\"| LMS Karras          | [`LMSDiscreteScheduler`]            | init with `use_karras_sigmas=True`    ...\"],[\"All schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*The past few years have witnessed the great success of Diffusion m...\"],[\"# Textual Inversion fine-tuning example\\n\\n[Textual inversion](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.01618) is a ...\"],[\"You have to be a registered user in 🤗 Hugging Face Hub, and you'll also need to use an access token ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [hohonathanho\\u002fdiffusion](https:\\u002f\\u002fgithub.com\\u002fhojonathanho\\u002fdiffu...\"],[\"Kandinsky2.2 text-to-image fine-tuning\\n\\nKandinsky 2.2 includes a prior pipeline that generates image...\"],[\"```bash\\naccelerate config\\n```\\nFor this example we want to directly store the trained LoRA embeddings...\"],[\"To train on your own training files, prepare the dataset according to the format required by `datase...\"],[\"pipe = AutoPipelineForText2Image.from_pretrained(\\\"kandinsky-community\\u002fkandinsky-2-2-decoder\\\", unet=u...\"],[\"pipe.enable_model_cpu_offload()\\nprompt='A robot pokemon, 4k photo'\\nimages = pipe(prompt=prompt, nega...\"],[\"## Training with LoRA\\n\\nLow-Rank Adaption of Large Language Models was first introduced by Microsoft ...\"],[\"#### Train decoder \\n\\n```bash\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-blip-captions\\\"\\n\\naccelerate laun...\"],[\"```python\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipe = AutoPipelineForText2I...\"],[\"e don't yet support training T2I-Adapters on Stable Diffusion yet. For training T2I-Adapters on Stab...\"],[\"Research projects\\n\\nThis folder contains various research projects using 🧨 Diffusers.\\nThey are not re...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Here's the overview from the [project page](https:\\u002f\\u002fvislearn.github.io\\u002fControlNet-XS\\u002f):\\n\\n*With incre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Use TensorFloat-32\\n\\nOn Ampere and later CUDA devices, matrix multiplications and convolutions can...\"],[\"[DreamBooth](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002ftree\\u002fmain\\u002fexamples\\u002fdreambooth) by [colossalai]...\"],[\"For each row the dataset contains `image` and `text` keys. `image` is a varying size PIL png, and `t...\"],[\"### Training with prior-preservation loss\\n\\nPrior-preservation is used to avoid overfitting and langu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before you begin, make sure you have the following libraries installed:\\n\\n```py\\n# uncomment to instal...\"],[\"img_url = \\\"https:\\u002f\\u002fgithub.com\\u002fXiang-cd\\u002fDiffEdit-stable-diffusion\\u002fraw\\u002fmain\\u002fassets\\u002forigin.png\\\"\\nraw_ima...\"],[\"## Generate source and target embeddings\\n\\nThe source and target embeddings can be automatically gene...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nLoad the text encoder model used by the [`StableDiffusionDiffEditPipeline`] to encode the te...\"],[\"img_url = \\\"https:\\u002f\\u002fgithub.com\\u002fXiang-cd\\u002fDiffEdit-stable-diffusion\\u002fraw\\u002fmain\\u002fassets\\u002forigin.png\\\"\\n  raw_i...\"],[\"# offload caption generator\\n    caption_generator.to(\\\"cpu\\\")\\n\\n    caption = caption_processor.batch_d...\"],[\"Community Examples\\n\\n\\u003e **For more information about community pipelines, please have a look at [this ...\"],[\"| Example                                                                                           ...\"],[\"| LLM-grounded Diffusion (LMD+)                                                                     ...\"],[\"| CLIP Guided Stable Diffusion                                                                      ...\"],[\"| Stable Diffusion Interpolation                                                                    ...\"],[\"| Long Prompt Weighting Stable Diffusion                                                            ...\"],[\"| Wild Card Stable Diffusion                                                                        ...\"],[\"| Seed Resizing Stable Diffusion                                                                    ...\"],[\"| Multilingual Stable Diffusion                                                                     ...\"],[\"| Text Based Inpainting Stable Diffusion                                                            ...\"],[\"| K-Diffusion Stable Diffusion                                                                      ...\"],[\"Stable Diffusion v1.1-1.4 Comparison                                                                ...\"],[\"| Stable UnCLIP                                                                                     ...\"],[\"| UnCLIP Image Interpolation Pipeline                                                               ...\"],[\"| CLIP Guided Img2Img Stable Diffusion Pipeline                                                     ...\"],[\"| EDICT Image Editing Pipeline                                                                      ...\"],[\"| TensorRT Stable Diffusion Image to Image Pipeline                                                 ...\"],[\"| TensorRT Stable Diffusion Inpainting Pipeline                                                     ...\"],[\"|   Zero1to3 Pipeline                                                                               ...\"],[\"|   Latent Consistency Pipeline                                                                     ...\"],[\"|   Latent Consistency Interpolation Pipeline                                                       ...\"],[\"|   Regional Prompting Pipeline                                                                     ...\"],[\"| AnimateDiff ControlNet Pipeline                                                                   ...\"],[\"To load a custom pipeline you just need to pass the `custom_pipeline` argument to `DiffusionPipeline...\"],[\"#### Use this pipeline with an LLM\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipeline\\n\\np...\"],[\"images[0].save(\\\".\\u002flmd_plus_generation.jpg\\\")\\n```\\n\\n### CLIP Guided Stable Diffusion\\n\\nCLIP guided stabl...\"],[\"![clip_guidance](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimages\\u002fresolve\\u002fmain\\u002fclip_guidance\\u002f...\"],[\"### Stable Diffusion Mega\\n\\nThe Stable Diffusion Mega Pipeline lets you use the main use cases of the...\"],[\"As shown above this one pipeline can run all both \\\"text-to-image\\\", \\\"image-to-image\\\", and \\\"inpainting...\"],[\"pipe.text2img(prompt, negative_prompt=neg_prompt, width=512,height=512,max_embeddings_multiples=3).i...\"],[\"model = WhisperForConditionalGeneration.from_pretrained(\\\"openai\\u002fwhisper-small\\\").to(device)\\nprocessor...\"],[\"A full example:\\n\\ncreate `animal.txt`, with contents like:\\n\\n```\\ndog\\ncat\\nmouse\\n```\\n\\ncreate `object.txt...\"],[\"pipe = DiffusionPipeline.from_pretrained(\\n    args.model_path,\\n    custom_pipeline=\\\"composable_stabl...\"],[\"```\\n\\n### Imagic Stable Diffusion\\nAllows you to edit an image using stable diffusion.\\n\\n```python\\nimpo...\"],[\"```python\\nimport torch as th\\nimport numpy as np\\nfrom diffusers import DiffusionPipeline\\n\\nhas_cuda = ...\"],[\"image = res.images[0]\\nimage.save('.\\u002fseed_resize\\u002fseed_resize_{w}_{h}_image_compare.png'.format(w=widt...\"],[\"torch_dtype=torch.float16,\\n)\\n\\ndiffuser_pipeline.enable_attention_slicing()\\ndiffuser_pipeline = diffu...\"],[\"torch_dtype=torch.float16\\n)\\npipe = pipe.to(\\\"cuda\\\")\\n\\nprompt = \\\"Your prompt here!\\\"\\nimage = pipe(prompt...\"],[\"```\\n\\n### Stable Diffusion with K Diffusion\\n\\nMake sure you have @crowsonkb's https:\\u002f\\u002fgithub.com\\u002fcrows...\"],[\"![diffusers_euler](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fpatrickvonplaten\\u002fimages\\u002fresolve\\u002fmain\\u002fk_diffusion\\u002f...\"],[\"prompt = \\\"An astronaut riding a horse on Mars\\\"\\n\\nimage = merged_pipe(prompt).images[0]\\n\\n```\\nSome exam...\"],[\"plt.subplots(2,2,1)\\nplt.imshow(output.images[0])\\nplt.title('Stable Diffusion v1.1')\\nplt.axis('off')\\n...\"],[\"img = Image.open('phone.jpg')\\nmix_img = pipe(\\n    img,\\n    prompt = 'bed',\\n    kmin = 0.3,\\n    kmax ...\"],[\"image = output.images[0]\\nimage.save(\\\".\\u002fshiba-inu.jpg\\\")\\n\\n# debug\\n\\n# `pipeline.decoder_pipe` is a regu...\"],[\"```python\\nimport torch\\nfrom diffusers import DiffusionPipeline\\n\\ndevice = torch.device(\\\"cpu\\\" if not t...\"],[\"```python\\nimport torch\\nfrom diffusers import DiffusionPipeline\\nfrom PIL import Image\\n\\ndevice = torch...\"],[\"### DDIM Noise Comparative Analysis Pipeline\\n#### **Research question: What visual concepts do the d...\"],[\"Here is the result of this pipeline (which is DDIM) on CelebA-HQ dataset.\\n\\n![noise-comparative-analy...\"],[\"The following code requires roughly 12GB of GPU RAM.\\n\\n```python\\nfrom io import BytesIO\\nimport reques...\"],[\"Output Image\\n\\n![img2img_clip_guidance](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fnjindal\\u002fimages\\u002fresolve\\u002fmain\\u002fc...\"],[\"```python\\nfrom diffusers import DiffusionPipeline, DDIMScheduler\\nfrom transformers import CLIPTextMo...\"],[\"display(result_image)\\n```\\n\\nInit Image\\n\\n![img2img_init_edict_text_editing](https:\\u002f\\u002fhuggingface.co\\u002fdat...\"],[\"### TensorRT Image2Image Stable Diffusion Pipeline\\n\\nThe TensorRT Pipeline can be used to accelerate ...\"],[\"Based on [this issue](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fissues\\u002f3566),\\n- `EulerAncestralDiscre...\"],[\"Based on [this issue](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fissues\\u002f3566),\\n- `EulerAncestralDiscre...\"],[\"To use this pipeline, you need to:\\n1. Install [IPEX](https:\\u002f\\u002fgithub.com\\u002fintel\\u002fintel-extension-for-py...\"],[\"Then you can use the ipex pipeline in a similar way to the default stable diffusion pipeline.\\n```pyt...\"],[\"# 2. Original Pipeline initialization\\npipe2 = StableDiffusionPipeline.from_pretrained(model_id)\\n\\n# 3...\"],[\"```python\\nfrom diffusers import DiffusionPipeline\\nfrom diffusers.utils import load_image\\nimport torc...\"],[\"For more results, checkout [PR #6114](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fpull\\u002f6114).\\n\\n## Examp...\"],[\"def download_image(url):\\n    response = requests.get(url)\\n    return PIL.Image.open(BytesIO(response...\"],[\"# Mixture of Diffusers generation\\nimage = pipeline(\\n    prompt=[[\\n        \\\"A charming house in the c...\"],[\"# re-use cached folder to save ONNX models and TensorRT Engines\\npipe.set_cached_folder(\\\"stabilityai\\u002f...\"],[\"# Mixture of Diffusers generation\\noutput = pipeline(\\n    canvas_height=800,\\n    canvas_width=352,\\n  ...\"],[\"```python\\n\\ndef sample_iadb(model, x0, nb_step):\\n    x_alpha = x0\\n    for t in range(nb_step):\\n      ...\"],[\"# load image\\n# H, W = (256, 256) # H, W = (512, 512)   # zero123 training is 256,256\\n\\n# for batch in...\"],[\"```\\n\\n### Stable Diffusion XL Reference\\n\\nThis pipeline uses the Reference . Refer to the [stable_diff...\"],[\"`prompt: A dog`\\n\\n`reference_attn=True, reference_adain=False, num_inference_steps=20`\\n![Output_image...\"],[\"prompt = \\\"photo, A blue colored car, fish eye\\\"\\nliked = [init_image]\\n## same goes with disliked\\n\\n# ca...\"],[\"pipeline = MaskedStableDiffusionImg2ImgPipeline.from_pretrained(\\\"frankjoshua\\u002ficbinpICantBelieveIts_v...\"],[\"outputs = pipe(prompt=prompts, height=512, width=512, num_inference_steps=50, cross_attention_kwargs...\"],[\"The abstract of the paper reads as follows:\\n\\n*Latent Diffusion models (LDMs) have achieved remarkabl...\"],[\"# Can be set to 1~50 steps. LCM support fast inference even \\u003c= 4 steps. Recommend: 1~8 steps.\\nnum_in...\"],[\"```py\\nimport torch\\nimport numpy as np\\n\\nfrom diffusers import DiffusionPipeline\\n\\npipe = DiffusionPipe...\"],[\"assert len(images) == (len(prompts) - 1) * num_interpolation_steps\\n```\\n\\n###  StableDiffusionUpscaleL...\"],[\"#Upscale the previous output to a resolution of (1024, 1024)\\npipe_ldm3d_upscale = DiffusionPipeline....\"],[\"controlnet_depth = ControlNetModel.from_pretrained(\\n    \\\"diffusers\\u002fcontrolnet-depth-sdxl-1.0\\\",\\n    t...\"],[\"```\\n\\n### ControlNet + T2I Adapter + Inpainting Pipeline\\n```py\\nimport cv2\\nimport numpy as np\\nimport t...\"],[\"image = load_image(img_url).resize((1024, 1024))\\nmask_image = load_image(mask_url).resize((1024, 102...\"],[\"time = time.strftime(r\\\"%Y%m%d%H%M%S\\\")\\ni = 1\\nfor image in images:\\n    i += 1\\n    fileName = f'img-{ti...\"],[\"prompt =\\\"\\\"\\\"\\nblue sky BREAK\\ngreen hair BREAK\\nbook shelf BREAK\\nterrarium on desk BREAK\\norange dress an...\"],[\"prompt =\\\"\\\"\\\"\\na girl in street with shirt, tie, skirt BREAK\\nred, shirt BREAK\\ngreen, tie BREAK\\nblue , s...\"],[\"### Mask\\nWhen an image is generated, the generated mask is displayed. It is generated at the same si...\"],[\"### Input Parameters\\nParameters are specified through the `rp_arg`(dictionary type).\\n\\n```\\nrp_args = ...\"],[\"# define the Gaussian blurring operator first\\n    class GaussialBlurOperator(nn.Module):\\n        def...\"],[\"def get_kernel(self):\\n                    return self.k\\n\\n            self.kernel_size = kernel_size\\n...\"],[\"# set up operator and measurement\\n    operator = GaussialBlurOperator(kernel_size=61, intensity=3.0)...\"],[\"# set up model\\n    model = UNet2DModel.from_pretrained(\\\"google\\u002fddpm-celebahq-256\\\").to(\\\"cuda\\\")\\n    ``...\"],[\"model_id = \\\"SG161222\\u002fRealistic_Vision_V5.1_noVAE\\\"\\npipe = DiffusionPipeline.from_pretrained(\\n    mode...\"],[\"from diffusers.utils import export_to_gif\\nexport_to_gif(result.frames[0], \\\"result.gif\\\")\\n```\\n\\n\\u003ctable\\u003e...\"],[\"- `cosine_scale_1` (`float`, defaults to 3):\\n  Control the strength of skip-residual. For specific i...\"],[\"w = 0\\n    for i, img in enumerate(imgs):\\n        h_, w_ = imgs[i].size\\n        w += w_\\n    h = h_\\n  ...\"],[\"# train_lora is optional, and in most cases, using train_lora can better preserve consistency with t...\"],[\"DreamBooth training example\\n\\n[DreamBooth](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2208.12242) is a method to personali...\"],[\"Let's first download it locally:\\n\\n```python\\nfrom huggingface_hub import snapshot_download\\n\\nlocal_dir...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport INSTANCE_DIR=\\\"dog\\\"\\nexport CLASS_DIR...\"],[\"### Training on a 12GB GPU:\\n\\nIt is possible to run dreambooth on a 12GB GPU by using the following o...\"],[\"Changing the default Adam optimizer to DeepSpeed's special version of Adam\\n`deepspeed.ops.adam.DeepS...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport INSTANCE_DIR=\\\"dog\\\"\\nexport CLASS_DIR...\"],[\"prompt = \\\"A photo of sks dog in a bucket\\\"\\nimage = pipe(prompt, num_inference_steps=50, guidance_scal...\"],[\"### Training\\n\\nLet's get started with a simple example. We will re-use the dog example of the [previo...\"],[\"```bash\\nhuggingface-cli login\\n```\\n\\nNow we can start training!\\n\\n```bash\\naccelerate launch train_dream...\"],[\"### Inference\\n\\nAfter training, LoRA weights can be loaded very easily into the original pipeline. Fi...\"],[\"lora_model_id = \\\"sayakpaul\\u002fdreambooth-text-encoder-test\\\"\\ncard = RepoCard.load(lora_model_id)\\nbase_mo...\"],[\"Before running the scripts, make sure to install the library's training dependencies:\\n\\n```bash\\npip i...\"],[\"### Training with xformers:\\nYou can enable memory efficient attention by [installing xFormers](https...\"],[\"```py\\nfrom diffusers import DiffusionPipeline\\n\\npipe = DiffusionPipeline.from_pretrained(\\\"DeepFloyd\\u002fI...\"],[\"We found experimentally that the DDPM scheduler with the default larger number of denoising steps to...\"],[\"python train_dreambooth_lora.py \\\\\\n    --report_to wandb \\\\\\n    --pretrained_model_name_or_path=$MODEL...\"],[\"```sh\\nexport MODEL_NAME=\\\"DeepFloyd\\u002fIF-I-XL-v1.0\\\"\\n\\nexport INSTANCE_DIR=\\\"dog\\\"\\nexport OUTPUT_DIR=\\\"dream...\"],[\"accelerate launch train_dreambooth.py \\\\\\n  --report_to wandb \\\\\\n  --pretrained_model_name_or_path=$MOD...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Let's load the [herge_style](https:\\u002f\\u002fhuggingface.co\\u002fsd-dreambooth-library\\u002fherge-style) checkpoint, w...\"],[\"```py\\npipeline.load_textual_inversion(\\\"sd-concepts-library\\u002fgta5-artwork\\\")\\nprompt = \\\"A cute brown bea...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"The [`~loaders.LoraLoaderMixin.load_lora_weights`] method loads LoRA weights into both the UNet and ...\"],[\"```py\\npipeline.unload_lora_weights()\\n```\\n\\n### Load multiple LoRAs\\n\\nIt can be fun to use multiple LoR...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nYou can't unfuse multiple LoRA checkpoints, so if you need to reset the model ...\"],[\"### Kohya and TheLastBen\\n\\nOther popular LoRA trainers from the community include those by [Kohya](ht...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nLoading a checkpoint from TheLastBen is very similar. For example, to load the [TheLastBen\\u002fW...\"],[\"pipeline = AutoPipelineForText2Image.from_pretrained(\\\"runwayml\\u002fstable-diffusion-v1-5\\\", torch_dtype=t...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftesting-image...\"],[\"pipeline = AutoPipelineForInpaint.from_pretrained(\\\"runwayml\\u002fstable-diffusion-v1-5\\\", torch_dtype=torc...\"],[\"\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhu...\"],[\"pipeline.set_ip_adapter_scale(0.7)\\n\\nimage = load_image(\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fYiYiXu\\u002ftesti...\"],[\"prompt = \\\"best quality, high quality\\\"\\nimage = load_image(\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f...\"],[\"pipeline.load_ip_adapter(\\\"h94\\u002fIP-Adapter\\\", subfolder=\\\"models\\\", weight_name=\\\"ip-adapter_sd15.bin\\\")\\n\\ng...\"],[\"# enable memory savings\\npipe.enable_vae_slicing()\\npipe.enable_model_cpu_offload()\\n\\n# load ip_adapter...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom diffusers import DiffusionPipeline\\nimport torch\\n\\nclass UnetSchedulerOneForwardPipelin...\"],[\"scheduler = DDPMScheduler()\\nunet = UNet2DModel()\\n\\npipeline = UnetSchedulerOneForwardPipeline(unet=un...\"],[\"Take a look at the following table to compare the two sharing workflows to help you decide the best ...\"],[\"Sometimes you can't load all the pipeline components weights from an official repository. In this ca...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"pipe = StableVideoDiffusionPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-video-diffusion-img2vid...\"],[\"```diff\\n- pipe.enable_model_cpu_offload()\\n+ pipe.to(\\\"cuda\\\")\\n+ pipe.unet = torch.compile(pipe.unet, m...\"],[\"Along with conditioning image Stable Diffusion Video also allows providing micro-conditioning that a...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"RealFill\\n\\n[RealFill](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2309.16668) is a method to personalize text2image inpaint...\"],[\"### Training on a low-memory GPU:\\n\\nIt is possible to run realfill on a low-memory GPU by using the f...\"],[\"More info: https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fgenerated\\u002ftorch.optim.Optimizer.zero_grad.html\\n\\n## Acknow...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Initialize an 🤗 Accelerate environment:\\n\\n```bash\\naccelerate config\\n```\\n\\nTo setup a default 🤗 Acceler...\"],[\"```bash\\naccelerate launch train_unconditional.py \\\\\\n  --mixed_precision=\\\"bf16\\\"\\n```\\n\\nSome basic and im...\"],[\"Next, the script initializes a [scheduler](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f096f84b05f9...\"],[\"```py\\ndataset = load_dataset(\\\"imagefolder\\\", data_dir=args.train_data_dir, cache_dir=args.cache_dir, ...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\nThe training script creates and saves a checkpoint file in your repository...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Contrastive models like CLIP have been shown to learn robust repre...\"],[\"prior_model_id = \\\"kakaobrain\\u002fkarlo-v1-alpha\\\"\\ndata_type = torch.float16\\nprior = PriorTransformer.from...\"],[\"pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\\n    \\\"stabilityai\\u002fstable-diffusion-2-1-unclip\\\", t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*Diffusion models have shown promising results in cross...\"],[\"This pipeline was contributed by [sanchit-gandhi](https:\\u002f\\u002fhuggingface.co\\u002fsanchit-gandhi).\\n\\n## Tips\\n\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Subject-driven text-to-image generation models create novel rendit...\"],[\"`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](http...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We present ControlNet, a neural network architecture to add spatia...\"],[\"## StableDiffusionControlNetImg2ImgPipeline\\n[[autodoc]] StableDiffusionControlNetImg2ImgPipeline\\n\\t- ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nYou'll notice throughout the guide, we use [`~DiffusionPipeline.enable_model_cpu_offload`] an...\"],[\"\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhu...\"],[\"### Stable Diffusion Inpainting\\n\\nStable Diffusion Inpainting is a latent diffusion model finetuned o...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load_...\"],[\"# load base and mask image\\ninit_image = load_image(\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"## Non-inpaint specific checkpoints\\n\\nSo far, this guide has used inpaint specific checkpoints such a...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"runwayml\\u002fstable-diffusion-inpainting\\\"\\u003e\\n\\n```py\\nimport torch\\nfrom diffusers ...\"],[\"\\u003chfoptions id=\\\"inpaint\\\"\\u003e\\n\\u003chfoption id=\\\"runwayml\\u002fstable-diffusion-v1-5\\\"\\u003e\\n\\n```py\\nimport torch\\nfrom dif...\"],[\"image = pipeline(prompt=\\\"road\\\", image=init_image, mask_image=mask_image).images[0]\\nmake_image_grid([...\"],[\"device = \\\"cuda\\\"\\npipeline = AutoPipelineForInpainting.from_pretrained(\\n    \\\"runwayml\\u002fstable-diffusion...\"],[\"### Strength\\n\\n`strength` is a measure of how much noise is added to the base image, which influences...\"],[\"\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhu...\"],[\"# load base and mask image\\ninit_image = load_image(\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocu...\"],[\"```py\\nimport torch\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load_...\"],[\"### Text-to-image-to-inpaint\\n\\nChaining a text-to-image and inpainting pipeline allows you to inpaint...\"],[\"\\u003cdiv class=\\\"flex flex-row gap-4\\\"\\u003e\\n  \\u003cdiv class=\\\"flex-1\\\"\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhu...\"],[\"Now let's pass the image to another inpainting pipeline with SDXL's refiner model to enhance the ima...\"],[\"image = pipeline(prompt=prompt, image=image).images[0]\\nmake_image_grid([init_image, mask_image, imag...\"],[\"### Prompt weighting\\n\\nPrompt weighting provides a quantifiable way to scale the representation of co...\"],[\"For example, let's condition an image with a ControlNet pretrained on inpaint images:\\n\\n```py\\nimport ...\"],[\"Now generate an image from the base, mask and control images. You'll notice features of the base ima...\"],[\"image_elden_ring = pipeline(prompt, negative_prompt=negative_prompt, image=image).images[0]\\nmake_ima...\"],[\"You can also offload the model to the CPU to save even more memory:\\n\\n```diff\\n+ pipeline.enable_xform...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about Text-to-Video on the [project page](https:\\u002f\\u002fmodelscope.cn\\u002f...\"],[\"It just takes **7 GBs of GPU memory** to generate the 64 video frames using PyTorch 2.0, \\\"fp16\\\" prec...\"],[\"```py\\nimport torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffuse...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## TextToVideoSDPipeline\\n[[autodoc]] TextToVideoSDPipeline\\n\\t- all\\n\\t- __call__\\n\\n## VideoToVid...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## CustomDiffusionXFormersAttnProcessor\\n[[autodoc]] models.attention_processor.CustomDiffusionXForme...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For models, you can also specify the [*variant*](loading#checkpoint-variants) of the weights to push...\"],[\"scheduler = DDIMScheduler(\\n    beta_start=0.00085,\\n    beta_end=0.012,\\n    beta_schedule=\\\"scaled_lin...\"],[\"```py\\ncontrolnet.push_to_hub(\\\"my-controlnet-model-private\\\", private=True)\\n```\\n\\nPrivate repositories ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [Fantasy-Studio\\u002fPaint-by-Example](https:\\u002f\\u002fgithub.com\\u002fFantasy-S...\"],[\"Dreambooth for the inpainting model\\n\\nThis script was added by @thedarkzeno .\\n\\nPlease note that this ...\"],[\"### Training with gradient checkpointing and 8-bit optimizer:\\n\\nWith the help of gradient checkpointi...\"],[\"```bash\\nexport MODEL_NAME=\\\"runwayml\\u002fstable-diffusion-inpainting\\\"\\nexport INSTANCE_DIR=\\\"path-to-instan...\"],[\"Create a dataset for training\\n\\nThere are many datasets on the [Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets?...\"],[\"💡 For more details and context about creating and uploading a dataset to the Hub, take a look at the...\"],[\"## Next steps\\n\\nNow that you've created a dataset, you can plug it into the `train_data_dir` (if your...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\ngenerator = [torch.Generator(device=\\\"cuda\\\").manual_seed(i) for i in range(4)]\\n```\\n\\n\\u003cTip wa...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Given a prompt, get the inference time for the original model:\\n\\n```py\\nimport time\\n\\nseed = 2023\\ngener...\"],[\"```py\\nfrom diffusers import AutoencoderTiny\\n\\ndistilled.vae = AutoencoderTiny.from_pretrained(\\n    \\\"s...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Model-based reinforcement learning methods often use learning only...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## ValueGuidedRLPipeline\\n[[autodoc]] diffusers.experimental.ValueGuidedRLPipeline...\"]],\"hovertemplate\":\"source=diffusers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"diffusers, circle\",\"marker\":{\"color\":\"#FECB52\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"diffusers, circle\",\"showlegend\":true,\"x\":[-21.7319,7.232949,-10.070834,1.1844635,7.431016,8.031367,7.9641905,-12.833175,-12.85389,-12.593168,-12.664577,-12.7390175,-12.502389,-12.530346,-12.563765,-12.810258,-12.830985,-12.756875,-12.859816,-12.835467,5.055025,8.310986,8.151325,7.937893,0.86210585,0.6375573,3.2125707,-4.121805,-4.396883,3.5559173,-5.4644823,-5.5858493,-3.4103665,-3.541679,-3.5390093,8.2553425,-9.445508,-5.696732,-6.1766896,-6.6487913,-6.569145,10.882228,-0.00803723,-2.5405025,-5.733907,-4.5169425,-4.598232,-4.267054,11.978607,11.420255,16.380577,16.68121,16.672974,16.03221,16.257689,16.15986,15.257409,16.823925,16.32452,15.762905,16.427145,16.499472,16.5149,16.885979,16.881245,16.818151,15.7912,15.670088,16.826567,16.880585,16.870815,16.579763,15.913488,16.832779,16.898417,16.416622,17.00915,16.423538,16.785824,15.003083,16.857224,16.443611,16.470186,16.138071,16.367472,16.378733,-2.997981,-3.7599528,-3.823407,-2.688633,-3.7495632,-3.8139746,12.8482065,-6.4810295,12.849348,12.33515,-4.1573644,-10.372812,3.451517,4.12305,3.8538826,7.7195587,6.0252833,6.3010263,4.3920703,0.9960067,2.6648438,3.942057,3.828999,-3.767462,-10.354251,7.6503797,-2.4219222,-1.8908232,-0.6789343,6.302118,7.4516883,9.0661545,9.055936,-4.32152,-3.0678318,7.516634,-3.9940007,-2.4398859,-0.92469555,-2.1660485,12.878681,13.112854,-6.3067484,-6.0773373,-1.9673599,-2.1286483,-2.283744,-5.7999053,3.887091,2.6513224,3.1908038,-18.407372,-18.794912,-18.803896,-18.67282,-18.727386,12.648218,-5.2960916,-4.8143053,0.56894493,7.1682205,7.050356,-3.7101564,-3.4879203,2.940158,-1.4231653,-4.9560575,-2.1633403,-5.4740725,-1.8333483,-2.2558727,-1.9704174,8.1441965,2.4615276,3.090316,-10.43852,-10.591924,-10.589908,-10.420897,-10.482243,1.804443,1.4032319,1.7806877,2.121988,0.2706639,-3.9467428,-1.2779205,-8.635756,-8.574296,-8.579374,-8.510892,8.893434,-7.8917036,-8.551165,-8.396668,-8.257204,-8.255752,-8.851514,-9.108154,-8.825768,-8.857913,-4.6527815,0.2947616,-14.988208,-13.604261,-17.415062,-18.305573,-18.832315,-9.301071,-9.560393,-9.555657,-9.670966,-9.441318,-1.1859343,9.081938,-7.062127,-9.393646,19.471037,18.465096,-4.386734,-2.6795723,-5.9918156,-7.107353,-3.3029437,-4.22647,-2.6101336,5.4898577,5.9320045,6.004083,2.2496011,2.8208756,13.106383,6.686826,14.4608,1.8046105,-3.6984754,9.323196,4.635167,-1.2049861,-10.036099,-10.114832,-3.0797217,-1.5116006,-6.5953364,3.356968,-4.1996875,-5.171784,-5.619874,3.62612,-4.6452327,-5.806346,-5.7027407,-6.0302663,-5.535055,-4.667481,-0.9705784,-10.324822,3.6655889,-3.7461557,-2.2389753,-1.5961424,-2.2501364,-1.6649256,-1.5670307,-2.6036158,-1.7982978,-2.849102,13.476093,-4.371421,-6.5234365,7.6515193,6.0554214,-5.990803,-3.3839953,-4.651488,-3.45115,4.119783,-3.4583914,0.8640782,0.9596619,-3.2057033,-2.8076406,-2.754319,-5.9220457,-5.961896,-5.7395086,-5.8920293,8.93469,0.31679595,-0.2382469,3.8269231,4.8967505,3.6852372,3.5753818,-14.861031,-4.391576,-4.5441723,-4.5013313,-4.513944,-5.4798717,-7.388878,-7.3875694,-7.325697,-1.9431528,-2.3002415,-2.1761,-4.126793,0.35053065,-2.0525775,-3.0806544,0.36907864,0.44028538,-7.683225,-4.162512,-4.310654,-4.1194324,-4.143924,-3.9907227,-0.18083633,-4.7431455,-4.4721546,-2.7810285,-2.275415,-2.57279,-3.3634825,-3.6634638,-2.723806,-3.8302817,-3.8644447,-4.6302276,-2.3205085,11.593773,-4.3161173,4.3802876,-1.6981729,-14.460618,-0.9585955,-21.731844,-18.284002,-18.879251,-18.669209,-18.840782,-18.686964,-18.835922,-18.505299,-6.7591286,7.2080793,-5.8889575,-5.1160703,-2.3294535,-6.1247272,-2.3606315,11.627878,11.6684,11.459946,11.511099,11.288574,11.323079,11.193788,10.667609,-4.115592,-6.7278914,-6.5069547,-6.5573115,16.523907,16.980637,17.566244,17.124548,13.049881,12.90564,10.771993,13.573646,-3.9444041,-2.386568,3.6653671,3.4536996,-0.060284533,0.6056376,0.16228907,0.45316067,0.5640557,0.54218113,-1.4580339,-4.8877926,-5.0516725,-5.060767,-0.119668975,7.8579674,8.0918255,7.7100735,7.1897197,7.6812716,7.5191317,7.8873415,0.1454786,7.905115,-2.8093252,8.511398,7.7986536,7.723144,1.2971196,7.39997,7.882986,-12.777866,-12.794673,-12.776007,-12.682746,-13.13146,-12.861881,-12.834576,-12.532682,-12.636417,-12.716034,13.317935,-5.789281,-7.415052,13.089771,9.357969,4.7708707,2.0609343,1.1558273,0.85789984,-14.931129,-13.605878,-2.3972256,-5.0564027,-12.755506,-12.516877,-12.616532,-2.0820663,-2.394086,-2.1573315,-2.2513537,-1.2614989,-2.249126,-1.6764926,-2.312251,-6.435748,8.832197,-5.259757,8.562013,2.2544992,2.233967,-7.4014306,-7.4957795,-7.492945,-7.094569,-6.8772855,1.2857968,-3.6321194,-6.432582,0.01188454,-12.649924,6.8820844,-4.066439,-3.9963124,-2.9019444,-2.6856606,-4.0078516,-3.8410633,-3.911169,-4.1162786,-3.4542506,8.281333,-13.00868,4.5005984,13.371471,14.264715,13.8630905,8.190041,8.281869,7.9957514,8.824917,8.821591,8.6864605,-6.27872,8.158505,8.007566,7.943011,7.9092984,2.957965,3.0980787,8.209097,8.126489,-7.987279,-7.3825293,-7.3581367,5.0310135,13.397286,13.71534,13.054454,-5.1924477,-4.388693,-1.099183,-5.1504097,8.605592,8.699527,-4.31975,-8.669769,-3.6636832,-4.5731373,-4.230919,-4.2900023,-14.827857,-21.731165,8.5649805,8.516418,-3.5946763,-3.7939043,-4.165545,-3.9214132,-2.7561662,6.8058505,-4.0617723,3.972158,7.320848,3.3113477,4.037965,-14.920745,-21.733541,1.6652641,-3.188449,1.5717136,-1.1898443,-1.8306018,5.7970867,5.6499567,-1.5818944,-1.6058896,17.96859,18.00375,19.89575,19.044588,18.824377,-2.5682204,-1.3220379,-2.6915274,-0.7321409,-2.5886405,-5.265745,-3.4977808,-3.830586,-4.708856,-2.825032,-4.9266605,13.280346,6.6722703,6.361696,4.229889,3.722114,6.9840536,6.7691574,-0.2540844,0.26323867,3.5062482,3.5003393,3.8540387,3.531089,3.0822587,3.3791451,16.881582,16.879274,16.133825,13.325846,13.306026,-3.6116695,-6.640144,15.769564,15.779661,15.779367,15.780934,15.779828,2.7097158,0.25498274,-0.616208,0.38449124,0.21196052,-0.748932,-4.3447094,-6.4224277,-5.809369,-6.993242,-7.0930567,-6.734699,-6.471212,-7.582785,-7.412341,-7.593562,-7.502551,-7.3840785,-7.284058,-7.437574,-7.6362934,-7.5273976,-7.7618895,-7.215699,-13.6061945,-17.414904,-18.467321,-3.9886007,-2.84673,1.5710734,-0.7926682,-0.59399843,1.3084741,1.1095202,2.932361,1.0712861,3.3448358,3.6435802,-0.49229327,-1.0907431,0.17103262,-0.79925174,-0.6106176,0.09105471,4.4424977,5.649264,-3.4096646,-1.0601022,-1.3132639,-1.6046118,-0.92398024,0.3467518,-1.9858203,-1.2574106,-1.110221,-0.92524314,-1.0309795,-6.811577,-5.7381988,0.46290848,-0.20662753,0.5533875,-7.3575697,-4.516308,-4.324262,-5.223188,19.548027,19.282864,17.543762,-3.7920356,-2.3829722,-4.0984397,0.1045713,-6.1876183,-4.4354825,-1.2773303,-3.6652668,-1.8473321,-6.599236,-6.488784,-0.21015616,-6.8126087,0.20200038,3.06144,-3.5545328,-3.636846,-3.2461674,-3.3683932,-3.5958486,-3.6815166,-3.1416502,-2.8559089,-3.1950989,-2.9764113,-2.9218657,-3.2627974,-3.369826,-2.6087441,0.29294628,2.6905825,2.7787943,-5.1321545,-3.2147489,-5.0068383,-3.6154542,-1.4701266,-3.3395991,-3.3039815,3.7671745,3.0224745,-1.069182,-0.6937844,-1.0625709,-6.164454,-2.4131913,-2.3722463,1.2860323,3.5119188,7.5105267,-3.0210662,-2.3066368,-5.829778,-2.3605368,13.044731,12.367552,19.524366,19.520979,17.931963,19.390905,19.544476,3.2759495,3.751311,5.094326,5.156218,3.863354,6.103032,6.0513396,7.3618913,-1.4120867,-0.11104401,1.3084583,11.286306,-9.839842,-9.919048,-8.680785,-5.1448545,1.1386708,-11.147399,8.593314,11.790969,10.433071,10.644555,-3.7683153,-6.72914,-4.2074447,-4.0795937,-4.1048923,0.83126616,13.2594185,13.477477,11.626238,11.066753,13.389416,-6.383349,-0.2154595,-6.462959,-3.6498444,-15.093558,-21.734419,-18.231295,-18.814447,-18.719326,-4.338254,-5.8940635,-3.6630104,-4.461093,-2.8293045,-1.3285847,-4.464499,7.3983474,6.929754,-5.463683,-3.0366476,-2.7953901,-2.570523,-2.7127566,-2.9182615,-4.09062,-4.9386525,-6.085006,-5.4344273,-4.4560843,-4.279314,-4.4655213,-3.7479706,-2.4949584,-4.0129023,-4.1936364,-3.6116962,-4.822801,-2.5878577,-3.849933,-1.1517214,-2.045091,-2.90681,-6.7990522,-3.1142607,12.531736,-2.954379,-1.9490676,-2.9841986,-2.1550896,-5.157022,-2.1777623,-2.7773232,-17.769556,12.456657,-4.993819,-5.982883,-3.6461606,-2.5268753,-4.3569913,-3.6806576,-2.0567076,-1.7482293,-0.9679877,12.440476,-9.426717,-6.8950896,-1.0403073,18.498343,10.560057,11.615429,6.9164996,6.856053,-0.65358526,-0.47590536,-0.22998144,0.86090255,-6.2990108,-0.54037064,-3.434388,-3.6093123,-3.8696039,-5.6475286,-6.1295485,-2.380758,-4.994751,-2.7857132,-1.2067578,-1.3724464,-2.2949154,-2.4269617,-1.1214917,-2.5604944,-2.8200963,-7.7935877,-3.1768994,-1.2659068,11.285338,11.644111,-0.20927417,0.48924682,-1.4829137,-1.8833332,-1.7475386,-14.984308,-21.733137,2.4001799,4.4741387,3.1085076,3.792204,-3.7150283,-3.6986525,-4.1642165,-4.3370547,-4.1116114,-3.9809237,-2.3361802,-1.3439159,-1.5630641,-1.2556596,-8.998065,-8.990861,-9.068644,-8.899398,-2.1383533,-9.013972,-9.011751,-8.954296,-9.064157,-9.029663,-9.052058,-6.488344,-2.7722692,-1.532147,-2.948725,-4.0119905,-3.753667,4.4283004,1.7388449,3.5771878,8.506301,0.80200285,8.34558,-6.538187,2.1380193,6.761151,-0.5209661,-0.19651987,7.364097,-6.4904118,-5.4998207,-9.563794,0.07934447,-2.3403306,-1.4937837,8.558196,-12.618309,-13.049577,-5.9085636,0.22963293,-1.4179786,-1.7164675,-1.3101531,7.8265347,-3.7272465,-1.518207,-1.2972735,-1.4202837,-2.7949996,4.047388,-0.9268559,-4.569095,5.246153,-1.4305191,-0.13581194,-0.5853447,-5.6037498,5.2115307,5.231406,5.232162,5.266634,-13.372374,-13.605361,-17.415077,-0.29561827,-1.436113,-0.6882396,-0.53712165,-4.2130723,-5.871735,-2.163301,-4.1837792,-4.726748,-5.1184874,8.566421,8.873855,-6.5720963,-5.7654543,-5.885592,-5.291327,-5.61915,-5.648167,-4.303273,-1.4166435,-3.8861382,-1.7028675,-3.9041047,-4.985735,-5.298409,-4.7114267,-4.7485957,8.139079,8.222579,-5.1606975,-1.0662047,-1.593558,-0.6129996,-1.1876012,-1.6437459,-4.6439614,-3.0116558,-2.3846948,-5.9618526,-4.1038737,-3.8229673,-2.2799292,-4.4686255,-4.524909,6.670043,1.4041111,0.66687715,0.41182855,-0.38243195,1.283097,0.43517599,0.67227536,-0.42215553,-6.892783,-6.633282,-6.6464343,4.7834864,8.57018,8.569539,-6.7718873,-6.753175,-6.823348,-2.293116,-2.8214345,-5.9564734,0.71344906,-0.033647116,-2.935449,-2.0998344,-2.228709,8.577941,8.563618,12.672645,12.17376,8.009089,-6.3131914,-6.118927,8.3143,-3.1288862,8.132673,-6.724507,-7.8201237,-4.696557,-4.4443326,-3.5165675,-2.9401798,-2.6142187,-2.6764026,-1.4788457,-5.0031085,-5.2497272,-3.408534,-3.4335692,0.5135001,-4.9538155,-8.124304,-8.041445,-3.714138,-7.4094896,-7.009425,-7.0935407,-6.81988,-7.1694374,1.2536784,2.1832433,-6.991052,-6.8292503,-5.84013,-5.4790716,-5.500314,-5.6569715,-4.522683,-5.53769,-1.190622,-5.393191,-0.45948485,-1.129511,-0.26266396,-0.50046813,-1.9734381,10.033184,-0.7485119,-5.8163195,-5.8814216,-5.7423635,-5.5928874,1.0195153,-0.21593536,8.080919,-6.4766207,-0.7731884,0.05405278,1.2000314,1.3739918,1.7586697,2.9051723,-3.163661,-3.4670043,-4.7554717,-5.0694003,3.079326,13.194924,-1.3030703,-8.849627,-3.4772558,2.0785306,-9.477484,-0.11448067,0.4532816,0.011474506,0.44544607,14.3826275,14.314611,14.244988,5.4308887,5.4315696,12.401404,11.929537,11.887262,10.937718,10.531783,10.54413,10.2186165,10.142418,10.534677,12.375118,-2.494367,-2.3443334,13.467306,-6.971951,-4.4884586,-4.4510865,-4.2474914,-3.9748127,-4.4985185,-5.211657,7.6883636,-3.4039333,-4.457601,2.8325768,-3.6873112,-3.7934773,-3.5821917,-4.1178617,-3.342686,7.511878,-4.8693957,-1.7355878,-0.9333907,-1.4405029,-0.9705838,0.021194246,0.11699227,-2.4477088,-2.0883303,-4.043549,-1.5609608,0.32118243,-7.189442,-2.9171913,2.5491912,-1.1840246,-1.8314701,-1.7998037,0.12358445,5.481661,4.609923],\"xaxis\":\"x\",\"y\":[8.5517025,2.9862008,-0.76149184,0.7452125,-0.17265947,0.102883905,0.22502756,-18.6889,-18.072287,-17.397297,-18.661627,-18.248919,-18.55972,-18.400204,-18.56912,-19.226387,-19.224133,-18.447668,-18.644619,-19.10212,2.0974107,-0.17653464,-0.05094658,0.08408934,7.377086,7.926084,-1.8669925,-2.386287,-2.243474,-1.6405565,-6.1459155,-6.1266866,6.6172223,6.504313,6.462636,4.7387223,-0.022207018,2.6267157,3.8834376,4.0056915,4.0069685,4.1109934,4.6526904,3.0272093,-1.3161205,-1.7449063,-1.4909266,-1.3900754,2.3737164,2.4338162,3.5623684,3.090724,2.8866196,3.0398767,3.526823,3.4471562,3.8295803,3.291289,3.457446,3.6099958,3.256651,3.068702,3.3471766,2.0134633,2.4209645,1.9216775,3.4794934,3.1721425,2.2949555,1.8585547,1.9421095,3.2793665,3.3732378,2.204845,1.9095415,2.5366201,2.5245697,2.9516582,2.7103863,3.4303293,3.1649485,3.0516052,3.1141307,3.1405082,2.9708755,3.057083,3.3646045,4.303269,4.759351,4.2225423,5.0707474,5.0961833,3.9875507,1.7884299,5.1609187,4.819321,3.3976176,-0.36107016,1.4800266,1.5643743,1.6299824,2.5828242,1.837281,1.8471501,0.34710938,0.6922345,0.13430934,1.0289475,1.0827351,-6.5882807,-1.0061647,1.6411643,-6.248957,-6.9607787,-0.3661113,-0.58335316,2.643535,1.6994836,1.7115391,-2.0304883,-1.6681566,2.764885,-0.91163486,-11.914921,-0.34325525,-0.055231337,5.28083,4.38159,0.27265748,0.29161322,6.1679745,5.683287,5.5969,0.018533684,2.1769543,3.235052,3.303289,-4.256027,-4.2891016,-4.3223863,-3.8729775,-4.293367,4.507075,-1.3543291,-1.0963933,0.16860013,-9.505611,-9.445089,-5.7926044,-8.265442,-0.3987684,-3.7916517,1.8847226,7.1005177,3.3336484,7.180091,6.931064,7.113099,2.9238627,4.494794,4.3313937,-0.90001285,-0.8024698,-1.0240211,-0.87600803,-1.085395,2.432373,3.046423,2.8972776,2.4757123,-6.287032,-3.949284,-6.18289,3.6732888,3.7322254,3.7200525,3.741642,2.611402,3.476469,3.5283718,3.6985865,3.7026567,2.9079971,3.1549277,3.0202498,3.278812,3.293735,4.560989,7.7495084,-2.1187222,19.448877,14.81251,-4.2260337,-4.322864,2.3038352,2.9403648,2.948707,2.906332,2.9450688,2.4773176,0.44709277,2.8476105,-0.6573076,2.3389792,3.8136308,0.69028383,3.1333234,1.0966108,-1.105724,-6.952175,-7.239729,-1.6731023,1.5570792,1.5947315,1.6300222,2.8632174,1.9497151,4.6366544,-0.61129427,4.8067155,4.1485434,4.016363,-19.994543,1.8786842,1.7243963,0.94028836,0.8404725,2.0901976,-3.6988754,-0.15083593,2.4517326,-1.2295834,1.0611838,-0.42524233,-1.5862391,-1.1266451,0.9207667,0.9680249,-0.05153312,-0.21466525,-0.2877947,3.7807944,-0.6956216,4.3024325,-6.915978,-7.3870516,-7.7574763,-7.4045587,-7.794473,-6.9186544,-6.6039085,-7.6470103,-3.7768114,5.3106065,3.7837596,2.9007761,2.3128994,1.7347363,2.711291,3.7643986,3.6277993,3.5336366,1.7053204,1.3917873,1.5044059,1.4845482,0.122842535,-0.014482749,0.24589595,1.5580657,1.5522779,1.2240044,1.4788451,2.7443361,4.997092,5.0368423,2.3235247,1.9943928,2.942516,4.1382976,-2.0378845,2.3014407,2.4997063,2.515958,2.3136346,1.0750774,-1.7060689,-1.6943167,-2.1727607,1.866213,3.0878108,3.8180373,2.7327008,4.4798346,4.495883,5.0760965,7.7303424,7.959604,-1.1414639,-4.3090243,-5.579311,-5.8605165,-3.5966496,-4.2638373,0.62997365,3.3251767,3.4639237,2.540786,2.3860075,3.2096205,3.8327792,3.9522793,3.8756557,3.0349212,3.609523,-0.9941757,3.9497533,17.955301,-6.3763843,0.95020884,1.3551445,-1.9133875,3.1317122,8.553542,-4.1757755,-4.3813977,-3.6828854,-4.295524,-3.6823475,-4.3038917,-3.5813515,1.3198295,0.8871124,3.4087977,3.476753,0.35147107,-0.51249343,-12.574284,2.0836215,3.2222528,2.2575762,2.3095129,2.4063528,2.209617,2.4161546,2.7304306,-0.13880704,-0.23320776,-0.28738335,-0.47621748,2.7573245,3.440772,3.2869203,2.6295252,5.359791,4.3612833,3.606629,4.071954,2.1446533,-12.359099,-1.7191043,-0.5858207,7.726394,8.088328,7.9170194,8.2296505,8.305667,8.44938,5.9916263,-6.601365,-6.467108,-6.6078215,9.412814,3.5060334,3.5862885,2.7881114,3.1025517,0.35845503,0.18251537,0.13119179,-6.034216,-0.118062355,3.3352904,-0.17668758,0.27729788,0.00088982907,0.7459593,-0.30414352,-0.15035169,-18.340363,-18.416536,-18.585857,-18.442936,-18.293182,-18.273066,-18.538248,-18.377916,-18.480276,-18.312439,3.6909783,2.7844002,-1.6724257,3.1506264,2.949508,0.61488754,5.457903,6.1049423,5.8028936,-2.1328964,19.450554,-6.706181,-6.64495,-17.805876,-17.718697,-17.549204,-1.763268,-2.3083184,0.051521488,-1.8954487,0.23416004,-2.0829797,-2.0162172,-1.6547774,1.6244988,2.9946494,3.2463346,2.4855812,-0.18383937,-0.25658748,7.080307,6.9549294,7.106979,6.3629093,7.3670506,-0.020666296,5.549069,0.6395452,-6.1373086,-1.5821644,-1.1533972,-7.7045803,-7.399823,-6.596023,-7.204461,-7.31999,-7.3312464,-6.462124,-7.900426,-7.2914734,4.402706,-1.7358844,3.9135942,3.717933,3.754961,3.6528087,-10.300992,-10.757996,-10.195769,-11.7239485,-11.7938175,-11.735875,0.0203231,-10.463756,-10.433004,-9.991198,-10.227758,-0.5759676,0.2319345,-10.304211,-10.151185,-1.9470407,-2.0213766,-2.1132748,-0.14806145,4.2366395,3.8877265,5.117531,3.4293408,3.604856,4.4984217,2.896049,2.8632429,2.8008425,-6.1098332,-0.27962872,-6.6175127,-6.035317,-6.178859,-6.037572,-2.041613,8.5533905,-11.149552,-11.062,-8.177594,-8.001106,-8.527897,-8.054417,-8.029932,2.371014,-8.625916,1.7386568,2.8302078,1.1085556,0.91745204,-2.1122508,8.552047,4.0784063,0.83961105,1.1472657,-0.46859068,-1.3859452,1.7213235,1.5764133,-6.298042,-6.410205,4.1137314,4.241757,2.3195384,2.4677587,2.5045109,-6.9100003,-6.8143897,-6.9736295,-6.626328,-7.1314826,0.8505363,4.4228683,4.3941746,-7.5856304,-10.036045,-6.3270454,5.588404,2.5011637,2.2956364,1.5769194,1.207685,2.6347423,2.5852065,9.69946,8.288378,1.174051,0.9530938,0.66510767,-0.16768876,-0.23880278,-0.551691,1.8737353,1.8462358,3.0145485,3.7262201,5.3907194,6.3328204,0.291808,-16.045069,-16.036182,-16.03693,-16.03959,-16.040173,3.3597758,8.263332,8.259872,7.9263415,7.835605,8.163156,1.2651924,7.0742097,7.1983733,6.7238727,6.8592057,7.0110617,6.072049,-2.4467523,-2.098077,-2.3228517,-2.4169667,-2.4397511,-2.2943707,-2.322486,-2.4919026,-2.4301548,-2.319001,-2.0310688,19.448849,14.812245,-4.205074,-7.791873,-6.8995357,-2.0615563,-2.2156816,-1.8674368,2.6001627,2.3282564,-2.381235,3.1658077,0.5582294,-0.8448134,7.340754,6.998281,8.094631,7.22431,8.9257145,7.737521,2.4506428,2.8933601,2.7854376,3.8828418,3.2030592,0.08604009,-0.44689965,6.0655684,-1.2261436,-0.7898345,-1.2476465,-0.39940852,-1.9703994,0.63869315,2.238449,8.144415,7.2945094,1.0738662,-1.3607992,0.73575383,-1.5879433,1.7882633,2.296516,2.4139528,2.8464117,-4.2598147,-4.5048943,-5.962051,-1.0895607,-1.3274755,3.425319,-1.7666917,6.6976013,0.67544913,7.8829393,7.5998535,3.0515976,7.2326965,-6.3251185,4.063758,6.269459,6.0820384,5.8195353,6.0256734,6.1951337,6.3639145,3.241957,3.5076966,3.160358,2.9854307,3.2999554,3.1846364,2.8327048,3.1785264,0.5275757,3.1761935,2.7259238,-0.59218055,0.1303948,-6.461774,-6.261554,-6.1372576,0.22529835,-0.022881115,2.267973,3.982419,4.4211426,4.666284,4.199959,3.645426,-12.117638,-12.473329,0.57040733,1.0276572,3.4683099,4.0543113,-0.14152431,-0.3312392,-12.568124,5.3500185,5.1622524,2.3149946,2.554042,4.343712,3.2684126,2.3893554,0.716694,1.4096925,1.5097451,1.1606477,1.231154,1.8031625,1.8222711,2.4517233,0.0077469656,-0.9152695,4.080122,3.765851,0.36446747,0.4452418,-0.4606923,0.023312485,3.6055412,-1.260608,2.725652,2.2494876,2.7758157,2.4809299,-1.6409949,7.431783,-2.3605912,-2.5059261,-2.4200208,1.2919562,5.615732,3.9133983,4.0684853,4.064004,5.6215553,7.832358,-6.031619,7.5746865,-4.1759915,-2.161697,8.551687,-4.2703424,-4.33845,-4.1950994,-6.306752,2.2728198,3.3856862,2.4048417,1.0313467,-1.6815989,1.8536465,1.5843374,2.055935,2.4603631,3.6623533,-8.088783,-7.5753703,-8.006068,-8.404312,-8.063418,-7.2372403,0.14171422,-0.43518385,-6.041816,-6.048232,-5.463258,-5.4825354,-4.7484875,-5.5573077,-5.624898,6.2714496,-6.943702,-9.696546,-6.7676682,-0.03816778,-5.8961763,1.5797613,-1.615756,1.3547467,4.8630586,4.4954033,3.7178488,4.4349427,3.3846045,-6.456924,-7.5173025,-7.496517,-3.9491704,4.734618,1.1879565,1.7193835,1.6221616,1.6524843,1.2113949,6.7249312,-2.2306914,-0.48542896,2.6932108,4.2162604,1.7117814,0.42836505,3.1619158,3.4584472,3.3555098,2.0880895,1.6006099,1.4262403,2.8789,3.1498713,3.7996945,3.3546696,7.7468114,2.5966306,6.2626696,6.1461754,3.425321,0.8029807,0.029764347,1.0852479,0.7011841,0.8644222,0.9210062,-1.4624746,-1.3707241,-1.2330371,-1.2233127,-1.3222611,-1.1914123,6.9543443,-1.1577661,1.8770801,3.6504169,1.9826988,-0.5151642,4.291593,4.3101397,3.4592168,3.6446457,-2.1313977,8.551926,1.0082529,1.3100997,2.0369585,0.70449114,-2.702375,-2.3619041,-1.9171474,-1.9901308,-2.4272754,-2.4056847,-1.6971194,-1.9650129,-1.5044695,2.1740487,2.7978232,2.5579915,2.6758823,2.7741773,5.1240425,2.8521042,2.6834607,2.7716978,2.7284222,2.7575881,2.8894458,2.1002235,-6.0447283,-7.5726914,-6.1304507,4.046775,4.3612337,0.6853942,2.1950457,1.1230156,-11.727315,1.1097747,-10.871662,0.57824236,0.45504227,2.2794883,7.8887606,8.433375,1.7080736,2.6233647,2.7875562,1.1463596,4.329558,3.919651,0.7294405,-0.12699087,-18.03585,-18.25157,-0.3615654,1.0242124,0.5633185,1.7102659,2.4148102,3.9795702,1.7629443,1.5707253,2.4103668,1.4115174,1.2648206,0.8972945,2.3030517,-0.8644084,2.7254205,3.227965,4.6210175,5.2187,0.3394491,-0.008783489,0.05056215,0.14961053,0.025155142,-1.0851651,19.448704,14.812992,-0.5059164,4.3834176,-0.95447284,-1.6698345,-6.6959047,-0.5070009,1.8710239,-0.8947044,-2.0774684,-1.3347877,-11.136841,-11.781271,-0.7751984,-0.7192436,-0.7949927,-1.5729929,-1.2884072,-0.50143784,-0.5870239,0.5998031,-0.6860939,1.3176404,-0.65731996,-1.487546,1.7252908,-1.2555784,-2.1634927,2.563478,2.8657827,-0.45145187,-0.59893805,-0.3266397,-0.3288932,-2.1537669,-0.24965812,-0.77141327,-0.42718157,-1.6520956,-1.1841311,-0.74005383,-0.51251596,-0.12567598,-1.0889422,-0.7609087,1.0880615,1.5422902,1.0334226,2.184428,1.9050254,1.6192725,1.2819092,1.2001301,0.8841305,-0.5096091,-0.24949063,-0.5251123,0.6246923,-11.158174,-11.228876,5.825701,6.69325,6.8726826,-0.85013926,-2.3038063,6.5949373,3.904194,-6.0675774,3.8900838,3.4452236,-0.29619107,-11.278351,-11.223733,4.733096,4.79526,3.2921164,7.715964,7.2493196,-10.2182045,-2.0051308,-10.136326,0.8815234,1.6642045,-1.6580715,-1.6036047,-2.1890767,-0.91712517,-2.1314619,-1.7166655,-1.2302821,-1.6063734,-1.5089903,-0.7796846,-1.2306874,2.9225066,-1.4536271,1.958801,1.880112,-1.7666018,6.357035,5.771415,6.1443915,5.840889,6.1444864,0.90296036,-0.34056324,6.622114,6.871988,7.284451,7.1484866,6.9538455,7.25762,6.406535,6.9660745,5.758368,6.6747303,4.0418954,-1.1120936,-0.5658428,-1.1051533,2.81401,2.93844,-7.1626654,3.700213,3.5807388,3.5655236,3.3107398,2.6572468,-6.184326,-9.3575115,0.58730507,4.69142,5.352143,4.614518,4.6830425,4.2327905,5.9143353,3.817934,-0.2991102,-0.6208048,0.1528945,2.8278098,5.501983,-6.6979613,0.6266854,-2.0665252,3.9566872,0.06616966,8.250492,8.271813,7.762977,8.24958,3.854407,3.728133,3.7718828,2.4906301,0.27274373,2.406495,2.383725,2.2428625,2.5963767,2.5339003,2.779674,2.4455283,2.5197382,2.7333941,2.5598464,0.4985827,0.73072845,5.648881,-0.3675747,-2.0796063,-2.113573,-2.4063146,-2.190395,-2.2963972,2.5759394,3.489825,-2.3003275,-2.413469,3.4881072,-6.8540025,-7.828578,-7.6176805,-7.65117,5.530295,-9.609861,3.3895686,-1.6386366,0.1231539,1.5628082,1.7145748,8.474539,8.444254,4.0867376,3.384787,2.5927172,0.698132,6.230188,6.512942,-5.6058507,-0.4551095,-3.0195827,-4.546762,-5.241815,3.660526,1.7639054,2.6639824],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Datasets server - worker\\n\\n\\u003e Workers that pre-compute and cache the response to \\u002fsplits, \\u002ffirst-rows,...\"],[\"- `WORKER_CONTENT_MAX_BYTES`: the maximum size in bytes of the response content computed by a worker...\"],[\"- `WORKER_MAX_MEMORY_PCT`: maximum memory (RAM + SWAP) usage of the machine (in percentage) allowed ...\"],[\"Also, it's possible to force the parent directory in which the temporary files (as the current job s...\"],[\"### Huggingface_hub library\\n\\nIf the Hub is not https:\\u002f\\u002fhuggingface.co (i.e., if you set the `COMMON_...\"],[\"- `PARQUET_AND_INFO_COMMIT_MESSAGE`: the git commit message when the worker uploads the parquet file...\"],[\"- `PARQUET_AND_INFO_URL_TEMPLATE`: the URL template to build the parquet file URLs. Defaults to `\\u002fda...\"],[\"### Duckdb Index worker\\n\\nSet environment variables to configure the `duckdb-index` worker (`DUCKDB_I...\"],[\"### Descriptive statistics worker\\n\\nSet environment variables to configure the `descriptive-statistic...\"],[\"`column_statistics` content depends on the feature type, see examples below.\\n##### class_label\\n\\n\\u003cdet...\"],[\"##### int\\n\\nAs bin edges for integer values also must be integers, bin size is counted as `np.ceil((m...\"],[\"```python\\n{\\n    \\\"column_name\\\": \\\"direction\\\",\\n    \\\"column_type\\\": \\\"int\\\",\\n    \\\"column_statistics\\\": {\\n   ...\"],[\"259\\n            ],\\n            \\\"bin_edges\\\": [\\n                54,\\n                59,\\n              ...\"],[\"\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n##### string_label\\n\\nIf the number of unique values in a column (within requested sp...\"],[\"##### bool\\n\\n\\u003cdetails\\u003e\\u003csummary\\u003eexample: \\u003c\\u002fsummary\\u003e\\n\\u003cp\\u003e\\n\\n```python\\n{\\n    'column_name': 'bool__nan_col...\"],[\"--\\ntitle: Datasets Server Admin UI\\nemoji: 📊\\ncolorFrom: gray\\ncolorTo: purple\\nsdk: gradio\\nsdk_version:...\"],[\"Filter rows in a dataset\\n\\nDatasets Server provides a `\\u002ffilter` endpoint for filtering rows in a data...\"],[\"\\u003cTip\\u003e\\n  Note that, following SQL syntax, string values in comparison predicates must be enclosed in ...\"],[\"List Parquet files\\n\\nDatasets can be published in any format (CSV, JSONL, directories of images, etc....\"],[\"The `\\u002fparquet` endpoint accepts the dataset name as its query parameter:\\n\\n\\u003cinferencesnippet\\u003e\\n\\u003cpython...\"],[\"The endpoint also gives the filename and size of each file:\\n\\n```json\\n{\\n  \\\"parquet_files\\\": [\\n    {\\n  ...\"],[\"## Sharded Parquet files\\n\\nBig datasets are partitioned into Parquet files (shards) of about 500MB ea...\"],[\"```json\\n{\\n  \\\"parquet_files\\\": [\\n    {\\n      \\\"dataset\\\": \\\"amazon_polarity\\\",\\n      \\\"config\\\": \\\"amazon_pol...\"],[\"In that case the Parquet files are generated up to 5GB and placed in a split directory prefixed with...\"],[\"```json\\n{\\n  \\\"ParaphraseRC\\\": {\\n    \\\"test\\\": [\\n      \\\"https:\\u002f\\u002fhuggingface.co\\u002fapi\\u002fdatasets\\u002fduorc\\u002fparquet...\"],[\"```json\\n[\\n  \\\"https:\\u002f\\u002fhuggingface.co\\u002fapi\\u002fdatasets\\u002fduorc\\u002fparquet\\u002fParaphraseRC\\u002ftrain\\u002f0.parquet\\\"\\n]\\n```\\n\\n...\"],[\"datasets-server Helm chart\\n\\nThe `datasets-server` Helm [chart](https:\\u002f\\u002fhelm.sh\\u002fdocs\\u002ftopics\\u002fcharts\\u002f) ...\"],[\"How to contribute to the Datasets Server?\\n\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fCon...\"],[\"**do not** work on the `main` branch.\\n\\n4. Set up a development environment by following the [develop...\"],[\"DuckDB\\n\\n[DuckDB](https:\\u002f\\u002fduckdb.org\\u002fdocs\\u002f) is a database that supports reading and querying Parquet ...\"],[\"To query multiple files - for example, if the dataset is sharded:\\n\\n\\u003cinferencesnippet\\u003e\\n\\u003cpython\\u003e\\n```py...\"],[\"Overview\\n\\nDatasets Server automatically converts and publishes public datasets less than 5GB on the ...\"],[\"Pandas\\n\\n[Pandas](https:\\u002f\\u002fpandas.pydata.org\\u002fdocs\\u002findex.html) is a popular DataFrame library for data ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"---\\n\\n## Adding a new element to the navigation bar\\n\\nAccepted files are Markdown (.md or .mdx).\\n\\nCrea...\"],[\"Check dataset validity\\n\\nBefore you download a dataset from the Hub, it is helpful to know if a speci...\"],[\"## Check if a dataset is valid\\n\\n`\\u002fis-valid` checks whether a specific dataset loads without any erro...\"],[\"Security Policy\\n\\n## Supported Versions\\n\\n\\u003c!--\\nUse this section to tell people about which versions of...\"],[\"Datasets server admin machine\\n\\n\\u003e Admin endpoints\\n\\n## Configuration\\n\\nThe worker can be configured usi...\"],[\"### Common\\n\\nSee [..\\u002f..\\u002flibs\\u002flibcommon\\u002fREADME.md](..\\u002f..\\u002flibs\\u002flibcommon\\u002fREADME.md) for more informatio...\"],[\"Get dataset information\\n\\nDatasets Server provides an `\\u002finfo` endpoint for exploring the general info...\"],[\"```json\\n{\\n    \\\"dataset_info\\\": {\\n        \\\"description\\\": \\\"DuoRC contains 186,089 unique question-answe...\"],[\"\\\"splits\\\": {\\n            \\\"train\\\": {\\n                \\\"name\\\": \\\"train\\\",\\n                \\\"num_bytes\\\": 239...\"],[\"Datasets server\\n\\n\\u003e Integrate into your apps over 10,000 datasets via simple HTTP requests, with pre-...\"],[\"Download slices of rows\\n\\nDatasets Server provides a `\\u002frows` endpoint for visualizing any slice of ro...\"],[\"\\u003cinferencesnippet\\u003e\\n\\u003cpython\\u003e\\n```python\\nimport requests\\nheaders = {\\\"Authorization\\\": f\\\"Bearer {API_TOKE...\"],[\"```json\\n\\u002f\\u002f https:\\u002f\\u002fdatasets-server.huggingface.co\\u002frows?dataset=duorc&config=SelfRC&split=train&offse...\"],[\"\\\"plot\\\": \\\"The film is centered on Mortal Kombat, a fighting tournament between the representatives of...\"],[\". Sonya worries that they may not win against Goro, but Raiden disagrees. He reveals their own fears...\"],[\". During the lengthy battle, Liu faces not only Tsung, but the souls that Tsung had forcibly taken i...\"],[\"\\\"title\\\": \\\"Mortal Kombat\\\",\\n        \\\"question_id\\\": \\\"40c1866a-b214-11ba-be57-8979d2cefa90\\\",\\n        \\\"qu...\"],[\"\\\"plot\\\": \\\"The film is centered on Mortal Kombat, a fighting tournament between the representatives of...\"],[\". Sonya worries that they may not win against Goro, but Raiden disagrees. He reveals their own fears...\"],[\". During the lengthy battle, Liu faces not only Tsung, but the souls that Tsung had forcibly taken i...\"],[\"\\\"title\\\": \\\"Mortal Kombat\\\",\\n        \\\"question_id\\\": \\\"f1fdefcf-1191-b5f9-4cae-4ce4d0a59da7\\\",\\n        \\\"qu...\"],[\"## Image and audio samples\\n\\nImage and audio are represented by a URL that points to the file.\\n\\n### I...\"],[\"Quickstart\\n\\n[[open-in-colab]]\\n\\nIn this quickstart, you'll learn how to use the Datasets Server's RES...\"],[\"| Endpoint                    | Method | Description                                             | Q...\"],[\"| [\\u002fsearch](.\\u002fsearch)         | GET    | Search text in a dataset split.                         | -...\"],[\"There is no installation or setup required to use Datasets Server.\\n\\n\\u003cTip\\u003e\\n  Sign up for a \\u003ca href=\\\"h...\"],[\"## Gated datasets\\n\\nFor gated datasets, you'll need to provide your user token in `headers` of your q...\"],[\"## Check dataset validity\\n\\nTo check whether a specific dataset is valid, for example, [Rotten Tomato...\"],[\"```json\\n{ \\\"preview\\\": true, \\\"viewer\\\": true, \\\"search\\\": true }\\n```\\n\\n## List configurations and splits\\n\\n...\"],[\"\\u003cinferencesnippet\\u003e\\n\\u003cpython\\u003e\\n```python\\nimport requests\\nAPI_URL = \\\"https:\\u002f\\u002fdatasets-server.huggingface...\"],[\"## Download slices of a dataset\\n\\nThe `\\u002frows` endpoint returns a JSON list of a slice of rows of a da...\"],[\"You can download slices of 100 rows maximum at a time.\\n\\nThe response looks like:\\n\\n```json\\n{\\n  \\\"featu...\"],[\"\\u003cinferencesnippet\\u003e\\n\\u003cpython\\u003e\\n```python\\nimport requests\\nAPI_URL = \\\"https:\\u002f\\u002fdatasets-server.huggingface...\"],[\"## Access Parquet files\\n\\nDatasets Server converts every public dataset on the Hub to the [Parquet](h...\"],[\"## Get the size of the dataset\\n\\nThe `\\u002fsize` endpoint returns a JSON with the size (number of rows an...\"],[\"This returns a URL to the Parquet file for each split:\\n\\n```json\\n{\\n  \\\"size\\\": {\\n    \\\"dataset\\\": {\\n     ...\"],[\"Datasets server SSE API\\n\\n\\u003e Server-sent events API for the Datasets server. It's used to update the H...\"],[\"libapi\\n\\nA Python library for the API services\\n\\n## Configuration\\n\\nThe APIs can be configured using en...\"],[\"### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`API_UVIC...\"],[\"ClickHouse\\n\\n[ClickHouse](https:\\u002f\\u002fclickhouse.com\\u002fdocs\\u002fen\\u002fintro) is a fast and efficient column-orient...\"],[\"Let's start by identifying the most popular artists:\\n\\n```bash\\n.\\u002fclickhouse local -q \\\"\\n    SELECT cou...\"],[\"┌─danceability─┬─dist───────────────────────────────────────────────────────────────────────────────...\"],[\"## User-defined function (UDFs)\\n\\nA user-defined function (UDF) allows you to reuse custom logic. Man...\"],[\"You can make this even easier by creating another function that calls `hugging_paths` and outputs al...\"],[\"Analyze a dataset on the Hub\\n\\n[[open-in-colab]]\\n\\nIn the Quickstart, you were introduced to various e...\"],[\"## Read dataset with Pandas\\n\\nWith the URL, you can read the Parquet file into a Pandas DataFrame:\\n\\n`...\"],[\"Polars \\n\\n[Polars](https:\\u002f\\u002fpola-rs.github.io\\u002fpolars-book\\u002fuser-guide\\u002f) is a fast DataFrame library wri...\"],[\"To read multiple Parquet files - for example, if the dataset is sharded - you'll need to use the [`c...\"],[\"```py\\nimport polars as pl\\n\\nq = (\\n    pl.scan_parquet(\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fblog_authorshi...\"],[\"Explore statistics over split data\\n\\nDatasets Server provides a `\\u002fstatistics` endpoint for fetching s...\"],[\"The response JSON contains two keys:\\n* `num_examples` - number of samples in a split\\n* `statistics` ...\"],[\"```json\\n{\\n  \\\"num_examples\\\": 8551,\\n  \\\"statistics\\\": [\\n    {\\n      \\\"column_name\\\": \\\"idx\\\",\\n      \\\"column_...\"],[\"## Response structure by data type\\n\\nCurrently, statistics are supported for strings, float and integ...\"],[\"* minimum, maximum, mean, and standard deviation values\\n* number and proportion of `null` values\\n* h...\"],[\"\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n### string_label\\n\\nIf string column has less than or equal to 30 unique values with...\"],[\"libcommon\\n\\nA Python library with common code (cache, queue, workers logic, processing steps, configu...\"],[\"## Common configuration\\n\\nSet the common environment variables to configure the following aspects:\\n\\n-...\"],[\"## Queue configuration\\n\\nSet environment variables to configure the job queues to precompute API resp...\"],[\"Splits and configurations\\n\\nMachine learning datasets are commonly organized in *splits* and they may...\"],[\"Configurations are flexible, and can be used to organize a dataset along whatever objective you'd li...\"],[\"e2e\\n\\nEnd to end tests, written in Python...\"],[\"Preview a dataset\\n\\nDatasets Server provides a `\\u002ffirst-rows` endpoint for visualizing the first 100 r...\"],[\"- `dataset`: the dataset name, for example `glue` or `mozilla-foundation\\u002fcommon_voice_10_0`\\n- `confi...\"],[\"```json\\n{\\n  \\\"dataset\\\": \\\"duorc\\\",\\n  \\\"config\\\": \\\"SelfRC\\\",\\n  \\\"split\\\": \\\"train\\\",\\n  \\\"features\\\": [\\n    {\\n    ...\"],[\"\\\"feature_idx\\\": 6,\\n      \\\"name\\\": \\\"no_answer\\\",\\n      \\\"type\\\": { \\\"dtype\\\": \\\"bool\\\", \\\"_type\\\": \\\"Value\\\" }\\n   ...\"],[\"\\\"question\\\": \\\"How did the police arrive at the Mars mining camp?\\\",\\n        \\\"answers\\\": [\\\"They arrived ...\"],[\"\\\"title\\\": \\\"Ghosts of Mars\\\",\\n        \\\"question_id\\\": \\\"a9f95c0d-121f-3ca9-1595-d497dc8bc56c\\\",\\n        \\\"q...\"],[\"## Truncated responses\\n\\nFor some datasets, the response size from `\\u002ffirst-rows` may exceed 1MB, in w...\"],[\"Search text in a dataset\\n\\nDatasets Server provides a `\\u002fsearch` endpoint for searching words in a dat...\"],[\"For example, let's search for the text `\\\"dog\\\"` in the `train` split of the `SelfRC` configuration of...\"],[\"```json\\n{\\n  \\\"features\\\": [\\n    {\\n      \\\"feature_idx\\\": 0,\\n      \\\"name\\\": \\\"plot_id\\\",\\n      \\\"type\\\": { \\\"dt...\"],[\"\\\"plot\\\": \\\"The film begins with clips that track a telephone call between London and Geneva, where a u...\"],[\". She leaves saying that she feels nothing but pity for him.\\\\nWhilst visiting Kern, Valentine hears ...\"],[\". It is his birthday and he offers her pear brandy for a toast. During their conversation he reminis...\"],[\". While preparing for his exam, he once went to the same theatre where the fashion show took place a...\"],[\"\\\"title\\\": \\\"Three Colors: Red\\\",\\n        \\\"question_id\\\": \\\"7c583513-0b7f-ddb3-be43-64befc7e90cc\\\",\\n       ...\"],[\"\\\"plot\\\": \\\"The film begins with clips that track a telephone call between London and Geneva, where a u...\"],[\". She leaves saying that she feels nothing but pity for him.\\\\nWhilst visiting Kern, Valentine hears ...\"],[\". It is his birthday and he offers her pear brandy for a toast. During their conversation he reminis...\"],[\". While preparing for his exam, he once went to the same theatre where the fashion show took place a...\"],[\"\\\"title\\\": \\\"Three Colors: Red\\\",\\n        \\\"question_id\\\": \\\"80becb22-908d-84bc-3a5f-00b620d551bc\\\",\\n       ...\"],[\"If the result has `partial: true` it means that the search couldn't be run on the full dataset becau...\"],[\"Developer guide\\n\\nThis document is intended for developers who want to install, test or contribute to...\"],[\"## Architecture\\n\\nThe repository is structured as a monorepo, with Python libraries and applications ...\"],[\"The API service exposes the `\\u002fwebhook` endpoint which is called by the Hub on every creation, update...\"],[\"The following environments contain all the modules: reverse proxy, API server, admin API server, wor...\"],[\"## Pull requests\\n\\nAll the contributions should go through a pull request. The pull requests must be ...\"],[\"If you need to have icu4c first in your PATH, run:\\n  echo 'export PATH=\\\"\\u002fopt\\u002fhomebrew\\u002fopt\\u002ficu4c\\u002fbin:...\"],[\"Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pledge...\"],[\"Community leaders have the right and responsibility to remove, edit, or reject\\ncomments, commits, co...\"],[\"### 2. Warning\\n\\n**Community Impact**: A violation through a single incident or series\\nof actions.\\n\\n*...\"],[\"[homepage]: https:\\u002f\\u002fwww.contributor-covenant.org\\n[v2.0]: https:\\u002f\\u002fwww.contributor-covenant.org\\u002fversio...\"],[\"Server infrastructure\\n\\nThe [Datasets Server](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdatasets-server) has two...\"],[\"You might've noticed the `\\u002frows` and `\\u002fsearch` endpoints don't have a job in the queue. The response...\"],[\"Datasets server maintenance job\\n\\n\\u003e Job to run maintenance actions on the datasets-server\\n\\nAvailable ...\"],[\"Datasets server - storage admin\\n\\n\\u003e A Ubuntu machine to log into and manage the storage manually...\"],[\"🤗 Datasets Server\\n\\nDatasets Server is a lightweight web API for visualizing and exploring all types ...\"],[\"\\u003cp style=\\\"text-align: center; font-style: italic; margin-top: 0;\\\"\\u003e\\n  Dataset viewer of the\\n  \\u003ca href...\"],[\"Get the number of rows and the size in bytes\\n\\nThis guide shows you how to use Datasets Server's `\\u002fsi...\"],[\"```json\\n{\\n  \\\"size\\\": {\\n    \\\"dataset\\\": {\\n      \\\"dataset\\\": \\\"duorc\\\",\\n      \\\"num_bytes_original_files\\\": 9...\"],[\"\\\"split\\\": \\\"validation\\\",\\n        \\\"num_bytes_parquet_files\\\": 3114390,\\n        \\\"num_bytes_memory\\\": 51662...\"],[\"If the size has `partial: true` it means that the actual size of the dataset couldn't been determine...\"],[\"Datasets server API - rows endpoint\\n\\n\\u003e \\u002frows endpoint\\n\\n## Configuration\\n\\nThe service can be configur...\"],[\"Datasets server - reverse proxy\\n\\n\\u003e Reverse-proxy in front of the API\\n\\nSee [docker-compose-datasets-s...\"],[\"Data types\\n\\nDatasets supported by Datasets Server have a tabular format, meaning a data point is rep...\"],[\"- The `label` column has a type of `ClassLabel`. The [`ClassLabel`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdata...\"],[\"List splits and configurations\\n\\nDatasets typically have splits and may also have configurations. A _...\"],[\"The `\\u002fsplits` endpoint accepts the dataset name as its query parameter:\\n\\n\\u003cinferencesnippet\\u003e\\n\\u003cpython\\u003e...\"],[\"Datasets server API\\n\\n\\u003e API on 🤗 datasets\\n\\n## Configuration\\n\\nThe service can be configured using envi...\"],[\"Datasets server API - search service\\n\\n\\u003e \\u002fsearch endpoint\\n\\u003e \\u002ffilter endpoint\\n\\n## Configuration\\n\\nThe s...\"],[\"Datasets server databases migrations\\n\\n\\u003e Scripts to migrate the datasets server databases\\n\\n## Configu...\"]],\"hovertemplate\":\"source=datasets-server\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"datasets-server, circle\",\"marker\":{\"color\":\"#636efa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"datasets-server, circle\",\"showlegend\":true,\"x\":[-18.272352,-18.899256,-0.20725437,0.42414188,0.18421587,0.4351481,-0.30093607,-7.2258725,-4.6163573,-5.3328996,-5.8351717,-6.0692663,-5.688871,-9.807694,-2.3954015,16.394388,15.402922,16.784868,16.552607,12.301434,16.743025,16.53518,13.410692,13.107753,16.382538,16.75719,3.1433032,-5.348089,-6.311917,7.7169056,-6.0069933,-6.4756894,-9.397778,-7.08341,-7.8670673,-7.5032673,-8.148295,-10.562162,-7.4252987,-3.8572893,-8.113871,-5.80015,-6.1168714,0.08378096,19.447426,18.535357,17.301744,-9.746283,-9.9352865,-9.735013,-9.44241,-9.486483,-2.424845,-2.3802547,-0.12739527,0.6176571,4.0834355,5.6031127,-4.5557423,4.565698,13.406547,0.07188458,-1.6920398,-6.3114734,-6.231788,-3.1386456,1.313894,-2.959025,-3.083952,-3.2196028,-7.3304396,-0.3296763,-3.5403852,-18.089573,12.091117,12.140582,-2.5489268,12.493693,-5.444437,-0.9386974,-3.534131,-2.7722423,-2.9246464,-3.5714974,-4.126509,-5.3632464,-5.36659,13.244826,12.945899,-3.995874,-1.2183065,-3.862526,8.586924,-7.3398356,8.119038,2.8976707,1.5521027,-4.1410875,-4.530612,-7.0166507,-6.829508,-6.137791,2.0903537,0.041510567,-0.035656344,0.55180436,-1.2206048,-0.47873586,11.057687,13.095445,-3.6665843,6.2051544,-2.3248699,-0.9369756,-2.3696108,-1.6315227,7.4794884,6.9097676,7.2190995,7.208303,7.048754,7.108572,-6.481402,-6.322966,-1.7492918,0.3772184,-5.354614,7.987158,-6.95669,-7.291796,-5.171495,-4.4691916,7.1425657,-6.723608,-6.6755586,2.0703955,4.400439,4.0723963,-6.759697,-10.04895,-6.951508,-4.686518,-3.2059112,-1.8410572,-2.954938],\"xaxis\":\"x\",\"y\":[-4.2019887,-4.3769636,9.851514,8.002468,7.7736154,7.9225945,7.752429,-1.0505248,-1.7592263,0.44547054,2.0042193,-1.5261712,-0.45169732,0.35085925,-12.318942,3.3045807,3.4405873,2.5781882,2.3306308,4.331028,2.38461,2.4026918,3.425862,3.616501,2.9727364,2.1402614,1.3200979,-6.72601,7.4986124,3.6485255,0.40477064,7.777659,0.9623032,0.7914785,3.3645866,3.3811612,3.3426104,-0.41793978,-1.6440359,-7.6326547,2.4979565,2.7103102,2.184088,-6.302566,3.2542505,2.3586504,2.389757,0.14924854,0.30838832,-0.18669215,-0.110384405,0.055892754,-12.065005,-12.419156,9.395716,7.6071477,1.6851629,0.0021775959,-6.422167,0.3205482,5.333489,3.9997804,3.9645803,0.002025619,0.18970433,0.44655275,1.2968814,4.3502216,4.403617,4.7361193,5.9910836,3.9335492,-8.152794,-4.1282144,2.2860074,2.4588268,3.0460382,2.8975456,-6.463367,-6.8352466,-7.6379533,-8.309417,-8.289139,0.7254741,1.1180606,1.9588046,1.6225246,5.4987288,4.303126,3.9201083,6.2632113,3.5003026,-10.989273,6.356731,-9.883391,0.5757673,-0.30767694,-2.0811722,-2.1250143,2.4654377,2.3532965,3.7618086,3.2016728,-6.0823145,5.3699317,7.445774,-0.14915997,-5.9402537,4.1058025,5.053513,-5.8033714,20.744629,-5.7986264,1.5013802,-1.3797392,-6.097587,2.1048217,2.0196218,2.1388116,2.0500314,2.0594327,2.076141,7.821266,7.3392572,4.3299346,6.3839464,0.85804904,-9.366731,6.460899,6.824306,-2.2211864,-5.7174554,1.8834695,-0.7598197,-0.56254804,3.2427208,-0.43922406,2.4651942,6.338344,0.79049647,-1.1792742,-1.1480935,6.0493,-4.1610126,-0.25788906],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Differences between Dataset and IterableDataset\\n\\nThere are two types of dataset objects, a [`Dataset...\"],[\"imagenet = load_dataset(\\\"imagenet-1k\\\", split=\\\"train\\\", streaming=True)  # will start loading the data...\"],[\"However, this requires a conversion step from CSV to Arrow format, which takes time and disk space i...\"],[\"# process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset\\nfo...\"],[\"# From a generator function\\ndef my_generator(n, sources):\\n    for source in sources:\\n        for exa...\"],[\"```python\\nfor example in enumerate(my_iterable_dataset):  # fast\\n    pass\\n\\nshuffled_iterable_dataset...\"],[\"Metric Card for F1\\n\\n\\n## Metric Description\\n\\nThe F1 score is the harmonic mean of the precision and r...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): Predicted labels.\\n- **references** (`list` of `int`)...\"],[\"### Output Values\\n- **f1**(`float` or `array` of `float`): F1 score or list of f1 scores, depending ...\"],[\"Example 4-A multiclass example, with different values for the `average` input.\\n```python\\n\\u003e\\u003e\\u003e predict...\"],[\"Cache management\\n\\nWhen you download a dataset, the processing scripts and data are stored locally on...\"],[\"```py\\n# Returns the number of removed cache files\\n\\u003e\\u003e\\u003e dataset.cleanup_cache_files()\\n2\\n```\\n\\n## Enable...\"],[\"Metric Card for MSE\\n\\n\\n## Metric Description\\n\\nMean Squared Error(MSE) represents the average of the s...\"],[\"### Output Values\\nThis metric outputs a dictionary, containing the mean squared error score, which i...\"],[\"## Limitations and Bias\\nMSE has the disadvantage of heavily weighting outliers -- given that it squa...\"],[\"All about metrics\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in 🤗 Datasets. To learn more about ho...\"],[\"## Distributed evaluation\\n\\nComputing metrics in a distributed environment can be tricky. Metric eval...\"],[\"Metric Card for METEOR\\n\\n## Metric description\\n\\nMETEOR (Metric for Evaluation of Translation with Exp...\"],[\"Refer to the [METEOR paper](https:\\u002f\\u002faclanthology.org\\u002fW05-0909.pdf) for more information about parame...\"],[\"Partial match:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e meteor = load_metric('meteor')\\n\\u003e\\u003e...\"],[\"Preprocess\\n\\nIn addition to loading datasets, 🤗 Datasets other main goal is to offer a diverse set of...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\\"rotte...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\nUse the [`~Dataset.set_format`] function to set the dataset format to be com...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoFeatureExtractor\\n\\u003e\\u003e\\u003e from datasets import load_dataset, Audio...\"],[\"**4**. Use the [`~Dataset.map`] function to resample the entire dataset to 16kHz. This function spee...\"],[\"**2**. Index into the first row of the dataset. When you call the `image` column of the dataset, the...\"],[\"Image classification\\n\\nImage classification datasets are used to train a model to classify an entire ...\"],[\"Create a function to apply the transformation to the images:\\n\\n```py\\n\\u003e\\u003e\\u003e def transforms(examples):\\n.....\"],[\"Beam Datasets\\n\\nSome datasets are too large to be processed on a single machine. Instead, you can pro...\"],[\"Load image data\\n\\nImage datasets have [`Image`] type columns, which contain PIL objects. \\n\\n\\u003cTip\\u003e\\n\\nTo ...\"],[\"## ImageFolder\\n\\nYou can also load a dataset with an `ImageFolder` dataset builder which does not req...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\\"imagefolder\\\", data_dir=\\\"\\u002fpa...\"],[\"Metric Card for Recall\\n\\n\\n## Metric Description\\n\\nRecall is the fraction of the positive examples that...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): The predicted labels.\\n- **references** (`list` of `i...\"],[\"- **sample_weight** (`list` of `float`): Sample weights Defaults to `None`.\\n- **zero_division** (): ...\"],[\"### Output Values\\n- **recall**(`float`, or `array` of `float`, for multiclass targets): Either the g...\"],[\"Example 4-A multiclass example, using different averages.\\n```python\\n\\u003e\\u003e\\u003e recall_metric = datasets.loa...\"],[\"Dataset features\\n\\n[`Features`] defines the internal structure of a dataset. It is used to specify th...\"],[\"If your data type contains a list of objects, then you want to use the [`Sequence`] feature. Remembe...\"],[\"When you load an audio dataset and call the audio column, the [`Audio`] feature automatically decode...\"],[\"## Image feature\\n\\nImage datasets have a column with type [`Image`], which loads `PIL.Image` objects ...\"],[\"And in this case the numpy arrays are encoded into PNG (or TIFF if the pixels values precision is im...\"],[\"Metric Card for GLUE\\n\\n## Metric description\\nThis metric is used to compute the GLUE evaluation metri...\"],[\"`pearson`: a measure of the linear relationship between two datasets (see [Pearson correlation](http...\"],[\"Minimal values for the STSB subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfrom datase...\"],[\"Metric Card for Matthews Correlation Coefficient\\n\\n## Metric Description\\nThe Matthews correlation coe...\"],[\"The same example as above, but also including sample weights:\\n```python\\n\\u003e\\u003e\\u003e matthews_metric = datase...\"],[\"Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pledge...\"],[\"Community leaders have the right and responsibility to remove, edit, or reject\\ncomments, commits, co...\"],[\"### 2. Warning\\n\\n**Community Impact**: A violation through a single incident or series\\nof actions.\\n\\n*...\"],[\"[homepage]: https:\\u002f\\u002fwww.contributor-covenant.org\\n[v2.0]: https:\\u002f\\u002fwww.contributor-covenant.org\\u002fversio...\"],[\"Table Classes\\n\\nEach `Dataset` object is backed by a PyArrow Table.\\nA Table can be loaded from either...\"],[\"## ConcatenationTable\\n\\n[[autodoc]] datasets.table.ConcatenationTable\\n    - validate\\n    - equals\\n   ...\"],[\"Metric Card for Precision\\n\\n\\n## Metric Description\\n\\nPrecision is the fraction of correctly labeled po...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): Predicted class labels.\\n- **references** (`list` of ...\"],[\"- 'samples': Calculate metrics for each instance, and find their average (only meaningful for multil...\"],[\"### Output Values\\n- **precision**(`float` or `array` of `float`): Precision score or list of precisi...\"],[\"Example 4-A multiclass example, with different values for the `average` input.\\n```python\\n\\u003e\\u003e\\u003e predict...\"],[\"Load tabular data\\n\\nA tabular dataset is a generic dataset used to describe any data stored in rows a...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import Dataset\\n\\u003e\\u003e\\u003e import pandas as pd\\n\\n# create a Pandas DataFrame\\n\\u003e\\u003e\\u003e df =...\"],[\"This creates a `states` table in the `us_covid_data.db` database which you can now load into a datas...\"],[\"Then you can use all of 🤗 Datasets process features like [`~datasets.Dataset.filter`] for example:\\n\\n...\"],[\"--\\nTODO: Add YAML tags here. Copy-paste the tags obtained with the online tagging app: https:\\u002f\\u002fhuggi...\"],[\"#### Initial Data Collection and Normalization\\n\\n[More Information Needed]\\n\\n#### Who are the source l...\"],[\"Use with Spark\\n\\nThis document is a quick introduction to using 🤗 Datasets with Spark, with a particu...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nIn a different session, a Spark DataFrame doesn't have the same [semantic hash...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cpicture\\u003e\\n    \\u003csource media=\\\"(prefers-color-scheme: dark)\\\" srcset=\\\"https:\\u002f\\u002fhuggi...\"],[\"🤗 Datasets is a lightweight library providing **two** main features:\\n\\n- **one-line dataloaders for m...\"],[\"🤗 Datasets is designed to let the community easily add and share new datasets.\\n\\n🤗 Datasets has many ...\"],[\"## Installation to use with PyTorch\\u002fTensorFlow\\u002fpandas\\n\\nIf you plan to use 🤗 Datasets with PyTorch (1...\"],[\"For more details on using the library, check the quick start page in the documentation: https:\\u002f\\u002fhugg...\"],[\"# Main differences between 🤗 Datasets and `tfds`\\n\\nIf you are familiar with the great TensorFlow Data...\"],[\"```bibtex\\n@inproceedings{lhoest-etal-2021-datasets,\\n    title = \\\"Datasets: A Community Library for N...\"],[\"publisher = \\\"Association for Computational Linguistics\\\",\\n    url = \\\"https:\\u002f\\u002faclanthology.org\\u002f2021.em...\"],[\"If you need to cite a specific version of our 🤗 Datasets library for reproducibility, you can use th...\"],[\"The cache\\n\\nThe cache is one of the reasons why 🤗 Datasets is so efficient. It stores previously down...\"],[\"An example of when 🤗 Datasets recomputes everything is when caching is disabled. When this happens, ...\"],[\"Metric Card for chrF(++)\\n\\n\\n## Metric Description\\nChrF and ChrF++ are two MT evaluation metrics that ...\"],[\"### Inputs\\n- **`predictions`** (`list` of `str`): The predicted sentences.\\n- **`references`** (`list...\"],[\"The chrF(++) score can be any value between `0.0` and `100.0`, inclusive.\\n\\n#### Values from Popular ...\"],[\"The same chrF++ example as above, but with `lowercase=True` to normalize all case:\\n```python\\n\\u003e\\u003e\\u003e pre...\"],[\"## Citation\\n```bibtex\\n@inproceedings{popovic-2015-chrf,\\n    title = \\\"chr{F}: character n-gram {F}-sc...\"],[\"Metric Card for BERT Score\\n\\n## Metric description\\n\\nBERTScore is an automatic evaluation metric for t...\"],[\"`device` (str): On which the contextual embedding model will be allocated on. If this argument is `N...\"],[\"## Examples \\n\\nMaximal values with the `distilbert-base-uncased` model:\\n\\n```python\\nfrom datasets impo...\"],[\"Finally, calculating the BERTScore metric involves downloading the BERT model that is used to comput...\"],[\"Metric Card for ROUGE\\n\\n## Metric Description\\nROUGE, or Recall-Oriented Understudy for Gisting Evalua...\"],[\"### Inputs\\n- **predictions** (`list`): list of predictions to score. Each prediction\\n        should ...\"],[\"If `rouge_types=['rouge1', 'rouge2']` and `use_aggregator=True`, the output is of the following form...\"],[\"The same example, but only calculating `rouge_1`:\\n```python\\n\\u003e\\u003e\\u003e rouge = datasets.load_metric('rouge'...\"],[\"Metric Card for Exact Match\\n\\n\\n## Metric Description\\nA given predicted string's exact match score is ...\"],[\"```python\\n{'exact_match': 100.0}\\n```\\n\\nThis metric's range is 0-100, inclusive. Here, 0.0 means no pr...\"],[\"Ignoring \\\"the\\\", \\\"yell\\\", and \\\"YELL\\\", as well as ignoring case, punctuation, and numbers:\\n```python\\n\\u003e\\u003e...\"],[\"Metric Card for COMET\\n\\n## Metric description\\n\\nCrosslingual Optimized Metric for Evaluation of Transl...\"],[\"`mean_score`: the mean value of COMET scores `scores` over all the input sentences, ranging from 0-1...\"],[\"No match:\\n\\n```python\\nfrom datasets import load_metric\\ncomet_metric = load_metric('comet') \\nsource = ...\"],[\"Thus, results for language pairs containing uncovered languages are unreliable, as per the [COMET we...\"],[\"Metric Card for seqeval\\n\\n## Metric description\\n\\nseqeval is a Python framework for sequence labeling ...\"],[\"## Output values\\n\\nThis metric returns a dictionary with a summary of scores for overall and per type...\"],[\"## Examples \\n\\nMaximal values (full match) :\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e seqe...\"],[\"## Limitations and bias\\n\\nseqeval supports following IOB formats (short for inside, outside, beginnin...\"],[\"Utilities\\n\\n## Configure logging\\n\\n🤗 Datasets strives to be transparent and explicit about how it work...\"],[\"[[autodoc]] datasets.logging.set_verbosity_error\\n\\n[[autodoc]] datasets.logging.disable_propagation\\n\\n...\"],[\"Use with PyTorch\\n\\nThis document is a quick introduction to using `datasets` with PyTorch, with a par...\"],[\"To get a single tensor, you must explicitly use the [`Array`] feature type and specify the shape of ...\"],[\"\\u003cTip\\u003e\\n\\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\\n`pip install d...\"],[\"#### Use multiple Workers\\n\\nYou can parallelize data loading with the `num_workers` argument of a PyT...\"],[\"If the dataset is split in several shards (i.e. if the dataset consists of multiple data files), the...\"],[\"Semantic segmentation\\n\\nSemantic segmentation datasets are used to train a model to classify every pi...\"],[\"Similarly, you can check out the respective segmentation mask:\\n\\n```py\\n\\u003e\\u003e\\u003e dataset[index][\\\"annotation...\"],[\"Create a function to apply the transformation to the images:\\n\\n```py\\n\\u003e\\u003e\\u003e def transforms(examples):\\n.....\"],[\"\\u003e\\u003e\\u003e dataset.set_transform(train_transforms)\\n\\n\\u003e\\u003e\\u003e image = np.array(dataset[index][\\\"pixel_values\\\"])\\n\\u003e\\u003e...\"],[\"Metric Card for *Current Metric*\\n\\n***Metric Card Instructions:*** *Copy this file into the relevant ...\"],[\"Metric Card for WikiSplit\\n\\n## Metric description\\n\\nWikiSplit is the combination of three metrics: [SA...\"],[\"```python\\n\\u003e\\u003e\\u003e print(results)\\n{'sari': 21.805555555555557, 'sacrebleu': 14.535768424205482, 'exact': ...\"],[\"No match between prediction and reference:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e wiki_...\"],[\"Metric Card for FrugalScore\\n\\n\\n## Metric Description\\nFrugalScore is a reference-based metric for Natu...\"],[\"## Output values\\n\\nThe output of FrugalScore is a dictionary with the list of scores for each predict...\"],[\"| FrugalScore                                        | Student     | Teacher        | Method     |\\n|...\"],[\"| [moussaKam\\u002ffrugalscore_tiny_bert-base_mover-score](https:\\u002f\\u002fhuggingface.co\\u002fmoussaKam\\u002ffrugalscore_ti...\"],[\"Depending on the size of the model picked, the loading time will vary: the `tiny` models will load v...\"],[\"How to add one new datasets\\n\\nAdd datasets directly to the 🤗 Hugging Face Hub!\\n\\nYou can share your da...\"],[\"Metric Card for Google BLEU (GLEU)\\n\\n\\n## Metric Description\\nThe BLEU score has some undesirable prope...\"],[\"### Inputs\\n- **predictions** (list of list of str): list of translations to score. Each translation ...\"],[\"#### Values from Popular Papers\\n\\n\\n### Examples\\nExample with one reference per sample:\\n```python\\n\\u003e\\u003e\\u003e ...\"],[\"\\u003e\\u003e\\u003e hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\\n...         'interested', 'in', 'wo...\"],[\"Example with multiple references for the first sample, with `min_len` adjusted to `2`, instead of th...\"],[\"## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@misc{wu2016googles,\\ntitle={Google's Neural Machine ...\"],[\"# Add Dummy data test\\n\\n**Important** In order to pass the `load_dataset_\\u003cdataset_name\\u003e` test, dummy ...\"],[\"2) The ``dl_manager.download_and_extract()`` is given a **dictionary of paths** of type `str` as its...\"],[\"Main classes\\n\\n\\n## DatasetInfo\\n\\n[[autodoc]] datasets.DatasetInfo\\n\\n## Dataset\\n\\nThe base class [`Datase...\"],[\"[[autodoc]] datasets.interleave_datasets\\n\\n[[autodoc]] datasets.distributed.split_dataset_by_node\\n\\n[[...\"],[\"## IterableDatasetDict\\n\\nDictionary with split names as keys ('train', 'test' for example), and `Iter...\"],[\"Metric Card for COVAL\\n\\n## Metric description\\n\\nCoVal is a coreference evaluation tool for the [CoNLL]...\"],[\"```python\\nfrom datasets import load_metric\\ncoval = load_metric('coval')\\nwords = ['bc\\u002fcctv\\u002f00\\u002fcctv_00...\"],[\"## Output values\\n\\nThe metric outputs a dictionary with the following key-value pairs:\\n\\n`mentions`: n...\"],[\"## Examples \\n\\nMaximal values\\n\\n```python\\nfrom datasets import load_metric\\ncoval = load_metric('coval'...\"],[\"| Column | Type                  | Description                                                      ...\"],[\"| 5      | Part-of-Speech        |                                                                  ...\"],[\"| 9      | Word sense            | This is the word sense of the word in Column 3.                  ...\"],[\"| N      | Coreference           | Coreference chain information encoded in a parenthesis structure....\"],[\"## Citations\\n\\n```bibtex\\n@InProceedings{moosavi2019minimum,\\n  author = { Nafise Sadat Moosavi, Leo Bo...\"],[\"```bibtex\\n@inproceedings{moosavi-strube-2016-coreference,\\n    title = \\\"Which Coreference Evaluation ...\"],[\"Overview\\n\\nThe how-to guides offer a more comprehensive overview of all the tools 🤗 Datasets offers a...\"],[\"Metric Card for SacreBLEU\\n\\n\\n## Metric Description\\nSacreBLEU provides hassle-free computation of shar...\"],[\"### Inputs\\n- **`predictions`** (`list` of `str`): list of translations to score. Each translation sh...\"],[\"### Output Values\\n- `score`: BLEU score\\n- `counts`: Counts\\n- `totals`: Totals\\n- `precisions`: Precis...\"],[\"Cloud storage\\n\\n🤗 Datasets supports access to cloud storage providers through a `fsspec` FileSystem i...\"],[\"3. Create your FileSystem instance\\n\\n```py\\n\\u003e\\u003e\\u003e import s3fs\\n\\u003e\\u003e\\u003e fs = s3fs.S3FileSystem(**storage_optio...\"],[\"## Load and Save your datasets using your cloud storage FileSystem\\n\\n### Download and prepare a datas...\"],[\"It is highly recommended to save the files as compressed Parquet files to optimize I\\u002fO by specifying...\"],[\"## Saving serialized datasets\\n\\nAfter you have processed your dataset, you can save it to your cloud ...\"],[\"Metric Card for BLEU\\n\\n\\n## Metric Description\\nBLEU (Bilingual Evaluation Understudy) is an algorithm ...\"],[\"### Inputs\\n- **predictions** (`list`): Translations to score. Each translation should be tokenized i...\"],[\"### Examples\\n\\nExample where each sample has 1 reference:\\n```python\\n\\u003e\\u003e\\u003e predictions = [\\n...     [\\\"hel...\"],[\"## Limitations and Bias\\nThis metric hase multiple known limitations and biases:\\n- BLEU compares over...\"],[\"Create an image dataset\\n\\nThere are two methods for creating and sharing an image dataset. This guide...\"],[\"You can also use `imagefolder` to load datasets involving multiple splits. To do so, your dataset di...\"],[\"\\u003cTip\\u003e\\n\\nIf metadata files are present, the inferred labels based on the directory name are dropped by...\"],[\"Upload your dataset with [`~datasets.DatasetDict.push_to_hub`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets import load...\"],[\"This structure allows your dataset to be loaded in one line:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets import load_da...\"],[\"def _info(self):\\n\\n    def _split_generators(self, dl_manager):\\n\\n    def _generate_examples(self, ima...\"],[\"1. Define your subsets with `Food101Config` in a list in `BUILDER_CONFIGS`.\\n2. For each configuratio...\"],[\"There is a lot of information you can specify about your dataset, but some important ones to include...\"],[\"2. After you've downloaded the dataset, use the [`SplitGenerator`] to organize the images and labels...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nTo stream a TAR archive file, the `metadata_path` needs to be opened and read ...\"],[\"Create a dataset card\\n\\nEach dataset should have a dataset card to promote responsible usage and info...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n3. Click on the **Import dataset card template** link to automatically create a template wit...\"],[\"Process\\n\\n🤗 Datasets provides many tools for modifying the structure and content of a dataset. These ...\"],[\"### Sort\\n\\nUse [`~Dataset.sort`] to sort column values according to their numerical values. The provi...\"],[\"### Select and Filter\\n\\nThere are two options for filtering rows in a dataset: [`~Dataset.select`] an...\"],[\"```py\\n\\u003e\\u003e\\u003e dataset.train_test_split(test_size=0.1)\\n{'train': Dataset(schema: {'sentence1': 'string', ...\"],[\"Provide [`~Dataset.rename_column`] with the name of the original column, and the new column name:\\n\\n`...\"],[\"\\u003e\\u003e\\u003e from datasets import ClassLabel, Value\\n\\u003e\\u003e\\u003e new_features = dataset.features.copy()\\n\\u003e\\u003e\\u003e new_featur...\"],[\"The `answers` field contains two subfields: `text` and `answer_start`. Use the [`~Dataset.flatten`] ...\"],[\"Let's take a look at another example, except this time, you'll remove a column with [`~Dataset.map`]...\"],[\"```py\\n\\u003e\\u003e\\u003e updated_dataset = dataset.map(lambda example, idx: {\\\"sentence2\\\": f\\\"{idx}: \\\" + example[\\\"sen...\"],[\"#### Split long examples\\n\\nWhen examples are too long, you may want to split them into several smalle...\"],[\"```py\\n\\u003e\\u003e\\u003e from random import randint\\n\\u003e\\u003e\\u003e from transformers import pipeline\\n\\n\\u003e\\u003e\\u003e fillmask = pipeline(...\"],[\"For each original sentence, RoBERTA augmented a random word with three alternatives. The original wo...\"],[\"\\u003e\\u003e\\u003e dataset1 = Dataset.from_dict({\\\"a\\\": [0, 1, 2]})\\n\\n\\u003e\\u003e\\u003e if training_args.local_rank \\u003e 0:\\n...     pri...\"],[\"You can define sampling probabilities for each of the original datasets to specify how to interleave...\"],[\"For example, create PyTorch tensors by setting `type=\\\"torch\\\"`:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e dataset.s...\"],[\"You can also use the [`~Dataset.set_transform`] function to decode formats not supported by [`Featur...\"],[\"Use the [`load_from_disk`] function to reload the dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets import load_from...\"],[\"Metric Card for SQuAD v2\\n\\n## Metric description\\nThis metric wraps the official scoring script for ve...\"],[\"```python\\nfrom datasets import load_metric\\nsquad_metric = load_metric(\\\"squad_v2\\\")\\nresults = squad_me...\"],[\"The range of `total` depends on the length of predictions\\u002freferences: its minimal value is 0, and ma...\"],[\"Partial match (2 out of 3 answers correct) : \\n\\n```python\\nfrom datasets import load_metric\\nsquad_metr...\"],[\"- [The Stanford Question Answering Dataset: Background, Challenges, Progress (blog post)](https:\\u002f\\u002fra...\"],[\"Metric Card for Perplexity\\n\\n## Metric Description\\nGiven a model and an input text sequence, perplexi...\"],[\"### Output Values\\nThis metric outputs a dictionary with the perplexity scores for the text input in ...\"],[\"## Citation\\n\\n```bibtex\\n@article{jelinek1977perplexity,\\ntitle={Perplexity—a measure of the difficulty...\"],[\"Overview\\n\\nWelcome to the 🤗 Datasets tutorials! These beginner-friendly tutorials will guide you thro...\"],[\"Metric Card for Pearson Correlation Coefficient (pearsonr)\\n\\n\\n## Metric Description\\n\\nPearson correlat...\"],[\"### Output Values\\n- **pearsonr**(`float`): Pearson correlation coefficient. Minimum possible value i...\"],[\"## Citation(s)\\n```bibtex\\n@article{2020SciPy-NMeth,\\nauthor  = {Virtanen, Pauli and Gommers, Ralf and ...\"],[\"Load audio data\\n\\nYou can load an audio dataset using the [`Audio`] feature that automatically decode...\"],[\"Your `metadata.csv` file must have a `file_name` column which links audio files with their metadata....\"],[\"To ignore the information in the metadata file, set `drop_metadata=True` in [`load_dataset`]:\\n\\n```py...\"],[\"Search index\\n\\n[FAISS](https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002ffaiss) and [Elasticsearch](https:\\u002f\\u002fwww.ela...\"],[\"4. Now you can query your dataset with the `embeddings` index. Load the DPR Question Encoder, and se...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\u003e\\u003e\\u003e squad = load_dataset('squad', split='validation')\\n``...\"],[\"For more advanced Elasticsearch usage, you can specify your own configuration with custom settings:\\n...\"],[\"How to contribute to Datasets?\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fContributor%20C...\"],[\"3. Create a new branch to hold your development changes:\\n\\n    ```bash\\n    git checkout -b a-descript...\"],[\"## How to add a dataset\\n\\nYou can share your dataset on https:\\u002f\\u002fhuggingface.co\\u002fdatasets directly usin...\"],[\"If you are a **user of a dataset**, the main source of information should be the dataset paper if it...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-4\\\"\\u003e\\n   \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nStart by installing 🤗 Datasets:\\n\\n```bash\\npip install datasets\\n```\\n\\n🤗 Datasets also support a...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\\n\\n\\u003e\\u003e\\u003e model ...\"],[\"\\u003e\\u003e\\u003e dataset = dataset.map(preprocess_function, batched=True)\\n```\\n\\n**5**. Use the [`~Dataset.rename_c...\"],[\"## Vision\\n\\nImage datasets are loaded just like text datasets. However, instead of a tokenizer, you'l...\"],[\"```py\\n\\u003e\\u003e\\u003e dataset = dataset.with_transform(transforms)\\n```\\n\\n**5**. Set the dataset format according ...\"],[\"\\u003e\\u003e\\u003e def transforms(examples):\\n...     examples[\\\"pixel_values\\\"] = [\\n...         transform(image=np.ar...\"],[\"```py\\n\\u003e\\u003e\\u003e from transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\n\\u003e\\u003e\\u003e model = Au...\"],[\"```py\\n\\u003e\\u003e\\u003e def encode(examples):\\n...     return tokenizer(examples[\\\"sentence1\\\"], examples[\\\"sentence2\\\"...\"],[\"```py\\n\\u003e\\u003e\\u003e dataset = dataset.map(lambda examples: {\\\"labels\\\": examples[\\\"label\\\"]}, batched=True)\\n```\\n\\n*...\"],[\"## What's next?\\n\\nThis completes the 🤗 Datasets quickstart! You can load any text, audio, or image da...\"],[\"Process text data\\n\\nThis guide shows specific methods for processing text datasets. Learn how to:\\n\\n- ...\"],[\"The [`~Dataset.map`] function converts the returned values to a PyArrow-supported format. But explic...\"],[\"Security Policy\\n\\n## Supported Versions\\n\\u003c!--\\nUse this section to tell people about which versions of ...\"],[\"Process audio data\\n\\nThis guide shows specific methods for processing audio datasets. Learn how to:\\n\\n...\"],[\"- For pretrained speech recognition models, load a feature extractor and tokenizer and combine them ...\"],[\"Task templates\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThe Task API is deprecated in favor of [`train-eval-index`](ht...\"],[\"Object detection\\n\\nObject detection models identify something in an image, and object detection datas...\"],[\"You can visualize the `bboxes` on the image using some internal torch utilities. To do that, you wil...\"],[\"Now when you visualize the result, the image should be flipped, but the `bboxes` should still be in ...\"],[\"```py\\n\\u003e\\u003e\\u003e ds['train'].set_transform(transforms)\\n```\\n\\nYou can verify the transform works by visualizi...\"],[\"Load\\n\\nYour data can be stored in various places; they can be on your local machine's disk, in a Gith...\"],[\"```py\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\n...   \\\"lhoestq\\u002fcustom_squad\\\",\\n...   revision=\\\"main\\\"  # tag name, o...\"],[\"## Local loading script\\n\\nYou may have a 🤗 Datasets loading script locally on your computer. In this ...\"],[\"For more details, check out the [how to load tabular datasets from CSV files](tabular_load#csv-files...\"],[\"To load remote Parquet files via HTTP, pass the URLs instead:\\n\\n```py\\n\\u003e\\u003e\\u003e base_url = \\\"https:\\u002f\\u002fstorage...\"],[\"\\u003cTip\\u003e\\n\\nFor more details, check out the [how to load tabular datasets from SQL databases](tabular_loa...\"],[\"### Python list of dictionaries\\n\\nLoad a list of Python dictionaries with [`~Dataset.from_list`]:\\n\\n``...\"],[\"If you know you won't have internet access, you can run 🤗 Datasets in full offline mode. This saves ...\"],[\"Finally, you can even create cross-validated splits. The example below creates 10-fold cross-validat...\"],[\"If you want equal sized splits, use `pct1_dropremainder` rounding instead. This treats the specified...\"],[\"For example, if you try to download a configuration from the [MATINF](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"Start by defining your own labels with the [`Features`] class:\\n\\n```py\\n\\u003e\\u003e\\u003e class_names = [\\\"sadness\\\", ...\"],[\"\\u003cTip\\u003e\\n\\nSee the [Metrics](.\\u002fhow_to_metrics#custom-metric-loading-script) guide for more details on ho...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e metric = load_metric('glue', 'mrpc', num_process=num_...\"],[\"Metric Card for SuperGLUE\\n\\n## Metric description\\nThis metric is used to compute the SuperGLUE evalua...\"],[\"```python\\nfrom datasets import load_metric\\nsuper_glue_metric = load_metric('super_glue', 'copa') \\npr...\"],[\"For more recent model performance, see the [dataset leaderboard](https:\\u002f\\u002fsuper.gluebenchmark.com\\u002flea...\"],[\"## Limitations and bias\\nThis metric works only with datasets that have the same format as the [Super...\"],[\"Stream\\n\\nDataset streaming lets you work with a dataset without downloading it.\\nThe data is streamed ...\"],[\"For example, you can stream a local dataset of hundreds of compressed JSONL files like [oscar-corpus...\"],[\"# faster 🐇\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\\"food101\\\")\\n\\u003e\\u003e\\u003e iterable_dataset = dataset.to_iterable_dataset(...\"],[\"\\u003cTip\\u003e\\n\\n[`IterableDataset.shuffle`] will also shuffle the order of the shards if the dataset is shard...\"],[\"\\u003e\\u003e\\u003e multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])\\n\\u003e\\u003e\\u003e list(multilingual_datas...\"],[\"Provide [`IterableDataset.rename_column`] with the name of the original column, and the new column n...\"],[\"\\u003cTip\\u003e\\n\\nCasting only works if the original feature type and new feature type are compatible. For exam...\"],[\"Let's take a look at another example, except this time, you will remove a column with [`IterableData...\"],[\"\\u003cTip\\u003e\\n\\nSee other examples of batch processing in the [batched map processing](.\\u002fprocess#batch-proces...\"],[\"\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n```py\\n\\u003e\\u003e\\u003e seed, buffer_size = 42, 10_000\\n\\u003e\\u003e\\u003e dataset = dataset.shuffle(seed,...\"],[\"Depth estimation\\n\\nDepth estimation datasets are used to train a model to approximate the relative di...\"],[\"The dataset has two fields:\\n\\n* `image`: a PIL PNG image object with `uint8` data type.\\n* `depth_map`...\"],[\"\\u003e\\u003e\\u003e def show_depthmap(depth_map):\\n...    if not isinstance(depth_map, np.ndarray):\\n...        depth_...\"],[\"* Random horizontal flipping\\n* Random cropping \\n* Random brightness and contrast \\n* Random gamma cor...\"],[\"```py\\n\\u003e\\u003e\\u003e example = train_dataset[index]\\n\\n\\u003e\\u003e\\u003e plt.imshow(example[\\\"pixel_values\\\"])\\n\\u003e\\u003e\\u003e plt.axis(\\\"off\\\"...\"],[\"Using Datasets with TensorFlow\\n\\nThis document is a quick introduction to using `datasets` with Tenso...\"],[\"To get a single tensor, you must explicitly use the [`Array`] feature type and specify the shape of ...\"],[\"String and binary objects are unchanged, since PyTorch only supports numbers.\\n\\nThe [`Image`] and [`A...\"],[\"## Data loading\\n\\nAlthough you can load individual samples and batches just by indexing into your dat...\"],[\"Since the entire data preprocessing pipeline can be compiled in a `tf.data.Dataset`, this approach a...\"],[\"```py\\n\\u003e\\u003e\\u003e model.fit(tf_ds, epochs=2)\\n```\\n\\nFor a full description of the arguments, please see the [`...\"],[\"- Your dataset is too large to fit in RAM. `to_tf_dataset()` streams only one batch at a time, so ev...\"],[\"Metric Card for Mahalanobis Distance\\n\\n## Metric Description\\nMahalonobis distance is the distance bet...\"],[\"```bibtex\\n@article{de2000mahalanobis,\\n  title={The Mahalanobis distance},\\n  author={De Maesschalck, ...\"],[\"Troubleshooting\\n\\nThis guide aims to provide you the tools and knowledge required to navigate some co...\"],[\"```bash\\nHfHubHTTPError: 429 Client Error: Too Many Requests for url: ...\\nYou have exceeded our hourl...\"],[\"Here are some ways to address this issue:\\n* A universal solution to pickle issues is to make sure th...\"],[\"Process image data\\n\\nThis guide shows specific methods for processing image datasets. Learn how to:\\n\\n...\"],[\"Both parameter values default to 1000, which can be expensive if you are storing images. Lower these...\"],[\"Builder classes\\n\\n## Builders\\n\\n🤗 Datasets relies on two main classes during the dataset building proc...\"],[\"Metrics\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in 🤗 Datasets. To learn more about how to use m...\"],[\"1. Load the SacreBLEU metric:\\n\\n```py\\n\\u003e\\u003e\\u003e import datasets\\n\\u003e\\u003e\\u003e metric = datasets.load_metric('sacreble...\"],[\"1. [`MetricInfo.description`] provides a brief description about your metric.\\n\\n2. [`MetricInfo.citat...\"],[\"1. Provide a dictionary of URLs that point to the metric files:\\n\\n```py\\nCHECKPOINT_URLS = {\\n    \\\"bleu...\"],[\"### Compute score\\n\\n[`DatasetBuilder._compute`] provides the actual instructions for how to compute a...\"],[\"Metric Card for MAUVE\\n\\n## Metric description\\n\\nMAUVE is a library built on PyTorch and HuggingFace Tr...\"],[\"`device_id`: Device for featurization. Supply a GPU id (e.g. `0` or `3`) to use GPU. If no GPU with ...\"],[\"Partial match between prediction and reference:\\n\\n```python\\nfrom datasets import load_metric\\nmauve = ...\"],[\"Create a dataset loading script\\n\\n\\n\\u003cTip\\u003e\\n\\nThe dataset loading script is likely not needed if your dat...\"],[\"Open the [SQuAD dataset loading script](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fsquad\\u002fblob\\u002fmain\\u002fsquad.py) te...\"],[\"After you've filled out all these fields in the template, it should look like the following example ...\"],[\"def __init__(self, features, data_url, citation, url, label_classes=(\\\"False\\\", \\\"True\\\"), **kwargs):\\n  ...\"],[\"```py\\nclass SuperGlue(datasets.GeneratorBasedBuilder):\\n    \\\"\\\"\\\"The SuperGLUE benchmark.\\\"\\\"\\\"\\n\\n    BUILD...\"],[\"```py\\nclass NewDataset(datasets.GeneratorBasedBuilder):\\n\\nVERSION = datasets.Version(\\\"1.1.0\\\")\\n\\nBUILDE...\"],[\"- `gen_kwargs` provides the file paths to the data files to load for each split.\\n\\nYour `DatasetBuild...\"],[\"answer_starts = [answer[\\\"answer_start\\\"] for answer in qa[\\\"answers\\\"]]\\n                    answers = [...\"],[\"To make it work, we consider lists of files in `gen_kwargs` to be shards.\\nTherefore 🤗 Datasets can a...\"],[\"def _generate_tables(self, filepaths):\\n        idx = 0\\n        for filepath in filepaths:\\n          ...\"],[\"Metric Card for Competition MATH\\n\\n## Metric description\\n\\nThis metric is used to assess performance o...\"],[\"Minimal values (no match):\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e math = load_metric(\\\"c...\"],[\"Metric Card for SARI\\n\\n\\n## Metric description\\nSARI (***s**ystem output **a**gainst **r**eferences and...\"],[\"This metric outputs a dictionary with the SARI score:\\n\\n```\\nprint(sari_score)\\n{'sari': 26.95360195360...\"],[\"## Limitations and bias\\n\\nSARI is a valuable measure for comparing different text simplification syst...\"],[\"Metric Card for Mean IoU \\n\\n\\n## Metric Description\\n\\nIoU (Intersection over Union) is the area of over...\"],[\"### Output Values\\nThe metric returns a dictionary with the following elements:\\n- `mean_iou` (`float`...\"],[\"### Examples\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e mean_iou = l...\"],[\"Metric Card for ROC AUC\\n\\n\\n## Metric Description\\nThis metric computes the area under the curve (AUC) ...\"],[\"See the [Examples Section Below](#examples_section) for more extensive examples....\"],[\"### Inputs\\n- **`references`** (array-like of shape (n_samples,) or (n_samples, n_classes)): Ground t...\"],[\"- **`sample_weight`** (array-like of shape (n_samples,)): Sample weights. Defaults to None.\\n- **`max...\"],[\"### Output Values\\nThis metric returns a dict containing the `roc_auc` score. The score is a `float`,...\"],[\"Example 3, the **multilabel** use case:\\n```python\\n\\u003e\\u003e\\u003e roc_auc_score = datasets.load_metric(\\\"roc_auc\\\"...\"],[\"```bibtex\\n@article{scikit-learn,\\ntitle={Scikit-learn: Machine Learning in {P}ython},\\nauthor={Pedrego...\"],[\"Use with JAX\\n\\nThis document is a quick introduction to using `datasets` with JAX, with a particular ...\"],[\"Finally, to load the data in the device of your choice, you can specify the `device` argument,\\nbut n...\"],[\"String and binary objects are unchanged, since JAX only supports numbers.\\n\\nThe [`Image`] and [`Audio...\"],[\"## Data loading\\n\\nJAX doesn't have any built-in data loading capabilities, so you'll need to use a li...\"],[\"Build and load\\n\\nNearly every deep learning workflow begins with loading a dataset, which makes it on...\"],[\"If the dataset has a dataset script, then it downloads and imports it from the Hugging Face Hub. \\nCo...\"],[\"### BuilderConfig[[datasets-builderconfig]]\\n\\n[`BuilderConfig`] is the configuration class of [`Datas...\"],[\"There are three main methods in [`DatasetBuilder`]:\\n\\n1. [`DatasetBuilder._info`] is in charge of def...\"],[\"The dataset is generated with a Python generator, which doesn't load all the data in memory. As a re...\"],[\"## Security\\n\\nThe dataset repositories on the Hub are scanned for malware, see more information [here...\"],[\"Metric Card for Accuracy\\n\\n\\n## Metric Description\\n\\nAccuracy is the proportion of correct predictions ...\"],[\"Example 2-The same as Example 1, except with `normalize` set to `False`.\\n```python\\n\\u003e\\u003e\\u003e accuracy_metr...\"],[\"Metric Card for CER\\n\\n## Metric description\\n\\nCharacter error rate (CER) is a common metric of the per...\"],[\"## Examples \\n\\nPerfect match between prediction and reference:\\n\\n```python\\nfrom datasets import load_m...\"],[\"## Citation\\n\\n\\n```bibtex\\n@inproceedings{morris2004,\\nauthor = {Morris, Andrew and Maier, Viktoria and ...\"],[\"Metric Card for XTREME-S\\n\\n\\n## Metric Description\\n\\nThe XTREME-S metric aims to evaluate model perform...\"],[\"- `wer_kwargs`: optional dict of keywords to be passed when computing `wer` and `cer`, which are com...\"],[\"- `bleu`: the BLEU score, calculated according to the SacreBLEU metric approach. It can take any val...\"],[\"For the `fleurs-lang_id` subset (which outputs `accuracy`):\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load...\"],[\"- [XTREME-S dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fgoogle\\u002fxtreme_s)\\n- [XTREME-S github repository]...\"],[\"Metric Card for WER\\n\\n## Metric description\\nWord error rate (WER) is a common metric of the performan...\"],[\"The **lower** the value, the **better** the performance of the ASR system, with a WER of 0 being a p...\"],[\"## Limitations and bias\\n\\nWER is a valuable tool for comparing different systems as well as for evalu...\"],[\"Share a dataset to the Hub\\n\\nThe [Hub](https:\\u002f\\u002fhuggingface.co\\u002fdatasets) is home to an extensive colle...\"],[\"Text file extensions are not tracked by Git LFS by default, and if they're greater than 10MB, they w...\"],[\"You can also look at the [Dataset Card specifications](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fblob\\u002f...\"],[\"\\u003e\\u003e\\u003e dataset = load_dataset(\\\"stevhliu\\u002fdemo\\\")\\n# dataset = dataset.map(...)  # do all your processing h...\"],[\"Metric Card for SQuAD\\n\\n## Metric description\\nThis metric wraps the official scoring script for versi...\"],[\"For more recent model performance, see the [dataset leaderboard](https:\\u002f\\u002fpaperswithcode.com\\u002fdataset\\u002f...\"],[\"## Limitations and bias\\nThis metric works only with datasets that have the same format as [SQuAD v.1...\"],[\"Loading methods\\n\\nMethods for listing and loading datasets and metrics:\\n\\n## Datasets\\n\\n[[autodoc]] dat...\"],[\"### Arrow\\n\\n[[autodoc]] datasets.packaged_modules.arrow.ArrowConfig\\n\\n[[autodoc]] datasets.packaged_mo...\"],[\"Metric Card for CUAD\\n\\n## Metric description\\n\\nThis metric wraps the official scoring script for versi...\"],[\"Note that `answer_start` values are not taken into account to compute the metric.\\n\\n```python\\nfrom da...\"],[\"`prec_at_90_recall`: The fraction of true examples among the predicted examples at a recall rate of ...\"],[\"Minimal values:\\n\\n```python\\nfrom datasets import load_metric\\ncuad_metric = load_metric(\\\"cuad\\\")\\npredic...\"],[\"In terms of the metric itself, the accuracy of AUPR has been debated because its estimates are quite...\"],[\"Datasets 🤝 Arrow\\n\\n## What is Arrow?\\n\\n[Arrow](https:\\u002f\\u002farrow.apache.org\\u002f) enables large amounts of dat...\"],[\"## Performance\\n\\nIterating over a memory-mapped dataset using Arrow is fast. Iterating over Wikipedia...\"],[\"Metric Card for IndicGLUE\\n\\n## Metric description\\nThis metric is used to compute the evaluation metri...\"],[\"`f1`: the harmonic mean of the precision and recall (see [F1 score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002ff...\"],[\"Partial match for the CVIT-Mann Ki Baat subset (which outputs `precision@10`) \\n\\n```python\\n\\u003e\\u003e\\u003e from d...\"],[\"Structure your repository\\n\\nTo host and share your dataset, create a dataset repository on the Huggin...\"],[\"Or you can use glob patterns to automatically list all the files you need:\\n\\n```yaml\\n---\\nconfigs:\\n- c...\"],[\"```yaml\\n- config_name: main_data\\n  data_files: \\\"main_data.csv\\\"\\n  default: true\\n```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n## Autom...\"],[\"Here is an example with three splits, `train`, `test`, and `random`:\\n\\n```\\nmy_dataset_repository\\u002f\\n├──...\"],[\"For convenience, you can also place your data files into different directories.\\nIn this case, the sp...\"],[\"Load a dataset from the Hub\\n\\nFinding high-quality datasets that are reproducible and accessible can ...\"],[\"If you're happy with the dataset, then load it with [`load_dataset`]:\\n\\n```py\\n\\u003e\\u003e\\u003e from datasets impor...\"],[\"Use the [`get_dataset_config_names`] function to retrieve a list of all the possible configurations ...\"],[\"Metric Card for Code Eval\\n\\n## Metric description\\n\\nThe CodeEval metric estimates the pass@k metric fo...\"],[\"The Code Eval metric outputs two things:\\n\\n`pass_at_k`: a dictionary with the pass rates for each k v...\"],[\"## Limitations and bias\\n\\nAs per the warning included in the metric code itself:\\n\\u003e This program exist...\"],[\"More information about the limitations of the code can be found on the [Human Eval Github repository...\"],[\"Create a dataset\\n\\nSometimes, you may need to create a dataset if you're working with your own data. ...\"],[\"* [`ImageFolder`] uses the [`~datasets.Image`] feature to decode an image file. Many image extension...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\\"audiofolder\\\", data_dir=\\\"\\u002fpa...\"],[\"A generator-based [`IterableDataset`] needs to be iterated over with a `for` loop for example:\\n\\n    ...\"],[\"To learn more about how to write loading scripts, take a look at the \\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co...\"],[\"Create an audio dataset\\n\\nYou can share a dataset with your team or with anyone in the community by c...\"],[\"```py\\naudio_dataset.push_to_hub(\\\"\\u003cusername\\u003e\\u002fmy_dataset\\\")\\n```\\n\\nThis will create a dataset repository ...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe metadata file should include a `file_name` column to link an audio file to it's metadata...\"],[\"```\\ndata\\u002ftrain\\u002ffirst_train_audio_file.mp3\\ndata\\u002ftrain\\u002fsecond_train_audio_file.mp3\\n\\ndata\\u002ftest\\u002ffirst_te...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\n\\u003cTip\\u003e\\n\\nSome audio datasets, like those found in [Kaggle competitions](https:\\u002f\\u002fwww.kaggle.co...\"],[\"Here is an example using TAR archives:\\n\\n```\\nmy_dataset\\u002f\\n├── README.md\\n├── my_dataset.py\\n└── data\\u002f\\n  ...\"],[\"```py\\nclass VivosDataset(datasets.GeneratorBasedBuilder):\\n    \\\"\\\"\\\"VIVOS is a free Vietnamese speech c...\"],[\"```py\\nclass LibriVoxIndonesiaConfig(datasets.BuilderConfig):\\n    \\\"\\\"\\\"BuilderConfig for LibriVoxIndone...\"],[\"```py\\nclass LibriVoxIndonesia(datasets.GeneratorBasedBuilder):\\n    DEFAULT_CONFIG_NAME = \\\"all\\\"\\n\\n    ...\"],[\"\\u003cTip\\u003e\\n\\nYou'll notice a lot of the dataset information is defined earlier in the loading script which...\"],[\"In the `gen_kwargs` parameter, specify the file path to the `prompts_path` and `path_to_clips`. For ...\"],[\"Files inside TAR archives are accessed and yielded sequentially. This means you need to have the met...\"],[\"Put these two steps together, and the whole `_generate_examples` method looks like:\\n\\n```py\\ndef _gene...\"],[\"#### Download and define the dataset splits\\n\\n1. Use the [`~DownloadManager.download`] method to down...\"],[\"5. Now use the [`SplitGenerator`] to organize the audio files and metadata in each split. Name each ...\"],[\"#### Generate the dataset\\n\\nHere `_generate_examples` accepts `local_extracted_archive`, `audio_files...\"],[\"Put both of these steps together, and the whole `_generate_examples` method should look like:\\n\\n```py...\"],[\"Share a dataset using the CLI\\n\\nAt Hugging Face, we are on a mission to democratize good Machine Lear...\"],[\"For more information on how to load a dataset from the Hub, take a look at the [load a dataset from ...\"],[\"### Upload your files\\n\\nYou can directly upload your files to your repository on the Hugging Face Hub...\"],[\"Metric Card for Spearman Correlation Coefficient Metric (spearmanr)\\n\\n\\n## Metric Description\\nThe Spea...\"],[\"If `return_pvalue=False`, the output is a `dict` with one value, as below:\\n```python\\n{'spearmanr': -...\"],[\"## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@book{kokoska2000crc,\\n  title={CRC standard probabil...\"],[\"Datasets\\n\\n\\u003cimg class=\\\"float-left !m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[150px]\\\" src...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eTechnical descriptions of how 🤗 Datasets classes and methods work.\\u003c\\u002fp\\u003e\\n    ...\"],[\"Load text data\\n\\nThis guide shows you how to load text datasets. To learn how to load any type of dat...\"],[\"Metric Card for TER\\n\\n## Metric Description\\nTER (Translation Edit Rate, also called Translation Error...\"],[\"### Inputs\\nThis metric takes the following as input:\\n- **`predictions`** (`list` of `str`): The syst...\"],[\"#### Values from Popular Papers\\n\\n\\n### Examples\\nBasic example with only predictions and references as...\"],[\"Example ignoring punctuation and capitalization, but with an extra (incorrect) sample:\\n```python\\n\\u003e\\u003e\\u003e...\"],[\"## Further References\\n- See [the sacreBLEU github repo](https:\\u002f\\u002fgithub.com\\u002fmjpost\\u002fsacreBLEU#ter) for...\"],[\"Metric Card for MAE\\n\\n\\n## Metric Description\\n\\nMean Absolute Error (MAE) is the mean of the magnitude ...\"],[\"Each MAE `float` value ranges from `0.0` to `1.0`, with the best value being 0.0.\\n\\nOutput Example(s)...\"],[\"## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n  title={Scikit-learn: Machine Learning in {P}ython}...\"],[\"--\\nYAML tags (full spec here: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fblob\\u002fmain\\u002fdatasetcard.md?plain...\"],[\"## Dataset Description\\n\\n- **Homepage:** [Add homepage URL here if available (unless it's a GitHub re...\"],[\"- `task-category-tag`: The dataset can be used to train a model for [TASK NAME], which consists in [...\"],[\"### Data Fields\\n\\nList and describe the fields present in the dataset. Mention their data type, and w...\"],[\"### Source Data\\n\\nThis section describes the source data (e.g. news text and headlines, social media ...\"],[\"### Annotations\\n\\nIf the dataset contains annotations which are not part of the initial data collecti...\"],[\"State whether the dataset contains other data that might be considered sensitive (e.g., data that re...\"],[\"If studies of the datasets have outlined other limitations of the dataset, such as annotation artifa...\"],[\"Know your dataset\\n\\nThere are two types of dataset objects, a regular [`Dataset`] and then an ✨ [`Ite...\"],[\"Indexing by the column name returns a list of all the values in the column:\\n\\n```py\\n\\u003e\\u003e\\u003e dataset[\\\"text...\"],[\"### Slicing\\n\\nSlicing returns a slice - or subset - of the dataset, which is useful for viewing sever...\"],[\"```py\\n\\u003e\\u003e\\u003e from datasets import load_dataset\\n\\n\\u003e\\u003e\\u003e dataset = load_dataset(\\\"rotten_tomatoes\\\", split=\\\"tr...\"],[\"To get more hands-on with these datasets types, check out the [Process](process) guide to learn how ...\"],[\"Metric Card for XNLI\\n\\n## Metric description\\n\\nThe XNLI metric allows to evaluate a model's score on t...\"],[\"Minimal values:\\n\\n```python\\n\\u003e\\u003e\\u003e from datasets import load_metric\\n\\u003e\\u003e\\u003e xnli_metric = load_metric(\\\"xnli\\\"...\"],[\"- [XNI Dataset GitHub](https:\\u002f\\u002fgithub.com\\u002ffacebookresearch\\u002fXNLI)\\n- [HuggingFace Tasks -- Text Classi...\"],[\"Evaluate predictions\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in 🤗 Datasets. To learn more about...\"],[\"```py\\n\\u003e\\u003e\\u003e metric = load_metric('glue', 'mrpc')\\n```\\n\\n## Metrics object\\n\\nBefore you begin using a [`Me...\"],[\"## Compute metric\\n\\nOnce you have loaded a metric, you are ready to use it to evaluate a models predi...\"],[\"Batch mapping\\n\\nCombining the utility of [`Dataset.map`] with batch mode is very powerful. It allows ...\"],[\"For example, from a dataset of 1 column and 3 rows, if you use `map` to return a new column with twi...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"To preview the docs, first install the `watchdog` module with:\\n\\n```bash\\npip install watchdog\\n```\\n\\nTh...\"],[\"```\\nSections that were moved:\\n\\n[ \\u003ca href=\\\"..\\u002fnew-file#section-b\\\"\\u003eSection A\\u003c\\u002fa\\u003e\\u003ca id=\\\"section-a\\\"\\u003e\\u003c\\u002fa\\u003e...\"],[\"The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or \\\\[~\\\\`XXXClass.method\\\\`\\\\]...\"],[\"#### Writing a multi-line code block\\n\\nMulti-line code blocks can be useful for displaying examples. ...\"],[\"#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that no fi...\"],[\"# process a batch of examples\\n    \\u003e\\u003e\\u003e ds = ds.map(lambda example: tokenizer(example[\\\"text\\\"]), batche...\"],[\"Installation\\n\\nBefore you start, you'll need to setup your environment and install the appropriate pa...\"],[\"```python\\n{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'A...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\n## Vision\\n\\nTo work with image datasets, you need to install the [`Image`] feature as an ext...\"]],\"hovertemplate\":\"source=datasets\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"datasets, circle\",\"marker\":{\"color\":\"#EF553B\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"datasets, circle\",\"showlegend\":true,\"x\":[-1.9808618,-1.3940986,-4.7052555,-3.1585965,2.978591,2.3351436,-12.963909,-12.946825,-13.049509,-12.661469,-5.036233,-2.6293159,16.545996,17.590487,16.268803,16.897472,16.851086,16.208742,16.540108,16.344904,-6.2166495,-6.048091,-6.165335,-5.3910217,-2.4643102,8.884677,10.062591,11.384334,6.6786737,5.4387956,2.0370042,3.1808724,-3.6915512,-12.912958,-13.788702,-13.5943365,-13.683072,4.158621,3.9068146,3.8201427,4.4031277,-2.2126427,-6.4591756,-3.936777,-1.559476,16.526136,17.96277,-0.50082105,0.32572353,0.020654885,7.22525,-3.918646,4.8972416,-0.2920051,0.58739334,0.5688541,-0.79128563,2.8607259,-1.0891198,-3.063183,-1.4021529,-2.9456522,-12.936921,-12.835219,-3.8473177,7.443388,3.1295865,2.5284111,0.71600914,2.5708609,1.572762,2.5181737,2.6123502,2.8676798,13.503662,-2.3996382,-1.9755971,-3.6697617,-3.2141662,-3.1046298,13.451459,-7.233818,4.9084773,-7.5588403,-7.649317,-6.2685785,2.4976509,2.3965547,2.8781488,2.8701994,-3.2537847,-2.2563167,-2.188726,9.293117,9.294869,9.293174,9.295602,-1.0252578,-0.08814478,-9.945691,-9.95862,-3.1604373,-2.5772293,2.9931765,2.8976178,-6.3386097,-1.1718807,-0.7195892,-2.6048265,0.33524963,-0.25746933,2.2177584,-21.732857,-2.1288702,-3.4592328,-1.4401108,-5.412385,-8.573293,-8.8851385,-3.627564,-6.0808516,3.7201724,-18.913471,-18.780363,-18.813082,-18.904526,-18.510506,-0.15545079,-3.4675343,-1.9655483,-3.87974,-3.8818429,-3.3911421,-3.7703831,-3.0921726,-2.0622396,-2.7331011,-2.1642387,-3.4388793,-4.2904334,-3.7455049,-5.3231754,6.874834,-2.0109751,19.848476,18.463648,17.706713,8.565306,12.49205,8.566999,9.222232,8.934127,6.1819196,4.0061197,3.9565792,4.1888804,-6.2273684,-6.4172945,-6.734252,-6.6060867,2.4799356,2.3967223,-6.946825,-7.3597937,-7.6466594,-6.9738035,-6.9917374,1.0212597,0.7594117,-4.787713,-4.4354863,0.9800559,-4.323463,-4.0052023,-3.8320994,-3.9257708,-3.8895903,-4.124413,-3.9263563,-3.900131,-3.7673955,-13.769791,-0.2625343,-2.6707263,-13.91307,-1.0540425,-8.767701,-1.4166785,-1.1540626,-8.210378,-12.431446,-12.449436,-12.484697,-15.11298,-0.9729389,-1.0869135,-0.9213365,-1.1517099,-2.9239485,3.9606113,14.215067,13.883601,13.9531975,13.908393,-10.331415,1.7235582,3.474183,1.2633892,-4.996531,-0.34342805,-0.3446784,-1.3580837,-4.0950947,5.171467,-4.114731,-4.048004,-4.177095,-4.2001357,-4.1438427,4.9767675,2.9985776,2.518993,-2.2911408,3.4777286,3.615219,1.2622561,10.643154,10.579089,14.45707,-4.178805,1.910086,1.9271276,3.3739715,2.1908042,2.2330585,2.3492143,2.025591,2.0414956,2.4253678,2.3633728,2.1309996,2.6046653,-2.5735006,-1.2181911,-4.9973726,12.828839,-3.2303376,-7.164545,12.522642,-5.3090305,-6.730525,-6.8795776,-6.9213676,-6.913976,-7.0272427,-6.9408536,-6.941255,-6.7173185,13.218287,12.834567,13.090569,13.592298,13.389669,-5.0893145,-1.0182022,2.6698515,2.69459,2.6831453,2.7204666,12.722132,-5.084517,-4.7254686,-4.3035765,-0.42550498,-7.0282736,10.559034,10.452756,10.460014,-1.7164253,-0.9516161,-1.6897613,-1.0998849,-0.9782389,4.599246,4.6459656,4.716094,0.012063251,-0.35542142,-1.2897707,5.982837,6.0035033,5.60192,0.22862104,0.20752236,0.15949245,0.76512337,2.6827714,2.5673392,0.9661933,0.48924154,1.6305915,-21.73045,13.125687,11.947399,-1.7593669,-2.4695559,0.014473506,-7.233567,-1.4827614,2.4027014,-6.3362584,0.8826991,0.6623118,-0.9061674,0.5175143,-1.1158773,-6.549501,-3.6625023,-2.8027394,-3.9271917,-0.68575585,-5.070085,-4.0472775,12.179176,8.593958,8.784071,-10.699883,-10.664897,-10.734498,-2.8229861,-2.8589854,8.256102,8.3737955,-3.3207886,-2.1841722,-2.2558205,-1.046879,-1.0194894,-3.63279,-3.5941055,-7.332555,13.600761,9.508274,-6.884416,-2.6274993,-5.776059,-1.1879333,-3.7104878,-5.8480835,-5.685426,8.816484,8.967939,8.743831,1.7058709,-3.39601,-3.6111107,-3.6350634,-3.5770445,-3.326321,-3.1315174,-2.8860388,8.518053,1.867719,-0.2995114,0.147474,11.666932,13.0282755,11.667613,11.661875,19.49592,-2.1120718,-1.4650321,-0.9419006,13.314764,3.175811,3.1798754,3.6602361,-4.906298,-4.056851,-4.474732,-4.579281,-4.306046,-4.497251,-4.4765735,-4.470152,-4.7890134,-4.349173,-1.4262416,-5.8530474,-5.9232283,-2.314539,-2.405257,-6.9495263,3.446154,3.7608793,4.486031,-11.419532,0.33559966,-1.2683904,-0.3578674,-0.14915308,-0.6025377,1.5390028,1.53786,0.054842874,19.335087,18.12243,-5.7159276,-5.3906875,-5.8050094,-4.826743,-4.9880714,-4.54552,-5.9182816,-6.0875735,-5.397689,-6.669919,-6.1430006,-2.7622209,-2.6375315,-2.3250935,-5.655314,-3.1609607,7.367821,5.7153406,4.5734606,-1.8020588,19.737864,18.22077,18.06378,19.251726,19.311342,17.579527,13.083375,-6.2071853,8.125845,8.159354],\"xaxis\":\"x\",\"y\":[4.461474,4.149109,-6.253917,-8.130991,2.5897489,2.6677382,-18.172558,-18.215824,-18.272137,-18.583107,-6.4999275,-9.865716,3.1986449,2.4932883,2.76326,1.9153243,2.297762,3.0780444,2.878127,2.9093456,7.593697,7.6377845,7.597415,7.5139494,-11.675352,-11.491719,2.5003376,2.335427,1.4230528,0.37154073,0.9214614,0.50131726,-6.5771766,-1.6473415,-1.8370066,-1.8073833,-1.8396356,1.0183775,1.0795846,1.0683932,0.39903003,-2.180214,3.7742581,3.5743551,1.1581455,3.4449866,3.5619724,6.4375353,6.7652717,6.923033,3.034656,-7.3320045,1.4885181,-0.4873418,6.5942492,6.93481,-0.35713777,0.39105305,-6.1219125,4.0778747,-5.9063125,4.090597,-18.820156,-18.959774,4.6675453,3.4111173,3.3870566,4.0018315,-0.17562275,3.924436,4.4880795,3.8088393,3.4564064,3.4838228,5.243651,-2.040845,-1.2696642,-2.1064835,-1.8905451,-2.51135,5.118168,-0.7557284,1.7167419,-2.259075,-2.3646057,0.8304537,-1.4128039,-1.6216174,-0.68132275,-2.2607343,6.72591,6.201871,5.9594584,-20.01009,-20.007273,-20.007843,-20.009745,1.7094129,-5.8295307,0.96369344,0.5934183,1.6828052,1.2686269,5.6594763,5.612913,0.3245245,0.5375328,0.3646524,3.1110008,6.808998,5.841158,0.13996612,8.55469,5.2390485,4.6936607,4.7245626,-1.3303765,2.1894166,2.4992328,-0.0029393767,-0.4895531,2.9455316,-4.4389734,-4.284347,-4.3362565,-4.358306,-3.6820173,9.343671,-5.570687,-3.002067,-0.32406947,-0.42349157,0.057106737,0.3697574,0.8878383,0.9959953,0.678645,0.8083321,0.0994185,-8.128643,2.4387429,0.6259348,-1.2079425,-1.2752429,2.3435268,3.7326095,3.9494863,3.870887,3.1563613,2.5501773,2.9124148,3.056115,0.7484786,1.0654016,2.2248778,2.6666508,7.5439754,7.7053556,7.3312206,7.475042,4.22089,4.370791,6.8431635,6.711947,6.9359565,6.76795,6.798181,6.388414,6.3522787,-5.8447156,-7.0720253,0.93921405,-7.4141383,-7.8740516,-7.965395,-7.911996,-8.2029915,-8.057442,-8.00722,-7.9471917,-7.995802,-1.7154868,3.0934544,2.1348696,-1.8000425,1.9557246,2.1924388,2.098968,2.0943418,1.1682372,-17.665028,-17.787992,-18.245249,-2.129555,0.04541709,1.5184115,1.8934776,-1.945599,-5.384845,1.841509,3.843089,3.5106559,4.0209413,4.0646358,-0.9192386,3.115126,2.6329806,3.7225215,0.89855534,6.7469716,6.543619,7.47822,-7.7775855,1.693918,-8.506621,-8.502161,-8.285059,-8.078001,-8.532676,1.3878922,3.529374,3.685477,3.4316816,0.8613616,1.9564453,0.50323135,3.556294,3.5425072,5.07093,3.1059456,3.8225033,3.6584241,2.755591,3.6850832,3.7209904,4.0131664,3.9051642,3.8447614,3.882014,3.9972878,4.1466928,3.690607,-2.3421957,-2.362276,-6.7022324,5.080451,0.33851585,1.1793165,4.0717206,1.2816254,0.8243397,0.6972205,0.5449508,0.51017314,0.2987817,0.7272359,0.4184004,-0.9518673,3.769554,3.8810983,3.767223,3.7130036,3.6890237,2.718993,-2.0913105,4.328114,4.41882,4.420951,4.2095346,3.690739,-1.2919877,-1.6187849,-5.5066075,-6.315956,6.5320387,4.012211,4.0197773,4.000722,-1.4325936,-1.272898,-1.2816688,-0.63806534,-2.3347225,3.7802696,3.850445,4.045024,-0.21257286,-0.17575169,-0.43267637,1.5836285,1.6610268,-0.23313908,0.42901334,-0.11518875,2.116697,1.743863,4.0074406,4.025419,4.4329653,4.6180115,4.175339,8.553013,5.440946,4.4904275,0.40045837,0.36719126,1.2534219,6.3881655,1.8399183,1.3285685,7.641453,4.356801,4.6129546,4.8396316,4.424085,-0.26591253,0.638458,0.023201734,2.0453176,0.5329572,0.21458836,-1.4102229,0.517564,5.056039,2.8340929,2.7202654,-0.67715204,-0.6966844,-0.7490319,-0.6981728,-4.4509153,-10.242252,-10.419965,-2.7953568,-1.4891979,-1.695653,-3.6417842,-2.8523836,3.8982456,3.670937,2.0575519,3.8311534,2.97314,2.5695915,3.1382222,3.9910192,0.663657,6.830206,-6.34178,-5.7779875,-11.712657,-11.370227,-11.351103,1.485263,5.0271587,5.635628,6.365604,6.088249,5.0311937,4.9071164,4.5183396,-11.549437,3.4517512,4.0827165,4.3571186,2.16695,4.477828,2.1361423,2.0659933,2.2788322,-0.5989972,-1.6230665,-2.0843077,5.447631,3.2961576,3.1056142,1.3859729,3.424574,3.9363554,3.9838762,3.7024927,3.5600803,3.3976672,3.715121,3.532521,3.5522277,3.9282718,-2.461343,3.195876,3.0594208,4.7111087,4.3764734,6.87211,3.6231883,3.0989134,1.8757877,-1.3862443,1.8232847,2.778265,-6.171467,-6.161504,-6.2931643,4.156963,4.336839,4.5275397,2.4363933,2.1089497,0.7168522,0.82363206,0.77485913,-1.4344592,1.7655854,2.316809,3.5173357,3.2294893,3.473389,3.8163974,3.5985055,0.60836184,0.52794605,0.8132424,2.3861103,0.42895842,3.0970976,1.4341197,0.78884006,1.547509,2.3535984,3.7435608,4.102946,3.1005912,2.558648,3.7531304,5.1688747,2.736261,0.7426863,1.5892301],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Q-Learning Recap [[q-learning-recap]]\\n\\n\\n*Q-Learning* **is the RL algorithm that** :\\n\\n- Trains a *Q-f...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q2: Which of the following statements are true, when talking about models with bias and\\u002for varia...\"],[\"### Q4: How would you describe, with your own words, the Actor-Critic Method (A2C)?\\n\\n\\u003cdetails\\u003e\\n\\u003csumm...\"],[\"Additional Readings\\n\\nThese are **optional readings** if you want to go deeper.\\n\\n\\n## Introduction to ...\"],[\"Hands-on\\n\\nNow that you learned the basics of multi-agents, you're ready to train your first agents i...\"],[\"More precisely, AI vs. AI is three tools:\\n\\n- A *matchmaking process* defining the matches (which mod...\"],[\"In order for your model to get correctly evaluated against others you need to follow these rules:\\n\\n1...\"],[\"```bash\\ncd ml-agents\\npip install -e .\\u002fml-agents-envs\\npip install -e .\\u002fml-agents\\n```\\n\\nFinally, you ne...\"],[\"### The observation space\\n\\nThe observation space is composed of vectors of size 336:\\n\\n- 11 ray-casts...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002f...\"],[\"The config file we’re going to use here is in  `.\\u002fconfig\\u002fpoca\\u002fSoccerTwos.yaml`. It looks like this:\\n...\"],[\"Depending on your hardware, 5M timesteps (the recommended value, but you can also try 10M) will take...\"],[\"Copy the token, run this, and paste the token\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nThen, we need to r...\"],[\"1. That you have this tag in your model: ML-Agents-SoccerTwos. This is the tag we use to select mode...\"],[\"Two types of value-based methods [[two-types-value-based-methods]]\\n\\nIn value-based methods, **we lea...\"],[\"\\u003cfigure\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolv...\"],[\"For each state, the state-value function outputs the expected return if the agent **starts at that s...\"],[\"In either case, whichever value function we choose (state-value or action-value function), **the ret...\"],[\"The advantages and disadvantages of policy-gradient methods\\n\\nAt this point, you might ask, \\\"but Deep...\"],[\"Under a deterministic policy, the policy will either always move right when in a red state or always...\"],[\"For instance, if during the training, the best action was left (with a Q-value of 0.22) and the trai...\"],[\"Glossary \\n\\nThis is a community-created glossary. Contributions are welcome!\\n\\n- **Deep Q-Learning:** ...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this unit! **That was the biggest one**, and there ...\"],[\"Introduction\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fres...\"],[\"Introduction to Q-Learning [[introduction-q-learning]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhu...\"],[\"Conclusion\\n\\nCongrats on finishing this unit! You’ve just trained your first ML-Agents and shared it ...\"],[\"Language models in RL\\n## LMs encode useful knowledge for agents\\n\\n**Language models** (LMs) can exhib...\"],[\"1) Sample inefficiency\\n\\n2) Unexpected behaviors from humans’ eyes\\n\\nAs a first attempt, the paper [“G...\"],[\"## Further reading\\n\\nFor more information we recommend you check out the following resources:\\n\\n- [Goo...\"],[\"The Deep Q-Network (DQN)  [[deep-q-network]]\\nThis is the architecture of our Deep Q-Learning network...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"The certification process\\n\\n\\nThe certification process is **completely free**:\\n\\n- To get a *certifica...\"],[\"Summary [[summary]]\\n\\nThat was a lot of information! Let's summarize:\\n\\n- Reinforcement Learning is a ...\"],[\"Glossary [[glossary]]\\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n\\n### Strat...\"],[\"### Greedy strategy:\\n\\n- Involves always choosing the action that is expected to lead to the highest ...\"],[\"The Reinforcement Learning Framework [[the-reinforcement-learning-framework]]\\n\\n## The RL Process [[t...\"],[\"Because RL is based on the **reward hypothesis**, which is that all goals can be described as the **...\"],[\"- *Observation o*: is a **partial description of the state.** In a partially observed environment.\\n\\n...\"],[\"To recap:\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve...\"],[\"As we can see in the diagram, **it’s more probable to eat the cheese near us than the cheese close t...\"],[\"Introduction [[introduction]]\\n\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"From Q-Learning to Deep Q-Learning [[from-q-to-dqn]]\\n\\nWe learned that **Q-Learning is an algorithm w...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"Hands-on\\n\\n\\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\nnotebooks={[\\n  {label: \\\"Goo...\"],[\"# Unit 5: An Introduction to ML-Agents\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-r...\"],[\"- Understand how **ML-Agents** works and the environment library.\\n- Be able to **train agents in Uni...\"],[\"Download the file SnowballTarget.zip from https:\\u002f\\u002fdrive.google.com\\u002ffile\\u002fd\\u002f1YHHLjyj6gaZ3Gemx1hQgqrPgS...\"],[\"We'll give you a preliminary version of this config (to copy and paste into your `SnowballTarget.yam...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"If you don't want to use Google Colab or a Jupyter Notebook, you need to use this command instead: `...\"],[\"1. Remember your repo-id\\n\\n2. Go here: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fThomasSimonini\\u002fML-Agents-Snowbal...\"],[\"Unzip it\\n\\n```python\\n%%capture\\n!unzip -d .\\u002ftraining-envs-executables\\u002flinux\\u002f .\\u002ftraining-envs-executabl...\"],[\"### Push the agent to the Hugging Face Hub\\n\\n- Now that we trained our agent, we’re **ready to push i...\"],[\"Decision Transformers\\n\\nThe Decision Transformer model was introduced by [\\\"Decision Transformer: Rein...\"],[\"Start the tutorial here 👉 https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002ftrain-decision-transformers\\n\\n## Further readin...\"],[\"Additional Readings [[additional-readings]]\\n\\n##  An introduction to multi-agents\\n\\n- [Multi-agent rei...\"],[\"Introducing Q-Learning [[q-learning]]\\n## What is Q-Learning? [[what-is-q-learning]]\\n\\nQ-Learning is a...\"],[\"The Q-table is initialized. That's why all values are = 0. This table **contains, for each state and...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggin...\"],[\"- *With probability 1 — ɛ* : we do **exploitation** (aka our agent selects the action with the highe...\"],[\"This means that to update our \\\\\\\\(Q(S_t, A_t)\\\\\\\\):\\n\\n- We need \\\\\\\\(S_t, A_t, R_{t+1}, S_{t+1}\\\\\\\\).\\n- To u...\"],[\"\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002f...\"],[\"Play with Huggy [[play]]\\n\\nNow that you've trained Huggy and pushed it to the Hub. **You will be able...\"],[\"Discord 101 [[discord-101]]\\n\\nHey there! My name is Huggy, the dog 🐕, and I'm looking forward to trai...\"],[\"The HF Community Server has a thriving community of human beings interested in many areas, so you ca...\"],[\"Introduction [[introduction]]\\n\\nOne of the most critical tasks in Deep Reinforcement Learning is to *...\"],[\"Designing Multi-Agents systems\\n\\nFor this section, you're going to watch this excellent introduction ...\"],[\"## Centralized approach\\n\\n\\u003cfigure\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-cour...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"Hands-on\\n\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      notebooks={[\\n ...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fgiphy.com\\u002fembed\\u002fpynZagVcYxVUk\\\" width=\\\"480\\\" height=\\\"480\\\" frameBorder=\\\"0\\\" class=\\\"...\"],[\"If you don't find your model, **go to the bottom of the page and click on the refresh button**\\n\\nFor ...\"],[\"👉 The video tutorial: https:\\u002f\\u002fyoutu.be\\u002fMEt6rrxH8W4\\n\\n```python\\nfrom IPython.display import HTML\\n\\nHTML...\"],[\"- These methods will:\\n  - `_evalutate_agent()`: evaluate the agent.\\n  - `_generate_model_card()`: ge...\"],[\"# First get datetime\\n        eval_datetime = datetime.datetime.now()\\n        eval_form_datetime = ev...\"],[\"while done is False:\\n            state = torch.Tensor(state).to(device)\\n            action, _, _, _ ...\"],[\"# Step 2: Generate the model card\\n    model_card = f\\\"\\\"\\\"\\n  # PPO Agent Playing {env_id}\\n\\n  This is a ...\"],[\"with readme_path.open(\\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n        f.write(readme)\\n\\n    # Save our metrics t...\"],[\"from pathlib import Path\\nimport datetime\\nimport tempfile\\nimport json\\nimport shutil\\nimport imageio\\n\\nf...\"],[\"# Algorithm specific arguments\\n    parser.add_argument(\\\"--env-id\\\", type=str, default=\\\"CartPole-v1\\\",\\n...\"],[\"help=\\\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\\\")\\n    p...\"],[\"# Adding HuggingFace argument\\n    parser.add_argument(\\\"--repo-id\\\", type=str, default=\\\"ThomasSimonini...\"],[\"# Step 3: Evaluate the model and build JSON\\n        mean_reward, std_reward = _evaluate_agent(eval_e...\"],[\"msg.info(f\\\"Your model is pushed to the Hub. You can view your model here: {repo_url}\\\")\\n    return re...\"],[\"def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\\n    \\\"\\\"\\\"\\n    ...\"],[\"# Merges both dictionaries\\n    metadata = {**metadata, **eval}\\n\\n    return metadata\\n\\n\\ndef _save_mode...\"],[\"return thunk\\n\\n\\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\\n    torch.nn.init.orthogonal_(...\"],[\"# TRY NOT TO MODIFY: seeding\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manu...\"],[\"for step in range(0, args.num_steps):\\n            global_step += 1 * args.num_envs\\n            obs[s...\"],[\"# bootstrap value if not done\\n        with torch.no_grad():\\n            next_value = agent.get_value...\"],[\"# Optimizing the policy and value network\\n        b_inds = np.arange(args.batch_size)\\n        clipfr...\"],[\"# Value loss\\n                newvalue = newvalue.view(-1)\\n                if args.clip_vloss:\\n      ...\"],[\"y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\\n        var_y = np.var(y_true)\\n    ...\"],[\"- Copy the token\\n- Run the cell below and paste the token\\n\\n```python\\nfrom huggingface_hub import not...\"],[\"The SnowballTarget Environment\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"What are the policy-based methods?\\n\\nThe main goal of Reinforcement learning is to **find the optimal...\"],[\"- On the other hand, in *policy-based methods*, we directly learn to approximate \\\\\\\\(\\\\pi^{*}\\\\\\\\) witho...\"],[\"The difference between these two methods **lies on how we optimize the parameter** \\\\\\\\(\\\\theta\\\\\\\\):\\n\\n- ...\"],[\"Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym 🤖 [[hands-on]]\\n\\n\\n      \\u003cCours...\"],[\"## Objectives of this notebook 🏆\\n\\nAt the end of the notebook, you will:\\n\\n- Be able to use **Panda-Gy...\"],[\"```bash\\n!pip install stable-baselines3[extra]\\n!pip install gymnasium\\n!pip install huggingface_sb3\\n!p...\"],[\"In `PandaReachDense-v3` the robotic arm must place its end-effector at a target position (green ball...\"],[\"#### Solution\\n\\n```python\\nenv = make_vec_env(env_id, n_envs=4)\\n\\nenv = VecNormalize(env, norm_obs=True...\"],[\"Now that we saw we got good results after the training, we can publish our trained model on the Hub ...\"],[\"```python\\nfrom huggingface_sb3 import package_to_hub\\n\\npackage_to_hub(\\n    model=model,\\n    model_nam...\"],[\"# 3\\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\\n\\n# 4\\nmodel = A2C(policy =...\"],[\"(Optional) What is Curiosity in Deep Reinforcement Learning?\\n\\nThis is an (optional) introduction to ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"The “Deep” in Reinforcement Learning [[deep-rl]]\\n\\n\\u003cTip\\u003e\\nWhat we've talked about so far is Reinforcem...\"],[\"What is RL? A short recap [[what-is-rl]]\\n\\nIn RL, we build an agent that can **make smart decisions**...\"],[\"Conclusion [[Conclusion]]\\n\\nThat’s all for today. Congrats on finishing this unit and the tutorial!\\n\\n...\"],[\"Diving deeper into policy-gradient methods\\n\\n## Getting the big picture\\n\\nWe just learned that policy-...\"],[\"Now that we got the big picture, let's dive deeper into policy-gradient methods.\\n\\n## Diving deeper i...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"2. We have another problem that I explain in the next optional section. To differentiate this object...\"],[\"- Update the weights of the policy: \\\\\\\\(\\\\theta \\\\leftarrow \\\\theta + \\\\alpha \\\\hat{g}\\\\\\\\)\\n\\nWe can interpre...\"],[\"Additional Readings [[additional-readings]]\\n\\n## Bias-variance tradeoff in Reinforcement Learning\\n\\nIf...\"],[\"Let's train and play with Huggy 🐶 [[train]]\\n\\n\\n\\n\\n          \\u003cCourseFloatingBanner classNames=\\\"absolute...\"],[\"## Objectives of this notebook 🏆\\n\\nAt the end of the notebook, you will:\\n\\n- Understand **the state sp...\"],[\"```bash\\nmkdir .\\u002ftrained-envs-executables\\nmkdir .\\u002ftrained-envs-executables\\u002flinux\\n```\\n\\n```bash\\nwget --...\"],[\"**Joint motors drive huggy legs**. This means that to get the target, Huggy needs to **learn to rota...\"],[\"- We need to create a config file for Huggy. \\n\\n- Go to `\\u002fcontent\\u002fml-agents\\u002fconfig\\u002fppo`\\n\\n- Create a n...\"],[\"The training will take 30 to 45min depending on your machine (don't forget to **set up a GPU**), go ...\"],[\"```bash\\nmlagents-push-to-hf --run-id=\\\"HuggyTraining\\\" --local-dir=\\\".\\u002fresults\\u002fHuggy\\\" --repo-id=\\\"Thomas...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fn...\"],[\"Mid-way Quiz [[mid-way-quiz]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n### Q4: What is the difference between Monte Carlo and Temporal Difference learning meth...\"],[\"Bonus: Learn to create your own environments with Unity and MLAgents\\n\\n**You can create your own rein...\"],[\"Train your first Deep Reinforcement Learning Agent 🤖 [[hands-on]]\\n\\n\\n\\n\\n      \\u003cCourseFloatingBanner cl...\"],[\"And you can check your progress here 👉 https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fThomasSimonini\\u002fCheck-my-progres...\"],[\"## This notebook is from Deep Reinforcement Learning Course\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"Let's do a small recap on what we learned in the first Unit:\\n\\n- Reinforcement Learning is a **comput...\"],[\"To find your result, go to the [leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhuggingface-projects\\u002fDeep...\"],[\"```bash\\nsudo apt-get update\\napt install python-opengl\\napt install ffmpeg\\napt install xvfb\\npip3 insta...\"],[\"Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https:\\u002f\\u002ffarama...\"],[\"**Let's look at an example!** Make sure to read the code\\n\\n\\n```python\\nimport gymnasium as gym\\n\\n# Firs...\"],[\"We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each va...\"],[\"```python\\n# Create the environment\\nenv = make_vec_env(\\\"LunarLander-v2\\\", n_envs=16)\\n```\\n\\n## Create th...\"],[\"3️⃣ You **train the agent** with `model.learn` and define the number of training timesteps\\n\\n```\\n# Cr...\"],[\"💡 When you evaluate your agent, you should not use your training environment but create an evaluatio...\"],[\"2️⃣ Sign in and then, you need to store your authentication token from the Hugging Face website.\\n- C...\"],[\"# TODO: Define the model architecture we used\\nmodel_architecture = \\\"\\\"\\n\\n## TODO: Define the commit me...\"],[\"# Create the evaluation env and set the render_mode=\\\"rgb_array\\\"\\neval_env = DummyVecEnv([lambda: Moni...\"],[\"You go to https:\\u002f\\u002fhuggingface.co\\u002fmodels?library=stable-baselines3 to see the list of all the Stable-...\"],[\"In the [Leaderboard](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fhuggingface-projects\\u002fDeep-Reinforcement-Learning-...\"],[\"Glossary \\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n- **Tabular Method:** ...\"],[\"- **Fixed Q-Target:** In order to calculate the **Q-Target** we need to estimate the discounted opti...\"],[\"Live 1: How the course work, Q&A, and playing with Huggy\\n\\nIn this first live stream, we explained ho...\"],[\"Quiz [[quiz]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera....\"],[\"### Q3: What's the difference between a state and an observation?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext...\"],[\"### Q4: A task is an instance of a Reinforcement Learning problem. What are the two types of tasks?\\n...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n\\n### Q7: What are value-based methods?\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsummary\\u003e\\n\\n- Value-b...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"(Optional) the Policy Gradient Theorem\\n\\nIn this optional section where we're **going to study how we...\"],[\"So this is our likelihood policy gradient:\\n\\n\\\\\\\\( \\\\nabla_\\\\theta J(\\\\theta) = \\\\sum_{\\\\tau} P(\\\\tau;\\\\theta)...\"],[\"\\\\\\\\(\\\\nabla_\\\\theta log P(\\\\tau^{(i)};\\\\theta) =   \\\\nabla_\\\\theta \\\\sum_{t=0}^{H} log \\\\pi_\\\\theta(a_{t}^{(i)...\"],[\"Introduction to Deep Reinforcement Learning [[introduction-to-deep-reinforcement-learning]]\\n\\n\\u003cimg sr...\"],[\"Additional Readings [[additional-readings]]\\n\\nThese are **optional readings** if you want to go deepe...\"],[\"Advantage Actor-Critic (A2C) [[advantage-actor-critic]]\\n\\n## Reducing variance with Actor-Critic meth...\"],[\"- Our Policy takes the state and **outputs an action**  \\\\\\\\( A_t \\\\\\\\).\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface....\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"(Automatic) Curriculum Learning for RL\\n\\nWhile most of the RL methods seen in this course work well i...\"],[\"\\u003e … a family of mechanisms that automatically adapt the distribution of training data by learning to...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this chapter! There was a lot of information. And c...\"],[\"Hands-on [[hands-on]]\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      no...\"],[\"For more information about the certification process, check this section 👉 https:\\u002f\\u002fhuggingface.co\\u002fde...\"],[\"## Objectives of this notebook 🏆\\n\\nAt the end of the notebook, you will:\\n\\n- Be able to use **Gymnasiu...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"## Install dependencies and create a virtual display 🔽\\n\\nIn the notebook, we'll need to generate a re...\"],[\"```python\\nimport numpy as np\\nimport gymnasium as gym\\nimport random\\nimport imageio\\nimport os\\nimport t...\"],[\"```python\\n# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_m...\"],[\"Reward function 💰:\\n- Reach goal: +1\\n- Reach hole: 0\\n- Reach frozen: 0\\n\\n## Create and Initialize the ...\"],[\"- Epsilon-greedy policy (acting policy)\\n- Greedy-policy (updating policy)\\n\\nThe greedy policy will al...\"],[\"return action\\n```\\n\\n#### Solution\\n\\n```python\\ndef epsilon_greedy_policy(Qtable, state, epsilon):\\n    #...\"],[\"The training loop goes like this:\\n\\n```\\nFor episode in the total of training episodes:\\n\\nReduce epsilo...\"],[\"# repeat\\n        for step in range(max_steps):\\n            # Choose the action At using epsilon gree...\"],[\"for step in range(max_steps):\\n            # Take the action (index) that have the maximum expected f...\"],[\"```python\\nfrom huggingface_hub import HfApi, snapshot_download\\nfrom huggingface_hub.repocard import ...\"],[\"eval_env = env\\n    api = HfApi()\\n\\n    # Step 1: Create the repo\\n    repo_url = api.create_repo(\\n    ...\"],[\"metadata = {}\\n    metadata[\\\"tags\\\"] = [env_name, \\\"q-learning\\\", \\\"reinforcement-learning\\\", \\\"custom-impl...\"],[\"### .\\n\\nBy using `push_to_hub` **you evaluate, record a replay, generate a model card of your agent a...\"],[\"Let's fill the `push_to_hub` function:\\n\\n- `repo_id`: the name of the Hugging Face Hub Repository tha...\"],[\"```python\\nstate_space = env.observation_space.n\\nprint(\\\"There are \\\", state_space, \\\" possible states\\\")...\"],[\"# Evaluation parameters\\nn_eval_episodes = 100  # Total number of test episodes\\n\\n# DO NOT MODIFY EVAL...\"],[\"## Create a model dictionary 💾 and publish our trained model to the Hub 🔥\\n\\n- We create a model dicti...\"],[\"#### Do not modify this code\\n\\n```python\\nfrom urllib.error import HTTPError\\n\\nfrom huggingface_hub imp...\"],[\"Here are some ideas to climb up the leaderboard:\\n\\n* Train more steps\\n* Try different hyperparameters...\"],[\"An Introduction to Unreal Learning Agents\\n\\n[Learning Agents](https:\\u002f\\u002fdev.epicgames.com\\u002fcommunity\\u002flea...\"],[\"3. Get the Big Picture of Learning Agents by [reading this informative overview](https:\\u002f\\u002fdev.epicgam...\"],[\"Conclusion\\n\\nThat’s all for today. Congrats on finishing this unit and the tutorial!\\n\\nThe best way to...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this bonus unit!\\n\\nYou can now sit and enjoy playing...\"],[\"Type of tasks [[tasks]]\\n\\nA task is an **instance** of a Reinforcement Learning problem. We can have ...\"],[\"The intuition behind PPO [[the-intuition-behind-ppo]]\\n\\n\\nThe idea with Proximal Policy Optimization (...\"],[\"The Bellman Equation: simplify our value estimation [[bellman-equation]]\\n\\nThe Bellman equation **sim...\"],[\"The Bellman equation is a recursive equation that works like this: instead of starting for each stat...\"],[\"Before going to the next section, think about the role of gamma in the Bellman equation. What happen...\"],[\"Conclusion\\n\\nThat's all for today. Congrats on finishing this Unit and the tutorial! ⭐️\\n\\nNow that you...\"],[\"Brief introduction to RL documentation\\n\\nIn this advanced topic, we address the question: **how shoul...\"],[\"Building on the documentation frameworks for [model cards](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1810.03993) and [da...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this unit and the tutorial. You've just trained you...\"],[\"Monte Carlo vs Temporal Difference Learning [[mc-vs-td]]\\n\\nThe last thing we need to discuss before d...\"],[\"- At the end of the episode, **we have a list of State, Actions, Rewards, and Next States tuples**\\nF...\"],[\"\\\\\\\\(V(S_0) = V(S_0) + lr * [G_0 — V(S_0)]\\\\\\\\)\\n\\n\\\\\\\\(V(S_0) = 0 + 0.1 * [3 – 0]\\\\\\\\)\\n\\n\\\\\\\\(V(S_0) = 0.3\\\\\\\\)\\n\\n\\n...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this chapter! There was a lot of information. And c...\"],[\"Glossary [[glossary]]\\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n### Agent\\n...\"],[\"### Policy\\n\\n- **Policy**: It is called the agent's brain. It tells us what action to take, given the...\"],[\"Offline vs. Online Reinforcement Learning\\n\\nDeep Reinforcement Learning (RL) is a framework **to buil...\"],[\"The process is as follows:\\n- **Create a dataset** using one or more policies and\\u002for human interactio...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n\\n### Q3: What's the difference between policy-based methods and policy-gradient methods?...\"],[\"Hands-on: advanced Deep Reinforcement Learning. Using Sample Factory to play Doom from pixels\\n\\n\\u003cCour...\"],[\"```python\\nfrom IPython.display import HTML\\n\\nHTML(\\n    \\\"\\\"\\\"\\u003cvideo width=\\\"640\\\" height=\\\"480\\\" controls\\u003e\\n ...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"### Key features\\n\\n- Highly optimized algorithm [architecture](https:\\u002f\\u002fwww.samplefactory.dev\\u002f06-archi...\"],[\"All of the above policies are available on the 🤗 hub. Search for the tag [sample-factory](https:\\u002f\\u002fhu...\"],[\"## ViZDoom\\n\\n[ViZDoom](https:\\u002f\\u002fvizdoom.cs.put.edu.pl\\u002f) is an **open-source python interface for the D...\"],[\"```python\\n# Install ViZDoom deps from\\n# https:\\u002f\\u002fgithub.com\\u002fmwydmuch\\u002fViZDoom\\u002fblob\\u002fmaster\\u002fdoc\\u002fBuilding...\"],[\"def register_vizdoom_components():\\n    register_vizdoom_envs()\\n    register_vizdoom_models()\\n\\n\\n# par...\"],[\"You can find out more about the scenarios available in ViZDoom [here](https:\\u002f\\u002fgithub.com\\u002fFarama-Foun...\"],[\"## Now lets upload your checkpoint and video to the Hugging Face Hub\\n\\n\\n\\n\\nTo be able to share your mo...\"],[\"```bash\\nls train_dir\\u002fdoom_health_gathering_supreme_2222\\n```\\n\\n```python\\nenv = \\\"doom_health_gathering_...\"],[\"You **can try to train your agent in this environment** using the code above, but not on colab.\\n**Go...\"],[\"Introducing the Clipped Surrogate Objective Function\\n## Recap: The Policy Objective Function\\n\\nLet’s ...\"],[\"As we can see, \\\\\\\\( r_t(\\\\theta) \\\\\\\\) denotes the probability ratio between the current and old policy:...\"],[\"**By clipping the ratio, we ensure that we do not have a too large policy update because the current...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q4: Explain in your own words what is the `Self-Play` approach\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsum...\"],[\"### Q6: What are the main motivations to use a ELO rating Score?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n   \\t\\t {\\n\\t\\t\\tt...\"],[\"The Problem of Variance in Reinforce [[the-problem-of-variance-in-reinforce]]\\n\\nIn Reinforce, we want...\"],[\"However, increasing the batch size significantly **reduces sample efficiency**. So we need to find a...\"],[\"An introduction to Multi-Agents Reinforcement Learning (MARL)\\n\\n## From single agent to multiple agen...\"],[\"## Different types of multi-agent environments\\n\\nGiven that, in a multi-agent system, agents interact...\"],[\"Student Works\\n\\nSince the launch of the Deep Reinforcement Learning Course, **many students have crea...\"],[\"Introduction [[introduction]]\\n\\nIn this bonus unit, we'll reinforce what we learned in the first unit...\"],[\"Conclusion\\n\\n\\n**Congrats on finishing this unit**! There was a lot of information.\\nAnd congrats on fi...\"],[\"Visualize the Clipped Surrogate Objective Function\\n\\nDon't worry. **It's normal if this seems complex...\"],[\"Since the ratio is between intervals, **we can decrease the probability that our policy takes that a...\"],[\"If the probability ratio is higher than \\\\\\\\( [1 + \\\\epsilon] \\\\\\\\), the probability of taking that actio...\"],[\"The final Clipped Surrogate Objective Loss for PPO Actor-Critic style looks like this, it's a combin...\"],[\"Welcome to the 🤗 Deep Reinforcement Learning Course [[introduction]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface....\"],[\"Let’s get started!\\n\\n## What to expect? [[expect]]\\n\\nIn this course, you will:\\n\\n- 📖 Study Deep Reinfor...\"],[\"Sign up  👉 \\u003ca href=\\\"http:\\u002f\\u002feepurl.com\\u002fic5ZUD\\\"\\u003ehere\\u003c\\u002fa\\u003e\\n\\n\\n## What does the course look like? [[course...\"],[\"You don't need to tell us which path you choose. **If you get more than 80% of the assignments done,...\"],[\"## What is the recommended pace? [[recommended-pace]]\\n\\nEach chapter in this course is designed **to ...\"],[\"## I found a bug, or I want to improve the course [[contribute]]\\n\\nContributions are welcomed 🤗\\n\\n- If...\"],[\"Hands on\\n\\n\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      notebooks={[\\n...\"],[\"We strongly **recommend students use Google Colab for the hands-on exercises** instead of running th...\"],[\"## Objectives of this notebook 🏆\\n\\nAt the end of the notebook, you will:\\n\\n- Be able to **code a Reinf...\"],[\"```python\\n# Virtual display\\nfrom pyvirtualdisplay import Display\\n\\nvirtual_display = Display(visible=...\"],[\"# PyTorch\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as o...\"],[\"The episode ends if:\\n- The pole Angle is greater than ±12°\\n- The Cart Position is greater than ±2.4\\n...\"],[\"def forward(self, x):\\n        # Define the forward pass\\n        # state goes to fc1 then we apply Re...\"],[\"def forward(self, x):\\n        x = F.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return F.softm...\"],[\"We use an interesting technique coded by [Chris1nexus](https:\\u002f\\u002fgithub.com\\u002fChris1nexus) to **compute ...\"],[\"# Line 6 of pseudocode: calculate the return\\n        returns = deque(maxlen=max_t)\\n        n_steps =...\"],[\"## standardization of the returns is employed to make training more stable\\n        eps = np.finfo(np...\"],[\"# Line 6 of pseudocode: calculate the return\\n        returns = deque(maxlen=max_t)\\n        n_steps =...\"],[\"## standardization of the returns is employed to make training more stable\\n        eps = np.finfo(np...\"],[\"## Define evaluation method 📝\\n- Here we define the evaluation method that we're going to use to test...\"],[\"from pathlib import Path\\nimport datetime\\nimport json\\nimport imageio\\n\\nimport tempfile\\n\\nimport os\\n```\\n...\"],[\"with tempfile.TemporaryDirectory() as tmpdirname:\\n    local_directory = Path(tmpdirname)\\n\\n    # Step...\"],[\"# Merges both dictionaries\\n    metadata = {**metadata, **eval}\\n\\n    model_card = f\\\"\\\"\\\"\\n  # **Reinforc...\"],[\"2️⃣ Sign in and then, you need to store your authentication token from the Hugging Face website.\\n- C...\"],[\"The action space(2) 🎮:\\n- Up (press accelerator)\\n- Do nothing (don't press accelerator)\\n\\nThe reward f...\"],[\"###  Train it\\n- We're now ready to train our agent 🔥.\\n\\n```python\\n# Create policy and place it to the...\"],[\"________________________________________________________________________\\n\\n**Congrats on finishing th...\"],[\"Setup [[setup]]\\n\\nAfter all this information, it's time to get started. We're going to do two things:...\"],[\"Second Quiz [[quiz2]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.c...\"],[\"\\u003cdetails\\u003e\\n\\u003csummary\\u003eSolution\\u003c\\u002fsummary\\u003e\\nEpsilon Greedy Strategy is a policy that handles the explorati...\"],[\"Two main approaches for solving RL problems [[two-methods]]\\n\\n\\u003cTip\\u003e\\nNow that we learned the RL framew...\"],[\"We have two types of policies:\\n\\n\\n- *Deterministic*: a policy at a given state **will always return t...\"],[\"“Act according to our policy” just means that our policy is **“going to the state with the highest v...\"],[\"The Exploration\\u002fExploitation trade-off [[exp-exp-tradeoff]]\\n\\nFinally, before looking at the differen...\"],[\"- *Exploitation*: You go to the same one that you know is good every day and **take the risk to miss...\"],[\"Generalization in Reinforcement Learning\\n\\nGeneralization plays a pivotal role in the realm of Reinfo...\"],[\"Godot RL Agents\\n\\n[Godot RL Agents](https:\\u002f\\u002fgithub.com\\u002fedbeeching\\u002fgodot_rl_agents) is an Open Source ...\"],[\"The environment we will be building today is called Ring Pong, the game of pong but the pitch is a r...\"],[\"### Loading the starter project\\n\\nWe provide two versions of the codebase:\\n- [A starter project, to d...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"In order to implement these methods, we will need to create a class that inherits from AIController3...\"],[\"```python\\nextends Node3D\\n\\n@export var rotation_speed = 3.0\\n@onready var ball = get_node(\\\"..\\u002fBall\\\")\\n@...\"],[\"## Author\\n\\nThis section was written by \\u003ca href=\\\"https:\\u002f\\u002ftwitter.com\\u002fedwardbeeching\\\"\\u003eEdward Beeching\\u003c...\"],[\"An Introduction to Unity ML-Agents [[introduction-to-ml-agents]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"],[\"Introduction [[introduction]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002f...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q2: What of the following statements are true about Unity ML-Agents?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n...\"],[\"### Q5: Which are the differences between capturing the environment using `frames` or `raycasts`?\\n\\n\\u003c...\"],[\"Model Based Reinforcement Learning (MBRL)\\n\\nModel-based reinforcement learning only differs from its ...\"],[\"## Further reading\\n\\nFor more information on MBRL, we recommend you check out the following resources...\"],[\"Hands-on [[hands-on]]\\n\\nNow that you've learned to use Optuna, here are some ideas to apply what you'...\"],[\"[The Hugging Face Deep Reinforcement Learning Course 🤗 (v2.0)](https:\\u002f\\u002fhuggingface.co\\u002fdeep-rl-course...\"],[\"How do Unity ML-Agents work? [[how-mlagents-works]]\\n\\nBefore training our agent, we need to understan...\"],[\"## Inside the Learning Component [[inside-learning-component]]\\n\\nInside the Learning Component, we ha...\"],[\"The Academy will be the one that will **send the order to our Agents and ensure that agents are in s...\"],[\"The Pyramid environment\\n\\nThe goal in this environment is to train our agent to **get the gold brick ...\"],[\"## The action space\\n\\nThe action space is **discrete** with four possible actions:\\n\\n\\u003cimg src=\\\"https:\\u002f...\"],[\"How Huggy works [[how-huggy-works]]\\n\\nHuggy is a Deep Reinforcement Learning environment made by Hugg...\"],[\"Remember that one of the foundations of Reinforcement Learning is the *reward hypothesis*: a goal ca...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fn...\"],[\"Interesting Environments to try\\n\\nHere we provide a list of interesting environments you can try to t...\"],[\"## Starcraft II\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002f...\"],[\"Self-Play: a classic technique to train competitive agents in adversarial games\\n\\nNow that we've stud...\"],[\"It’s the same way humans learn in competition:\\n\\n- We start to train against an opponent of similar l...\"],[\"Training against a set of slowly changing or unchanging adversaries with low diversity **results in ...\"],[\"Instead, we’re using an ***ELO rating system*** (named after Arpad Elo) that calculates the **relati...\"],[\"So if A and B have rating Ra, and Rb, then the **expected scores are** given by:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002f...\"],[\"Quiz [[quiz]]\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera....\"],[\"\\u003c\\u002fdetails\\u003e\\n\\n\\n### Q4: What are the two phases of Deep Q-Learning?\\n\\n\\u003cQuestion\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext:...\"],[\"- Use our *Target network* to calculate **the target Q value of taking that action at the next state...\"],[\"Deep Q-Learning [[deep-q-learning]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-c...\"],[\"Apache License\\n                           Version 2.0, January 2004\\n                        http:\\u002f\\u002fw...\"],[\"\\\"Derivative Works\\\" shall mean any work, whether in Source or Object\\n      form, that is based on (or...\"],[\"\\\"Contributor\\\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contr...\"],[\"(a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this Lice...\"],[\"You may add Your own copyright statement to Your modifications and\\n      may provide additional or d...\"],[\"8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including ...\"],[\"END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To ap...\"],[\"Optuna Tutorial [[optuna]]\\n\\nThe content below comes from [Antonin's Raffin ICRA 2022 presentations](...\"],[\"Hands-on [[hands-on]]\\n\\n\\n\\n      \\u003cCourseFloatingBanner classNames=\\\"absolute z-10 right-0 top-0\\\"\\n      ...\"],[\"[![Open In Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fassets\\u002fcolab-badge.svg)](https:\\u002f\\u002fcolab.research....\"],[\"We're constantly trying to improve our tutorials, so **if you find some issues in this hands-on**, p...\"],[\"IF AND ONLY IF THE VERSION ABOVE DOES NOT EXIST ANYMORE. UNCOMMENT AND INSTALL THE ONE BELOW\\n\\n```pyt...\"],[\"Here we see that:\\n- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,grayscale...\"],[\"#### Solution\\n\\n```bash\\npython -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-...\"],[\"3️⃣ We're now ready to push our trained agent to the 🤗 Hub 🔥\\n\\nLet's run push_to_hub.py file to uploa...\"],[\"- The Stable-Baselines3 team uploaded **more than 150 trained Deep Reinforcement Learning agents on ...\"],[\"Here's a list of environments you can try to train your agent with:\\n- BeamRiderNoFrameskip-v4\\n- Brea...\"],[\"Introduction to PPO with Sample-Factory\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-...\"],[\"The Deep Q-Learning Algorithm [[deep-q-algorithm]]\\n\\nWe learned that Deep Q-Learning **uses a deep ne...\"],[\"Let's go through them!\\n\\n## Experience Replay to make more efficient use of experiences [[exp-replay]...\"],[\"In the Deep Q-Learning pseudocode, we **initialize a replay memory buffer D with capacity N** (N is ...\"],[\"At each time step, you’re trying to approach the cow, which also moves at each time step (because yo...\"],[\"We know that the accuracy of Q-values depends on what action we tried **and** what neighboring state...\"],[\"A Q-Learning example [[q-learning-example]]\\n\\nTo better understand Q-Learning, let's take a simple ex...\"],[\"Training timestep 1:\\n\\n## Step 2: Choose an action using the Epsilon Greedy Strategy [[step2]]\\n\\nBecau...\"],[\"## Step 4: Update Q(St, At) [[step4-4]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-...\"],[\"Congratulations\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002f...\"],[\"What is Reinforcement Learning? [[what-is-reinforcement-learning]]\\n\\nTo understand Reinforcement Lear...\"],[\"### A formal definition [[a-formal-definition]]\\n\\nWe can now make a formal definition:\\n\\n\\u003cTip\\u003e\\nReinfor...\"],[\"Introduction [[introduction]]\\n\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-cours...\"],[\"Mid-way Recap [[mid-way-recap]]\\n\\nBefore diving into Q-Learning, let's summarize what we've just lear...\"],[\"Introduction [[introduction]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002f...\"],[\"RLHF\\n\\nReinforcement learning from human feedback (RLHF) is a **methodology for integrating human dat...\"],[\"## Additional readings\\n\\n*Note, this is copied from the Illustrating RLHF blog post above*.\\nHere is a...\"],[\"And here is a snapshot of the growing set of papers that show RLHF's performance for LMs:\\n- [Fine-Tu...\"],[\"- [Scaling Laws for Reward Model Overoptimization](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2210.10760) (Gao et al. 202...\"],[\"## Author\\n\\nThis section was written by \\u003ca href=\\\"https:\\u002f\\u002ftwitter.com\\u002fnatolambert\\\"\\u003e Nathan Lambert \\u003c\\u002fa...\"]],\"hovertemplate\":\"source=deep-rl-class\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"deep-rl-class, circle\",\"marker\":{\"color\":\"#00cc96\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"deep-rl-class, circle\",\"showlegend\":true,\"x\":[1.518768,0.4899634,0.44991583,0.44672543,0.5648448,-12.686781,-12.645068,-12.832039,-12.789995,-12.372341,-12.91437,-12.799515,-12.237931,-12.913642,-12.70499,-3.519839,-3.4718394,-14.917367,-13.60609,-4.8974404,-8.146845,8.263088,16.45444,16.22401,16.905188,12.477221,7.094574,-2.9907398,-1.9528548,-4.201217,-1.0334631,-3.4608483,-9.637943,5.698935,1.2574642,-2.1615167,-1.685265,-1.7523024,-2.1209705,-1.7841445,-2.042682,7.3199863,17.934664,18.007805,2.4863148,-0.3897234,-9.813918,0.9338187,3.6857042,3.2580101,1.0363195,0.90542585,0.80548406,0.27340546,0.58287334,9.301129,9.297059,9.297216,9.294657,9.294591,9.295319,13.096803,3.6932204,-1.24006,-7.1757126,-1.5561476,-1.5152124,0.66973245,6.6151247,5.006625,-5.9804707,-6.7410693,-7.0143733,8.860565,-6.9989934,-5.353457,-0.20943512,-0.28904098,7.8531394,7.793647,7.771415,7.732786,7.856212,7.9360223,-2.5657861,-1.8278334,-0.17184235,-2.9743946,-2.70839,-1.2857364,-1.9692167,-3.361666,-2.9841793,-3.7216628,-4.587344,-4.71746,-3.7172253,-3.636989,-3.606989,-3.7297883,-3.4859712,-6.5310254,-6.910363,-6.549441,-2.6514268,-6.4707956,-3.112183,-3.9556775,-4.001426,-2.7981553,-2.9782882,-2.9811058,-1.5634472,-4.036294,8.792735,0.2005743,0.18681355,0.23061685,0.2236983,-10.201565,17.560516,-6.222779,2.5128508,-2.97307,-1.2285889,-2.7363336,-3.5272374,-2.7953694,-2.8599803,8.593269,4.994782,-6.507972,13.028562,-3.0799239,-2.4405262,4.1846223,2.4262981,-3.0443757,3.513345,6.375238,-6.9678116,-6.816675,-6.415172,-2.3863726,-3.7564902,-3.648349,-7.0527067,-6.7606683,-3.6463344,5.0099945,5.5325236,-9.375416,-1.8681484,-3.047647,-2.06461,-1.7386492,-4.1561685,8.720168,-7.999121,-7.2704053,-21.732962,-1.7154928,3.851029,4.120435,3.8439395,0.6230645,0.6831419,3.3862643,-7.0270295,-5.542485,-2.9095716,-0.7284642,8.203525,-4.5955095,-4.0619783,-0.9760991,-2.278292,-0.80830425,-3.0323114,0.40533057,3.1684148,0.107677095,9.282225,3.812438,-1.5927174,0.18391028,-2.5447187,-3.3236105,-1.3376375,-1.4584203,-1.69328,18.042772,19.275894,8.32255,8.4815035,12.195716,-4.06283,-6.5528774,1.8473489,2.5519419,-3.9071212,6.129496,-4.0880384,4.77893,7.420086,-4.116531,-2.9279757,-3.0034351,-2.4358168,0.71975756,0.7827388,-2.956806,-3.983149,4.8950496,4.568042,-7.1802516,-6.3611627,-3.6006927,-3.525139,-3.6956322,-3.7822797,-3.6119409,-3.4066463,-3.6453826,-3.0432208,-2.873443,-3.436862,8.5931835,6.3375983,6.2775993,3.738852,4.315699,4.903773,-3.346954,-2.8542829,10.275316,2.2816892,0.9891103,-1.2224839,0.021150293,-21.733107,-18.293434,-18.815054,-18.746828,-2.4943,-1.9808401,-2.1700401,-1.2742771,-1.6567941,-1.352592,-10.214421,-9.698822,-6.5283127,-6.8803005,-2.3627574,-14.823798,-21.731665,-18.455462,-18.71539,-5.3911104,-6.3111,-5.941322,-6.49271,-5.7219086,-5.907262,-3.0531442,-3.390728,-4.4112124,-5.8584294,-5.6753483,-5.7270474,-5.546926,-12.582743,-12.882657,10.389708,8.02317,7.923958,8.019743,8.214384,-5.0233426,11.742996,11.883738,11.745326,11.661849,11.799247,11.633829,11.582006,8.640753,-4.5597725,2.0106175,3.3966177,-0.7507791,-1.4285953,-1.466977,0.14243746,-1.0496615,8.631923,-2.468001,8.657704,13.066252,-4.355384,-4.758801,7.6652226,7.9526067,-3.015444,-12.858063,-13.217927,-1.2705965,-1.665294,-1.2831211,-2.0042832,-2.4737713,10.132053,2.691658,1.9257531,-8.237937,8.7255945,8.712205,8.614462,-6.595116,-5.516639,-3.907809,-3.775182,5.751434,-11.424,-11.323336,-11.271749,-11.382111,-11.442267,-11.538398,-11.509488,-11.459131,3.6149604,-5.8827147,1.0705757,1.946714,2.8372505,7.962564,8.092556,3.9051065,4.8135953,-7.838574,-6.3763433,-10.216358,-9.433513,1.4218227,-3.3947523,3.7480001,3.2648628,3.0843263,3.979284,3.94809,3.6357005],\"xaxis\":\"x\",\"y\":[1.6438923,4.5677686,4.9975443,4.9376307,5.0476494,-17.48589,-17.046242,-18.432808,-18.377718,-18.484116,-18.179853,-18.606558,-18.466372,-18.309542,-18.379908,6.43422,6.4284487,-2.0745254,19.449438,-5.637185,0.15534954,4.9420834,3.4275997,3.5289915,1.7872049,3.8563216,2.7116575,-5.9586487,-4.084778,-3.2566357,-3.1496592,-7.862307,0.9867043,-0.059154384,0.41423288,-5.8566437,-5.629389,-5.9264483,-5.512811,-5.72342,-5.8337617,-0.2731692,3.5204134,4.2996097,-2.3283517,7.6319294,0.17715164,2.278952,0.84280556,1.0927078,6.0883512,5.232506,5.9420366,7.6931276,6.4217734,-20.002111,-20.005974,-20.0068,-20.009113,-20.00948,-20.007858,5.025574,1.2891282,0.82831264,-1.8230234,3.3784838,4.14223,-0.115656815,-0.38912964,0.26473156,2.8657563,3.8571234,3.7541308,0.65788126,2.3830886,3.0888748,4.4260893,4.0576806,3.2231953,3.0593488,3.1395578,3.2810562,3.1456115,3.012095,5.567639,5.4378934,5.756816,4.903952,4.2022657,3.6965806,3.280835,2.2668128,1.7628428,1.8148477,2.4414847,2.5076642,6.3062944,6.2413154,6.493367,6.242604,5.759875,0.1288626,2.937227,3.0212355,3.9664783,3.160888,3.6238124,4.544922,3.6777425,1.895198,1.6222503,1.6955101,1.2069572,-0.5523346,3.07906,-6.2220044,-6.223,-6.3128047,-6.288606,-0.9184967,-5.397403,1.897114,0.8005125,-6.8594174,-6.8626695,-6.9066067,-6.7298675,-8.131254,-6.7905955,2.7543705,3.8954315,4.2601275,5.4239235,0.84808683,-11.816139,2.8522353,2.9338675,-1.0023665,3.0928705,3.089469,2.7720153,3.10697,3.1001592,-12.317909,5.4932494,4.9565854,1.9426796,3.0915406,4.5938306,3.950319,3.8158581,-0.6043268,-6.5419784,-2.9868503,-6.607677,-0.5022147,-8.579908,-11.480042,-1.8494273,-1.9874284,8.553864,-6.925,0.8989632,0.41108748,0.57697785,4.5343666,4.6633396,0.22066535,6.400377,4.6577315,5.199909,4.8172507,4.2956038,-2.2344337,-2.1429925,4.5239463,5.0714517,4.686888,5.0280466,1.2563198,1.7069246,2.6981587,2.369518,2.5336335,4.8288946,2.8188596,4.656129,-5.441637,-2.7134898,-3.7117252,-2.6334078,4.1658726,3.2772622,-10.633316,-10.867131,5.117902,-2.4139435,6.8219147,4.4230084,4.6713395,-6.59452,1.592134,-1.9943347,0.85603875,1.6854312,1.962026,2.784068,2.1147768,-6.927032,5.0261645,4.8535,-1.1878573,-1.0895342,1.5222442,1.9520886,-0.60105264,-1.2949706,6.10953,6.2047515,6.046863,6.2713943,6.4394,6.0220914,6.529387,7.0351877,6.974756,6.5776877,-11.07059,0.17691988,0.17134796,0.8993967,1.2535833,1.2828307,0.27252522,1.7839346,3.4810152,-0.22393,0.25871095,-6.6312265,-6.1047945,8.552031,-4.20802,-4.3388824,-4.2517443,-11.745112,0.0154097555,0.18562329,-0.2694382,-0.14276381,0.13202158,-0.5686425,-0.44708586,-0.16698116,0.39917862,-12.553334,-2.0773387,8.5520735,-4.2365656,-4.30171,-1.4355484,-0.2187795,-0.4985396,1.2034942,-0.39813876,0.20997131,-0.21806009,3.9217608,-1.273882,-0.08093401,-0.6497377,0.36196423,-0.5963858,-18.484484,-18.761684,3.4175053,-9.933241,-9.470794,-9.460948,-9.538129,3.7243848,1.9804939,2.2823365,2.204994,2.038874,2.0995457,2.0531251,2.1499379,2.9205832,-6.902088,4.025644,1.3299887,3.198104,1.9636962,1.0623075,0.9497644,2.1332803,-11.528583,-11.705355,-10.526517,5.2585816,3.8072433,3.4884553,0.020447895,0.011487874,3.3086693,-18.049292,-18.216404,0.92379797,1.0305318,0.16631688,1.5058181,1.6880668,3.4427,-0.56410486,0.31826654,2.2143664,-11.828026,-11.794742,-10.505921,2.247023,2.9644017,4.9046364,4.464155,-0.015159179,-1.4383159,-1.3566465,-1.3613108,-1.3808601,-1.3975927,-1.4676251,-1.4400284,-1.3954506,3.4143496,-0.45614442,2.4474578,-0.047223475,-0.4793656,-10.425271,-10.685464,0.8243082,1.5145835,-1.7424552,2.2719796,0.54837394,-0.6678271,4.3571625,-2.0147355,2.4758952,3.5228038,4.3100777,1.5341572,2.5183632,2.7700877],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"__The following example applies the acceleration features powered by ONNX Runtime.__\\n\\n\\n### Onnxrunti...\"],[\"Inference pipelines with the ONNX Runtime accelerator\\n\\nThe [`~pipelines.pipeline`] function makes it...\"],[\"### Using vanilla Transformers model and converting to ONNX\\n\\nThe [`~pipelines.pipeline`] function ac...\"],[\"\\u003e\\u003e\\u003e onnx_qa = pipeline(\\\"question-answering\\\", model=model, tokenizer=tokenizer, accelerator=\\\"ort\\\")\\n\\u003e\\u003e...\"],[\"Below you can find two examples of how you could use the [`~optimum.onnxruntime.ORTOptimizer`] and t...\"],[\"### Optimizing with `ORTOptimizer`\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e from op...\"],[\"Quantization\\n\\n## AutoGPTQ Integration\\n\\n🤗 Optimum collaborated with [AutoGPTQ library](https:\\u002f\\u002fgithub...\"],[\"With 🤗 Transformers integration, you don't need to pass the `block_name_to_quantize` and `model_seql...\"],[\"### Exllama kernels for faster inference\\n\\nWith the release of exllamav2 kernels, you can get faster ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Audio\\n\\nThe following ORT classes are available for the following audio tasks.\\n\\n### ORTModelForAud...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https:\\u002f\\u002fgithub....\"],[\"## Optimum Intel\\n\\n### OpenVINO...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to run inference with OpenVINO](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-intel\\u002fblob\\u002fmain\\u002fnotebo...\"],[\"| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https:...\"],[\"### Neural Compressor...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to quantize a model with Intel Neural Compressor for text classification](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## Optimum ONNX Runtime...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to quantize a model with ONNX Runtime for text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"| [How to fine-tune a model for summarization with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnote...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003e *The inference will use PyTorch by default, if you want to use ONNX Runtime backend instead, add t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Let's see now how we can apply dynamic quantization with ONNX Runtime:\\n\\n```python\\n\\u003e\\u003e\\u003e from optimum.o...\"],[\"```python\\n\\u003e\\u003e\\u003e qconfig = AutoQuantizationConfig.arm64(is_static=True, per_channel=False)\\n```\\n\\nStatic ...\"],[\"As a final example, let's take a look at applying _graph optimizations_ techniques such as operator ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Run inference!\\n  classifier = pipeline(\\\"text-classification\\\", model=model, tokenizer=tokenizer)\\n  ...\"],[\"```python\\n\\u003e\\u003e\\u003e from optimum.onnxruntime.configuration import AutoQuantizationConfig\\n\\u003e\\u003e\\u003e from optimum....\"],[\"```diff\\n- from transformers import Trainer, TrainingArguments\\n+ from optimum.habana import GaudiTrai...\"],[\"# Use ONNX Runtime for training!\\n  trainer.train()\\n```\\n\\nYou can find more examples in the [documenta...\"],[\"## `torch.fx` integration\\n\\nOptimum integrates with `torch.fx`, providing as a one-liner several grap...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"__Note__\\n\\u003e *To enable ONNX Runtime training, your devices need to be equipped with GPU. Install the ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003e\\u003cdiv class=\\\"w-full text-center bg-gradient-to-br from-orange-400 to-orange-500 rounded-lg py-1.5 fo...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"\\u003e *The inference will use PyTorch by default, if you want to use ONNX Runtime backend instead, add t...\"],[\"Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pledge...\"],[\"Community leaders have the right and responsibility to remove, edit, or reject\\ncomments, commits, co...\"],[\"### 2. Warning\\n\\n**Community Impact**: A violation through a single incident or series\\nof actions.\\n\\n*...\"],[\"[homepage]: https:\\u002f\\u002fwww.contributor-covenant.org\\n[v2.0]: https:\\u002f\\u002fwww.contributor-covenant.org\\u002fversio...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e model_id = \\\"roberta-base\\\"\\n\\u003e\\u003e\\u003e model = AutoModel.from_pretrained(model_id, device_map=\\\"auto\\\")\\n```...\"],[\"\\u003e\\u003e\\u003e pipe = pipeline(\\\"fill-mask\\\", \\\"distilbert-base-uncased\\\", accelerator=\\\"bettertransformer\\\", device=...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"After implementing it, your transformation can be used as a regular function:\\n\\n```python\\n\\u003e\\u003e\\u003e from tr...\"],[\"```python\\n\\u003e\\u003e\\u003e from optimum.fx.optimization import compose\\n\\u003e\\u003e\\u003e composition = compose(MulToMulTimesTwo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Register commands in the Optimum CLI from a subpackage\\n\\nIt is possible to register a command in the ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"options:\\n  -h, --help            show this help message and exit\\n  --arm64               Quantizatio...\"],[\"2. Using a local ONNX model from a directory.\\n\\n```python\\n\\u003e\\u003e\\u003e from optimum.onnxruntime import ORTQuan...\"],[\"\\u003e\\u003e\\u003e model_id = \\\"distilbert-base-uncased-finetuned-sst-2-english\\\"\\n\\n# Load PyTorch model and convert t...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nCurrently, only dynamic quantization is supported for Seq2Seq models.\\n\\n\\u003c\\u002fTip\\u003e\\n...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Stable Diffusion Text-to-Image Fine-Tuning\\n\\nThis example shows how to leverage ONNX Runtime Training...\"],[\"Run the following command to authenticate your token\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nIf you have...\"],[\"```bash\\nexport MODEL_NAME=\\\"CompVis\\u002fstable-diffusion-v1-4\\\"\\nexport TRAIN_DIR=\\\"path_to_your_dataset\\\"\\n\\na...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https:\\u002f\\u002fgithub....\"],[\"## Optimum Intel\\n\\n### OpenVINO...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to run inference with OpenVINO](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-intel\\u002fblob\\u002fmain\\u002fnotebo...\"],[\"| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https:...\"],[\"### Neural Compressor...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to quantize a model with Intel Neural Compressor for text classification](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## Optimum ONNX Runtime...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to quantize a model with ONNX Runtime for text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"| [How to fine-tune a model for summarization with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnote...\"],[\"Overview\\n\\n🤗 Optimum provides an integration with Torch FX, a library for PyTorch that allows develop...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"optional arguments:\\n  -h, --help            show this help message and exit\\n\\nRequired arguments:\\n  -...\"],[\"Input shapes:\\n  --batch_size BATCH_SIZE\\n                        Batch size that the TFLite exported ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Accelerated inference on NVIDIA GPUs\\n\\nBy default, ONNX Runtime runs inference on CPU devices. Howeve...\"],[\"\\u003e\\u003e\\u003e ort_model = ORTModelForSequenceClassification.from_pretrained(\\n...   \\\"philschmid\\u002ftiny-bert-sst2-...\"],[\"```python\\n\\u003e\\u003e\\u003e from optimum.pipelines import pipeline\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\n\\u003e\\u003e\\u003e...\"],[\"Due to current limitations in ONNX Runtime, it is not possible to use quantized models with `CUDAExe...\"],[\"To avoid the slowdown, 🤗 Optimum adopts the IOBinding to copy inputs onto GPUs and pre-allocate memo...\"],[\"\\u003ctable\\u003e\\u003ctr\\u003e\\n\\u003ctd\\u003e\\n  \\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg alt=\\\"GPT2\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptim...\"],[\"Environment:\\n\\n```\\n+-----------------------------------------------------------------------------+\\n| ...\"],[\"### TensorRT installation\\n\\nThe easiest way to use TensorRT as the execution provider for models opti...\"],[\"In case this code runs gracefully, congratulations, the installation is successful!\\n\\nIn case the abo...\"],[\"TensorRT builds its engine depending on specified input shapes. Unfortunately, in the [current ONNX ...\"],[\"For example, for text generation, the engine can be built with:\\n\\n```python\\n\\u003e\\u003e\\u003e import os\\n\\u003e\\u003e\\u003e from tr...\"],[\"\\u003e\\u003e\\u003e text = [\\\"Replace me by any text you'd like.\\\"]\\n\\u003e\\u003e\\u003e encoded_input = tokenizer(text, return_tensors...\"],[\"### Use TensorRT execution provider with quantized models\\n\\nWhen it comes to quantized models, Tensor...\"],[\"\\u003e\\u003e\\u003e inp = tokenizer(\\\"TensorRT is a bit painful to use, but at the end of day it runs smoothly and bl...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Since many architectures share similar properties for their ONNX configuration, 🤗 Optimum adopts a 3...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```bash\\npython -m pip install optimum\\n```\\n\\nIf you'd like to use the accelerator-specific features of...\"],[\"The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Supported architectures from [🤗 Diffusers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdiffusers\\u002findex):\\n- Stable Di...\"],[\"Optimum Inference with ONNX Runtime\\n\\nOptimum is a utility package for building and running inference...\"],[\"```python\\n\\u003e\\u003e\\u003e from optimum.onnxruntime import ORTModelForSequenceClassification\\n\\n\\u003e\\u003e\\u003e # Load the mode...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, pipeline\\n\\u003e\\u003e\\u003e from optimum.onnxruntime import O...\"],[\"# Don't forget to save the ONNX model\\nsave_directory = \\\"a_local_path\\\"\\npipeline.save_pretrained(save_...\"],[\"init_image = download_image(img_url).resize((512, 512))\\nmask_image = download_image(mask_url).resize...\"],[\"### Refining the image output\\n\\nThe image can be refined by making use of a model like [stabilityai\\u002fs...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"1. Using an already initialized [`~onnxruntime.ORTModel`] class.\\n\\n```python\\n\\u003e\\u003e\\u003e from optimum.onnxrun...\"],[\"`enable_transformers_specific_optimizations=True` means that `transformers`-specific graph fusion an...\"],[\"Below you will find an easy end-to-end example on how to optimize [distilbert-base-uncased-finetuned...\"],[\"\\u003e\\u003e\\u003e # Optimize the model\\n\\u003e\\u003e\\u003e optimizer.optimize(save_dir=save_dir, optimization_config=optimization_...\"],[\"Optimizing an ONNX model can be done as follows:\\n\\n```bash\\n optimum-cli onnxruntime optimize --onnx_m...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## How to convert a model into its `BetterTransformer` format?\\n\\n### Step 1: Identifying the source l...\"],[\"Let us try to do it step by step for `Bert`, first we need to identify the layers that needs to be r...\"],[\"### Step 2: Building the `xxxLayerBetterTransformer` module\\n\\nCheck that the identified module is not...\"],[\"### Step 3: Building the forward pass\\n\\nFirst of all, start with the line `super().forward_checker()`...\"],[\"Once the `hidden_states` are nested, call `torch._transformer_encoder_layer_fwd` using the right arg...\"],[\"Symbolic tracer\\n\\nIn Torch FX, the symbolic tracer feeds dummy values through the code to record the ...\"],[\"Accelerated inference on AMD GPUs supported by ROCm\\n\\nBy default, ONNX Runtime runs inference on CPU ...\"],[\"##### 2.2 ONNX Runtime with ROCm Execution Provider\\n\\n```bash\\n# pre-requisites\\npip install -U pip\\npip...\"],[\"then something is wrong with the ROCM or ONNX Runtime installation.\\n\\n## Use ROCM Execution Provider ...\"],[\"BetterTransformer benchmark\\n\\nPlease refer to https:\\u002f\\u002fmedium.com\\u002fpytorch\\u002fbettertransformer-out-of-the...\"],[\"Additional benchmarks could be done in the act-order case.\\n\\nFrom the bencharmk, it appears that Exll...\"],[\"### Batch size = 2\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency ...\"],[\"### Batch size = 8\\n\\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency ...\"],[\"Run\\n\\n```shell\\n# pytorch fp16\\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model meta-llama\\u002fLlam...\"],[\"### Batch size = 1\\n\\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok...\"],[\"### Batch size = 4\\n\\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok...\"],[\"### Batch size = 16\\n\\n|quantization |act_order|bits|group_size|kernel    |prompt_length|new_tokens|Lo...\"],[\"# using bitsandbytes fp4\\u002ffp16 scheme\\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model meta-ll...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In the 2.0 version, PyTorch includes a native scaled dot-product attention operator (SDPA) as part o...\"],[\"- [AlBERT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1909.11942)\\n- [Bark](https:\\u002f\\u002fgithub.com\\u002fsuno-ai\\u002fbark)\\n- [BART](http...\"],[\"- [MarkupLM](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2110.08518)\\n- [Marian](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1804.00344)\\n- [MBart...\"],[\"Let us know by opening an issue in 🤗 Optimum if you want more models to be supported, or check out t...\"],[\"More details on `tutorials` section to deeply understand how to use it, or check the [Google colab d...\"],[\"ONNX 🤝 ONNX Runtime\\n\\nONNX is an open standard that defines a common set of operators and a common fi...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003e\\u003e\\u003e model_type = \\\"distilbert\\\"\\n\\u003e\\u003e\\u003e # For instance, for the ONNX export.\\n\\u003e\\u003e\\u003e backend = \\\"onnx\\\"\\n\\u003e\\u003e\\u003e dist...\"],[\"### TensorFlow\\n\\n| Task                                 | Auto Class                             |\\n|-...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# How to contribute to Optimum?\\n\\nOptimum is an open source project, so all contributions and suggest...\"],[\"5. Develop the features on your branch.\\n\\n6. Format your code. Run black and ruff so that your newly ...\"],[\"Overview\\n\\n🤗 Optimum provides an integration with ONNX Runtime, a cross-platform, high performance en...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Performance\\n\\nWe get the following results for [meta-llama\\u002fLlama-2-7b-hf](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"__The following example applies the acceleration features powered by ONNX Runtime.__\\n\\n\\n### ONNX Runt...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"* Computation graph optimizations: constant foldings, node eliminations, node fusions\\n* Efficient me...\"],[\"To use `ORTTrainer` or `ORTSeq2SeqTrainer`, you need to install ONNX Runtime Training module and Opt...\"],[\"```bash\\ndocker build -f Dockerfile-ort-nightly-rocm57 -t ort\\u002ftrain:nightly .\\n```\\n\\n* If you want to i...\"],[\"# Step 2: Create your ONNX Runtime Trainer\\n-trainer = Trainer(\\n+trainer = ORTTrainer(\\n    model=mode...\"],[\"Check out more detailed [example scripts](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002ftree\\u002fmain\\u002fexamples\\u002f...\"],[\"```diff\\n-from transformers import Seq2SeqTrainingArguments\\n+from optimum.onnxruntime import ORTSeq2S...\"],[\"+vae = ORTModule(vae)\\n+text_encoder = ORTModule(text_encoder)\\n+unet = ORTModule(unet)\\n\\n+optimizer = ...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"__The following example applies the acceleration features powered by ONNX Runtime.__\\n\\n\\n### Onnx Runt...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"---\\n\\n# Writing documentation - specification\\n\\nThe 🤗 Optimum documentation follows the [Google\\ndocume...\"],[\"[ \\u003ca href=\\\"..\\u002fnew-file#section-b\\\"\\u003eSection A\\u003c\\u002fa\\u003e\\u003ca id=\\\"section-a\\\"\\u003e\\u003c\\u002fa\\u003e ]\\n```\\n\\nUse the relative style ...\"],[\"The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or\\n\\\\[~\\\\`XXXClass.method\\\\`\\\\]...\"],[\"### Writing a multi-line code block\\n\\nMulti-line code blocks can be useful for displaying examples. T...\"],[\"## Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that no\\nfile...\"],[\"## Writing documentation examples\\n\\nThe syntax for Example docstrings can look as follows:\\n\\n```\\n    E...\"],[\"Including the documentation for a subpackage involves four main steps:\\n\\n1. Adding a `docs\\u002fsource` fo...\"],[\"```\\nSHELL := \\u002fbin\\u002fbash\\nCURRENT_DIR = $(shell pwd)\\n\\n...\\n\\nbuild_doc_docker_image:\\n\\tdocker build -t doc...\"],[\"![ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum\\u002factions\\u002fworkflows\\u002ftest_onnxruntime.yml\\u002fbadge....\"],[\"```bash\\npython -m pip install optimum\\n```\\n\\nIf you'd like to use the accelerator-specific features of...\"],[\"To install from source:\\n\\n```bash\\npython -m pip install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum.gi...\"],[\"The [export](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fexporters\\u002foverview) and optimizations can be done b...\"],[\"```bash\\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbe...\"],[\"Dynamic quantization can be applied on your model:\\n\\n```bash\\noptimum-cli inc quantize --model distilb...\"],[\"#### Run the exported model using ONNX Runtime\\n\\nOnce the model is exported to the ONNX format, we pr...\"],[\"### Habana\\n\\nBefore you begin, make sure you have all the necessary libraries installed :\\n\\n```bash\\npi...\"],[\"# Use ONNX Runtime for training!\\n  trainer.train()\\n```\\n\\nYou can find more examples in the [documenta...\"],[\"Quantization\\n\\nQuantization is a technique to reduce the computational and memory costs of running in...\"],[\"- Does my operation have a `float16` implementation?\\n- Does my hardware suport `float16`? For instan...\"],[\"```\\nx_q = clip(round(x\\u002fS + Z), round(a\\u002fS + Z), round(b\\u002fS + Z))\\n\\n```\\n\\n\\u003cTip\\u003e\\n\\nUsually `round(a\\u002fS + Z)`...\"],[\"### Per-tensor and per-channel quantization\\n\\nDepending on the accuracy \\u002f latency trade-off you are t...\"],[\"1. Post training **dynamic quantization**: the range for each activation is computed on the fly at *...\"],[\"For both post training static quantization and quantization aware training, it is necessary to defin...\"],[\"### Pratical steps to follow to quantize a model to `int8`\\n\\nTo effectively quantize a model to `int8...\"],[\"## Going further: How do machines represent numbers?\\n\\n\\u003cTip\\u003e\\n\\nThe section is not fundamental to under...\"],[\"The floating-point representation can represent bigger ranges of values, and this is the one we will...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nOnce exported, a model can be optimized for inference via techniques such as\\ngraph optimizati...\"],[\"optional arguments:\\n  -h, --help            show this help message and exit\\n\\nRequired arguments:\\n  -...\"],[\"Optional arguments:\\n  --task TASK           The task to export the model for. If not specified, the ...\"],[\"--cache_dir CACHE_DIR\\n                        Path indicating where to store cache.\\n  --trust-remote...\"],[\"```\\n\\nExporting a checkpoint can be done as follows:\\n\\n```bash\\noptimum-cli export onnx --model distilb...\"],[\"```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e from optimum.onnxruntime import ORTModelFor...\"],[\"```bash\\noptimum-cli export onnx --model keras-io\\u002ftransformers-qa distilbert_base_cased_squad_onnx\\u002f\\n`...\"],[\"\\u003e\\u003e\\u003e inputs = tokenizer(\\\"My name is Arthur and I live in\\\", return_tensors=\\\"pt\\\")  # doctest: +SKIP\\n\\n\\u003e\\u003e...\"],[\"To support these use cases, [`~exporters.main_export`] supports two arguments: `model_kwargs` and `c...\"],[\"return common_outputs\\n\\n    @property\\n    def torch_to_onnx_output_map(self):\\n        if self._behavi...\"],[\"Examples of such models are [THUDM\\u002fchatglm2-6b](https:\\u002f\\u002fhuggingface.co\\u002fTHUDM\\u002fchatglm2-6b) and [mosai...\"],[\"DEFAULT_ONNX_OPSET = 14  # aten::tril operator requires opset\\u003e=14\\n    NORMALIZED_CONFIG_CLASS = Norm...\"],[\"main_export(\\n    model_id,\\n    output=\\\"mpt_onnx\\\",\\n    task=\\\"text-generation-with-past\\\",\\n    trust_re...\"],[\"Helpful tips for testing & debugging optimum\\n\\n## VSCODE\\n\\nIf you are using vscode you might have hard...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\nWhen inheriting from a middle-end class, look for the one handling the same modality \\u002f cate...\"],[\"Then comes the model-specific class, `BertOnnxConfig`. Two class attributes are specified here:\\n- `N...\"],[\"You can also view the outputs associated with the model as follows:\\n\\n```python\\n\\u003e\\u003e\\u003e print(onnx_config...\"],[\"For BERT, it looks as follows:\\n\\n```python\\n    \\\"bert\\\": supported_tasks_mapping(\\n        \\\"default\\\",\\n  ...\"],[\"```python\\n\\u003e\\u003e\\u003e import onnx\\n\\n\\u003e\\u003e\\u003e onnx_model = onnx.load(\\\"model.onnx\\\")\\n\\u003e\\u003e\\u003e onnx.checker.check_model(onn...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"We observe the gain of ONNX Runtime compared to PyTorch as follow:\\n\\n|       | Latency | Throughput |...\"]],\"hovertemplate\":\"source=optimum\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"optimum, circle\",\"marker\":{\"color\":\"#ab63fa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"optimum, circle\",\"showlegend\":true,\"x\":[0.9481,-6.1138797,9.108184,-3.6684127,-8.486558,-12.8787985,-12.878405,-12.854081,8.479292,-11.188941,16.192225,16.485748,17.046858,17.221561,15.892349,16.859104,16.52701,16.388258,16.197763,16.049618,16.890047,16.870268,16.870699,16.936302,16.869387,16.674904,16.932417,5.627821,13.944116,16.414316,16.673407,14.144508,14.203725,16.412155,16.793438,7.6400776,7.5433397,-9.614362,8.982346,5.5161743,-7.1053915,2.688788,-2.202351,-0.70968664,-13.225395,-21.731926,-18.038418,-18.547628,-8.607382,0.3734978,-1.9776931,-8.445332,-3.0641334,-2.3161042,-2.4256763,-4.027961,-17.801167,5.145751,5.214754,-4.757605,-4.4650345,-4.003967,-1.5298494,12.833797,9.300566,9.298409,9.295112,9.296604,9.29604,-0.26935637,-5.4010735,-6.2670035,-8.059142,1.9246286,-0.9515422,-1.6189706,3.069037,5.0329885,-14.734764,-13.605835,-17.414284,-17.98068,-3.070526,-1.2539903,-0.26934433,0.24913622,0.18303056,-0.5482999,-0.99868315,1.5486039,-3.676989,4.1824484,12.316913,-7.369427,-7.901262,-7.922524,-7.4690237,-7.1677637,-7.6146293,-8.286146,-8.644501,-7.4254637,-6.90151,-6.628863,-6.4607196,-5.705405,-1.8409938,11.880721,11.91752,-1.5536418,7.9942374,-8.462839,7.4317756,-4.742725,12.970694,-3.7043223,3.5344021,-5.490324,-4.7680817,-3.7395897,-2.337447,-4.5789623,-2.385849,-0.9305596,-3.063448,-2.6289861,-2.6096528,-2.0115871,-6.650021,15.776661,15.7717,15.777016,15.782506,15.782325,15.777333,15.761896,-6.3805685,-6.4814754,-6.7380333,-12.381782,-12.3714,-12.809213,-12.721756,-12.98175,-12.502581,-12.8475275,7.8056498,8.059477,7.3067145,-2.8985844,8.377276,8.338578,3.007996,2.0879712,-1.5720285,-4.1023846,-5.0918994,13.011982,-21.732998,-5.7454686,-5.912842,-5.554356,-3.6723113,-0.5990655,-0.8002688,-3.6303608,-7.2157326,-5.671584,8.896466,9.121131,9.078433,9.108664,9.08184,9.268729,-9.012276,-2.6138523,-3.6977117,-0.6494985,-0.7795422,0.5127847,-1.2716156,-1.2623405,-0.45754915,-2.489087,-7.008479,-6.619408,3.3751776,-6.3510237,-5.9097433,-6.2373424,-6.7458277,8.5624,8.496548,7.738963,4.9658546,1.3094139,-5.255275,-5.239279,-3.7083242,-5.293214,-4.9681625,-5.247359,-5.2794585,-5.2026258,0.31033194,-1.152502,-4.4703255,-3.6489344,-1.9824437,-2.249786,-3.157658,-2.4310863,-2.7183383,-2.4964309,-2.9422116,-3.0590973,-3.8807628,6.402277,7.3532267,-7.6345143,11.197896,0.51481366,-1.028921,8.650729,0.5485428,0.604133,3.198697,2.503421,3.3186228,2.6925745,1.3988295,-14.9079485,-21.732838],\"xaxis\":\"x\",\"y\":[4.5566063,3.6436687,1.739686,4.3767147,0.08632739,-18.766449,-18.78742,-19.04491,-0.3016899,4.5602627,3.4339473,3.300918,2.8287265,3.394856,3.1430933,1.959005,2.9156785,2.8696826,2.913486,3.4689095,2.8417342,2.6042366,1.9580874,1.902681,1.8649607,2.9426877,1.9712309,1.6510845,3.496759,2.912792,2.3982718,4.056253,3.7067394,3.0034635,1.9749453,0.49693188,0.12071761,0.81596994,2.363194,2.0282645,2.9423878,2.052946,4.9090567,2.9152071,-0.9433115,8.553497,-4.0754905,-4.204095,3.6924918,7.66212,0.08264651,2.6163878,-7.1972375,-7.1735144,-7.039775,-6.9947915,-4.0114503,0.16348973,2.2361708,-6.770389,-7.0419745,-7.0617604,5.4871526,3.9196281,-20.004204,-20.00358,-20.006842,-20.006504,-20.00766,4.6392093,1.2147369,3.0573156,2.5557332,4.186639,2.5531085,7.3774266,5.4636936,2.9495676,-1.9806812,19.449924,14.812019,-4.086261,0.3366787,1.4036828,1.1767286,-6.196625,-6.2777944,1.5457679,-0.34171855,0.92771596,1.1765449,0.63895714,3.8237371,3.5913808,3.5031552,3.5128677,3.3049972,3.7618988,3.6078646,3.3974168,3.3021657,3.728288,3.7641594,3.4674082,3.1783607,3.377552,7.286133,2.1131675,2.124961,7.535879,-0.2870151,1.1258206,0.064196594,-7.173456,5.347291,-6.712783,0.21445356,-0.9738334,-1.7565397,-5.0653567,-2.0284886,-6.6122675,-7.1310987,-7.091126,-6.668489,-6.9616284,-7.2355175,-7.0722,2.9258888,-16.039835,-16.042212,-16.040085,-16.035967,-16.034182,-16.035463,-16.048315,7.7785583,7.905262,6.426543,-17.864733,-17.908022,-18.378777,-18.314484,-18.219608,-18.441545,-19.190565,0.06144161,-0.09955053,-9.730124,-8.332589,-10.060699,-10.123425,1.0893703,1.2002271,1.1584672,-0.5540908,-2.0914342,5.2078586,8.552316,-0.3656251,0.5597963,-0.25269023,-1.3072785,1.240669,1.2878827,6.150479,2.9498029,3.8653414,-11.04323,-11.2710085,-11.380655,-11.402174,-11.504506,-11.4845915,2.6900675,-0.8300056,-0.5473794,-0.9468983,-0.06748479,6.6650357,4.300421,-1.2743791,-0.20642564,4.0544453,2.914352,2.9603457,-2.2380226,6.037804,6.6553307,7.1953773,6.260866,-10.93069,-10.907208,3.650908,0.63187087,3.9847434,-1.39944,-1.4572722,-0.3156272,-1.4265002,-1.4359353,-1.5044763,-1.5532054,-1.3931922,7.601829,-4.214848,2.0014741,0.6558959,-1.867647,-1.7210782,-2.1670742,-1.936687,-1.9262704,-1.3984402,-1.7805169,-1.9623636,-1.9318187,2.4160004,1.3302433,-0.081460804,3.460204,4.614353,-1.7418985,-10.647366,0.596841,0.22537403,0.75271255,0.46452138,0.51161194,0.64121866,0.98227984,-2.0764282,8.552625],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Fine-tuning for image classification using LoRA and 🤗 PEFT\\n\\n## Vision Transformer model from transfo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*In this work, we propose a communication-efficient parameterizatio...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```bash\\npip install watchdog\\n```\\n\\nThen run the following command:\\n\\n```bash\\ndoc-builder preview {pack...\"],[\"```\\nSections that were moved:\\n\\n[ \\u003ca href=\\\"..\\u002fnew-file#section-b\\\"\\u003eSection A\\u003c\\u002fa\\u003e\\u003ca id=\\\"section-a\\\"\\u003e\\u003c\\u002fa\\u003e...\"],[\"The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or \\\\[~\\\\`XXXClass.method\\\\`\\\\]...\"],[\"Note that we always omit the \\\"defaults to \\\\`None\\\\`\\\" when None is the default for any argument. Also ...\"],[\"This script may have some weird failures if you make a syntax mistake or if you uncover a bug. There...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"os.environ[\\\"TOKENIZERS_PARALLELISM\\\"] = \\\"false\\\"\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"3\\\"\\n\\ndevice = \\\"c...\"],[\"## Preprocess dataset\\n\\nInitialize a tokenizer, and create a function to pad and truncate the `model_...\"],[\"## Train model\\n\\nNow you can setup your model and make sure it is ready for training. Specify the tas...\"],[\"model.eval()\\n    eval_loss = 0\\n    eval_preds = []\\n    for step, batch in enumerate(tqdm(eval_datalo...\"],[\"```py\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()\\n```\\n\\nUpload the model to a specif...\"],[\"Fine-tuning a multilayer perceptron using LoRA and 🤗 PEFT\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.research....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In this guide, you'll see how to quantize a model to 4-bits and train it with LoRA.\\n\\n## Quantize a m...\"],[\"config = LoraConfig(\\n    r=16,\\n    lora_alpha=8,\\n    target_modules=[\\\"q_proj\\\", \\\"k_proj\\\", \\\"v_proj\\\", \\\"...\"],[\"model = get_peft_model(model, lora_config)\\n```\\n\\n## Next steps\\n\\nIf you're interested in learning more...\"],[\"Using PEFT with timm\\n\\n`peft` allows us to train any model with LoRA as long as the layer type is sup...\"],[\"```python\\nds = load_dataset(\\\"beans\\\")\\n```\\n\\n\\n```python\\nds_train = ds[\\\"train\\\"]\\nds_valid = ds[\\\"validatio...\"],[\"train_loss_total = (train_loss \\u002f len(train_dataloader)).item()\\n        valid_loss_total = (valid_los...\"],[\"```python\\n%time train(peft_model, optimizer, criterion, train_loader, valid_dataloader=valid_loader,...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Below is a basic example usage of how to inject LoRA adapters into the submodule `linear` of the mod...\"],[\"```python\\nfrom peft import get_peft_model_state_dict\\n\\npeft_state_dict = get_peft_model_state_dict(mo...\"],[\"``python\\nimport os\\n\\nimport torch\\nfrom transformers import (\\n    AutoTokenizer,\\n    default_data_coll...\"],[\"dataset[\\\"train\\\"][0]\\n```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(m...\"],[\"training_args = Seq2SeqTrainingArguments(\\n    \\\"out\\\",\\n    per_device_train_batch_size=batch_size,\\n   ...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```py\\nimport os\\n\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"0\\\"\\nmodel_name_or_path = \\\"openai\\u002fwhisper-large...\"],[\"You'll only be training on the `sentence` and `audio` columns, so you can remove the rest of the met...\"],[\"Once you've cleaned up the dataset, you can write a function to generate the correct model inputs. T...\"],[\"labels = labels_batch[\\\"input_ids\\\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\\n\\n        if ...\"],[\"model = prepare_model_for_int8_training(model)\\n```\\n\\nLet's also apply LoRA to the training to make it...\"],[\"```py\\nfrom transformers import Seq2SeqTrainingArguments\\n\\ntraining_args = Seq2SeqTrainingArguments(\\n ...\"],[\"```py\\nfrom transformers import Seq2SeqTrainer, TrainerCallback, Seq2SeqTrainingArguments, TrainerSta...\"],[\"```py\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\nimport numpy as np\\nimport gc\\n\\nev...\"],[\"```py\\nfrom peft import PeftModel, PeftConfig\\n\\npeft_model_id = \\\"smangrul\\u002fopenai-whisper-large-v2-LORA...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()\\n```\\n\\n## Load a dataset\\n\\nTo en...\"],[\"```python\\nfrom transformers import AutoImageProcessor\\n\\ncheckpoint = \\\"nvidia\\u002fmit-b0\\\"\\nimage_processor ...\"],[\"def val_transforms(example_batch):\\n    images = [handle_grayscale_image(x) for x in example_batch[\\\"i...\"],[\"per_category_accuracy = metrics.pop(\\\"per_category_accuracy\\\").tolist()\\n        per_category_iou = met...\"],[\"## Wrap the base model as a PeftModel for LoRA training\\n\\nTo leverage the LoRa method, you need to wr...\"],[\"In addition to specifying the `target_modules` within `LoraConfig`, we also need to specify the `mod...\"],[\"This confirms that only the LoRA parameters appended to the attention blocks and the `decode_head` p...\"],[\"```python\\nmodel_id = \\\"segformer-scene-parse-150-lora\\\"\\nlora_model.save_pretrained(model_id)\\n```\\n\\nWe c...\"],[\"```python\\nimport matplotlib.pyplot as plt\\n\\ncolor_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" ...\"],[\"\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fstevhliu-peft-methods.hf.space\\\"\\n\\tframeborder=\\\"0\\\"\\n\\twidth=\\\"850\\\"\\n\\theight=\\\"620\\\"\\n\\u003e\\u003c...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"[Prompt tuning](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2104.08691) was developed for text classification tasks on T5 m...\"],[\"The main difference is that the prefix parameters are inserted in **all** of the model layers, where...\"],[\"The results suggest that P-tuning is more efficient than manually crafting prompts, and it enables G...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Fine-tuning large pre-trained language models on downstream tasks ...\"],[\"Training PEFT models with new tokens being added to the embedding layers and tokenizer\\n\\nIn this exam...\"],[\"from enum import Enum\\n```\\n\\n## Prepare Model and Tokenizer\\n\\nNow, we will be adding 27 new tokens as w...\"],[\"## Apply LoRA\\n\\n\\n```python\\nconfig = LoraConfig(\\n    r=64, lora_alpha=128, lora_dropout=0.0, target_mo...\"],[\"dataset = load_dataset(\\\"smangrul\\u002fassistant_chatbot_dataset\\\")\\ndataset = dataset[\\\"train\\\"].train_test_s...\"],[\"processed_datasets = dataset.map(\\n    preprocess_function,\\n    batched=True,\\n    num_proc=1,\\n    rem...\"],[\"# Check the model output on a sample from evaluation dataset\\n\\n\\n```python\\nimport random\\n\\ni = random.r...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*While GPTs with traditional fine-tuning fail to achieve strong res...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For example, load a base model and then load the [artificialguybr\\u002f3DRedmond-V1](https:\\u002f\\u002fhuggingface....\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fybelkada\\u002fdocumentati...\"],[\"```py\\nfrom transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\\"y...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Navigate to the directory containing the training scripts for fine-tuning Dreambooth with LoRA:\\n\\n```...\"],[\"Here's what the full set of script arguments may look like:\\n\\n```bash\\naccelerate launch train_dreambo...\"],[\"if base_model_name_or_path is None:\\n        raise ValueError(\\\"Please specify the base model name or ...\"],[\"First, you'll need to perform all the steps as in the single adapter inference example:\\n\\n1. Specify ...\"],[\"Create a pipeline using weighted adapters:\\n\\n```python\\npipe = create_weighted_lora_adapter(pipe, [\\\"cr...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import get_peft_config, get_peft_m...\"],[\"dataset[\\\"train\\\"][0]\\n```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(m...\"],[\"model.eval()\\n    eval_loss = 0\\n    eval_preds = []\\n    for step, batch in enumerate(tqdm(eval_datalo...\"],[\"```python\\nmodel.eval()\\ni = 107\\ninputs = tokenizer(dataset[\\\"validation\\\"][text_column][i], return_tens...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import get_peft_config, get_peft_m...\"],[\"dataset[\\\"train\\\"][0]\\n```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(m...\"],[\"model.eval()\\n    eval_loss = 0\\n    eval_preds = []\\n    for step, batch in enumerate(tqdm(eval_datalo...\"],[\"```python\\nmodel.eval()\\ni = 13\\ninputs = tokenizer(dataset[\\\"validation\\\"][text_column][i], return_tenso...\"],[\"``python\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import get_peft_config, get_peft_mo...\"],[\"def preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    inputs = [f\\\"{text...\"],[\"train_dataset = processed_datasets[\\\"train\\\"]\\neval_dataset = processed_datasets[\\\"train\\\"]\\n\\n\\ntrain_datal...\"],[\"```python\\nmodel.print_trainable_parameters()\\n```\\n\\n\\n```python\\nmodel\\n```\\n\\n\\n```python\\nmodel.peft_config...\"],[\"```python\\nmodel.eval()\\ni = 16\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"test\\\"][i][\\\"Tweet text\\\"]...\"],[\"```python\\nfrom peft import PeftModel, PeftConfig\\n\\npeft_model_id = f\\\"{dataset_name}_{model_name_or_pa...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"LoftQ: LoRA-fine-tuning-aware Quantization\\n\\n## Introduction\\n\\nLoftQ finds quantized LoRA initializati...\"],[\"Below is an example of obtaining 4bit LLAMA-2-7b with 16-rank LoRA adapters by 5 alternating steps.\\n...\"],[\"## LoftQ Fine-tuning\\n\\nWe also provide an example to fine-tune LoftQ on GSM8K. \\nWe load the quantized...\"],[\"Finetuning Whisper-large-V2 on Colab using PEFT-Lora + BNB INT8 training\\n\\nIn this Colab, we present ...\"],[\"## Load Dataset\\n\\n\\n```python\\nfrom datasets import load_dataset, DatasetDict\\n\\ncommon_voice = DatasetDi...\"],[\"Re-loading the first audio sample in the Common Voice dataset will resample \\nit to the desired sampl...\"],[\"```python\\nimport torch\\n\\nfrom dataclasses import dataclass\\nfrom typing import Any, Dict, List, Union\\n...\"],[\"```python\\nimport evaluate\\n\\nmetric = evaluate.load(\\\"wer\\\")\\n```\\n\\nWe then simply have to define a functi...\"],[\"```python\\nmodel.config.forced_decoder_ids = None\\nmodel.config.suppress_tokens = []\\n```\\n\\n### Post-pro...\"],[\"**Few Important Notes:**\\n1. `remove_unused_columns=False` and `label_names=[\\\"labels\\\"]` are required ...\"],[\"# Evaluation and Inference\\n\\n**Important points to note while inferencing**:\\n1. As `predict_with_gene...\"],[\"| Model          | DrishtiSharma\\u002fwhisper-large-v2-marathi | smangrul\\u002fopenai-whisper-large-v2-LORA-co...\"],[\"```python\\nfrom torch.utils.data import DataLoader\\nfrom tqdm import tqdm\\nimport numpy as np\\nimport gc...\"],[\"```\\n\\n\\n```python\\nimport torch\\nimport gradio as gr\\nfrom transformers import (\\n    AutomaticSpeechRecog...\"],[\"``python\\nimport argparse\\nimport gc\\nimport hashlib\\nimport itertools\\nimport logging\\nimport math\\nimport...\"],[\"logger = get_logger(__name__)\\n\\n\\nMODEL_NAME = \\\"CompVis\\u002fstable-diffusion-v1-4\\\"  # \\\"stabilityai\\u002fstable-...\"],[\"def set_adapter(pipe, adapter_name):\\n    pipe.unet.set_adapter(adapter_name)\\n    if isinstance(pipe....\"],[\"```python\\n%%time\\nset_adapter(pipe, adapter_name=\\\"toy\\\")\\n```\\n\\n\\n```python\\nprompt = \\\"narendra modi rende...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## Configuration\\n\\nStart by running the following command to [create a DeepSpeed configuratio...\"],[\"An example [configuration file](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fpeft\\u002fblob\\u002fmain\\u002fexamples\\u002fconditional_g...\"],[\"```diff\\n def main():\\n+    accelerator = Accelerator()\\n     model_name_or_path = \\\"facebook\\u002fbart-large...\"],[\"Inside the training loop, the usual `loss.backward()` is replaced by 🤗 Accelerate's [`~accelerate.Ac...\"],[\"You'll see some output logs that track memory usage during training, and once it's completed, the sc...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Prompt tuning, in which a base pretrained model is adapted to each...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```sh\\nmake test\\n```\\n\\nRun one of the following to either check or check and fix code quality and styl...\"],[\"## Providing a bugfix\\n\\nPlease give a description of the circumstances that lead to the bug. If there...\"],[\"Once you have something that seems to be working, don’t hesitate to create a draft PR, even if it’s ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*In this work, we explore \\\"prompt tuning\\\", a simple yet effective m...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nimport peft\\nfrom peft import get_peft_config...\"],[\"dataset[\\\"train\\\"][0]\\n```\\n\\n\\n```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(m...\"],[\"model.eval()\\n    eval_loss = 0\\n    eval_preds = []\\n    for step, batch in enumerate(tqdm(eval_datalo...\"],[\"```python\\nmodel.eval()\\ni = 13\\ninputs = tokenizer(dataset[\\\"validation\\\"][text_column][i], return_tenso...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*We present LLaMA-Adapter, a lightweight adaption method to efficie...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Tips\\n\\n- Not all adapter types can be combined. See `peft.tuners.mixed.COMPATIBLE_TUNER_TYPES` for...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"The configuration file is used to set the default options when you launch the training script.\\n\\n```b...\"],[\"For example, your FSDP configuration file may look like the following:\\n\\n```yaml\\ncommand_file: null\\nc...\"],[\"Next, the script wraps the base model and `peft_config` with the [`get_peft_model`] function to crea...\"],[\"From here, the remainder of the script handles the training loop, evaluation, and sharing your model...\"],[\"``python\\nfrom datasets import load_dataset\\nfrom transformers import set_seed, AutoModelForSeq2SeqLM,...\"],[\"result_examples[-1][\\\"input\\\"] = example[\\\"premise\\\"].strip() + \\\" \\\" + example[\\\"hypothesis\\\"].strip() + \\\"\\u003c...\"],[\"task_ids = [i[\\\"task_id\\\"] for i in batch]\\n    task_ids = torch.tensor(task_ids)\\n\\n    return {\\n       ...\"],[\"optimizer = AdamW(model.parameters(), lr=1e-4)\\nscheduler = get_cosine_schedule_with_warmup(optimizer...\"],[\"n = 1000\\nstep = 0\\ntrain_ = tqdm(train)\\n\\nval_loss, f1 = evaluate(model, val)\\nprint(\\n    f\\\"\\\"\\\"\\nbefore t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```py\\nfrom peft import LoraConfig\\n\\nconfig = LoraConfig(init_lora_weights=\\\"gaussian\\\", ...)\\n```\\n\\nThere...\"],[\"```py\\nfrom peft import LoraConfig\\n\\nconfig = LoraConfig(use_rslora=True, ...)\\n```\\n\\n## Merge adapters\\n...\"],[\"```py\\nmodel.add_weighted_adapter(\\n    adapters=[\\\"adapter_1\\\", \\\"adapter_2\\\"],\\n    weights=[0.7, 0.3],\\n ...\"],[\"``python\\n!pip install -q git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers.git\\n!pip install -q git+htt...\"],[\"def evaluate(\\n    instruction,\\n    input=None,\\n    temperature=0.1,\\n    top_p=0.75,\\n    top_k=40,\\n  ...\"],[\"``python\\nfrom transformers import AutoModelForSeq2SeqLM\\nfrom peft import PeftModel, PeftConfig\\nimpor...\"],[\"```python\\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\\ntarget_max_lengt...\"],[\"```python\\nmodel.eval()\\neval_preds = []\\nfor _, batch in enumerate(tqdm(eval_dataloader)):\\n    batch =...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PEFT configurations\\n\\n\\u003cTip\\u003e\\n\\nLearn more about the parameters you can configure for each PEFT metho...\"],[\"\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"PromptEncoderConfig\\\"\\u003e\\n\\n```json\\n{\\n  \\\"base_model_name_or_path\\\": \\\"roberta-lar...\"],[\"lora_model = get_peft_model(model, lora_config)\\nlora_model.print_trainable_parameters()\\n\\\"trainable p...\"],[\"Take a look at the [AutoPeftModel](package_reference\\u002fauto_class) API reference to learn more about t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Fine-tune FLAN-T5 using `bitsandbytes`, `peft` & `transformers` 🤗 \\n\\nIn this notebook we will see how...\"],[\"model = prepare_model_for_int8_training(model)\\n```\\n\\n## Load your `PeftModel` \\n\\nHere we will use LoRA...\"],[\"classes = dataset[\\\"train\\\"].features[\\\"label\\\"].names\\ndataset = dataset.map(\\n    lambda x: {\\\"text_label...\"],[\"```python\\nfrom transformers import TrainingArguments, Trainer\\n\\ntraining_args = TrainingArguments(\\n  ...\"],[\"```python\\nimport torch\\nfrom peft import PeftModel, PeftConfig\\nfrom transformers import AutoModelForS...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"def collate_fn(examples):\\n    return tokenizer.pad(examples, padding=\\\"longest\\\", return_tensors=\\\"pt\\\")...\"],[\"## Load adapters from the Hub\\n\\nYou can also directly load adapters from the Hub using the commands b...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```bash\\ncd peft\\u002fexamples\\u002ffeature_extraction\\n```\\n\\nInstall all the necessary required libraries with:\\n...\"],[\"## Dataset for semantic similarity\\n\\nThe dataset we'll be using is a small subset of the [esci-data](...\"],[\"For this task guide, we will explore the first stage of training an embedding model to predict seman...\"],[\"def get_cosine_embeddings(query_embs, product_embs):\\n    return torch.sum(query_embs * product_embs,...\"],[\"1. Get a list of ids to products which we can call `ids_to_products_dict`:\\n\\n```bash\\n{0: 'RamPro 10\\\" ...\"],[\"device = \\\"cuda\\\"\\nmodel.to(device)\\nmodel.eval()\\nmodel = model.merge_and_unload()\\n\\nimport numpy as np\\nn...\"],[\"# Query dataset, k - number of the closest elements (returns 2 numpy arrays)\\n    labels, distances =...\"],[\"print(f\\\"{query=}\\\") \\nfor product, cosine_sim_score in search_results:\\n    print(f\\\"cosine_sim_score={r...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"def collate_fn(examples):\\n    return tokenizer.pad(examples, padding=\\\"longest\\\", return_tensors=\\\"pt\\\")...\"],[\"## Load adapters from the Hub\\n\\nYou can also directly load adapters from the Hub using the commands b...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"model_name_or_path = \\\"roberta-large\\\"\\ntask = \\\"mrpc\\\"\\nnum_epochs = 20\\nlr = 1e-3\\nbatch_size = 32\\n```\\n\\n##...\"],[\"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)\\nif getattr(...\"],[\"```py\\npeft_config = PromptEncoderConfig(task_type=\\\"SEQ_CLS\\\", num_virtual_tokens=20, encoder_hidden_s...\"],[\"trainer.train()\\n```\\n\\n## Share model\\n\\nYou can store and share your model on the Hub if you'd like. Lo...\"],[\"Dreambooth with OFT\\nThis Notebook assumes that you already ran the train_dreambooth.py script to cre...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Large text-to-image diffusion models have impressive capabilities ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Before you begin, make sure you have all the necessary libraries installed:\\n\\n```bash\\n!pip install -q...\"],[\"```py\\ndataset = load_dataset(\\\"ought\\u002fraft\\\", dataset_name)\\ndataset[\\\"train\\\"][0]\\n{\\\"Tweet text\\\": \\\"@HMRCcu...\"],[\"```py\\ndef preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    inputs = [f...\"],[\"Use the [`~datasets.Dataset.map`] function to apply the `preprocess_function` to the entire dataset....\"],[\"Move the model to the GPU, then write a training loop to start training!\\n\\n```py\\nmodel = model.to(dev...\"],[\"Once the model is uploaded, you'll see the model file size is only 33.5kB! 🤏\\n\\n## Inference\\n\\nLet's tr...\"],[\"``python\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import get_peft_config, get_peft_mo...\"],[\"def preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    inputs = [f\\\"{text...\"],[\"train_dataset = processed_datasets[\\\"train\\\"]\\neval_dataset = processed_datasets[\\\"train\\\"]\\n\\n\\ntrain_datal...\"],[\"```python\\n# creating model\\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path)\\nmodel = ...\"],[\"```python\\nmodel.eval()\\ni = 33\\ninputs = tokenizer(f'{text_column} : {dataset[\\\"test\\\"][i][\\\"Tweet text\\\"]...\"],[\"```python\\nfrom peft import PeftModel, PeftConfig\\n\\npeft_model_id = f\\\"{dataset_name}_{model_name_or_pa...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PrefixTuningConfig\\n\\n[[autodoc]] tuners.prefix_tuning.config.PrefixTuningConfig\\n\\n## PrefixEncoder\\n...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Check the versions of all required libraries to make sure you are up to date:\\n\\n```python\\nimport tran...\"],[\"id2label[2]\\n\\\"baklava\\\"\\n```\\n\\nNext, load the image processor of the model you're fine-tuning:\\n\\n```pytho...\"],[\"Finally, set the transformation functions for the datasets accordingly:\\n\\n```python\\ntrain_ds.set_tran...\"],[\"```python\\nprint_trainable_parameters(model)\\n\\\"trainable params: 85876325 || all params: 85876325 || t...\"],[\"`r` and `alpha` together control the total number of final trainable parameters when using LoRA, giv...\"],[\"The `compute_metrics` function takes a named tuple as input: `predictions`, which are the logits of ...\"],[\"Once the fine-tuning is done, share the LoRA parameters with the community like so:\\n\\n```python\\nrepo_...\"],[\"First, instantiate an `image_processor` from the underlying model repo.\\n\\n```python\\nimage_processor =...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"When opening an issue, it helps a lot if you provide a minimal code example that reproduces the issu...\"],[\"In PEFT, we try to correctly guess the `modules_to_save` if you provide the `task_type` argument in ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nAs mentioned briefly earlier, [LoRA](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2106.09685) is a technique that ac...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"## Orthogonal Finetuning (OFT)\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface....\"],[\"## Adaptive Low-Rank Adaptation (AdaLoRA)\\n\\n[AdaLoRA](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2303.10512) manages the pa...\"],[\"To avoid adding noise to the tokens, the adapter uses zero-initialized attention. On top of this, th...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"def collate_fn(examples):\\n    return tokenizer.pad(examples, padding=\\\"longest\\\", return_tensors=\\\"pt\\\")...\"],[\"## Load adapters from the Hub\\n\\nYou can also directly load adapters from the Hub using the commands b...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"def collate_fn(examples):\\n    return tokenizer.pad(examples, padding=\\\"longest\\\", return_tensors=\\\"pt\\\")...\"],[\"## Share adapters on the 🤗 Hub\\n\\n\\n```python\\nmodel.push_to_hub(\\\"SumanthRH\\u002froberta-large-peft-ia3\\\", use...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Few-shot in-context learning (ICL) enables pre-trained language mo...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"model_checkpoint = \\\"roberta-large\\\"\\nlr = 1e-3\\nbatch_size = 16\\nnum_epochs = 10\\n```\\n\\n## Load dataset an...\"],[\"```py\\nlabel_list = [\\n    \\\"O\\\",\\n    \\\"B-DNA\\\",\\n    \\\"I-DNA\\\",\\n    \\\"B-protein\\\",\\n    \\\"I-protein\\\",\\n    \\\"B-cel...\"],[\"```py\\ndef tokenize_and_align_labels(examples):\\n    tokenized_inputs = tokenizer(examples[\\\"tokens\\\"], ...\"],[\"model = AutoModelForTokenClassification.from_pretrained(\\n    model_checkpoint, num_labels=11, id2lab...\"],[\"Pass the model, `TrainingArguments`, datasets, tokenizer, data collator and evaluation function to t...\"],[\"```py\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\n\\ntokens = inputs.tokens()\\nprediction...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Being similar to LoRA, IA3 carries many of the same advantages: \\n\\n* IA3 makes fine-tuning more effic...\"],[\"`IA3Config` allows you to control how IA3 is applied to the base model through the following paramet...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## LoraConfig\\n\\n[[autodoc]] tuners.lora.config.LoraConfig\\n\\n## LoraModel\\n\\n[[autodoc]] tuners.lora.mode...\"],[\"``python\\nimport os\\n\\nimport torch\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, defa...\"],[\"```python\\n# data preprocessing\\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\\ntarget_...\"],[\"model.eval()\\n    eval_loss = 0\\n    eval_preds = []\\n    for step, batch in enumerate(tqdm(eval_datalo...\"],[\"```python\\nmodel.eval()\\ni = 107\\ninput_ids = tokenizer(dataset[\\\"validation\\\"][text_column][i], return_t...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"``python\\nimport argparse\\nimport json\\nimport logging\\nimport math\\nimport os\\nimport random\\nfrom pathlib...\"],[\"def __getattr__(self, name: str):\\n        \\\"\\\"\\\"Forward missing attributes to the wrapped module.\\\"\\\"\\\"\\n  ...\"],[\"```python\\n# base model\\nmodel = AutoModelForSentenceEmbedding(model_name_or_path, tokenizer)\\n\\n# peft ...\"],[\"return search_index\\n```\\n\\n\\n```python\\nproduct_search_index = construct_search_index(d, num_products, p...\"],[\"``python\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import PeftModel, PeftConfig\\nimport...\"],[\"def preprocess_function(examples):\\n    batch_size = len(examples[text_column])\\n    inputs = [f\\\"{text...\"],[\"train_dataset = processed_datasets[\\\"train\\\"]\\n\\n\\ntrain_dataloader = DataLoader(\\n    train_dataset, shuf...\"],[\"```python\\nfrom peft import PeftModel, PeftConfig\\n\\nmax_memory = {0: \\\"1GIB\\\", 1: \\\"1GIB\\\", 2: \\\"2GIB\\\", 3: ...\"],[\"```python\\nmodel.eval()\\ntest_preds = []\\n\\nfor _, batch in enumerate(tqdm(test_dataloader)):\\n    batch ...\"],[\"Fine-tuning for semantic segmentation using LoRA and 🤗 PEFT\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.researc...\"],[\"Using PEFT with custom models\\n\\n`peft` allows us to fine-tune models efficiently with LoRA. In this s...\"],[\"def forward(self, X):\\n        return self.seq(X)\\n```\\n\\n## Training\\n\\nHere are just a few training hype...\"],[\"Now let's train with `peft`. First we check the names of the modules, so that we can configure `peft...\"],[\"```python\\nfor name, param in peft_model.base_model.named_parameters():\\n    if \\\"lora\\\" not in name:\\n  ...\"],[\"Let's check that the two models produce the same output:\\n\\n\\n```python\\ny_peft = peft_model(X.to(device...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"def forward(self, X):\\n        return self.seq(X)\\n```\\n\\nThis is a straightforward multilayer perceptro...\"],[\"Finally, we can use any training framework we like, or write our own fit loop, to train the `peft_mo...\"],[\"```\\n[('', timm.models.metaformer.MetaFormer),\\n ('stem', timm.models.metaformer.Stem),\\n ('stem.conv',...\"],[\"('stages.0.blocks.1.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),\\n ...\\n ('head.global_pool...\"],[\"Upon closer inspection, we see that the 2D conv layers have names such as `\\\"stages.0.blocks.0.mlp.fc...\"],[\"As a first step, it is a good idea is to check the existing models for inspiration. You can find the...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```python\\nfrom peft import LoraConfig, TaskType\\n\\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_S...\"],[\"Pass the model, training arguments, dataset, tokenizer, and any other necessary component to the [`~...\"],[\"## Inference\\n\\n\\u003cTip\\u003e\\n\\nTake a look at the [AutoPeftModel](package_reference\\u002fauto_class) API reference ...\"],[\"Then you can train it however you like! To load a PEFT model for inference, you can use the [`AutoPe...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"Seamlessly integrated with 🤗 Accelerate for large scale models leveraging DeepSpeed and Big Model In...\"],[\"peft_config = LoraConfig(\\n    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha...\"],[\"Performance of PEFT-LoRA tuned [`bigscience\\u002fT0_3B`](https:\\u002f\\u002fhuggingface.co\\u002fbigscience\\u002fT0_3B) on [`ou...\"],[\"**Training**\\nAn example of using LoRA for parameter efficient dreambooth training is given in [`exam...\"],[\"**NEW** ✨ Dreambooth training for Stable Diffusion using LoHa and LoKr adapters [`examples\\u002fstable_di...\"],[\"An example of using LoRA for the task of adapting `LayoutLMForTokenClassification` on `FUNSD` datase...\"],[\"### Example of PEFT model training using 🤗 Accelerate's DeepSpeed integration\\n\\nDeepSpeed version req...\"],[\"c. output logs:\\n  ```bash\\n  GPU Memory before entering the train : 1916\\n  GPU Memory consumed at the...\"],[\"## Models support matrix\\n\\nFind models that are supported out of the box below. Note that PEFT works ...\"],[\"### Token Classification\\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | IA3...\"],[\"The same principle applies to our [segmentation models](https:\\u002f\\u002fhuggingface.co\\u002fmodels?pipeline_tag=i...\"],[\"...\\n\\n  if os.environ.get(\\\"ACCELERATE_USE_FSDP\\\", None) is not None:\\n      accelerator.state.fsdp_plug...\"],[\"## 🤗 PEFT as a utility library\\n\\n### Injecting adapters directly into the model\\n\\nInject trainable ada...\"],[\"```python\\nfrom peft import get_peft_model, LoraConfig, LoKrConfig\\n\\nbase_model = ...\\nconfig0 = LoraCo...\"],[\"``python\\nimport argparse\\nimport os\\n\\nimport torch\\nfrom torch.optim import AdamW\\nfrom torch.utils.data...\"],[\"def collate_fn(examples):\\n    return tokenizer.pad(examples, padding=\\\"longest\\\", return_tensors=\\\"pt\\\")...\"],[\"```python\\nmodel.push_to_hub(\\\"smangrul\\u002froberta-large-peft-lora\\\", use_auth_token=True)\\n```\\n\\n## Load ad...\"]],\"hovertemplate\":\"source=peft\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"peft, circle\",\"marker\":{\"color\":\"#FFA15A\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"peft, circle\",\"showlegend\":true,\"x\":[-0.5041594,-6.698407,8.592847,-1.0704656,8.165504,-1.1898171,-0.8525605,8.373011,-1.2906328,-1.7470055,-1.4687008,13.548416,10.7175865,8.84904,16.562302,16.246862,16.876463,3.6027977,3.28842,-6.66191,-5.7333655,-4.815733,-3.6404126,-2.1032605,-2.3926218,-3.5024514,-18.850424,-18.476477,11.423248,-10.257405,-9.092562,-2.7593532,-5.0211053,3.680709,-8.990861,-7.483854,-6.1645226,3.2367148,3.5723746,3.292088,4.0815105,2.913663,-2.2130105,-1.5830545,1.1696488,1.8805673,1.5719998,2.0778768,2.0416353,-0.23731826,13.638476,13.088909,-3.5678542,-3.1643965,-3.4838765,-2.9379878,4.6677337,2.0468316,0.07363964,-0.45466122,-7.299479,-3.4711287,9.298989,9.307684,9.297329,9.294307,9.297868,9.294633,-0.06611704,-0.56108254,-6.141466,-5.748599,2.602375,3.633283,-0.48456448,-5.363462,5.118901,3.6016362,-3.7498276,13.409811,-2.1417263,-5.7239876,-6.928523,-6.768513,2.361125,2.5519366,3.0103228,1.5259454,-2.3190384,-2.8608446,-0.97379774,11.200678,11.696952,9.350242,2.7042878,-0.031059446,0.049319427,0.3183269,7.585248,7.5914927,7.5680113,2.6695838,1.4333868,3.1994562,3.108839,3.057964,1.6675375,8.055717,-4.897763,-5.904783,-6.2731614,11.445452,10.818029,-7.2772064,-7.26613,-7.150106,-7.2734847,-7.2742224,-7.296221,3.2946966,2.608715,-1.6552362,15.58514,15.780652,15.777944,15.776037,0.5277799,0.6188838,-18.642391,-18.739304,-18.596392,-18.737482,-4.5966916,-0.16322488,3.54879,3.182063,-21.731491,-17.885386,-6.759325,8.5483885,9.063323,-2.3044546,-2.2124228,-2.3104677,-1.875293,-2.5218961,15.849929,-12.896725,-13.701536,-13.642389,-5.22505,-3.7539222,-2.3972201,13.638121,3.5365226,0.8138035,10.415044,10.315952,9.491451,8.03176,-2.7487009,1.9348019,8.73991,-4.5884337,-4.887422,-1.8625419,-1.7778013,-21.730263,-18.19102,-18.47988,-1.371523,-2.9164927,2.7929838,2.6523163,2.662125,2.6048055,11.591394,14.578737,13.527035,-8.238405,14.485195,2.6208172,-3.76341,-6.8851457,-6.665394,-2.424507,13.085745,2.506912,-3.767057,-3.7265403,2.5983634,-13.352073,-21.732306,-18.346493,-18.671967,-18.750685,-18.86836,-10.746881,-10.724851,-10.724682,-10.750562,-10.835118,-10.747946,11.916295,12.028943,-3.8236964,3.4766107,2.9439785,7.6837006,7.9332814,8.276212,7.9276285,8.136185,7.8838882,-12.963337,-12.633369,-13.270102,-10.357598,-10.526884,-10.412789,8.683642,8.542865,1.6878374,-3.8987644,-9.476132,-3.900142,9.348758,-3.4319444,-3.0570507,-1.1930798,-9.578422,-7.20087,-6.371476,8.0128,-10.2842045,8.415055,-10.053359,-4.293892,-9.906942,-12.345158,-12.784334,-12.832947,2.9596314,2.773817,3.3565974,2.8466413,2.4574583,-0.9388108,-1.5236627,1.8472356,-2.8708692,-1.8547839,-2.996653,-1.9209467,-1.5042963,0.36157814,-1.3252764,-1.0758461,-7.32561,8.352241,-0.289192,-1.3986105,-0.7407669,-0.4841044,-3.086768,-3.9967763,-3.8074384,-7.194127,-2.3614347,8.343568,8.414539,11.525748,11.959866,11.846039,-7.4636045,8.098678,2.6330247,8.18716,3.8469453,4.3769712,4.5116324,-8.193172,-8.282055,-8.369001,-8.283086,-8.082162,-8.295308,-8.330953,-8.117743,-8.308615,-8.0702305,-2.5028384,-2.0884976,19.260048],\"xaxis\":\"x\",\"y\":[-6.077502,3.9935215,1.5900317,4.7154202,-9.716547,1.183371,3.324278,3.9317985,0.9920664,1.1128683,0.56418306,3.8940198,3.7091553,2.2233856,2.9106364,3.2246115,2.1204276,1.524331,1.188828,0.9776014,-0.85173446,-1.3983455,-2.1926358,-1.7469915,-0.84313464,-2.143417,-4.35155,-3.7213943,2.1504266,-0.15786162,-0.6635026,-6.777998,1.5171918,-1.3398479,-0.47240236,-1.4465928,1.0804955,1.0360888,0.77124345,1.6466618,0.2134022,0.61696476,-1.7053977,-1.8890262,-0.0062736887,0.19957316,0.74007833,1.4924744,0.51107126,-0.6526644,3.9315403,3.379033,-2.4396038,-2.3000574,-2.318166,-2.4633117,1.9882007,1.4973161,2.1935544,3.0299478,-0.9375148,-0.18114442,-20.007584,-20.004675,-20.0084,-20.005795,-20.00437,-20.006865,1.0454623,-6.1252165,2.4627302,2.9565794,0.42213765,-1.0622033,4.997459,2.935794,1.1340147,1.3976719,6.806975,5.3454623,0.12334998,-0.6547642,1.7094941,7.2861943,3.7487833,3.304437,2.706405,4.4140925,0.93461853,2.3186386,1.7336715,2.2277994,2.2889576,0.3697929,2.5095696,7.546826,7.867611,8.258676,-0.042884998,-0.03439997,-0.09164493,1.1156136,-0.024051463,4.1195664,4.2836285,5.0485086,4.1209426,-9.561336,-6.6865597,2.943293,1.2994157,2.8028152,2.3661978,6.862127,6.6826134,6.6518345,6.6379156,6.389859,6.6182523,2.8261654,2.3440676,-1.3993833,-16.199144,-16.038122,-16.034595,-16.040556,4.778077,4.6732397,-4.1577663,-4.2436657,-4.101407,-4.2275333,1.8314999,7.963434,3.3051968,3.01672,8.552441,-4.052729,3.0701945,2.1785529,0.3501797,-4.5302305,-4.7963715,-4.436501,-6.832207,-4.615829,3.3174648,-1.6387097,-1.7846869,-1.7970232,6.7593617,6.077515,-12.252829,4.093981,2.560865,3.584639,3.884567,3.8702917,3.919726,-9.503438,3.537248,0.5418887,-11.942697,-7.0738254,-6.8869324,-6.694939,-6.7094626,8.553723,-4.17019,-4.1847672,-7.068235,-8.5526905,4.319541,4.3965945,4.338336,4.3476577,17.956087,5.2076817,5.608645,-2.921148,5.136158,2.9704015,1.9220369,2.3123538,2.596708,-12.042055,5.3467402,3.96466,-3.856897,-3.6062908,3.7397845,-1.0793343,8.554633,-4.2089143,-4.2795033,-4.3266754,-4.384526,-0.71968615,-0.72571325,-0.6852408,-0.7053492,-0.7310136,-0.7184011,2.6087282,2.5320878,5.425437,0.7588426,0.5975866,0.57425827,-0.35785532,0.15501969,3.5990381,3.6886263,3.4466462,-18.153852,-18.381351,-18.264109,0.30883855,0.22447196,-0.38963538,-10.351116,-10.179784,0.9022324,-6.4989996,-0.4775946,-6.76552,3.287494,5.9276743,3.5394244,0.37733275,-0.48690516,-0.73989093,1.4671865,0.16274159,0.10384664,2.7929819,-0.24684241,-1.3045213,-0.29843867,-18.317406,-17.905863,-18.181656,-2.4267507,-2.3691406,5.2616796,5.306423,5.73865,7.1195555,-7.8556805,0.57632804,-6.4174185,0.23754013,-6.396894,-1.4722079,0.62798756,5.8370714,0.66062355,-1.6302066,-0.82377183,-10.2481985,-0.48697683,4.2823544,-0.7850692,-1.656259,4.497863,4.424825,4.927486,4.495549,-12.568765,-10.69608,-10.664501,3.1824996,2.8238456,2.7605913,-1.1115371,-9.899718,-0.09471608,-9.947821,0.52326214,0.83381915,0.8814999,-2.416394,-3.0035398,-3.0776463,-3.0810766,-2.556312,-3.0444517,-3.042409,-2.9246373,-2.9840925,-2.7103422,4.617606,5.243021,2.3705554],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"**सोर्स कोड**: \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\\" target=\\\"_blank\\\"\\u003ehttps:\\u002f\\u002fgith...\"],[\"पैकेज को डिफ़ॉल्ट रूप से न्यूनतम रखने के लिए, `huggingface_hub` कुछ उपयोग मामलों के लिए उपयोगी वैकल्...\"],[\"[अपलोड गाइड](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fupload) में विवरण के लिए।\\n\\n## हब ...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"### From specific version\\n\\nBy default, the latest version from the `main` branch is downloaded. Howe...\"],[\"[`snapshot_download`] downloads the latest revision by default. If you want a specific repository re...\"],[\"## Download file(s) to local folder\\n\\nThe recommended (and default) way to download files from the Hu...\"],[\"Here is a table that summarizes the different options to help you choose the parameters that best su...\"],[\"You can download multiple files at once which displays a progress bar and returns the snapshot path ...\"],[\"--\\n# For reference on dataset card metadata, see the spec: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fb...\"],[\"{{ dataset_structure | default(\\\"[More Information Needed]\\\", true)}}\\n\\n## Dataset Creation\\n\\n### Curati...\"],[\"{{ who_are_annotators_section | default(\\\"[More Information Needed]\\\", true)}}\\n\\n#### Personal and Sens...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import HfApi\\n\\u003e\\u003e\\u003e api = HfApi()\\n\\n# Upload all the content from the loc...\"],[\"The example below uploads the local `.\\u002flogs` folder to the remote `\\u002fexperiment\\u002flogs\\u002f` folder. Only t...\"],[\"### Non-blocking uploads\\n\\nIn some cases, you want to push data without blocking your main thread. Th...\"],[\"### Upload a folder by chunks\\n\\n[`upload_folder`] makes it easy to upload an entire folder to the Hub...\"],[\"\\u003cTip warning={true}\\u003e\\n\\n`multi_commits` is still an experimental feature. Its API and behavior is subj...\"],[\"# Schedule regular uploads. Remote repo and local folder are created if they don't already exist.\\n\\u003e\\u003e...\"],[\"For more details about the [`CommitScheduler`], here is what you need to know:\\n- **append-only:**\\n  ...\"],[\"#### Space persistence demo\\n\\nPersisting data from a Space to a Dataset on the Hub is the main use ca...\"],[\"# 3. Upload archive\\n            self.api.upload_file(..., path_or_fileobj=archive_path)\\n\\n        # 4...\"],[\"For example, if you want to upload two files and delete a file in a Hub repository:\\n\\n1. Use the appr...\"],[\"For more detailed information, take a look at the [`HfApi`] reference.\\n\\n### Preupload LFS files befo...\"],[\"\\u003e\\u003e\\u003e # Create commit\\n\\u003e\\u003e\\u003e create_commit(repo_id, operations=operations, commit_message=\\\"Commit all sha...\"],[\"- **Start small**: We recommend starting with a small amount of data to test your upload script. It'...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nAlthough [`Repository`] is not formally deprecated, we recommend using the HTT...\"],[\"```python\\n\\u003e\\u003e\\u003e with repo.commit(commit_message=\\\"My cool model :)\\\", blocking=False)\\n```\\n\\nYou can check...\"],[\"```py\\n\\u003e\\u003e\\u003e repo.git_add(\\\"path\\u002fto\\u002ffile\\\")\\n\\u003e\\u003e\\u003e repo.git_commit(commit_message=\\\"add my first model config...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"There are many ways you can contribute to this client library:\\n* Fixing outstanding issues with the ...\"],[\"If your issue is well written we're already 80% of the way there by the time you post it!\\n\\n## Submit...\"],[\"Once your `main` branch is synchronized, create a new branch from it:\\n\\n   ```bash\\n   $ git checkout ...\"],[\"\\u003e For the commands leveraging the `make` utility, we recommend using the WSL system when running on\\n...\"],[\"11. It's ok if maintainers ask you for changes. It happens all the time to core contributors\\n   too!...\"],[\"We use `pytest` in order to run the tests for the library.\\nFrom the root of the repository they can ...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"Community leaders have the right and responsibility to remove, edit, or reject\\ncomments, commits, co...\"],[\"### 2. Warning\\n\\n**Community Impact**: A violation through a single incident or series\\nof actions.\\n\\n*...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"options:\\n  -h, --help            show this help message and exit\\n```\\n\\nIf the CLI is correctly instal...\"],[\"```\\n_|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      ...\"],[\"```bash\\nhuggingface-cli whoami                                                                     \\n...\"],[\"### Download an entire repository\\n\\nIn some cases, you just want to download all the files from a rep...\"],[\"```bash\\n\\u003e\\u003e\\u003e huggingface-cli download stabilityai\\u002fstable-diffusion-xl-base-1.0 --include \\\"*.safetenso...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```bash\\n\\u003e\\u003e\\u003e huggingface-cli download adept\\u002ffuyu-8b model-00001-of-00002.safetensors --local-...\"],[\"```bash\\n\\u003e\\u003e\\u003e huggingface-cli upload --help\\n```\\n\\n### Upload an entire folder\\n\\nThe default usage for th...\"],[\"```bash\\n# Sync local Space with Hub (upload new files except from logs\\u002f, delete removed files)\\n\\u003e\\u003e\\u003e h...\"],[\"```bash\\n# Upload new logs every 10 minutes\\nhuggingface-cli upload training-model logs\\u002f --every=10\\n``...\"],[\"Scanning your cache directory is useful if you want to know which repos you have downloaded and how ...\"],[\"For more details about how to scan your cache directory, please refer to the [Manage your cache](.\\u002fm...\"],[\"Running Tests\\n\\nTo run the test suite, please perform the following from the root directory of this r...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"## Integrations\\n\\nThe [`HfFileSystem`] can be used with any library that integrates `fsspec`, provide...\"],[\"```python\\n  \\u003e\\u003e\\u003e import numpy as np\\n  \\u003e\\u003e\\u003e import zarr\\n\\n  \\u003e\\u003e\\u003e embeddings = np.random.randn(50000, 1000...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Let's fetch the collection with, `\\\"TheBloke\\u002frecent-models-64f9a55bb3115b4f513ec026\\\"`:\\n\\n```py\\n\\u003e\\u003e\\u003e fro...\"],[\"In addition to these base attributes, returned items can have additional attributes depending on the...\"],[\"Parameter `sort` must be one of  `\\\"last_modified\\\"`,  `\\\"trending\\\"` or `\\\"upvotes\\\"`. Parameter `item` a...\"],[\"\\u003e\\u003e\\u003e collection = create_collection(title=\\\"OS Week Highlights - Sept 18 - 24\\\", namespace=\\\"osanseviero...\"],[\"Let's reuse our example above:\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_hub import get_collection, update_collect...\"],[\"--\\nlanguage:\\n- en\\nlicense: mit\\nlibrary_name: pytorch-lightning\\ntags:\\n- pytorch\\n- image-classificatio...\"],[\"--\\n[]\\n---\\n\\n# invalid-card-data\\n\\nThis card should fail when trying to load it in because the card dat...\"],[\"his document covers all steps that need to be done in order to do a release of the `huggingface_hub`...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```py\\n\\u003e\\u003e\\u003e api.add_space_secret(repo_id=repo_id, key=\\\"HF_TOKEN\\\", value=\\\"hf_api_***\\\")\\n\\u003e\\u003e\\u003e api.add_spac...\"],[\"# Or simply pass a string value\\n\\u003e\\u003e\\u003e api.request_space_hardware(repo_id=repo_id, hardware=\\\"t4-medium\\\"...\"],[\"```py\\n# Pause your Space to avoid getting billed\\n\\u003e\\u003e\\u003e api.pause_space(repo_id=repo_id)\\n# (...)\\n# Rest...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import SpaceStorage\\n\\u003e\\u003e\\u003e api.request_space_storage(repo_id=repo_id, st...\"],[\"### App skeleton\\n\\nHere is what your app would look like. On startup, check if a task is scheduled an...\"],[\"### Task scheduler\\n\\nScheduling tasks can be done in many ways. Here is an example how it could be do...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\"\\n       hr...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Inference Endpoints\\n\\nInference Endpoints provides a secure production solution to easily deploy mode...\"],[\"[[autodoc]] InferenceEndpoint\\n  - from_raw\\n  - client\\n  - async_client\\n  - all\\n\\n## InferenceEndpoint...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"### SafetensorsRepoMetadata\\n\\n[[autodoc]] huggingface_hub.utils.SafetensorsRepoMetadata\\n\\n### Safetens...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"**Source Code**: \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\\" target=\\\"_blank\\\"\\u003ehttps:\\u002f\\u002fgi...\"],[\"```bash\\npip install huggingface_hub[inference]\\n```\\n\\nTo learn more installation and optional dependen...\"],[\"The advantages are:\\n\\n- Free model or dataset hosting for libraries and their users.\\n- Built-in file ...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"For more information about webhooks payload, you can refer to the Webhooks Payload [guide](https:\\u002f\\u002fh...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002fp...\"],[\"Contributors should also be respectful of our [code of\\nconduct](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggi...\"],[\"--\\n# For reference on model card metadata, see the spec: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fblo...\"],[\"{{ downstream_use | default(\\\"[More Information Needed]\\\", true)}}\\n\\n### Out-of-Scope Use\\n\\n\\u003c!-- This se...\"],[\"#### Speeds, Sizes, Times [optional]\\n\\n\\u003c!-- This section provides information about throughput, start...\"],[\"## Technical Specifications [optional]\\n\\n### Model Architecture and Objective\\n\\n{{ model_specs | defau...\"],[\"MyCoolModel\\n\\nIn this example, we don't have any metadata at the top of the file. In cases like these...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```python\\n# app.py\\nfrom huggingface_hub import webhook_endpoint, WebhookPayload\\n\\n@webhook_endpoint\\na...\"],[\"Good job! You just launched a webhook server! Let's break down what happened exactly:\\n\\n1. By decorat...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\n## Configure a Webhook\\n\\nNow that you have a webhook server running, you want to configure a...\"],[\"And this is it! Your Space is now ready to receive webhooks from the Hub. Please keep in mind that i...\"],[\"### Custom server\\n\\nTo get more flexibility, you can also create a [`WebhooksServer`] object directly...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"## Previewing the documentation\\n\\nTo preview the docs, run the following command:\\n\\n```bash\\ndoc-builde...\"],[\"```\\nSections that were moved:\\n\\n[ \\u003ca href=\\\"..\\u002fnew-file#section-b\\\"\\u003eSection A\\u003c\\u002fa\\u003e\\u003ca id=\\\"section-a\\\"\\u003e\\u003c\\u002fa\\u003e...\"],[\"If you want to create a link to some internal class or function, you need to\\nprovide its path. For i...\"],[\"```\\n    Args:\\n        x (`str`, *optional*):\\n            This argument controls ...\\n        a (`floa...\"],[\"#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that no fi...\"],[\"The docstring should give a minimal, clear example of how the respective model \\nis to be used in inf...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"### Create a repository\\n\\nCreate an empty repository with [`create_repo`] and give it a name with the...\"],[\"## Upload and download files\\n\\nNow that you have created your repository, you are interested in pushi...\"],[\"## Change repository settings\\n\\nRepositories come with some settings that you can configure. Most of ...\"],[\"The [`Repository`] class allows you to interact with files and repositories on the Hub with function...\"],[\"```py\\n\\u003e\\u003e\\u003e repo = Repository(\\n...   \\\"my-dataset\\\",\\n...   clone_from=\\\"\\u003cuser\\u003e\\u002f\\u003cdataset_id\\u003e\\\",\\n...   token...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"**源代码**: `\\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\\" target=\\\"_blank\\\"\\u003e`https:\\u002f\\u002fgithub.c...\"],[\"```bash\\nhuggingface-cli login\\n# or using an environment variable\\nhuggingface-cli login --token $HUGG...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"**Quellcode**: \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\\" target=\\\"_blank\\\"\\u003ehttps:\\u002f\\u002fgith...\"],[\"```bash\\npip install huggingface_hub\\n```\\n\\nWenn Sie möchten, können Sie es auch mit [conda](https:\\u002f\\u002fhu...\"],[\"upload_file(\\n    path_or_fileobj=\\\"\\u002fhome\\u002flysandre\\u002fdummy-test\\u002fREADME.md\\\",\\n    path_in_repo=\\\"README.md\\\"...\"],[\"## Beiträge (Feature-Anfragen, Fehler usw.) sind super willkommen 💙💚💛💜🧡❤️\\n\\nJeder ist willkommen beiz...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Contrib test suite\\n\\nThe contrib folder contains simple end-to-end scripts to test integration of `hu...\"],[\"Optionally, it is possible to setup and run all tests in a single command. However this\\ntake more ti...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"--\\nlanguage: en\\nlicense: mit\\nlibrary_name: timm\\ntags:\\n- pytorch\\n- image-classification\\ndatasets:\\n- b...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"**소스 코드**: \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\\" target=\\\"_blank\\\"\\u003ehttps:\\u002f\\u002fgithub.c...\"],[\"레포지토리 전체의 경우:\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_download(\\\"stabilityai\\u002fs...\"],[\"여러분의 라이브러리를 통합하고 싶다면, 이슈를 열어서 의견을 나눠주세요. 통합 과정을 안내하기 위해 ❤️을 담아 [단계별 가이드](https:\\u002f\\u002fhuggingface.co\\u002fdocs...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"## Translating the `huggingface_hub` documentation into your language\\n\\nAs part of our mission to dem...\"],[\"**✍️ Start translating**\\n\\nThe fun part comes - translating the text!\\n\\nThe first thing we recommend i...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"[[autodoc]] logging.get_logger\\n\\n## Configure progress bars\\n\\nProgress bars are a useful tool to displ...\"],[\"Since `requests.Session` is not guaranteed to be thread-safe, `huggingface_hub` creates one session ...\"],[\"[[autodoc]] huggingface_hub.utils.HfHubHTTPError\\n\\n#### RepositoryNotFoundError\\n\\n[[autodoc]] huggingf...\"],[\"Usage:\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_hub.utils import validate_hf_hub_args\\n\\n\\u003e\\u003e\\u003e @validate_hf_hub_args\\n...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"您还可以直接将令牌传递给 [`login`]，如下所示：`login(token=\\\"hf_xxx\\\")`。这将使用您的用户访问令牌登录到 Hugging Face 模型库，而无需您输入任何内容。但是，如...\"],[\"Hugging Face Hub Client library\\n\\n## Download files from the Hub\\n\\nThe `hf_hub_download()` function is...\"],[\"### `hf_hub_url`\\n\\nInternally, the library uses `hf_hub_url()` to return the URL to download the actu...\"],[\"Those API utilities are also exposed through the `huggingface-cli` CLI:\\n\\n```bash\\nhuggingface-cli log...\"],[\"The `clone_from` method can also take any Hugging Face model ID as input, and\\nwill clone that reposi...\"],[\"The repository can be managed through this object, through wrappers of\\ntraditional Git methods:\\n\\n- `...\"],[\"### Non-blocking behavior\\n\\nThe pushing methods have access to a `blocking` boolean parameter to indi...\"],[\"\\u003cbr\\u003e\\n\\n## Using the Inference API wrapper\\n\\n`huggingface_hub` comes with a wrapper client to make call...\"],[\"Some tasks might also require additional params in the request. Here is an\\nexample using a `zero-sho...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"# Install dependencies for both torch-specific and CLI-specific features.\\npip install 'huggingface_h...\"],[\"```bash\\n# First, clone repo locally\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub.git\\n\\n# ...\"],[\"- `huggingface_hub`'s cache system relies on symlinks to efficiently cache files downloaded\\nfrom the...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"`HfApi.get_repo_discussions` returns a [generator](https:\\u002f\\u002fdocs.python.org\\u002f3.7\\u002fhowto\\u002ffunctional.html...\"],[\"## Create and edit a Discussion or Pull Request programmatically\\n\\nThe [`HfApi`] class also offers wa...\"],[\"Managing Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\\n\\n...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"If you are interested in Inference and Widgets, you can follow [this guide](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"# Save all files in a temporary directory and push them in a single commit\\n   with TemporaryDirector...\"],[\"All of these parameters can be added to the implementations we saw above and passed to the `huggingf...\"],[\"1. Make your Model class inherit from [`ModelHubMixin`].\\n2. Implement the private methods:\\n    - [`~...\"],[\"...     def forward(self, x):\\n...         return self.linear(x + self.param)\\n\\u003e\\u003e\\u003e model = MyModel()\\n\\n...\"],[\"3. Implement the `_from_pretrained` method:\\n\\n```python\\nclass PyTorchModelHubMixin(ModelHubMixin):\\n  ...\"],[\"And that's it! Your library now enables users to upload and download files to and from the Hub.\\n\\n## ...\"],[\"Inference Endpoints\\n\\nInference Endpoints provides a secure production solution to easily deploy any ...\"],[\"In this example, we created a `protected` Inference Endpoint named `\\\"my-endpoint-name\\\"`, to serve [g...\"],[\"```python\\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\\n\\u003e\\u003e\\u003e from huggingface_hub impor...\"],[\"# Get one\\n\\u003e\\u003e\\u003e get_inference_endpoint(\\\"my-endpoint-name\\\")\\nInferenceEndpoint(name='my-endpoint-name', ...\"],[\"```py\\n\\u003e\\u003e\\u003e endpoint.fetch()\\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repositor...\"],[\"# Or in an asyncio context:\\n\\u003e\\u003e\\u003e await endpoint.async_client.text_generation(\\\"I am\\\")\\n```\\n\\nIf the Infe...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n```py\\n# Pause and resume endpoint\\n\\u003e\\u003e\\u003e endpoint.pause()\\nInferenceEndpoint(name='my-endpoint-n...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\n## An end-to-end example\\n\\nA typical use case of Inference Endpoints is to process a batch o...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"While filtering, you can also sort the models and take only the top results. For example,\\nthe follow...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003cTip warning={true}\\u003e\\n\\n[`Repository`] is now deprecated in favor of the http-based alternatives. Give...\"],[\"This preference of the http-based [`HfApi`] over the git-based [`Repository`] does not mean that git...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"For more details and options, see the API reference for [`hf_hub_download`].\\n\\n\\u003ca id=\\\"login\\\"\\u003e\\u003c\\u002fa\\u003e \\u003c!-...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nOnce logged in, all requests to the Hub - even methods that don't necessarily ...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import HfApi\\n\\u003e\\u003e\\u003e api = HfApi()\\n\\u003e\\u003e\\u003e api.create_repo(repo_id=\\\"super-coo...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"### 克隆一个仓库（仅适用于 Spaces）\\n\\n在某些情况下，你可能想要复制别人的仓库并根据自己的用例进行调整。对于 Spaces，你可以使用 [`duplicate_space`] 方法来实现。它...\"],[\"## 修改存储库设置\\n\\n存储库具有一些可配置的设置。大多数情况下，您通常会在浏览器中的存储库设置页面上手动配置这些设置。要配置存储库，您必须具有对其的写访问权限（拥有它或属于组织）。在本节中，我们将看...\"],[\"当你克隆一个存储库时，通过在克隆时指定`git_user`和`git_email`参数，你还可以为克隆的存储库配置Git用户名和电子邮件。当用户提交到该存储库时，Git将知道提交的作者是谁。\\n\\n请运行...\"],[\"--\\nlanguage: en\\nlicense: mit\\nlibrary_name: timm\\ntags:\\n- pytorch\\n- image-classification\\ndatasets:\\n- b...\"],[\"--\\nlicense: mit\\nlanguage: eo\\nthumbnail: https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fassets\\u002f01_how-to-train\\u002fEsperBERT...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"- Use [`ModelCardData.to_yaml`] to convert metadata we defined to YAML so we can use it to insert th...\"],[\"# Model Card for MyCoolModel\\n\\nThis model does this and that.\\n\\nThis model was created by [@nateraw](h...\"],[\"First, we'll create a new repo called 'hf-hub-modelcards-pr-test' under the authenticated user's nam...\"],[\"Let's start with a first example:\\n\\n```python\\n\\u003e\\u003e\\u003e from huggingface_hub import metadata_update\\n\\u003e\\u003e\\u003e met...\"],[\"card = ModelCard.from_template(card_data)\\nprint(card.data)\\n```\\n\\nThe resulting `card.data` should loo...\"],[\"--\\nlanguage:\\n- en\\nlicense:\\n- bsd-3-clause\\nannotations_creators:\\n- crowdsourced\\n- expert-generated\\nla...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"For more details about authentication, check out [this section](..\\u002fquick-start#authentication).\\n\\n###...\"],[\"### HF_HUB_OFFLINE\\n\\nIf set, no HTTP calls will me made to the Hugging Face Hub. If you try to downlo...\"],[\"### HF_HUB_DISABLE_SYMLINKS_WARNING\\n\\nIf you are on a Windows machine, it is recommended to enable th...\"],[\"### HF_HUB_ENABLE_HF_TRANSFER\\n\\nSet to `True` for faster uploads and downloads from the Hub using `hf...\"],[\"## From external tools\\n\\nSome environment variables are not specific to `huggingface_hub` but are sti...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\u003cCACHE_DIR\\u003e\\n├─ datasets--glue\\n│  ├─ refs\\n│  ├─ blobs\\n│  ├─ snapshots\\n...\\n```\\n\\nEach folder is des...\"],[\"### .no_exist (advanced)\\n\\nIn addition to the `blobs`, `refs` and `snapshots` folders, you might also...\"],[\"### In practice\\n\\nIn practice, your cache should look like the following tree:\\n\\n```text\\n    [  96]  ....\"],[\"When symlinks are not supported, a warning message is displayed to the user to alert\\nthem they are u...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Assets in practice\\n\\nIn practice, your assets cache should look like the following tree:\\n...\"],[\"The snippet below shows a scan report in a folder in which 4 models and 2 datasets are\\ncached.\\n\\n```t...\"],[\"Done in 0.0s. Scanned 6 repo(s) for a total of 3.4G.\\nGot 1 warning(s) while scanning. Use -vvv to pr...\"],[\"```text\\n➜ huggingface-cli scan-cache -v\\nREPO ID                     REPO TYPE REVISION              ...\"],[\"bert-base-cased             model     a8d257ba9925ef39f3036bfc338acf5283c512d9         1.4G        9...\"],[\"Done in 0.0s. Scanned 6 repo(s) for a total of 3.4G.\\nGot 1 warning(s) while scanning. Use -vvv to pr...\"],[\"Here is a simple usage example. See reference for details.\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_hub import sc...\"],[\"## Clean your cache\\n\\nScanning your cache is interesting but what you really want to do next is usual...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n### Clean cache from the terminal\\n\\nThe easiest way to delete some revisions from your HF cac...\"],[\"```txt\\n✗ huggingface-cli delete-cache --dir ~\\u002f.cache\\u002fhuggingface\\u002fhub\\n? Select revisions to delete: 2...\"],[\"```sh\\nhuggingface-cli delete-cache --disable-tui\\n```\\n\\nExample of command file:\\n\\n```txt\\n# INSTRUCTION...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import scan_cache_dir\\n\\n\\u003e\\u003e\\u003e delete_strategy = scan_cache_dir().delete_...\"],[\"--\\n{{card_data}}\\n---\\n\\n# {{ model_name | default(\\\"MyModelName\\\", true)}}\\n\\n{{ some_data }}...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"### ConversationalOutputConversation\\n\\n[[autodoc]] huggingface_hub.inference._types.ConversationalOut...\"],[\"[[autodoc]] huggingface_hub.inference._text_generation.Details\\n\\n[[autodoc]] huggingface_hub.inferenc...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"For web development, a [JS client](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface.js\\u002finference\\u002fREADME) has ...\"],[\"\\u003cTip\\u003e\\n\\nThere are more than 200k models on the Hugging Face Hub! Each task in the [`InferenceClient`]...\"],[\"```python\\n\\u003e\\u003e\\u003e from huggingface_hub import InferenceClient\\n\\u003e\\u003e\\u003e client = InferenceClient(token=\\\"hf_***...\"],[\"| Domain | Task                           | Supported    | Documentation                            ...\"],[\"| | [Visual Question Answering](https:\\u002f\\u002fhuggingface.co\\u002ftasks\\u002fvisual-question-answering)      | ✅ | [...\"],[\"\\u003cTip\\u003e\\n\\nCheck out the [Tasks](https:\\u002f\\u002fhuggingface.co\\u002ftasks) page to learn more about each task, how t...\"],[\"\\u003e\\u003e\\u003e async for token in await client.text_generation(\\\"The Huggingface Hub is\\\", stream=True):\\n...     ...\"],[\"```py\\n\\u003e\\u003e\\u003e from huggingface_hub import InferenceClient\\n\\u003e\\u003e\\u003e client = InferenceClient()\\n\\u003e\\u003e\\u003e client.imag...\"],[\"to\\n\\n```python\\n\\u003e\\u003e\\u003e from huggingface_hub import InferenceClient\\n\\u003e\\u003e\\u003e client = InferenceClient()\\n\\u003e\\u003e\\u003e res...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```bash\\npip install git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub  # 使用pip从GitHub仓库安装Hugging Fa...\"],[\"- `huggingface_hub`的缓存系统依赖于符号链接来高效地缓存从Hub下载的文件。在Windows上，您必须激活开发者模式或以管理员身份运行您的脚本才能启用符号链接。如果它们没有被激活，缓...\"],[\"--\\n{card_data}\\n---\\n\\n# {{ pretty_name | default(\\\"Dataset Name\\\", true)}}\\n\\n{{ some_data }}...\"],[\"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"`\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002f...\"]],\"hovertemplate\":\"source=huggingface_hub\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"huggingface_hub, circle\",\"marker\":{\"color\":\"#19d3f3\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"huggingface_hub, circle\",\"showlegend\":true,\"x\":[3.840112,3.5237174,-0.23674905,0.024525087,-3.690209,-3.715634,-3.570811,-3.5423884,-3.5133085,-3.396097,-21.729216,-18.403095,-18.938156,-3.2376592,-7.2134957,-7.3140535,-3.6035151,-6.27907,-2.9947236,-2.452885,-5.8153024,-5.4891825,17.529089,16.148619,16.16308,16.086458,16.36883,16.785055,16.466417,17.395948,16.168081,16.078154,16.381487,16.383823,16.318861,17.005936,16.624275,16.225048,16.22801,16.423065,9.747198,15.990625,15.749157,16.1206,15.809169,16.33162,16.239243,15.531514,16.219349,16.159536,16.159813,16.089058,11.698058,15.386488,16.888168,14.304656,3.5037751,11.680295,11.118413,11.222125,11.562686,11.975047,11.431659,-10.367323,9.074369,-1.0966704,-0.23609284,6.7307563,8.64939,8.748994,0.6238602,0.2826415,-0.7298093,-6.311267,9.071555,0.6502489,7.329362,-1.6601533,-3.1693487,5.7865496,-10.065142,-9.598169,2.498633,-1.5015969,-0.70018184,-1.7060548,-1.4539214,-1.566356,-2.6755133,-2.805975,-6.6106205,-6.156404,-4.88641,12.01567,10.686868,12.0723715,12.191128,-1.5814867,11.823375,-3.3801446,-3.565734,-3.3557267,-2.997219,-3.4903936,5.223522,5.2195625,5.1893487,12.767702,12.689042,12.813058,12.832492,3.6103055,0.16228753,1.0002277,1.3264922,0.7033644,0.96414906,0.46426427,0.8256905,-0.19807622,0.59295815,-2.5146525,0.91111904,0.99994177,0.4124165,-12.4822035,-0.14175265,-0.16158035,0.47741544,-5.32908,-3.4298973,-18.698574,2.4759815,-0.814547,-0.91406876,-3.4360693,-2.8565333,-3.1255352,-2.5690444,-1.0451651,-2.3011892,-8.068928,-2.9454477,1.5763216,0.9034179,-0.4610384,-4.622963,0.5754882,1.584449,-3.879826,-8.24121,-8.450119,-3.2013164,-5.021259,8.150727,-4.513147,-4.9331503,-3.7801042,-3.692757,-3.6875389,-3.767954,-18.832653,-18.720398,-3.8217597,-2.4767187,-2.4473717,-2.6976151,-2.842588,-2.8405073,13.444311,8.876042,8.640017,8.496122,8.644164,8.900064,8.557312,3.4395635,11.939567,-10.765892,-10.784831,-4.562603,4.5870147,0.19592053,0.20938145,0.40734667,-8.46917,-8.645436,-7.0171595,8.422889,-6.281501,8.915571,-9.025611,3.068827,-12.8673,-12.558936,-1.9554739,-1.2181934,-2.392566,-2.2579317,-2.3420317,-1.2139765,4.657487,-1.3356311,-1.5400829,-14.942748,-21.731833,-3.4902723,-2.597106,-3.2285624,-7.442797,-5.6963243,0.4696768,-0.5560663,8.641804,7.6724806,-5.9881825,-6.236903,-6.3278546,-6.2502494,-8.158758,-6.120683,-6.260275,-6.24906,-6.9965587,-6.171932,-6.2752137,-6.160782,-4.362104,-2.6231353,-5.643737,-5.123099,-3.2017777,0.21212864,-1.9301958,-2.60429,-3.210403,-3.1916935,-3.2705004,-3.0605948,-3.0108497,-0.4691339,8.8211775,8.837781,8.763447,0.6566443,-6.228557,-6.0585446],\"xaxis\":\"x\",\"y\":[1.2291284,-1.7013487,9.92151,8.103422,6.223643,6.3451867,6.4352784,6.3800898,6.155192,6.2644343,8.553143,-4.1854334,-4.463686,-8.372564,6.315033,6.558933,3.7240164,7.098099,4.3640494,-11.7751,1.2723533,1.411592,3.0472832,3.253732,3.3848717,3.4471757,3.4042683,3.325021,3.1339643,3.0505483,3.51217,3.5730183,3.4735513,3.5160687,3.554405,3.6716578,3.5799222,3.5075052,3.1389399,3.0377386,3.5077143,3.5360456,3.2196305,3.2280035,3.3628316,3.5104637,3.5191443,3.4521966,3.2623873,3.2686708,3.3790092,3.1087437,4.636949,3.606876,1.9066,3.6950202,1.2506806,2.9325764,2.3269618,2.6961136,2.2133076,4.754151,2.0717883,-0.29703847,1.9467425,-3.173853,9.915134,-0.9578892,2.6156666,2.6248007,4.402729,4.2697654,4.7692804,7.8378835,0.4337546,0.7827302,3.1482003,4.5918183,-0.9121988,1.6359706,0.92206,1.0797302,3.050677,4.5720034,0.70866895,-1.21231,-0.5497261,0.08567592,3.471086,4.1202955,0.16906454,1.0674798,1.8635178,2.1380386,2.4188268,2.3137126,2.3386173,-1.2288536,2.4007804,-1.9690734,-1.9592897,-2.028286,-1.9435445,-2.279159,-0.016013585,-0.03625472,0.002892732,4.046299,3.4088976,4.265568,3.7581935,3.4392216,4.3434467,4.48503,4.620946,4.8277082,4.411609,4.201114,4.247992,4.768324,4.5820136,5.048791,4.0141726,4.4050207,4.1287894,-18.446878,9.260534,9.272308,7.865508,2.0182254,-8.289103,-4.1842523,-0.14152803,2.6265368,2.1964397,2.9064345,3.073975,3.1284184,-1.3632933,2.5318635,-0.6746897,1.3050393,-1.0330731,0.87454873,0.84361315,-0.1622524,1.5859567,8.146004,4.038026,3.5337203,2.9729183,3.2458808,0.9007937,1.0399014,4.318807,1.913384,2.0023046,-3.8476088,-3.7938106,-3.8845372,-3.732932,-4.331791,-4.181946,-7.4557133,-7.9725437,-7.3397737,-7.983931,-8.049249,-8.050822,5.56614,-10.685779,-10.540026,-10.287521,-10.524977,-11.158816,-10.257929,0.99411535,3.9583707,-0.74687093,-0.7264263,-5.7625184,3.473397,6.718326,6.6141415,6.890726,2.6086142,2.692289,-0.67484814,-11.890593,3.3701317,3.3631232,2.1924014,1.2232499,-19.122526,-18.426565,-4.4563384,-3.8699665,-4.722555,-2.952443,-2.6860557,-3.1924489,1.5685391,0.26600078,-3.3394084,-2.099589,8.55352,-6.8867683,-5.398058,-6.896,-2.0119207,3.1364496,0.8579042,3.8324769,-11.616035,0.08123198,2.8936143,3.3085072,2.216155,3.5091364,-0.22092511,3.6774151,3.528013,3.0779157,0.11043622,3.277533,3.6964567,3.5172155,-2.1920729,2.3745632,1.3820925,2.779688,5.631343,4.4599757,4.9836345,4.4912252,6.132654,6.1686463,5.6040177,5.878622,6.1294703,3.1021724,-11.469755,-11.305153,-11.441806,0.71351373,2.59033,2.7179952],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"The tokenization pipeline\\n\\nWhen calling `Tokenizer.encode` or\\n`Tokenizer.encode_batch`, the input\\nte...\"],[\"Each normalization operation is represented in the 🤗 Tokenizers library\\nby a `Normalizer`, and you c...\"],[\"When building a `Tokenizer`, you can\\ncustomize its normalizer by just changing the corresponding att...\"],[\"An easy way to pre-tokenize inputs is to split on spaces and\\npunctuations, which is done by the\\n`pre...\"],[\"You can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that wil...\"],[\"Of course, if you change the way the pre-tokenizer, you should probably\\nretrain your tokenizer from ...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"Then we know that BERT preprocesses texts by removing accents and\\nlowercasing. We also use a unicode...\"],[\"And the post-processing uses the template we saw in the previous\\nsection:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003c...\"],[\"The `decoder` will first convert the IDs back to tokens\\n(using the tokenizer's vocabulary) and remov...\"],[\"But by changing it to a proper decoder, we get:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{...\"],[\"Quicktour\\n\\nLet's have a quick look at the 🤗 Tokenizers library features. The\\nlibrary provides an imp...\"],[\"The main API of the library is the `class` `Tokenizer`, here is how\\nwe instantiate one with a BPE mo...\"],[\"We can set the training arguments like `vocab_size` or `min_frequency` (here\\nleft at their default v...\"],[\"Now, we can just call the `Tokenizer.train` method with any list of files we want to use:\\n\\n\\u003ctokenize...\"],[\"and you can reload your tokenizer from that file with the\\n`Tokenizer.from_file`\\n`classmethod`:\\n\\n\\u003ctok...\"],[\"This `Encoding` object then has all the\\nattributes you need for your deep learning model (or other)....\"],[\"An important feature of the 🤗 Tokenizers library is that it comes with\\nfull alignment tracking, mean...\"],[\"### Post-processing\\n\\nWe might want our tokenizer to automatically add special tokens, like\\n`\\\"[CLS]\\\"`...\"],[\"Here is how we can set the post-processing to give us the traditional\\nBERT inputs:\\n\\n\\u003ctokenizerslangc...\"],[\"Lastly, we specify the special tokens we used and their IDs in our\\ntokenizer's vocabulary.\\n\\nTo check...\"],[\"You can then check the type IDs attributed to each token is correct with\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cp...\"],[\"The output is then a list of `Encoding`\\nobjects like the ones we saw before. You can process togethe...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"In this case, the `attention mask` generated by the\\ntokenizer takes the padding into account:\\n\\n\\u003ctoke...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"```python\\nfrom tokenizers import Tokenizer\\nfrom tokenizers.models import BPE\\n\\ntokenizer = Tokenizer(...\"],[\"`tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n  \\u003ch1\\u003e\\u003ccode\\u003ewasm-pack-template\\u003c\\u002fcode\\u003e\\u003c\\u002fh1\\u003e\\n\\n  \\u003cstrong\\u003eA template for kick start...\"],[\"### 🔬 Test in Headless Browsers with `wasm-pack test`\\n\\n```\\nwasm-pack test --headless --firefox\\n```\\n\\n...\"],[\"Post-processors\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertProcessing\\n\\n[[autodoc]] tokenizers.processo...\"],[\"Training from memory\\n\\nIn the [Quicktour](quicktour), we saw how to build and train a\\ntokenizer using...\"],[\"Let's start by loading our dataset:\\n\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocument...\"],[\"Models\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BPE\\n\\n[[autodoc]] tokenizers.models.BPE\\n\\n## Model\\n\\n[[auto...\"],[\"Added Tokens\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## AddedToken\\n\\n[[autodoc]] tokenizers.AddedToken\\n    ...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"```bash\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\ncd tokenizers\\u002fbindings\\u002fpython\\n\\n# Create ...\"],[\"All of these can be used and trained as explained above!\\n\\n### Build your own\\n\\nWhenever these provide...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"fn main() -\\u003e Result\\u003c()\\u003e {\\n    # #[cfg(feature = \\\"http\\\")]\\n    # {\\n        let tokenizer = Tokenizer::...\"],[\"let mut tokenizer = TokenizerBuilder::new()\\n        .with_model(BPE::default())\\n        .with_normal...\"],[\"`tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`...\"],[\"`tokenizers-freebsd-x64`\\n\\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`...\"],[\"`tokenizers-win32-ia32-msvc`\\n\\nThis is the **i686-pc-windows-msvc** binary for `tokenizers`...\"],[\"Visualizer\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## Annotation\\n\\n[[autodoc]] tokenizers.tools.Annotation\\n...\"],[\"Components\\n\\nWhen building a Tokenizer, you can attach various types of components to\\nthis Tokenizer ...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| NFD | NFD...\"],[\"| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\\\"a\\\", \\\"e\\\"...\"],[\"## Pre-tokenizers\\n\\nThe `PreTokenizer` takes care of splitting the input according to a set\\nof rules....\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel...\"],[\"| Digits | Splits the numbers from any other characters. | Input: `\\\"Hello123there\\\"` \\u003cbr\\u003e  Output: ``...\"],[\"| Whitespace | Splits on word boundaries (using the following regular expression: `\\\\w+&#124;[^\\\\w\\\\s]+...\"],[\"\\u003c\\u002frust\\u003e\\n\\u003cnode\\u003e\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel | Splits on white...\"],[\"| Digits | Splits the numbers from any other characters. | Input: `\\\"Hello123there\\\"` \\u003cbr\\u003e  Output: ``...\"],[\"## Models\\n\\nModels are the core algorithms used to actually tokenize, and therefore,\\nthey are the onl...\"],[\"## Post-Processors\\n\\nAfter the whole pipeline, we sometimes want to insert some special\\ntokens before...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n# Tokenizers\\n\\nFast State-of-the-art tokenizers, optimized for ...\"],[\"Decoders\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BPEDecoder\\n\\n[[autodoc]] tokenizers.decoders.BPEDecoder...\"],[\"`tokenizers-darwin-arm64`\\n\\nThis is the **aarch64-apple-darwin** binary for `tokenizers`...\"],[\"Input Sequences\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\nThese types represent all the different kinds of s...\"],[\"Encoding\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## Encoding\\n\\n[[autodoc]] tokenizers.Encoding\\n    - all\\n  ...\"],[\"Pre-tokenizers\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertPreTokenizer\\n\\n[[autodoc]] tokenizers.pre_tok...\"],[\"Normalizers\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertNormalizer\\n\\n[[autodoc]] tokenizers.normalizers....\"],[\"Trainers\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BpeTrainer\\n\\n[[autodoc]] tokenizers.trainers.BpeTrainer...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers-log...\"],[\"## License\\n\\n[Apache License 2.0](..\\u002f..\\u002fLICENSE)...\"],[\"# Requirements\\n\\nIn order to generate the documentation, it is necessary to have a Python environment...\"],[\"`tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`...\"],[\"Changelog\\nAll notable changes to this project will be documented in this file.\\n\\nThe format is based ...\"],[\"## [0.11.2]\\n\\n- [#884] Fixing bad deserialization following inclusion of a default for Punctuation\\n\\n#...\"],[\"### Changed\\n- [#234]: Completely changed the alignement mappings available on `Encoding`. Previous m...\"],[\"### Added\\n- [#236]: RobertaProcessing is now also taking care of trimming offsets, and works just as...\"],[\"### Fixed\\n- [#205]: Trim the decoded string in `BPEDecoder`\\n- [b770f36]: Fix a bug with added tokens...\"],[\"### Fixed\\n- [#193]: Fix some issues with the offsets being wrong with the `ByteLevel` BPE:\\n\\t- when `...\"],[\"[#1072]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f1072\\n[#956]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002f...\"],[\"[#280]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f280\\n[#276]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"`tokenizers-win32-arm64-msvc`\\n\\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`...\"],[\"`tokenizers-android-arm-eabi`\\n\\nThis is the **armv7-linux-androideabi** binary for `tokenizers`...\"],[\"`tokenizers-linux-arm-gnueabihf`\\n\\nThis is the **armv7-unknown-linux-gnueabihf** binary for `tokenize...\"],[\"`tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`...\"],[\"`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`...\"],[\"`tokenizers-android-arm64`\\n\\nThis is the **aarch64-linux-android** binary for `tokenizers`...\"],[\"Changelog\\nAll notable changes to this project will be documented in this file.\\n\\nThe format is based ...\"],[\"## [0.11.5]\\n\\n- [#895] Build `python 3.10` wheels.\\n\\n## [0.11.4]\\n\\n- [#884] Fixing bad deserialization ...\"],[\"## [0.10.0]\\n\\n### Added\\n- [#508]: Add a Visualizer for notebooks to help understand how the tokenizer...\"],[\"## [0.9.2]\\n\\n### Fixed\\n- [#464]: Fix a problem with RobertaProcessing being deserialized as BertProce...\"],[\"## [0.8.0]\\n\\n### Highlights of this release\\n- We can now encode both pre-tokenized inputs, and raw st...\"],[\"### Fixed\\n- [#286]: Fix various crash when training a BPE model\\n- [#309]: Fixed a few bugs related t...\"],[\"## [0.7.0]\\n\\n### Changed\\n- Only one progress bar while reading files during training. This is better ...\"],[\"### Added\\n- [#188]: `ByteLevel` is also a `PostProcessor` now and handles trimming the offsets if ac...\"],[\"### How to migrate\\n- Add the `ByteLevel` `PostProcessor` to your byte-level BPE tokenizers if releva...\"],[\"### Changed\\n- `name` argument is now optional when saving a `Model`'s vocabulary. When the name is n...\"],[\"### Added\\n- Added `CharDelimiterSplit`: a new `PreTokenizer` that allows splitting sequences on the ...\"],[\"[#1096]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f1096\\n[#1072]: https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"[#770]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f770\\n[#762]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#492]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f492\\n[#481]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"[#273]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftokenizers\\u002fpull\\u002f273\\n[#272]: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fto...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n  \\u003ch1\\u003e\\u003ccode\\u003ecreate-wasm-app\\u003c\\u002fcode\\u003e\\u003c\\u002fh1\\u003e\\n\\n  \\u003cstrong\\u003eAn \\u003ccode\\u003enpm init\\u003c\\u002fcode\\u003e tem...\"],[\"## 🚴 Usage\\n\\n```\\nnpm init wasm-app\\n```\\n\\n## 🔋 Batteries Included\\n\\n- `.gitignore`: ignores `node_module...\"],[\"Encode Inputs\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\nThese types represent all the different kinds of inp...\"],[\"alias of `Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Uni...\"],[\"Tokenizer\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## Tokenizer\\n\\n[[autodoc]] tokenizers.Tokenizer\\n    - all...\"],[\"Installation\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n🤗 Tokenizers is tested on Python 3.5+.\\n\\nYou should in...\"],[\"`tokenizers-darwin-x64`\\n\\nThis is the **x86_64-apple-darwin** binary for `tokenizers`...\"],[\"# How to release\\n\\n# Before the release\\n\\nSimple checklist on how to make releases for `tokenizers`.\\n\\n...\"],[\"# Rust\\n\\n- `tokenizers` (rust, python & node) versions don't have to be in sync but it's\\n  very commo...\"],[\"# Python\\n\\n- Edit `bindings\\u002fpython\\u002fsetup.py` to reflect new version.\\n- Edit `bindings\\u002fpython\\u002fpy_src\\u002ft...\"],[\"# Node\\n\\n- Edit `bindings\\u002fnode\\u002fpackage.json` to reflect new version.\\n- Edit `CHANGELOG.md`:\\n    - Add...\"]],\"hovertemplate\":\"source=tokenizers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"tokenizers, circle\",\"marker\":{\"color\":\"#FF6692\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"tokenizers, circle\",\"showlegend\":true,\"x\":[-7.6153574,-3.282806,-3.5411599,8.395502,8.369789,8.171587,2.017632,8.117329,8.408721,8.420789,-0.8139971,8.216901,-4.354456,-3.9425135,-3.8990793,7.7170186,7.74381,-3.5471768,7.639955,-3.8818638,7.682098,7.5215087,-3.9118524,-8.400133,-3.8304238,16.793879,16.61083,15.680361,-2.2062871,-5.872317,-1.1911582,-7.456542,-7.396744,-1.5304849,-12.795739,11.149298,11.197442,6.732622,2.6874,-6.6936035,-4.637435,1.1555274,14.595594,-10.454203,-3.8600652,-1.5211645,-1.534105,-1.8701527,-9.524102,-15.034081,-21.731276,-18.055958,-7.196571,3.4906251,3.8255465,4.9900684,-18.790533,-0.5336911,-2.776676,-18.824368,2.88444,-14.851047,-3.276266,0.09959774,6.2009363,-1.1313289,-4.206838,-1.8621331,-4.116645,-4.7187147,-3.9831572,-3.8590906,-4.18117,-4.2744737,-4.210389,9.987617,1.785,-10.084938,-1.043445,-0.98575205,-2.0058577,-4.501601,4.82943,-3.6273382,-1.5368313,-3.510936,-3.7690365,-3.085507,-2.9309897,12.927634,4.7134743,4.7430134,3.77458,0.29823288,0.9678589,-0.1119224,0.24102639,-15.070852,-13.605402,-11.537026,-4.223121,-1.4010776,-4.827397,2.132512,7.4285526,0.32148913,0.40036774,0.47367832],\"xaxis\":\"x\",\"y\":[6.968925,-8.382204,6.5189548,-10.212548,-10.176771,-10.026677,0.63361776,-9.834686,-10.405255,-10.415473,-0.57912505,-9.97067,4.2825155,4.235157,4.4444857,3.4721992,3.2122533,4.116446,3.418799,4.20367,3.452136,3.401669,4.092761,0.1293834,3.946881,2.644239,2.8192718,3.1589723,-0.14637943,0.83985114,1.5426188,-2.0277328,-2.071901,-0.9643008,-18.718765,4.5517626,3.653662,-0.7465928,-2.2913136,5.483938,4.2221904,5.886352,5.169209,-0.79436266,-1.6217791,3.1729422,3.359118,2.3177416,0.19669448,-2.1428878,8.5533695,-4.112588,7.111749,-1.6669446,-1.3409973,0.7427429,-4.3022594,6.4048686,-8.336354,-4.346428,2.074896,-2.0434148,5.7022667,4.0908155,20.733057,-2.408082,5.118345,-6.3384237,-1.6897002,-2.138088,-2.3788095,-2.4931614,-2.6008391,-2.5735435,-2.8077672,3.233794,3.637085,0.09186385,-2.941238,2.906233,1.4691722,-5.6711335,1.694073,4.927262,4.3378706,5.8099284,5.104795,4.591582,5.095526,5.189291,2.2782288,2.2969897,4.163819,-6.320432,4.0479803,-6.3993955,-6.3251247,-2.1405969,19.450472,-1.2898402,-1.5705574,-2.3100307,-6.2360682,3.7293243,3.0611985,4.4517503,5.7910824,1.4305544],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# add actors, sensors, reward functions etc ...\\n\\n    return root\\n```\\n\\nYou can then provide the `gene...\"],[\"How to contribute to simulate?\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fContributor%20C...\"],[\"**do not** work on the `main` branch.\\n\\n4. Set up a development environment by running the following ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Simulate with Godot\\n\\n### Install in Godot 4\\nThis integration has been developed for Godot 4.x. You...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Loading a scene from the hub or a local file\\n\\nLoading a scene from a local file or the hub is don...\"],[\"Most of these objects can be visualized by running the following [example](https:\\u002f\\u002fgithub.com\\u002fhuggin...\"],[\"# Remove the last added sphere\\n\\u003e\\u003e\\u003e scene.remove(scene.sphere_04)\\n\\u003e\\u003e\\u003e Scene(dimensionality=3, engine=...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f10695622\\u002f1926638...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"docs\\u002fsource\\u002fassets\\u002fsimulate_library.png\\\" width=\\\"400\\\"\\u002f\\u003e\\n    ...\"],[\"The saving\\u002fsharing format is engine agnostic and using a graphic industry standard.\\n\\nLet's do a quic...\"],[\"### Creating a Scene and adding\\u002fmanaging Objects in the scene\\n\\nBasic example of creating a scene wit...\"],[\"### Objects are organized in a tree structure\\n\\nAdding\\u002fremoving objects:\\n- Using the addition (`+`) o...\"],[\"### Editing and moving objects\\n\\nObjects can be easily translated, rotated, scaled\\n\\nHere are a couple...\"],[\"# Unity Integration\\n\\n### Install with the Unity editor\\nCurrently we use Unity version `2021.3.2f1` a...\"],[\"The above example will only work if `MyCommand` is implemented in the backend. To implement this in ...\"],[\"This currently only supports Box, Sphere, and Capsule colliders (the Unity\\u002fPhysX colliders).\\n\\nDiffer...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"---\\n\\n## Adding a new element to the navigation bar\\n\\nAccepted files are Markdown (.md or .mdx).\\n\\nCrea...\"],[\"### Adding a new tutorial\\n\\nAdding a new tutorial or section is done in two steps:\\n\\n- Add a new file ...\"],[\"```\\n## XXXTokenizer\\n\\n[[autodoc]] XXXTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_speci...\"],[\"If the description is too long to fit in one line, another indentation is necessary before writing t...\"],[\"We follow the [doctest](https:\\u002f\\u002fdocs.python.org\\u002f3\\u002flibrary\\u002fdoctest.html) syntax for the examples to a...\"],[\"#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that no fi...\"],[\"Security Policy\\n\\n## Supported Versions\\n\\u003c!--\\nUse this section to tell people about which versions of ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"his package provides core backend functionality for the Hugging Face Simulate project: (https:\\u002f\\u002fgith...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The \\\"leaf\\\" reward functions can be combined in a tree structure with the following predicate functio...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Next, run the simulation for 30 timesteps:\\n```\\nfor i in range(60):\\n    event = scene.step()\\n```\\nYou ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The simulate library is an exploration on how one could use python to easily build & share complex a...\"],[\"# Blender Integration\\n\\n### Install addon in Blender\\nThis integration has been developed for Blender ...\"],[\"Tests examples taken from the original great gltflib\\n\\nFind the great gltflib by Lukas Shawford here:...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Examples for Simulate\\n\\nThe examples are organized by level of complexity or application. \\nCurrentl...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"We can now start an X server:\\n\\n```\\nsudo Xorg :0\\n```\\n\\nRun the following to confirm that offscreen ren...\"],[\"Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pledge...\"],[\"Community leaders have the right and responsibility to remove, edit, or reject\\ncomments, commits, co...\"],[\"### 2. Warning\\n\\n**Community Impact**: A violation through a single incident or series\\nof actions.\\n\\n*...\"],[\"[homepage]: https:\\u002f\\u002fwww.contributor-covenant.org\\n[v2.0]: https:\\u002f\\u002fwww.contributor-covenant.org\\u002fversio...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Reward function:\\n- A dense reward based on improvement in best euclidean distance to the object\\n- A ...\"],[\"Action space:\\n- A discrete action space with 3 possible actions\\n- Turn left 10 degrees\\n- Turn right ...\"],[\"Objective: Move the agent so the box is within the agents its field of view\\n\\nActors: An EgoCentric C...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"]],\"hovertemplate\":\"source=simulate\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"simulate, circle\",\"marker\":{\"color\":\"#B6E880\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"simulate, circle\",\"showlegend\":true,\"x\":[7.4714875,6.9895806,-9.648714,-9.910589,-3.4938927,16.321583,16.902449,-9.326808,-9.37814,-9.620113,-9.64198,-9.632654,-2.719649,5.2841697,-3.406306,4.229451,-12.826831,-12.791737,-12.629783,-12.714423,-12.834095,-9.405731,-2.5774226,-2.9504697,-12.491108,-8.200371,-8.350127,-8.665044,-8.655634,-8.592831,-7.774471,-8.439387,2.6040094,1.7901679,-10.058616,1.84946,3.7534938,9.22954,8.825228,-6.9378424,-7.0847816,8.320373,12.980639,-3.5749373,-3.5440972,4.9523883,-0.4829199,8.647556,-13.732295,11.82015,1.1855736,1.7310512,13.856002,-3.6338978,6.1950164,6.20879,-0.86395967,-1.65553,7.130012,7.0107026,7.1374774,14.068373,-3.8431013,-0.023641778],\"xaxis\":\"x\",\"y\":[0.6836264,0.2648149,0.50800467,0.53669477,6.487428,3.324824,1.9452248,0.90490127,0.88961864,0.9562924,0.9374244,0.9289043,4.126522,-0.37834346,-4.071553,0.84803945,-18.304426,-18.48719,-18.683372,-17.614208,-18.141691,-0.66995585,3.9178064,4.1941185,-18.442438,3.3971727,3.4170132,3.7449493,3.587345,3.6738875,3.5707104,3.626262,-2.384442,4.0266314,0.89426523,3.9591827,2.9847405,2.8832068,3.0019321,0.8191519,0.37091646,4.432144,4.374967,6.387363,6.338595,3.8013704,1.2618406,-11.092497,-1.7974099,2.1500933,2.9776657,-0.12500702,3.605542,-6.678103,20.721165,20.753958,4.0606217,4.146875,-9.418798,-9.421311,-9.395039,3.842231,3.1597064,3.7102482],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"# How to release\\n\\n# Before the release\\n\\nSimple checklist on how to make releases for `safetensors`.\\n...\"],[\"# Rust\\n\\n- `safetensors` (rust, python & node) versions don't have to be in sync but it's\\n  very comm...\"],[\"# Python\\n\\n- Edit `bindings\\u002fpython\\u002fsetup.py` to reflect new version.\\n- Edit `bindings\\u002fpython\\u002fpy_src\\u002fs...\"],[\"# Node\\n\\n- Edit `bindings\\u002fnode\\u002fpackage.json` to reflect new version.\\n- Edit `CHANGELOG.md`:\\n    - Add...\"],[\"Flax API\\n\\n[[autodoc]] safetensors.flax.load_file\\n[[autodoc]] safetensors.flax.load\\n[[autodoc]] safet...\"],[\"Convert weights to safetensors\\n\\nPyTorch model weights are commonly saved and stored as `.bin` files ...\"],[\"Numpy API\\n\\n[[autodoc]] safetensors.numpy.load_file\\n[[autodoc]] safetensors.numpy.load\\n[[autodoc]] sa...\"],[\"Speed Comparison\\n\\n\\u003ca href=\\\"https:\\u002f\\u002fcolab.research.google.com\\u002fgithub\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002f...\"],[\"This speedup is due to the fact that this library avoids unnecessary copies by mapping the file dire...\"],[\"# Installation\\n\\n```\\npip install safetensors\\n```\\n\\n\\n## Usage\\n\\n### Numpy\\n\\n```python\\nfrom safetensors.nu...\"],[\"PaddlePaddle API\\n\\n[[autodoc]] safetensors.paddle.load_file\\n[[autodoc]] safetensors.paddle.load\\n[[aut...\"],[\"he purpose of this directory is to showcase various attacks (and creating your own).\\n\\n\\n# Torch Arbit...\"],[\"# Safetensors abuse attempts\\n\\nIn order to try and check the limits, we also try to abuse the current...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cpicture\\u003e\\n    \\u003csource media=\\\"(prefers-color-scheme: dark)\\\" srcset=\\\"https:\\u002f\\u002fhuggi...\"],[\"### Installation\\n#### Pip\\n\\nYou can install safetensors via the pip manager:\\n\\n```bash\\npip install saf...\"],[\"Notes:\\n - Duplicate keys are disallowed. Not all parsers may respect this.\\n - In general the subset ...\"],[\"Let's take a look at alternatives and why this format is deemed interesting.\\nThis is my very persona...\"],[\"### Main oppositions\\n\\n- Pickle: Unsafe, runs arbitrary code\\n- H5: Apparently now discouraged for TF\\u002f...\"],[\"- Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\\nmome...\"],[\"Metadata Parsing\\n\\nGiven the simplicity of the format, it's very simple and efficient to fetch and pa...\"],[\"```ts\\nimport { parseSafetensorsMetadata } from \\\"@huggingface\\u002fhub\\\";\\n\\nconst info = await parseSafetens...\"],[\"### Python\\n\\nIn this example python script, we are parsing metadata of [gpt2](https:\\u002f\\u002fhuggingface.co\\u002f...\"],[\"For instance, here are the number of params per dtype for a few models on the HuggingFace Hub. Also ...\"],[\"Torch API\\n\\n[[autodoc]] safetensors.torch.load_file\\n[[autodoc]] safetensors.torch.load\\n[[autodoc]] sa...\"],[\"Tensorflow API\\n\\n[[autodoc]] safetensors.tensorflow.load_file\\n[[autodoc]] safetensors.tensorflow.load...\"],[\"!-- DISABLE-FRONTMATTER-SECTIONS --\\u003e\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:h...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocument...\"],[\"Torch shared tensors\\n\\n\\n## TL;DR\\n\\nUsing specific functions, which should work in most cases for you.\\n...\"],[\"## Why are shared tensors not saved in `safetensors` ?\\n\\nMultiple reasons for that:\\n\\n- *Not all frame...\"],[\"## How does it work ?\\n\\nThe design is rather simple.\\nWe're going to look for all shared tensors, then...\"],[\"p align=\\\"center\\\"\\u003e\\n  \\u003cpicture\\u003e\\n    \\u003csource media=\\\"(prefers-color-scheme: dark)\\\" srcset=\\\"https:\\u002f\\u002fhuggi...\"],[\"### Installation\\n#### Pip\\n\\nYou can install safetensors via the pip manager:\\n\\n```bash\\npip install saf...\"],[\"Notes:\\n - Duplicate keys are disallowed. Not all parsers may respect this.\\n - In general the subset ...\"],[\"Let's take a look at alternatives and why this format is deemed interesting.\\nThis is my very persona...\"],[\"### Main oppositions\\n\\n- Pickle: Unsafe, runs arbitrary code\\n- H5: Apparently now discouraged for TF\\u002f...\"],[\"- Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\\nmome...\"]],\"hovertemplate\":\"source=safetensors\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"safetensors, circle\",\"marker\":{\"color\":\"#FF97FF\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"safetensors, circle\",\"showlegend\":true,\"x\":[5.7745385,-5.176481,-5.171575,5.9442763,-1.7210348,-13.303105,-0.28575662,-2.0276926,-1.751208,15.711645,-13.60489,-8.205961,-2.4103553,-1.9196311,-2.2264724,-5.662627,-7.987206,-4.196478,-2.460069,-5.9333024,-6.616086,-1.6681465,-1.6180145,1.1664577,-9.494479,2.616352,-1.0007535,2.603088,1.4246515,2.2775722,8.974145,8.705663,-6.8909235,-6.6444635,-6.3603053,-2.413323],\"xaxis\":\"x\",\"y\":[1.628394,-0.247553,-0.20245957,1.8405302,2.5456998,-18.173996,0.6159435,-6.5033,-6.6140313,3.4203646,-1.8210596,1.4039589,-1.2999015,-0.45639545,0.22166449,-0.5437272,1.8958439,0.7769484,3.656517,7.158755,6.7624865,4.6270447,4.5197606,-0.33934176,0.77584547,0.5688381,2.8240452,4.0397573,4.473998,4.106563,-11.3436365,-10.520645,2.7032855,2.6144872,2.5546713,-12.068371],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"How to create a pipeline object?\"]],\"hovertemplate\":\"source=User query\\u003cbr\\u003esymbol=star\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"User query, star\",\"marker\":{\"color\":\"black\",\"size\":[100],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"diamond\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"User query, star\",\"showlegend\":true,\"x\":[-2.6366208],\"xaxis\":\"x\",\"y\":[-8.937956],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"\\u003cb\\u003eChunk source\\u003c\\u002fb\\u003e\"},\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"height\":700,\"width\":1000,\"title\":{\"text\":\"\\u003cb\\u003e2D Projection of Chunk Embeddings via PaCMAP\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('044727af-3d89-4bfc-99ab-f7a15391e29c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Au-n4ewrEQzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "<|system|>\n",
        "Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "<|user|>\n",
        "Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iC8gyTCSsSOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFaceHub\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
        "HF_API_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "READER_LLM = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=HF_API_TOKEN,\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"top_k\": 30,\n",
        "        \"temperature\": 0.1,\n",
        "        \"repetition_penalty\": 1.03,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh-GkdPEsTNW",
        "outputId": "7ffdaa30-cdea-4ab1-8c6d-4523b76490f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2308440224.py:9: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  READER_LLM = HuggingFaceHub(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_core.language_models.llms import LLM\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: LLM,\n",
        "    knowledge_index: VectorStore,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 7,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
        "    # Gather documents with retriever\n",
        "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    llm_client = InferenceClient(model=repo_id, timeout=120)\n",
        "\n",
        "    answer = call_llm(llm_client, final_prompt)\n",
        "\n",
        "    return answer, relevant_docs"
      ],
      "metadata": {
        "id": "rd698Rv3spX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Question Couples\n"
      ],
      "metadata": {
        "id": "0QUt2LqqtytJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import json\n",
        "\n",
        "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "llm_client = InferenceClient(\n",
        "    model=repo_id,\n",
        "    timeout=120,\n",
        ")\n",
        "\n",
        "def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "    # using the built-in text_generation method\n",
        "    resp = inference_client.text_generation(\n",
        "        prompt,\n",
        "        max_new_tokens=1000,\n",
        "    )\n",
        "    # Depending on version, resp may be a dict or list\n",
        "    # E.g. resp might be {\"generated_text\": \"...\"} or a list of such dicts\n",
        "    if isinstance(resp, list):\n",
        "        resp0 = resp[0]\n",
        "    else:\n",
        "        resp0 = resp\n",
        "    # Try to extract generated text field\n",
        "    if \"generated_text\" in resp0:\n",
        "        return resp0[\"generated_text\"]\n",
        "    # fallback: return str(resp0)\n",
        "    return str(resp0)\n",
        "\n",
        "print(call_llm(llm_client, \"This is a test context\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "t0mnPzoltF7K",
        "outputId": "b3e7f44f-d815-4f55-8c27-ea22dbc24d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Model mistralai/Mixtral-8x7B-Instruct-v0.1 is not supported for task text-generation and provider together. Supported task: conversational.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3192906405.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This is a test context\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3192906405.py\u001b[0m in \u001b[0;36mcall_llm\u001b[0;34m(inference_client, prompt)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference_client\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInferenceClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# using the built-in text_generation method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     resp = inference_client.text_generation(\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mtext_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2370\u001b[0m         \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2371\u001b[0m         \u001b[0mprovider_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2372\u001b[0;31m         request_parameters = provider_helper.prepare_request(\n\u001b[0m\u001b[1;32m   2373\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2374\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_providers/_common.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# mapped model from HF model ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mprovider_mapping_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_mapping_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# default HF headers + user headers (to customize in subclasses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_providers/_common.py\u001b[0m in \u001b[0;36m_prepare_mapping_info\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprovider_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;34mf\"Model {model} is not supported for task {self.task} and provider {self.provider}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;34mf\"Supported task: {provider_mapping.task}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model mistralai/Mixtral-8x7B-Instruct-v0.1 is not supported for task text-generation and provider together. Supported task: conversational."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show huggingface_hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1IBiHhnvd-g",
        "outputId": "c8356fe1-f307-4e5d-858f-b6b1a196608f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: huggingface-hub\n",
            "Version: 0.35.3\n",
            "Summary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\n",
            "Home-page: https://github.com/huggingface/huggingface_hub\n",
            "Author: Hugging Face, Inc.\n",
            "Author-email: julien@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: filelock, fsspec, hf-xet, packaging, pyyaml, requests, tqdm, typing-extensions\n",
            "Required-by: accelerate, datasets, diffusers, gradio, gradio_client, peft, sentence-transformers, timm, tokenizers, torchtune, transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "llm_client = InferenceClient(model=repo_id, timeout=120)\n",
        "\n",
        "def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "    response = inference_client.chat.completions.create(\n",
        "        model=repo_id,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "    # response.choices[0].message.content contains the text\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "print(call_llm(llm_client, \"This is a test context\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPRIoqzwvkLt",
        "outputId": "2ccb48dd-07d9-450e-a471-ab343c3c7abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello! I'm here to help answer your questions to the best of my ability. Since this appears to be a general statement and not a question, please let me know how I can assist you further.\n",
            "\n",
            "In a test context, it's important to clearly communicate what you are trying to achieve and what you need help with. This will allow me to provide you with accurate and relevant information.\n",
            "\n",
            "If you have a specific question related to programming, data science, or machine learning, please feel free to ask and I will do my best to assist you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a factoid question and an answer given a context.\n",
        "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
        "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
        "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Factoid question: (your factoid question)\n",
        "Answer: (your answer to the factoid question)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ],
      "metadata": {
        "id": "legX3rq8vx3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "langchain_docs = [LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "docs_processed = []\n",
        "for doc in langchain_docs:\n",
        "    docs_processed += text_splitter.split_documents([doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fe42ddb217e84c32b306f86e08e314b8",
            "7e108c3085bd493a94c0a33d79aab835",
            "0699c0a52bb34b6f81dbebd09bf5ce07",
            "b5dae48529c3461397db5943da6114bb",
            "8e0d2d065c714d699d8b415c639d037d",
            "c9bc730201e948b1bb1497a32635e3a7",
            "f3adf322cfb3404889fd77d03174f372",
            "29542d66fecd49a48432510bb72e4617",
            "efe56e57e1124ef49fd4ef5052735b47",
            "f8694e4356504eae8a3378da5d507b68",
            "f66bcc1c26f84406bcade7125958e85a"
          ]
        },
        "id": "0d0m2P1OwB8l",
        "outputId": "9ff41d4e-5a41-4854-979a-6fcc8fdd0719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe42ddb217e84c32b306f86e08e314b8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "N_GENERATIONS = 10  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
        "\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
        "    # Generate QA couple\n",
        "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
        "    try:\n",
        "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
        "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
        "        assert len(answer) < 300, \"Answer is too long\"\n",
        "        outputs.append(\n",
        "            {\n",
        "                \"context\": sampled_context.page_content,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "8becc5ea39124d0ebc453808589c5a94",
            "3d6f4775b6204fd49f04cac08229c914",
            "1795b23359d34a36b2bb4206c0fe9f3c",
            "f8dfc32b8a1446f28446a7ef4cbd7ab1",
            "b10b23c9f99a457f91e45228c87b662d",
            "8aeaff77971443c982656c7db1d7cda7",
            "4cf211fb0c8847ec83f91d1cfccfa101",
            "3ccff14cd6ee48449b07c12933622e8a",
            "bea59f3f8bb04e07ae3fe66f2045982c",
            "6d12390fbc62410aa2641ac6c1c9a919",
            "13153e5a2f874629a83b77808b50a2a7"
          ]
        },
        "id": "LG1tvMEzv2i7",
        "outputId": "5854ee6a-6ff9-42e0-8977-a6cf853846b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 10 QA couples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8becc5ea39124d0ebc453808589c5a94"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEqp4ymJwHid",
        "outputId": "900c202f-e8b1-4e37-d830-1c632c27d1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': 'It has two sub-properties: `event.action` and `event.scope`.\\n\\n`event.scope` will be one of the following values:\\n\\n- `\"repo\"` - Global events on repos. Possible values for the associated `action`: `\"create\"`, `\"delete\"`, `\"update\"`, `\"move\"`.\\n- `\"repo.content\"` - Events on the repo\\'s content, such as new commits or tags. It triggers on new Pull Requests as well due to the newly created reference/commit. The associated `action` is always `\"update\"`.\\n- `\"repo.config\"` - Events on the config: update Space secrets, update settings, update DOIs, disabled or not, etc. The associated `action` is always `\"update\"`.\\n- `\"discussion\"` - Creating a discussion or Pull Request, updating the title or status, and merging. Possible values for the associated `action`: `\"create\"`, `\"delete\"`, `\"update\"`.\\n- `\"discussion.comment\"` - Creating, updating, and hiding a comment. Possible values for the associated `action`: `\"create\"`, `\"update\"`.\\n\\nMore scopes can be added in the future. To handle unknown events, your webhook handler can consider any action on a narrowed scope to be an `\"update\"` action on the broader scope.\\n\\nFor example, if the `\"repo.config.dois\"` scope is added in the future, any event with that scope can be considered by your webhook handler as an `\"update\"` action on the `\"repo.config\"` scope.\\n\\n### Repo\\n\\nIn the current version of webhooks, the top-level property `repo` is always specified, as events can always be associated with a repo. For example, consider the following value:\\n\\n```json\\n\"repo\": {\\n\\t\"type\": \"model\",\\n\\t\"name\": \"some-user/some-repo\",\\n\\t\"id\": \"6366c000a2abcdf2fd69a080\",\\n\\t\"private\": false,\\n\\t\"url\": {\\n\\t\\t\"web\": \"https://huggingface.co/some-user/some-repo\",\\n\\t\\t\"api\": \"https://huggingface.co/api/models/some-user/some-repo\"\\n\\t},\\n\\t\"headSha\": \"c379e821c9c95d613899e8c4343e4bfee2b0c600\",\\n\\t\"tags\": [\\n\\t\\t\"license:other\",\\n\\t\\t\"has_space\"\\n\\t],\\n\\t\"owner\": {\\n\\t\\t\"id\": \"61d2000c3c2083e1c08af22d\"\\n\\t}\\n}\\n```',\n",
              " 'question': 'What is the top-level property always specified in the current version of webhooks?\\n',\n",
              " 'answer': 'The top-level property `repo` is always specified in the current version of webhooks.',\n",
              " 'source_doc': 'huggingface/hub-docs/blob/main/docs/hub/webhooks.md'}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(pd.DataFrame(outputs).head(1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "qy-aDjrjwa83",
        "outputId": "196a314e-641d-4524-991c-5b11524c9236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                context  \\\n",
              "0  It has two sub-properties: `event.action` and `event.scope`.\\n\\n`event.scope` will be one of the following values:\\n\\n- `\"repo\"` - Global events on repos. Possible values for the associated `action`: `\"create\"`, `\"delete\"`, `\"update\"`, `\"move\"`.\\n- `\"repo.content\"` - Events on the repo's content, such as new commits or tags. It triggers on new Pull Requests as well due to the newly created reference/commit. The associated `action` is always `\"update\"`.\\n- `\"repo.config\"` - Events on the config: update Space secrets, update settings, update DOIs, disabled or not, etc. The associated `action` is always `\"update\"`.\\n- `\"discussion\"` - Creating a discussion or Pull Request, updating the title or status, and merging. Possible values for the associated `action`: `\"create\"`, `\"delete\"`, `\"update\"`.\\n- `\"discussion.comment\"` - Creating, updating, and hiding a comment. Possible values for the associated `action`: `\"create\"`, `\"update\"`.\\n\\nMore scopes can be added in the future. To handle unknown events, your webhook handler can consider any action on a narrowed scope to be an `\"update\"` action on the broader scope.\\n\\nFor example, if the `\"repo.config.dois\"` scope is added in the future, any event with that scope can be considered by your webhook handler as an `\"update\"` action on the `\"repo.config\"` scope.\\n\\n### Repo\\n\\nIn the current version of webhooks, the top-level property `repo` is always specified, as events can always be associated with a repo. For example, consider the following value:\\n\\n```json\\n\"repo\": {\\n\\t\"type\": \"model\",\\n\\t\"name\": \"some-user/some-repo\",\\n\\t\"id\": \"6366c000a2abcdf2fd69a080\",\\n\\t\"private\": false,\\n\\t\"url\": {\\n\\t\\t\"web\": \"https://huggingface.co/some-user/some-repo\",\\n\\t\\t\"api\": \"https://huggingface.co/api/models/some-user/some-repo\"\\n\\t},\\n\\t\"headSha\": \"c379e821c9c95d613899e8c4343e4bfee2b0c600\",\\n\\t\"tags\": [\\n\\t\\t\"license:other\",\\n\\t\\t\"has_space\"\\n\\t],\\n\\t\"owner\": {\\n\\t\\t\"id\": \"61d2000c3c2083e1c08af22d\"\\n\\t}\\n}\\n```   \n",
              "\n",
              "                                                                                question  \\\n",
              "0  What is the top-level property always specified in the current version of webhooks?\\n   \n",
              "\n",
              "                                                                                  answer  \\\n",
              "0  The top-level property `repo` is always specified in the current version of webhooks.   \n",
              "\n",
              "                                            source_doc  \n",
              "0  huggingface/hub-docs/blob/main/docs/hub/webhooks.md  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b14d5c4c-977f-4e94-8b89-0ae37b4c1deb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It has two sub-properties: `event.action` and `event.scope`.\\n\\n`event.scope` will be one of the following values:\\n\\n- `\"repo\"` - Global events on repos. Possible values for the associated `action`: `\"create\"`, `\"delete\"`, `\"update\"`, `\"move\"`.\\n- `\"repo.content\"` - Events on the repo's content, such as new commits or tags. It triggers on new Pull Requests as well due to the newly created reference/commit. The associated `action` is always `\"update\"`.\\n- `\"repo.config\"` - Events on the config: update Space secrets, update settings, update DOIs, disabled or not, etc. The associated `action` is always `\"update\"`.\\n- `\"discussion\"` - Creating a discussion or Pull Request, updating the title or status, and merging. Possible values for the associated `action`: `\"create\"`, `\"delete\"`, `\"update\"`.\\n- `\"discussion.comment\"` - Creating, updating, and hiding a comment. Possible values for the associated `action`: `\"create\"`, `\"update\"`.\\n\\nMore scopes can be added in the future. To handle unknown events, your webhook handler can consider any action on a narrowed scope to be an `\"update\"` action on the broader scope.\\n\\nFor example, if the `\"repo.config.dois\"` scope is added in the future, any event with that scope can be considered by your webhook handler as an `\"update\"` action on the `\"repo.config\"` scope.\\n\\n### Repo\\n\\nIn the current version of webhooks, the top-level property `repo` is always specified, as events can always be associated with a repo. For example, consider the following value:\\n\\n```json\\n\"repo\": {\\n\\t\"type\": \"model\",\\n\\t\"name\": \"some-user/some-repo\",\\n\\t\"id\": \"6366c000a2abcdf2fd69a080\",\\n\\t\"private\": false,\\n\\t\"url\": {\\n\\t\\t\"web\": \"https://huggingface.co/some-user/some-repo\",\\n\\t\\t\"api\": \"https://huggingface.co/api/models/some-user/some-repo\"\\n\\t},\\n\\t\"headSha\": \"c379e821c9c95d613899e8c4343e4bfee2b0c600\",\\n\\t\"tags\": [\\n\\t\\t\"license:other\",\\n\\t\\t\"has_space\"\\n\\t],\\n\\t\"owner\": {\\n\\t\\t\"id\": \"61d2000c3c2083e1c08af22d\"\\n\\t}\\n}\\n```</td>\n",
              "      <td>What is the top-level property always specified in the current version of webhooks?\\n</td>\n",
              "      <td>The top-level property `repo` is always specified in the current version of webhooks.</td>\n",
              "      <td>huggingface/hub-docs/blob/main/docs/hub/webhooks.md</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b14d5c4c-977f-4e94-8b89-0ae37b4c1deb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b14d5c4c-977f-4e94-8b89-0ae37b4c1deb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b14d5c4c-977f-4e94-8b89-0ae37b4c1deb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"It has two sub-properties: `event.action` and `event.scope`.\\n\\n`event.scope` will be one of the following values:\\n\\n- `\\\"repo\\\"` - Global events on repos. Possible values for the associated `action`: `\\\"create\\\"`, `\\\"delete\\\"`, `\\\"update\\\"`, `\\\"move\\\"`.\\n- `\\\"repo.content\\\"` - Events on the repo's content, such as new commits or tags. It triggers on new Pull Requests as well due to the newly created reference/commit. The associated `action` is always `\\\"update\\\"`.\\n- `\\\"repo.config\\\"` - Events on the config: update Space secrets, update settings, update DOIs, disabled or not, etc. The associated `action` is always `\\\"update\\\"`.\\n- `\\\"discussion\\\"` - Creating a discussion or Pull Request, updating the title or status, and merging. Possible values for the associated `action`: `\\\"create\\\"`, `\\\"delete\\\"`, `\\\"update\\\"`.\\n- `\\\"discussion.comment\\\"` - Creating, updating, and hiding a comment. Possible values for the associated `action`: `\\\"create\\\"`, `\\\"update\\\"`.\\n\\nMore scopes can be added in the future. To handle unknown events, your webhook handler can consider any action on a narrowed scope to be an `\\\"update\\\"` action on the broader scope.\\n\\nFor example, if the `\\\"repo.config.dois\\\"` scope is added in the future, any event with that scope can be considered by your webhook handler as an `\\\"update\\\"` action on the `\\\"repo.config\\\"` scope.\\n\\n### Repo\\n\\nIn the current version of webhooks, the top-level property `repo` is always specified, as events can always be associated with a repo. For example, consider the following value:\\n\\n```json\\n\\\"repo\\\": {\\n\\t\\\"type\\\": \\\"model\\\",\\n\\t\\\"name\\\": \\\"some-user/some-repo\\\",\\n\\t\\\"id\\\": \\\"6366c000a2abcdf2fd69a080\\\",\\n\\t\\\"private\\\": false,\\n\\t\\\"url\\\": {\\n\\t\\t\\\"web\\\": \\\"https://huggingface.co/some-user/some-repo\\\",\\n\\t\\t\\\"api\\\": \\\"https://huggingface.co/api/models/some-user/some-repo\\\"\\n\\t},\\n\\t\\\"headSha\\\": \\\"c379e821c9c95d613899e8c4343e4bfee2b0c600\\\",\\n\\t\\\"tags\\\": [\\n\\t\\t\\\"license:other\\\",\\n\\t\\t\\\"has_space\\\"\\n\\t],\\n\\t\\\"owner\\\": {\\n\\t\\t\\\"id\\\": \\\"61d2000c3c2083e1c08af22d\\\"\\n\\t}\\n}\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"What is the top-level property always specified in the current version of webhooks?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"The top-level property `repo` is always specified in the current version of webhooks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"huggingface/hub-docs/blob/main/docs/hub/webhooks.md\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Critique Agents"
      ],
      "metadata": {
        "id": "wqW2FADjw1xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ],
      "metadata": {
        "id": "Ou_hrLADw41o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating critique for each QA couple...\")\n",
        "for output in tqdm(outputs):\n",
        "    evaluations = {\n",
        "        \"groundedness\": call_llm(\n",
        "            llm_client,\n",
        "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
        "        ),\n",
        "        \"relevance\": call_llm(\n",
        "            llm_client,\n",
        "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
        "        ),\n",
        "        \"standalone\": call_llm(\n",
        "            llm_client,\n",
        "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "        ),\n",
        "    }\n",
        "    try:\n",
        "        for criterion, evaluation in evaluations.items():\n",
        "            score, eval = (\n",
        "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "            )\n",
        "            output.update(\n",
        "                {\n",
        "                    f\"{criterion}_score\": score,\n",
        "                    f\"{criterion}_eval\": eval,\n",
        "                }\n",
        "            )\n",
        "    except Exception as e:\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522,
          "referenced_widgets": [
            "74463bc872bd40cd9fec800f9f26e085",
            "5306bf055ed44ed2b927eaa352af3246",
            "ccdcaa9466a64a8f96997954ff5a9b4a",
            "371c37e49f634155b1055f269197ff61",
            "500f5a10354143edba44820c3cc92f97",
            "063d8d17af354619869622cfdf4e72f6",
            "6417ef798dc14ecda835d3d0c75ae1ee",
            "55e17de1d66445b795c7938922620caa",
            "7017625800c543c9b8f9be2ae20d5369",
            "29f7cc67ea6346f2ba492fb4ba5475ca",
            "0d9a7643f28d41d4b4699973652f6bc0"
          ]
        },
        "id": "o4MgnKU1w9nF",
        "outputId": "7bdff4bb-eba9-4ed6-f39f-aff00b6c09b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating critique for each QA couple...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74463bc872bd40cd9fec800f9f26e085"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-68e0b5a9-7b331a3a4dcfcd497df745c2;c64db98e-2f95-43b3-8c25-a9f36003ce13)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-122694099.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     evaluations = {\n\u001b[0;32m----> 4\u001b[0;31m         \"groundedness\": call_llm(\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mllm_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mquestion_groundedness_critique_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-783427706.py\u001b[0m in \u001b[0;36mcall_llm\u001b[0;34m(inference_client, prompt)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference_client\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInferenceClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     response = inference_client.chat.completions.create(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         )\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;31m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/together/v1/chat/completions (Request ID: Root=1-68e0b5a9-7b331a3a4dcfcd497df745c2;c64db98e-2f95-43b3-8c25-a9f36003ce13)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "generated_questions = generated_questions.loc[\n",
        "    (generated_questions[\"groundedness_score\"] >= 4)\n",
        "    & (generated_questions[\"relevance_score\"] >= 4)\n",
        "    & (generated_questions[\"standalone_score\"] >= 4)\n",
        "]\n",
        "print(\"============================================\")\n",
        "print(\"Final evaluation dataset:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "q1cvWnr5xCVZ",
        "outputId": "8d0da4ab-56f6-42ac-a9d9-03d9e186be83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation dataset before filtering:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['relevance_score', 'standalone_score'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1526841804.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation dataset before filtering:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m display(\n\u001b[0;32m----> 9\u001b[0;31m     generated_questions[\n\u001b[0m\u001b[1;32m     10\u001b[0m         [\n\u001b[1;32m     11\u001b[0m             \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['relevance_score', 'standalone_score'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
      ],
      "metadata": {
        "id": "HYOkbv91xnUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNefPbxNxrnV",
        "outputId": "48342136-889a-4ad4-84ad-d8987dc62a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WztOBoRxvDF",
        "outputId": "e80a1a7f-4a31-478d-d399-9c00ab915adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': ' `tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\\n',\n",
              " 'question': 'What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n',\n",
              " 'answer': 'x86_64-unknown-linux-musl',\n",
              " 'source_doc': 'huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md',\n",
              " 'standalone_score': 5,\n",
              " 'standalone_eval': 'The question is asking about the specific architecture for which a binary file, named `tokenizers-linux-x64-musl`, is designed. The terms used in the question are technical but do not depend on a specific context to be understood. The question is clear and can be understood by someone familiar with computer architecture and software without additional context.\\n\\n',\n",
              " 'relatedness_score': 5,\n",
              " 'relatedness_eval': 'The context directly specifies the architecture for which the `tokenizers-linux-x64-musl` binary is designed. It states that this is the binary for `tokenizers` intended for the **x86_64-unknown-linux-musl** architecture. The \"x64\" in the name suggests it is for 64-bit systems, and \"musl\" indicates that it uses the musl libc library, which is common in lightweight Linux distributions. Therefore, the question is clearly and unambiguously answerable with the given context.\\n\\n',\n",
              " 'relevance_score': 3,\n",
              " 'relevance_eval': 'The question is asking for specific technical information regarding a binary file provided by the Hugging Face `tokenizers` library. This information is useful for developers who need to understand the compatibility of the binary with their system architecture, particularly when working on a Linux system with the `musl` libc. This knowledge is practical and relevant for ensuring that the correct binaries are used for deployment or development, which is important for machine learning developers who are setting up their NLP applications. However, the question is quite specific and may not be broadly applicable to all developers in the Hugging Face ecosystem, as not everyone will be using this specific binary or working with a Linux system that requires it.\\n\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset =eval_dataset[0:11]"
      ],
      "metadata": {
        "id": "o0jTJEwaxzKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Aj5Wqy1x4B4",
        "outputId": "2f86daf8-d3ac-47c9-cedd-61e68184e373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate RAG"
      ],
      "metadata": {
        "id": "0CekUGpsyGtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.language_models import BaseChatModel\n",
        "\n",
        "\n",
        "def run_rag_tests(\n",
        "    eval_dataset: datasets.Dataset,\n",
        "    llm,\n",
        "    knowledge_index: VectorStore,\n",
        "    output_file: str,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    verbose: Optional[bool] = True,\n",
        "    test_settings: Optional[str] = None,  # To document the test settings used\n",
        "):\n",
        "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
        "    try:  # load previous generations if they exist\n",
        "        with open(output_file, \"r\") as f:\n",
        "            outputs = json.load(f)\n",
        "    except:\n",
        "        outputs = []\n",
        "\n",
        "    for example in tqdm(eval_dataset):\n",
        "        question = example[\"question\"]\n",
        "        if question in [output[\"question\"] for output in outputs]:\n",
        "            continue\n",
        "\n",
        "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
        "        if verbose:\n",
        "            print(\"=======================================================\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'True answer: {example[\"answer\"]}')\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"true_answer\": example[\"answer\"],\n",
        "            \"source_doc\": example[\"source_doc\"],\n",
        "            \"generated_answer\": answer,\n",
        "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
        "        }\n",
        "        if test_settings:\n",
        "            result[\"test_settings\"] = test_settings\n",
        "        outputs.append(result)\n",
        "\n",
        "        with open(output_file, \"w\") as f:\n",
        "            json.dump(outputs, f)"
      ],
      "metadata": {
        "id": "NKMLWK8RyJWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
        "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
        "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
        "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
        "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
        "\n",
        "###The instruction to evaluate:\n",
        "{instruction}\n",
        "\n",
        "###Response to evaluate:\n",
        "{response}\n",
        "\n",
        "###Reference Answer (Score 5):\n",
        "{reference_answer}\n",
        "\n",
        "###Score Rubrics:\n",
        "[Is the response correct, accurate, and factual based on the reference answer?]\n",
        "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
        "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
        "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
        "Score 4: The response is mostly correct, accurate, and factual.\n",
        "Score 5: The response is completely correct, accurate, and factual.\n",
        "\n",
        "###Feedback:\"\"\"\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "\n",
        "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
        "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "3HzMRiAhyN4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "qaZ0Q52ny3if",
        "outputId": "f3403f9a-932d-45fb-9d0b-72aba4ef33be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (0.3.77)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (2.11.9)\n",
            "Requirement already satisfied: filetype<2,>=1.2 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain_google_genai) (0.4.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain_google_genai) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain_google_genai) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain_google_genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.32.5)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.75->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.7.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.7.0 langchain_google_genai-2.1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "77fed62938d04c93b68ab8530db37bdc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Initialize the Gemini chat model\n",
        "eval_chat_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",  # Use a powerful model like gemini-2.5-pro or gemini-2.5-flash\n",
        "    temperature=0,\n",
        "    google_api_key= userdata.get('GEMINI')  # Pass the key if not using env var\n",
        ")\n",
        "evaluator_name = \"GEMINI\" # Update the evaluator name\n",
        "\n",
        "\n",
        "def evaluate_answers(\n",
        "    answer_path: str,\n",
        "    eval_chat_model,\n",
        "    evaluator_name: str,\n",
        "    evaluation_prompt_template: ChatPromptTemplate,\n",
        ") -> None:\n",
        "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
        "    answers = []\n",
        "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
        "        answers = json.load(open(answer_path, \"r\"))\n",
        "\n",
        "    for experiment in tqdm(answers):\n",
        "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
        "            continue\n",
        "\n",
        "        eval_prompt = evaluation_prompt_template.format_messages(\n",
        "            instruction=experiment[\"question\"],\n",
        "            response=experiment[\"generated_answer\"],\n",
        "            reference_answer=experiment[\"true_answer\"],\n",
        "        )\n",
        "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
        "        # Note: The original code assumes a specific output format (\"[RESULT]\").\n",
        "        # Ensure your evaluation_prompt_template guides Gemini to produce this format.\n",
        "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
        "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
        "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
        "\n",
        "        with open(answer_path, \"w\") as f:\n",
        "            json.dump(answers, f)"
      ],
      "metadata": {
        "id": "cUiw-nzlyR-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = eval_dataset.select(range(10)) # Testing"
      ],
      "metadata": {
        "id": "oRWoE51934OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"./output\"):\n",
        "    os.mkdir(\"./output\")\n",
        "\n",
        "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
        "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
        "        for rerank in [False]:\n",
        "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
        "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
        "\n",
        "            print(f\"Running evaluation for {settings_name}:\")\n",
        "\n",
        "            print(\"Loading knowledge base embeddings...\")\n",
        "            knowledge_index = load_embeddings(\n",
        "                RAW_KNOWLEDGE_BASE,\n",
        "                chunk_size=chunk_size,\n",
        "                embedding_model_name=embeddings,\n",
        "            )\n",
        "\n",
        "            print(\"Running RAG...\")\n",
        "            reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
        "            run_rag_tests(\n",
        "                eval_dataset=eval_dataset, # Pass the original sliced dataset\n",
        "                llm=READER_LLM,\n",
        "                knowledge_index=knowledge_index,\n",
        "                output_file=output_file_name,\n",
        "                reranker=reranker,\n",
        "                verbose=False,\n",
        "                test_settings=settings_name,\n",
        "            )\n",
        "\n",
        "            print(\"Running evaluation...\")\n",
        "            evaluate_answers(\n",
        "                output_file_name,\n",
        "                eval_chat_model,\n",
        "                evaluator_name,\n",
        "                evaluation_prompt_template,\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152,
          "referenced_widgets": [
            "a1f6c01a895d41bb93cdbbe2cab43acf",
            "9942e0217d41450f90741e2756942db7",
            "b33a9de34e164b11b3439d2cce85817a",
            "0f5a36943f30458c9474499fb523424e",
            "3c78a0199cfd4b448aa9a310285a6b38",
            "b357a59177fd476c9e098829385d0a6a",
            "ecf56464164745959acf3edcf3e5aa31",
            "529ced00d4684177b2d8ff7b3678d4cd",
            "cbf7da0971a14cf5b035cb153c52f948",
            "c119386623f74d7bb2353ddb6c33012c",
            "de72614d783c469994cbfc0d2d9becf0",
            "22ebf8903aac497ab7aa5390376c925b",
            "4f030132f5d74e0abcce8ddce3f66574",
            "7c5d2cccd66743a9a7151109f18cf48b",
            "b3a1289acd994b3cb86198f3de48c9f0",
            "cd3425b0697b429cbecddcebf906db4e",
            "83cc393b17d8493bb7f78efd8ccd74bb",
            "bf7a0824b43c447886615aaadfc8730d",
            "2ecb235348024d65ab92fbad3de336c8",
            "dce7bc331af5483fb5706d3e6ce056c6",
            "4b2941fc537b489c90952c2a86b85009",
            "39cf0e05499c4a35a9678f39bd780c27"
          ]
        },
        "id": "-pJLAGoozFgb",
        "outputId": "6f2eb148-6d9d-403c-85c3-0c19b3826daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta:\n",
            "Loading knowledge base embeddings...\n",
            "Running RAG...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1f6c01a895d41bb93cdbbe2cab43acf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22ebf8903aac497ab7aa5390376c925b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "outputs = []\n",
        "for file in glob.glob(\"./output/*.json\"):\n",
        "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
        "    output[\"settings\"] = file\n",
        "    outputs.append(output)\n",
        "result = pd.concat(outputs)"
      ],
      "metadata": {
        "id": "d7Ftr0zo7dk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"eval_score_GEMINI\"] = result[\"eval_score_GEMINI\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
        "result[\"eval_score_GEMINI\"] = (result[\"eval_score_GEMINI\"] - 1) / 4"
      ],
      "metadata": {
        "id": "raIMkOdL3xSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_scores = result.groupby(\"settings\")[\"eval_score_GEMINI\"].mean()\n",
        "average_scores.sort_values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "mOap3Tr17jTU",
        "outputId": "c38a854c-94a5-47fd-f98a-8164b71e3dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "settings\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json    0.75\n",
              "Name: eval_score_GEMINI, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eval_score_GEMINI</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>settings</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json</th>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    }
  ]
}